<!--
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Mon Jun 03 15:20:10 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/sr/jira.issueviews:searchrequest-xml/temp/SearchRequest.xml?jqlQuery=project+%3D+LUCENE+AND+fixVersion+%3D+%224.4%22&tempMax=1000&field=key&field=summary
-->
<!-- If you wish to do custom client-side styling of RSS, uncomment this:
<?xml-stylesheet href="https://issues.apache.org/jira/styles/jiraxml2html.xsl" type="text/xsl"?>
-->
<rss version="0.92">
    <channel>
        <title>ASF JIRA</title>
        <link>https://issues.apache.org/jira/secure/IssueNavigator.jspa?reset=true&amp;jqlQuery=project+%3D+LUCENE+AND+fixVersion+%3D+%224.4%22</link>
        <description>An XML representation of a search request</description>
                <language>en-uk</language>
                        <issue start="0" end="234" total="234"/>
                <build-info>
            <version>5.2.8</version>
            <build-number>851</build-number>
            <build-date>26-02-2013</build-date>
        </build-info>
<item>
            <title>[LUCENE-5029] factor out a generic 'TermState' for better sharing in FST-based term dict</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5029</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently, those two FST-based term dict (memory codec &amp;amp; blocktree) all use FST&amp;lt;BytesRef&amp;gt; as a base data structure, this might not share much data in parent arcs, since the encoded BytesRef doesn't guarantee that 'Outputs.common()' always creates a long prefix. &lt;/p&gt;


&lt;p&gt;While for current postings format, it is guaranteed that each FP (pointing to .doc, .pos, etc.) will increase monotonically with 'larger' terms. That means, between two Outputs, the Outputs from smaller term can be safely pushed towards root. However we always have some tricky TermState to deal with (like the singletonDocID for pulsing trick), so as Mike suggested, we can simply cut the whole TermState into two parts: one part for comparation and intersection, another for restoring generic data. Then the data structure will be clear: this generic 'TermState' will consist of a fixed-length LongsRef and variable-length BytesRef. &lt;/p&gt;</description>
                <environment/>
            <key id="12650461">LUCENE-5029</key>
            <summary>factor out a generic 'TermState' for better sharing in FST-based term dict</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                    <parent id="12506174">LUCENE-3069</parent>
                        <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="billy">Han Jiang</assignee>
                                <reporter username="billy">Han Jiang</reporter>
                        <labels>
                    </labels>
                <created>Sat, 1 Jun 2013 18:50:58 +0100</created>
                <updated>Sat, 1 Jun 2013 18:54:40 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13672181" author="billy" created="Sat, 1 Jun 2013 18:54:40 +0100">&lt;p&gt;A patch full of nocommits...&lt;/p&gt;

&lt;p&gt;For 'common()' operation, simply pickup the smaller one, vice versa. And when two LongsRef are not comparable (e.g. one intermediate TermState vs. another intermediate), we can ignore that.&lt;/p&gt;

&lt;p&gt;This behavior is quite different from current Outputs like PairOutputs, and of course we have other alternatives to test.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585734" name="LUCENE-5029.patch" size="7951" author="billy" created="Sat, 1 Jun 2013 18:54:40 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330800</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330782</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-5028] doShare is pointless in PositiveIntOutputs</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5028</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We recently use this in oal.core.fst.PositiveIntOutputs to indicate whether to share outputs. The comment mentioned 'with doShare=false, in some case this may result in a smaller FST'. However, this is not intuitive, as for long type, we always have the smallest output reduced to NO_OUTPUT, thus the smallest one is 'moved' towards root, and no extra output is created.&lt;/p&gt;

&lt;p&gt;However, if there are many many small outputs around root arcs, when we share outputs, a large output might be pushed into the root arcs. When root arcs are packed as fixed-array, yes the size of FST is increased. But, I suppose this should invoke other intuitive heuristics, instead of the confusing 'doShare'?&lt;/p&gt;

&lt;p&gt;Besides, this only exist in PositiveIntOutputs.&lt;/p&gt;</description>
                <environment/>
            <key id="12650374">LUCENE-5028</key>
            <summary>doShare is pointless in PositiveIntOutputs</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="billy">Han Jiang</assignee>
                                <reporter username="billy">Han Jiang</reporter>
                        <labels>
                    </labels>
                <created>Fri, 31 May 2013 18:00:00 +0100</created>
                <updated>Mon, 3 Jun 2013 15:00:27 +0100</updated>
                    <resolved>Mon, 3 Jun 2013 14:54:41 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/FSTs</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13671648" author="rcmuir" created="Fri, 31 May 2013 18:02:17 +0100">&lt;p&gt;I think instead of having this boolean in PositiveIntOutputs, if someone wants a non-sharing outputs impl, it should just be a different outputs implementation?&lt;/p&gt;

&lt;p&gt;if anything is using this / anyone is upset by this, we could add one to the sandbox.&lt;/p&gt;</comment>
                    <comment id="13671651" author="mikemccand" created="Fri, 31 May 2013 18:05:27 +0100">&lt;p&gt;+1 to nuke it!&lt;/p&gt;

&lt;p&gt;I think, except for array-arc effects, the FST is never smaller with doShare=false (I had thought it was but now I disagree with my past self!).&lt;/p&gt;

&lt;p&gt;Han do you wanna make a patch?  Thanks.&lt;/p&gt;</comment>
                    <comment id="13671658" author="billy" created="Fri, 31 May 2013 18:08:46 +0100">&lt;p&gt;yes, I think this should be kept intuitive for general usage.&lt;/p&gt;

&lt;p&gt;I take a glimpse of the grep result, strange that we use doShare=false in codecs.simpletext.SimpleTextFieldsReader&lt;/p&gt;</comment>
                    <comment id="13671723" author="billy" created="Fri, 31 May 2013 19:29:19 +0100">&lt;p&gt;ok, patch, tests pass. also updated related javadocs. sorry my laptop is slow on tests &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13671772" author="mikemccand" created="Fri, 31 May 2013 20:42:15 +0100">&lt;p&gt;Patch looks great!  Maybe also remove doShare from UpToTwoPositiveIntOutputs?&lt;/p&gt;</comment>
                    <comment id="13671942" author="billy" created="Sat, 1 Jun 2013 02:08:16 +0100">&lt;p&gt;hmm, maybe not? I suppose, for TwoLong case, when the first long is 'moved' towards root and minus to zero, the second one might still be greater than zero? So, when output is shared, we might create an extra output.&lt;/p&gt;</comment>
                    <comment id="13672009" author="billy" created="Sat, 1 Jun 2013 08:43:58 +0100">&lt;p&gt;the second patch also removes a silly equal check in PositiveIntOutputs&lt;/p&gt;</comment>
                    <comment id="13672010" author="billy" created="Sat, 1 Jun 2013 08:47:51 +0100">&lt;p&gt;The third one nukes 'doShare' in UpToTwoPositiveIntOutputs as well, since those codes are in 'misc', and doShare=false sometimes makes sense, I'm not sure whether this is the right way &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;If we're going to totally omit 'doShare', maybe we need some javadocs in fst.Builder, so that this 'sharing increases fst size' is not forgotten?&lt;/p&gt;</comment>
                    <comment id="13672153" author="mikemccand" created="Sat, 1 Jun 2013 17:52:07 +0100">&lt;p&gt;Hmm, I agree it's not clear that doShare is useless on UpToTwoPositiveIntOutputs.  Let's just leave that one for now then?  I'll look at the 2nd patch soon ... but I'll be offline for next ~36 hours or so ...&lt;/p&gt;</comment>
                    <comment id="13673035" author="mikemccand" created="Mon, 3 Jun 2013 12:37:57 +0100">&lt;p&gt;The 2nd patch looks great, I'll commit shortly!  Thanks Han.&lt;/p&gt;

&lt;p&gt;I'll also go improve the javadocs in UpToTwo/List outputs impls...&lt;/p&gt;</comment>
                    <comment id="13673113" author="mikemccand" created="Mon, 3 Jun 2013 14:54:41 +0100">&lt;p&gt;Thanks Han!&lt;/p&gt;</comment>
                    <comment id="13673121" author="billy" created="Mon, 3 Jun 2013 15:00:27 +0100">&lt;p&gt;Mike, thanks for the commit!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585708" name="LUCENE-5028.patch" size="2380" author="billy" created="Sat, 1 Jun 2013 08:47:51 +0100"/>
                    <attachment id="12585707" name="LUCENE-5028.patch" size="23818" author="billy" created="Sat, 1 Jun 2013 08:43:58 +0100"/>
                    <attachment id="12585653" name="LUCENE-5028.patch" size="23884" author="billy" created="Fri, 31 May 2013 19:29:19 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 31 May 2013 17:02:17 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330713</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330695</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5026] PagedGrowableWriter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5026</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We already have packed data structures that support more than 2B values such as AppendingLongBuffer and MonotonicAppendingLongBuffer but none of them supports random write-access.&lt;/p&gt;

&lt;p&gt;We could write a PagedGrowableWriter for this, which would essentially wrap an array of GrowableWriters.&lt;/p&gt;</description>
                <environment/>
            <key id="12650174">LUCENE-5026</key>
            <summary>PagedGrowableWriter</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="jpountz">Adrien Grand</reporter>
                        <labels>
                    </labels>
                <created>Thu, 30 May 2013 18:20:32 +0100</created>
                <updated>Mon, 3 Jun 2013 11:38:30 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13671248" author="jpountz" created="Fri, 31 May 2013 09:08:21 +0100">&lt;p&gt;Patch. I didn't make PagedGrowableWriter used anywhere but I think Mike is planning to use it for FST.&lt;/p&gt;</comment>
                    <comment id="13672720" author="jpountz" created="Mon, 3 Jun 2013 00:38:49 +0100">&lt;p&gt;New patch that:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;doesn't over-allocate the last page as requested per Mike,&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;shares more code with &lt;tt&gt;BlockPackedWriter|Reader&lt;/tt&gt; (regarding fixed-size block handling)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Unless someone objects, I'd like to commit it tomorrow.&lt;/p&gt;</comment>
                    <comment id="13672985" author="mikemccand" created="Mon, 3 Jun 2013 11:38:30 +0100">&lt;p&gt;+1, thanks Adrien.&lt;/p&gt;

&lt;p&gt;I think you don't need that super() in PagedGrowableWriter's ctor.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585787" name="LUCENE-5026.patch" size="21513" author="jpountz" created="Mon, 3 Jun 2013 00:38:49 +0100"/>
                    <attachment id="12585566" name="LUCENE-5026.patch" size="12642" author="jpountz" created="Fri, 31 May 2013 09:08:21 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 3 Jun 2013 10:38:30 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330513</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330495</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-5025] Allow more than 2.1B "tail nodes" when building FST</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5025</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We recently relaxed some of the limits for big FSTs, but there is&lt;br/&gt;
one more limit I think we should fix.  E.g. Aaron hit it in building&lt;br/&gt;
the world's biggest FST: &lt;a href="http://aaron.blog.archive.org/2013/05/29/worlds-biggest-fst/" class="external-link"&gt;http://aaron.blog.archive.org/2013/05/29/worlds-biggest-fst/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The issue is NodeHash, which currently uses a GrowableWriter (packed&lt;br/&gt;
ints impl that can grow both number of bits and number of values):&lt;br/&gt;
it's indexed by int not long.&lt;/p&gt;

&lt;p&gt;This is a hash table that's used to share suffixes, so we need random&lt;br/&gt;
get/put on a long index of long values, i.e. this is logically a long[].&lt;/p&gt;

&lt;p&gt;I think one simple way to do this is to make a "paged"&lt;br/&gt;
GrowableWriter...&lt;/p&gt;

&lt;p&gt;Along with this we'd need to fix the hash codes to be long not&lt;br/&gt;
int.&lt;/p&gt;</description>
                <environment/>
            <key id="12650162">LUCENE-5025</key>
            <summary>Allow more than 2.1B "tail nodes" when building FST</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Thu, 30 May 2013 16:30:11 +0100</created>
                <updated>Sun, 2 Jun 2013 19:03:59 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/FSTs</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13671590" author="mikemccand" created="Fri, 31 May 2013 16:44:09 +0100">&lt;p&gt;Initial patch, also folding in Adrien's patch from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-5026" title="PagedGrowableWriter"&gt;LUCENE-5026&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I had to make one change to that patch so that the last page (GrowableWriter) is sized down, ie if I use 1&amp;lt;&amp;lt;30 page size but only need 100 values then that single GrowableWriter should be size=100 I think.&lt;/p&gt;

&lt;p&gt;I'm running Test2BFSTs now ...&lt;/p&gt;</comment>
                    <comment id="13671904" author="rcmuir" created="Sat, 1 Jun 2013 00:46:16 +0100">&lt;blockquote&gt;
&lt;p&gt;I had to make one change to that patch so that the last page (GrowableWriter) is sized down, ie if I use 1&amp;lt;&amp;lt;30 page size but only need 100 values then that single GrowableWriter should be size=100 I think.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I played with this in the patch, seems to work fine!&lt;br/&gt;
There is a stray print left in rehash(), but precommit should find that &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13672159" author="mikemccand" created="Sat, 1 Jun 2013 17:59:34 +0100">&lt;p&gt;Thanks Rob.&lt;/p&gt;

&lt;p&gt;Test2BFSTs also passed, after 4.5 hours and 40 GB heap!&lt;/p&gt;

&lt;p&gt;I think it's ready ... I'll clean up that print and the nocommits and commit ... thanks Adrien for creating PagedGrowableWriter!&lt;/p&gt;</comment>
                    <comment id="13672160" author="mikemccand" created="Sat, 1 Jun 2013 18:07:23 +0100">&lt;p&gt;Hmm just hit a failure in the test case for PagedGrowableWriter:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;ant test  -Dtestcase=TestPackedInts -Dtests.method=testPagedGrowableWriter -Dtests.seed=5F62E708DEB71329 -Dtests.slow=true -Dtests.locale=ar_BH -Dtests.timezone=America/Tijuana -Dtests.file.encoding=UTF-8
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I haven't looked into it yet ...&lt;/p&gt;</comment>
                    <comment id="13672215" author="rcmuir" created="Sat, 1 Jun 2013 20:52:37 +0100">&lt;p&gt;I think that bug is in PagedGrowableWriter.resize().&lt;/p&gt;

&lt;p&gt;Currently, if you resize one where the last page isnt full, you try to access values that dont exist,&lt;br/&gt;
because it just copies pagesize() values.&lt;/p&gt;

&lt;p&gt;I think instead it should be (something like):&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; i = 0; i &amp;lt; numCommonPages; ++i) {
      ...
      &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt; valuesToCopy;
      &lt;span class="code-comment"&gt;// &lt;span class="code-keyword"&gt;if&lt;/span&gt; its the last page, it might be sized down
&lt;/span&gt;      &lt;span class="code-keyword"&gt;if&lt;/span&gt; (i == subWriters.length - 1) {
        valuesToCopy = (&lt;span class="code-object"&gt;int&lt;/span&gt;)(size() % pageSize());
      } &lt;span class="code-keyword"&gt;else&lt;/span&gt; {
        valuesToCopy = pageSize();
      }
      PackedInts.copy(subWriters[i], 0, newWriter.subWriters[i], 0, valuesToCopy, copyBuffer);
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13672621" author="mikemccand" created="Sun, 2 Jun 2013 19:03:59 +0100">&lt;p&gt;Thanks Rob!  That's it (due to a similar fix I made in the ctor ...).  I'll fix.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585631" name="LUCENE-5025.patch" size="19329" author="mikemccand" created="Fri, 31 May 2013 16:44:09 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 31 May 2013 23:46:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330501</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330483</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-5024] Can we reliably detect an incomplete first commit vs index corruption?</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5024</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Normally, if something bad happens (OS, JVM, hardware crashes) while&lt;br/&gt;
IndexWriter is committing, we will just fallback to the prior commit&lt;br/&gt;
and no intervention necessary from the app.&lt;/p&gt;

&lt;p&gt;But if that commit is the first commit, then on restart IndexWriter&lt;br/&gt;
will now throw CorruptIndexException, as of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4738" title="Killed JVM when first commit was running will generate a corrupted index"&gt;&lt;del&gt;LUCENE-4738&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Prior to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4738" title="Killed JVM when first commit was running will generate a corrupted index"&gt;&lt;del&gt;LUCENE-4738&lt;/del&gt;&lt;/a&gt;, in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2812" title="IndexReader.indexExists sometimes returns true when an index isn&amp;#39;t present"&gt;&lt;del&gt;LUCENE-2812&lt;/del&gt;&lt;/a&gt;, we used to try to detect the&lt;br/&gt;
corrupt first commit, but that logic was dangerous and could result in&lt;br/&gt;
falsely believing no index is present when one is, e.g. when transient&lt;br/&gt;
IOExceptions are thrown due to file descriptor exhaustion.&lt;/p&gt;

&lt;p&gt;But now two users have hit this change ... see "CorruptIndexException&lt;br/&gt;
when opening Index during first commit" and "Calling&lt;br/&gt;
IndexWriter.commit() immediately after creating the writer", both on&lt;br/&gt;
java-user.&lt;/p&gt;

&lt;p&gt;It would be nice to get back to not marking an incomplete first commit&lt;br/&gt;
as corruption ... but we have to proceed carefully.&lt;/p&gt;</description>
                <environment/>
            <key id="12650159">LUCENE-5024</key>
            <summary>Can we reliably detect an incomplete first commit vs index corruption?</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Thu, 30 May 2013 16:19:24 +0100</created>
                <updated>Thu, 30 May 2013 20:03:04 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13670410" author="rcmuir" created="Thu, 30 May 2013 16:31:02 +0100">&lt;p&gt;even if we can, i'm not sure we should.&lt;/p&gt;

&lt;p&gt;real users hit corruption issues too. sorry to the two java-users for the inconvenience, but corruption/dataloss is WAY worse.&lt;/p&gt;</comment>
                    <comment id="13670416" author="rcmuir" created="Thu, 30 May 2013 16:35:04 +0100">&lt;p&gt;The best solution here i think, is removal of create-or-append.&lt;/p&gt;

&lt;p&gt;really index creation can be a one time thing you must do separately before you can use the directory. this is typically how its done: lucene is wierd and has this broken mechanism today instead.&lt;/p&gt;</comment>
                    <comment id="13670604" author="gcooney" created="Thu, 30 May 2013 20:03:04 +0100">&lt;p&gt;Hi.  I'm one of the users who reported/asked about this.  &lt;/p&gt;

&lt;p&gt;Specifically, i was wondering if it's possible to deal with this by being explicit about the segments_n file being in the pre-committed state?  That is, add one byte to segments_n file representing a boolean "isCommitted".  Then you could treat an index that only has a segments_1 file set to "isCommitted"=false as a non-existant index.  &lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 30 May 2013 15:31:02 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330498</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330480</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-5022] Add FacetResult.mergeHierarchies</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5022</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When you DrillSideways on a hierarchical dimension, and especially when you OR multiple drill-downs together, you get several FacetResults back, one for each category you drill down on. So for example, if you want to drill-down on Date/2010 OR Date/2011/May, the FacetRequests that you need to create (to get the sideways effect) are: Date/, Date/2010, Date/2011 and Date/2011/May. Date/ is because you want to get sideways counts as an alternative to Date/2010, and Date/2011 in order to get months count as an alternative to Date/2011/May.&lt;/p&gt;

&lt;p&gt;That results in 4 FacetResult objects. Having a utility which merges all FacetResults of the same dimension into a single hierarchical one will be very useful for e.g. apps that want to display the hierarchy. I'm thinking of FacetResult.mergeHierarchies which takes a List&amp;lt;FacetResult&amp;gt; and returns the merged ones, one FacetResult per dimension.&lt;/p&gt;</description>
                <environment/>
            <key id="12649761">LUCENE-5022</key>
            <summary>Add FacetResult.mergeHierarchies</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Tue, 28 May 2013 19:57:49 +0100</created>
                <updated>Wed, 29 May 2013 14:10:01 +0100</updated>
                    <resolved>Wed, 29 May 2013 14:10:01 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13668588" author="shaie" created="Tue, 28 May 2013 20:41:35 +0100">&lt;p&gt;Patch with the new mergeHierarchies method + test. I created it using git, so hopefully it applies ok (I tried patch --dry-run and it didn't complain).&lt;/p&gt;</comment>
                    <comment id="13668642" author="mikemccand" created="Tue, 28 May 2013 21:26:01 +0100">&lt;p&gt;This looks great Shai!  Can it be used for non-hierarchical dims as well?  E.g. if I have one FacetRequest for top 10 under User/ and then a second FacetRequest for User/Bob (a "leaf" ... hmm can one make a FacetRequest like that?), will it merge them?&lt;/p&gt;

&lt;p&gt;Or, what would it do if I (oddly) had one FacetRequest asking for top 10 under User/ and another FacetRequest asking for top 20 under User/?&lt;/p&gt;</comment>
                    <comment id="13669110" author="shaie" created="Wed, 29 May 2013 10:20:03 +0100">&lt;p&gt;Good questions Mike! They say a test is worth more than a thousand words, so I added tests and they pass. I guess he answer is Yes! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13669178" author="mikemccand" created="Wed, 29 May 2013 12:55:36 +0100">&lt;p&gt;+1, thanks Shai!  Very useful...&lt;/p&gt;</comment>
                    <comment id="13669213" author="shaie" created="Wed, 29 May 2013 14:10:01 +0100">&lt;p&gt;Committed to trunk and 4x.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585174" name="LUCENE-5022.patch" size="18369" author="shaie" created="Wed, 29 May 2013 10:20:03 +0100"/>
                    <attachment id="12585067" name="LUCENE-5022.patch" size="16399" author="shaie" created="Tue, 28 May 2013 20:41:35 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 28 May 2013 20:26:01 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330100</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>330082</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5020] Make DrillSidewaysResult ctor public</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5020</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;DrillSidewaysResult has a package-private ctor which prevents initializing it by an app. I found that it's sometimes useful for e.g. doing some post-processing on the returned TopDocs or List&amp;lt;FacetResult&amp;gt;. Since you cannot return two values from a method, it will be convenient if  method could return a new 'processed' DSR.&lt;/p&gt;

&lt;p&gt;I would also like to make the hits member final.&lt;/p&gt;</description>
                <environment/>
            <key id="12649619">LUCENE-5020</key>
            <summary>Make DrillSidewaysResult ctor public</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Tue, 28 May 2013 06:08:07 +0100</created>
                <updated>Tue, 28 May 2013 13:04:04 +0100</updated>
                    <resolved>Tue, 28 May 2013 13:04:04 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13668097" author="shaie" created="Tue, 28 May 2013 06:16:17 +0100">&lt;p&gt;Trivial patch. I also clarified the jdocs of DSR and made hits final. I plan to commit this later today.&lt;/p&gt;</comment>
                    <comment id="13668240" author="mikemccand" created="Tue, 28 May 2013 12:14:54 +0100">&lt;p&gt;+1, looks great!&lt;/p&gt;</comment>
                    <comment id="13668261" author="shaie" created="Tue, 28 May 2013 13:04:04 +0100">&lt;p&gt;Thanks Mike. Committed to trunk and 4x.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12584995" name="LUCENE-5020.patch" size="2125" author="shaie" created="Tue, 28 May 2013 06:16:17 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 28 May 2013 11:14:54 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329958</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329940</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5018] Never update offsets in CompoundWordTokenFilterBase</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5018</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;CompoundWordTokenFilterBase and its children DictionaryCompoundWordTokenFilter and HyphenationCompoundWordTokenFilter update offsets. This can make OffsetAttributeImpl trip an exception when chained with other filters that group tokens together such as ShingleFilter, see &lt;a href="http://www.gossamer-threads.com/lists/lucene/java-dev/196376?page=last" class="external-link"&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/196376?page=last&lt;/a&gt;.&lt;/p&gt;</description>
                <environment/>
            <key id="12649589">LUCENE-5018</key>
            <summary>Never update offsets in CompoundWordTokenFilterBase</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="jpountz">Adrien Grand</reporter>
                        <labels>
                    </labels>
                <created>Mon, 27 May 2013 18:36:32 +0100</created>
                <updated>Tue, 28 May 2013 08:40:37 +0100</updated>
                    <resolved>Tue, 28 May 2013 08:40:24 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13667890" author="jpountz" created="Mon, 27 May 2013 19:14:03 +0100">&lt;p&gt;Here is a patch.&lt;/p&gt;</comment>
                    <comment id="13668155" author="jpountz" created="Tue, 28 May 2013 08:40:24 +0100">&lt;p&gt;I just committed the patch on trunk and branch_4x.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                                <inwardlinks description="is part of">
                            <issuelink>
            <issuekey id="12624823">LUCENE-4641</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12584956" name="LUCENE-5018.patch" size="5738" author="jpountz" created="Mon, 27 May 2013 19:14:03 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329928</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329910</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5017] SpatialOpRecursivePrefixTreeTest is failing</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5017</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This has been failing lately on trunk (e.g. on rev 1486339):&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;ant test  -Dtestcase=SpatialOpRecursivePrefixTreeTest -Dtestmethod=testContains -Dtests.seed=456022665217DADF:2C2A2816BD2BA1C5 -Dtests.slow=true -Dtests.locale=nl_BE -Dtests.timezone=Poland -Dtests.file.encoding=ISO-8859-1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not sure what's up ...&lt;/p&gt;</description>
                <environment/>
            <key id="12649454">LUCENE-5017</key>
            <summary>SpatialOpRecursivePrefixTreeTest is failing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="dsmiley">David Smiley</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sat, 25 May 2013 16:50:47 +0100</created>
                <updated>Tue, 28 May 2013 21:38:09 +0100</updated>
                    <resolved>Tue, 28 May 2013 21:38:09 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/spatial</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13667773" author="dsmiley" created="Mon, 27 May 2013 15:46:19 +0100">&lt;p&gt;Thanks for bringing this to my attention Mike.  I'll look into it.  I wish I could subscribe to test failures in spatial, and if somehow test failures that still fail for a given seed could be tracked somewhere such that we can see outstanding problems that haven't been fixed.&lt;/p&gt;</comment>
                    <comment id="13668637" author="dsmiley" created="Tue, 28 May 2013 21:21:36 +0100">&lt;p&gt;The problem is a bug in my test, relating to a shape-pair of adjacent shapes when testing for Contains.  I fixed this bug.&lt;/p&gt;

&lt;p&gt;I also found out I could make the test "Repeat" annotation refer to a constant for the "iterations", so I did that to make it easy to dial-up the testing temporarily.&lt;/p&gt;

&lt;p&gt;I'll commit this shortly.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585079" name="LUCENE-5017_SpatialOpRecursivePrefixTreeTest_bug.patch" size="2072" author="dsmiley" created="Tue, 28 May 2013 21:21:36 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 27 May 2013 14:46:19 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329793</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329775</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5016] Sampling can break FacetResult labeling </title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5016</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When sampling FacetResults, the TopKInEachNodeHandler is used to get the FacetResults.&lt;/p&gt;

&lt;p&gt;This is my case:&lt;br/&gt;
A FacetResult is returned (which matches a FacetRequest) from the StandardFacetAccumulator. The facet has 0 results. The labelling of the root-node seems incorrect. I know, from the StandardFacetAccumulator, that the rootnode has a label, so I can use that one.&lt;/p&gt;

&lt;p&gt;Currently the recursivelyLabel method uses the taxonomyReader.getPath() to retrieve the label. I think we can skip that for the rootNode when there are no children (and gain a little performance on the way too?)&lt;/p&gt;


</description>
                <environment/>
            <key id="12649286">LUCENE-5016</key>
            <summary>Sampling can break FacetResult labeling </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="robau">Rob Audenaerde</reporter>
                        <labels>
                    </labels>
                <created>Fri, 24 May 2013 10:16:35 +0100</created>
                <updated>Thu, 30 May 2013 11:40:14 +0100</updated>
                    <resolved>Thu, 30 May 2013 11:40:14 +0100</resolved>
                            <version>4.3</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13666201" author="shaie" created="Fri, 24 May 2013 11:53:06 +0100">&lt;p&gt;Can you attach a simple testcase exposing the problem? Not sure that I follow what's wrong. About not labeling, I doubt it will gain us anything. Labeling is not very expensive, and labels are LRU-cached. Also, considering all the work that's done during search processing, the labeling part is less than marginal.&lt;/p&gt;</comment>
                    <comment id="13666401" author="robau" created="Fri, 24 May 2013 16:45:34 +0100">&lt;p&gt;Now that I wrote the tests, I realise that maybe the behaviour of the StandardFacetAccumulator is incorrect, as it labels a FacetResult of a Facet that does not exist in the taxonomy...&lt;/p&gt;

&lt;p&gt;The behaviour of the SamplingAccumulator and the Standard differ.&lt;/p&gt;

&lt;p&gt;For my use case, it is very helpful if all the FacetRequests return a FacetResult with the same label as the request, but I can imagine that this is not desired.&lt;/p&gt;</comment>
                    <comment id="13666436" author="shaie" created="Fri, 24 May 2013 17:33:10 +0100">&lt;p&gt;I am not near the code and actually read the test in Notepad &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. It looks like you're indexing 100K docs with categories A/docnum and then ask to count the categories "A" and "B". If I understand correctly, the assert in the end fails?&lt;/p&gt;

&lt;p&gt;Basically, the FacetRestult that you get back should have the same label as the request. If it's not like that (and I will validate that when I'm near the code), then it's probably a bug in SamplingAccumulator.&lt;/p&gt;

&lt;p&gt;BTW the test actually indexed 200K docs while passing 'j' which is 0 for the first 100K and 1 for the second. But 'j' seems to be unused in addDocument. This shouldn't affect the test behavior but just FYI.&lt;/p&gt;

&lt;p&gt;Thanks for reporting this, I'll take a deeper look later.&lt;/p&gt;</comment>
                    <comment id="13669445" author="shaie" created="Wed, 29 May 2013 18:04:31 +0100">&lt;p&gt;I checked that and indeed there is inconsistency here. StandardFacetsAccumulator and FacetsAccumulator return an empty result with the root node labeled, while the sampling accumulators return the root node not labeled. There isn't anything technically wrong here, because the category does not exist, but I think we should be consistent.&lt;/p&gt;

&lt;p&gt;I was able to reproduce this behavior with an even simpler test Rob: index a single document with category "A" and ask to count category "B". The problem is as follows:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;SamplingAccumulator delegates to SFA.&lt;/li&gt;
	&lt;li&gt;SFA detects this category does not exist and creates an empty FacetResult, which sets the label of the root node to the request's CategoryPath.&lt;/li&gt;
	&lt;li&gt;SamplingAccumulator receives the results, and potentially runs SampleFixer. Then it labels the result, which then sets the label to null, after not finding it in the taxonomy.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Perhaps at some point of the code lifecycle this additional labeling was needed, I'm not sure &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. But I think we should either remove the call to label the results in SamplingAccumulator, or at least not call taxoReader.getPath if the node.label is not null. For instance, if you ask to count "A" (which does exist), then labeling happens twice, once by SFA.accumulate and second time by SamplingAccumulator, which is just a waste.&lt;/p&gt;

&lt;p&gt;I'll attach later a short test which reproduces this and checks all existing accumulators.&lt;/p&gt;</comment>
                    <comment id="13669531" author="shaie" created="Wed, 29 May 2013 19:17:15 +0100">&lt;p&gt;Patch fixes the following:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;OverSampledFacetRequest sets numLabel=0, as otherwise it will label categories that will be thrown away (over-sample). This was introduced incorrectly in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4411" title="Depth requested in a facetRequest is reset when Sampling is in effect"&gt;&lt;del&gt;LUCENE-4411&lt;/del&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;SamplingAccumulator and SamplingWrapper now return an emptyResult() when the ordinal of the root node is INVALID, which makes them consistent with the other accumulators.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added TestFacetsCollector.testLabeling which ensures all current accumulators behave the same.
	&lt;ul&gt;
		&lt;li&gt;This uncovered a bug in RangeAccumulator when some readers did not have the requested numeric DV field. I fixed that too.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think it's ready - I intend to commit it tomorrow.&lt;/p&gt;</comment>
                    <comment id="13670200" author="gilad" created="Thu, 30 May 2013 11:17:59 +0100">&lt;p&gt;Patch looks good.&lt;br/&gt;
+1 for commit &lt;/p&gt;</comment>
                    <comment id="13670218" author="shaie" created="Thu, 30 May 2013 11:40:14 +0100">&lt;p&gt;Committed to trunk and 4x. Thanks Rob for reporting this!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585268" name="LUCENE-5016.patch" size="10786" author="shaie" created="Wed, 29 May 2013 19:17:15 +0100"/>
                    <attachment id="12584701" name="test-labels.zip" size="4580" author="robau" created="Fri, 24 May 2013 16:42:11 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 24 May 2013 10:53:06 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329625</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329607</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5015] Unexpected performance difference between SamplingAccumulator and StandardFacetAccumulator</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5015</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I have an unexpected performance difference between the SamplingAccumulator and the StandardFacetAccumulator. &lt;/p&gt;

&lt;p&gt;The case is an index with about 5M documents and each document containing about 10 fields. I created a facet on each of those fields. When searching to retrieve facet-counts (using 1 CountFacetRequest), the SamplingAccumulator is about twice as fast as the StandardFacetAccumulator. This is expected and a nice speed-up. &lt;/p&gt;

&lt;p&gt;However, when I use more CountFacetRequests to retrieve facet-counts for more than one field, the speeds of the SampingAccumulator decreases, to the point where the StandardFacetAccumulator is faster. &lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt; 
FacetRequests  Sampling    Standard
 1               391 ms     1100 ms
 2               531 ms     1095 ms 
 3               948 ms     1108 ms
 4              1400 ms     1110 ms
 5              1901 ms     1102 ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Is this behaviour normal? I did not expect it, as the SamplingAccumulator needs to do less work? &lt;/p&gt;

&lt;p&gt;Some code to show what I do:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
	searcher.search( facetsQuery, facetsCollector );
	&lt;span class="code-keyword"&gt;final&lt;/span&gt; List&amp;lt;FacetResult&amp;gt; collectedFacets = facetsCollector.getFacetResults();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;final&lt;/span&gt; FacetSearchParams facetSearchParams = &lt;span class="code-keyword"&gt;new&lt;/span&gt; FacetSearchParams( facetRequests );

FacetsCollector facetsCollector;

&lt;span class="code-keyword"&gt;if&lt;/span&gt; ( isSampled )
{
	facetsCollector =
		FacetsCollector.create( &lt;span class="code-keyword"&gt;new&lt;/span&gt; SamplingAccumulator( &lt;span class="code-keyword"&gt;new&lt;/span&gt; RandomSampler(), facetSearchParams, searcher.getIndexReader(), taxo ) );
}
&lt;span class="code-keyword"&gt;else&lt;/span&gt;
{
	facetsCollector = FacetsCollector.create( FacetsAccumulator.create( facetSearchParams, searcher.getIndexReader(), taxo ) );
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</description>
                <environment/>
            <key id="12649058">LUCENE-5015</key>
            <summary>Unexpected performance difference between SamplingAccumulator and StandardFacetAccumulator</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="robau">Rob Audenaerde</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 May 2013 09:02:44 +0100</created>
                <updated>Wed, 29 May 2013 14:20:42 +0100</updated>
                    <resolved>Wed, 29 May 2013 09:29:19 +0100</resolved>
                            <version>4.3</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13664976" author="gilad" created="Thu, 23 May 2013 09:18:21 +0100">&lt;p&gt;Hello Rob,&lt;/p&gt;

&lt;p&gt;Indeed that looks unexpected.&lt;br/&gt;
The immediate suspect is the "fixing" part of the sampling, where after sampled top-cK are computed for each facet request, each of the candidates for top-K gets a real count computation, rather than a count over the sampled set of results.&lt;/p&gt;

&lt;p&gt;How many results are in the result set? All the documents?&lt;/p&gt;</comment>
                    <comment id="13664980" author="robau" created="Thu, 23 May 2013 09:22:45 +0100">&lt;p&gt;I use a MatchAddDocsQuery(), so I retrieve all the 5 million documents as hits.&lt;/p&gt;</comment>
                    <comment id="13665018" author="gilad" created="Thu, 23 May 2013 10:09:41 +0100">&lt;p&gt;Sampling, with its defaults, has its toll. &lt;/p&gt;

&lt;p&gt;In its defaults, Sampling aims to produce the exact top-K results for each request, as if a &lt;tt&gt;StandardFacetAccumulator&lt;/tt&gt; would have been used. Meaning it aims at producing the same top-K with the same counts.&lt;/p&gt;

&lt;p&gt;The process begins with sampling the result set and computers the top-&lt;b&gt;cK&lt;/b&gt; candidates for each of the &lt;b&gt;M&lt;/b&gt; facet requests, producing amortized results. That part is faster than &lt;tt&gt;StandardFacetAccumulator&lt;/tt&gt; because less documents' facets information gets processed.&lt;/p&gt;

&lt;p&gt;The next part is the "fixing", using a &lt;tt&gt;SampleFixer&lt;/tt&gt; retrieved from a &lt;tt&gt;Sampler&lt;/tt&gt;, in which "fixed" counts are produced which correlate better with the original document result set, rather than the sampled one. The default (and currently only implementation) for such fixer is &lt;tt&gt;TakmiSampleFixer&lt;/tt&gt; which produced &lt;em&gt;exact&lt;/em&gt; counts for each of the &lt;b&gt;cK&lt;/b&gt; candidates for each of the &lt;b&gt;M&lt;/b&gt; facet requests. The counts are not computed against the facet information of each document, but rather matching the skiplist of the drill-down term, of each such candidate category with the bitset of the (actual) document results. The amount of matches is the count. &lt;br/&gt;
This is equivalent to total-hit collector with a drilldown query for the candidate category over original query. &lt;br/&gt;
There's tipping point in which not sampling is faster than sampling and fixing using &lt;b&gt;c&lt;/b&gt; x &lt;b&gt;K&lt;/b&gt; x &lt;b&gt;M&lt;/b&gt; skiplists matches against the bitset representing the document results. &lt;b&gt;c&lt;/b&gt; defaults to 2 (see overSampleFactor in SamplingParams); &lt;/p&gt;

&lt;p&gt;Over-sampling (a.k.a &lt;b&gt;c&lt;/b&gt;) is important for exact counts, as it is conceivable that the accuracy of a sampled top-k is not 100%, but according to some measures we once ran it is very likely that the true top-K results are within the sampled &lt;b&gt;2K&lt;/b&gt; results. Fixing those 2K with their actual counts and re-sorting them accordingly yields much more accurate top-K. &lt;/p&gt;


&lt;p&gt;E.g Requesting 5 count requests for top-10 with overSampleFactor of 2, results in 100 skiplist matching against the document results bitset.&lt;/p&gt;


&lt;p&gt;If amortized results suffice, a different &lt;tt&gt;SampleFixer&lt;/tt&gt; could be coded - which E.g amortize the true count from the sampling ration. E.g if category C got count of 3, and the sample was of 1,000 results out of a 1,000,000 than the "AmortizedSampleFixer" would fix the count of C to be 3,000.&lt;br/&gt;
Such fixing is very fast, and the overSampleFactor should be set to 1.0.&lt;/p&gt;

&lt;p&gt;Edit:&lt;br/&gt;
I now see that it is not that easy to code a different SampleFixer, nor get it the information needed for the amortized result fixing as suggested above. &lt;br/&gt;
I'll try to open the API some and make it more convenient.&lt;/p&gt;</comment>
                    <comment id="13665047" author="robau" created="Thu, 23 May 2013 11:01:55 +0100">&lt;p&gt;Hi Gilad,&lt;/p&gt;

&lt;p&gt;Thanks for the swift and very useful reply, it has given me good insight in the sampling process. &lt;/p&gt;

&lt;p&gt;As I see it, I could benefit from a more straight-forward SampleFixer; like the behaviour of the AmortizedSampleFixer you described. It would be great to have one, or be able to code one up, so +1 for the API improvements which are needed for this. &lt;/p&gt;</comment>
                    <comment id="13665468" author="gilad" created="Thu, 23 May 2013 19:36:33 +0100">&lt;p&gt;Added a parameter to &lt;tt&gt;SamplingParams&lt;/tt&gt; named &lt;tt&gt;fixToExact&lt;/tt&gt; which defaults to &lt;tt&gt;false&lt;/tt&gt;. &lt;br/&gt;
I think it is probable that one who uses sampling may not be interested in exact results.&lt;/p&gt;

&lt;p&gt;In the proposed approach, the &lt;tt&gt;Sampler&lt;/tt&gt; would create either the old, slow, and accurate &lt;tt&gt;TakmiSampleFixer&lt;/tt&gt; if &lt;tt&gt;SamplingParams.shouldFixToExact()&lt;/tt&gt; is &lt;tt&gt;true&lt;/tt&gt;. Otherwise the much (much!} faster &lt;tt&gt;AmortizedSampleFixer&lt;/tt&gt; would be used, when it only take under account the sampling ratio, assuming the sampled set represent the whole set with 100% accuracy.&lt;/p&gt;

&lt;p&gt;With these changes, the code above should already use the amortized fixer, as the default is now it.&lt;br/&gt;
If the old fixer is to be used - for comparison - the code could look as follows:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;final&lt;/span&gt; FacetSearchParams facetSearchParams = &lt;span class="code-keyword"&gt;new&lt;/span&gt; FacetSearchParams( facetRequests );

FacetsCollector facetsCollector;

&lt;span class="code-keyword"&gt;if&lt;/span&gt; ( isSampled )
{
	&lt;span class="code-comment"&gt;// Create SamplingParams which denotes fixing to exact
&lt;/span&gt;	SamplingParams samplingParams = &lt;span class="code-keyword"&gt;new&lt;/span&gt; SamplingParams();
	samplingParams.setFixToExact(&lt;span class="code-keyword"&gt;true&lt;/span&gt;);

	&lt;span class="code-comment"&gt;// Use the custom sampling params &lt;span class="code-keyword"&gt;while&lt;/span&gt; creating the RandomSampler
&lt;/span&gt;	facetsCollector =
		FacetsCollector.create( &lt;span class="code-keyword"&gt;new&lt;/span&gt; SamplingAccumulator( &lt;span class="code-keyword"&gt;new&lt;/span&gt; RandomSampler(samplingParams, &lt;span class="code-keyword"&gt;new&lt;/span&gt; Random(someSeed)), facetSearchParams, searcher.getIndexReader(), taxo ) );
}
&lt;span class="code-keyword"&gt;else&lt;/span&gt;
{
	facetsCollector = FacetsCollector.create( FacetsAccumulator.create( facetSearchParams, searcher.getIndexReader(), taxo ) );
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The sampling tests still use the "exact" fixer, as it is not easy asserting against amortized results. I'm still looking into creating a complete faceted search flow test with the amortized-fixer.&lt;/p&gt;</comment>
                    <comment id="13665469" author="gilad" created="Thu, 23 May 2013 19:39:39 +0100">&lt;p&gt;Older patch was against trunk/lucene/facet. This one is rooted with trunk. &lt;/p&gt;</comment>
                    <comment id="13666007" author="shaie" created="Fri, 24 May 2013 06:27:37 +0100">&lt;p&gt;Gilad this looks good! I have few comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;AmortizedSampleFixer's jdocs need a &amp;lt;p&amp;gt; tag instead of the empty line. Otherwise I think this does not render as expected.
	&lt;ul&gt;
		&lt;li&gt;Same in TakmiSamplerFixer&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;SampleFixer has a TODO next to the new param&lt;/li&gt;
	&lt;li&gt;AmortizedSamplerFixerTest and SamplerTest should extend FacetTestCase so that it doesn't use Lucene3x codec accidentally (which doesn't support DocValues and hence facets)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In general, what do you think if SamplingParams take a SampleFixer instead of &lt;tt&gt;fixToExact&lt;/tt&gt;?&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;We could default to Amortized, while the current sampling tests will set Takmi&lt;/li&gt;
	&lt;li&gt;It will allow someone who doesn't care about the value at all to not fix it. I.e., if I just want to show 5% in the UI, I don't really need Amortized right?&lt;/li&gt;
	&lt;li&gt;It will allow to experiment with other SampleFixers implementations, e.g. maybe Takmi can be made more efficient or something.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Currently SampleFixer is public though there's really no point to override it since you cannot pass it anywhere? Therefore I think that taking a fixer is better.&lt;/p&gt;</comment>
                    <comment id="13666009" author="gilad" created="Fri, 24 May 2013 06:37:15 +0100">&lt;p&gt;Shai, thanks for the comments.&lt;br/&gt;
First three points are taken care of in the new patch.&lt;br/&gt;
As for SamplingParams taking a SampleFixer, it's a good idea, and I've been there, but it makes it harder on the e.g. &lt;tt&gt;SamplingAccumulator&lt;/tt&gt; to figure out whether to oversample - and trim - for this SampleFixer. It would than move this logic to the SampleFixer. &lt;br/&gt;
That's not bad, but it changes the API a bit more, also the name SampleFixer does not match the functionality any more (perhaps it should oversample and trim itself?)&lt;/p&gt;</comment>
                    <comment id="13666091" author="shaie" created="Fri, 24 May 2013 09:03:55 +0100">&lt;p&gt;Well, as long as we keep SampleFixer hidden, users will not be able to solve sampling issues on their own. So the API has to be open on that end too. Maybe SamplingAccumulator can have a protected shouldOverSample with a default impl that handles the two known fixers and otherwise returns false? Then the user who provides his own fixer, can provide his accumulator too.&lt;/p&gt;</comment>
                    <comment id="13666190" author="shaie" created="Fri, 24 May 2013 11:50:33 +0100">&lt;p&gt;Looking at SamplingParams, isn't overSampleFactor enough to decide whether to over sample or not? It can default to not oversample, with the default Amortized fixer or some other default (2.0?) if fixer is Takmi. Then user can change it, and if he passes a fixer which requires over sampling, he should set that too.&lt;/p&gt;</comment>
                    <comment id="13667221" author="gilad" created="Sun, 26 May 2013 08:40:55 +0100">&lt;p&gt;True, looking at overSampleFactor is enough, but it's not obvious that TakmiFixer should be used with overSampleFactor &amp;gt; 1, to better the chances of the result top-k being accurate.&lt;br/&gt;
I'll add some documentation w.r.t this issue, I hope it will do.&lt;/p&gt;

&lt;p&gt;New patch defaults to &lt;tt&gt;NoopSampleFixer&lt;/tt&gt; which does not touch the results at all - if the need is only for a top-k and their counts does not matter, this is the least expensive one. &lt;br/&gt;
Also if instead of counts, a percentage sould be displayed (as how much of the results match this category), the sampled valued out of the sample size would yield the same result as the amortized fixed results out of the actual result set size. That might render the amortized fixer moot..&lt;/p&gt;

&lt;p&gt;New patch account of &lt;tt&gt;SampleFixer&lt;/tt&gt; being set in &lt;tt&gt;SamplingParams&lt;/tt&gt;&lt;/p&gt;</comment>
                    <comment id="13667223" author="shaie" created="Sun, 26 May 2013 08:52:04 +0100">&lt;p&gt;Thanks Gilad. Now that we have SampleFixer on SamplingParams, I wonder why we need Noop and Amortized? Could we just make the default fixer null and not oversample + fix if it's null? And Amortized ... well as you said, it looks kind of redundant now... I don't think it's rocket science for an app to do value/ratio on its own, yet it's one more class that we need to maintain going forward?&lt;/p&gt;</comment>
                    <comment id="13667243" author="gilad" created="Sun, 26 May 2013 10:15:32 +0100">&lt;p&gt;Shai, I think you're right, a null &lt;tt&gt;SampleFixer&lt;/tt&gt; makes more sense. &lt;/p&gt;

&lt;p&gt;While working on a test which validates that a flow works with the &lt;tt&gt;null&lt;/tt&gt; fixer, I found it it did not. The reason is Complements. By default the complements kicks in when enough results are found. I think this may hold the key to the performance differences as well.&lt;/p&gt;

&lt;p&gt;Rod, could you please try the following code and report the results?&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    SamplingAccumulator accumulator = &lt;span class="code-keyword"&gt;new&lt;/span&gt; SamplingAccumulator( &lt;span class="code-keyword"&gt;new&lt;/span&gt; RandomSampler(),  facetSearchParams, searcher.getIndexReader, taxo);

    &lt;span class="code-comment"&gt;// Make sure no complements are in action
&lt;/span&gt;    accumulator.setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);
    
    facetsCollector = FacetsCollector.create(accumulator);

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the mean time, made the changes to the patch, and added the test for &lt;tt&gt;null&lt;/tt&gt; fixer.&lt;/p&gt;</comment>
                    <comment id="13668134" author="robau" created="Tue, 28 May 2013 08:08:21 +0100">&lt;p&gt;Hi all, thanks for all the progress. &lt;/p&gt;

&lt;p&gt;I will try to build a Lucene with the latests patch and give it a go.. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;(do I take the 4.3 release version? or is there a 4.3 development branch where the patch has to be applied?)&lt;/p&gt;</comment>
                    <comment id="13668143" author="robau" created="Tue, 28 May 2013 08:22:26 +0100">&lt;p&gt;I took the revisionnumber that was in the patchfile and checked that out.&lt;/p&gt;

&lt;p&gt; svn checkout &lt;a href="http://svn.apache.org/repos/asf/lucene/dev/trunk@1486401" class="external-link"&gt;http://svn.apache.org/repos/asf/lucene/dev/trunk@1486401&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;After installing Ivy I'm now building Lucene myself for the first time&lt;/p&gt;</comment>
                    <comment id="13668151" author="shaie" created="Tue, 28 May 2013 08:28:38 +0100">&lt;p&gt;Rob, you don't need to build Lucene to try what Gilad suggested, just modify your search code to disable complements. The problem is that if complements indeed kick in, and from the setup your describe it seems that they do because you search with MADQ, then sampling isn't done at all, yet the accumulator still corrects the counts.&lt;/p&gt;

&lt;p&gt;After you try it, we can tell if the performance overhead is indeed because of complements or that the counts are corrected. In either case, I think it will be good to open up the SampleFixer.&lt;/p&gt;</comment>
                    <comment id="13668267" author="robau" created="Tue, 28 May 2013 13:11:14 +0100">&lt;p&gt;Hi Shai,&lt;/p&gt;

&lt;p&gt;I will check tomorrow. Just to be sure, this is what I will do:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Lucene 4.3 release&lt;/li&gt;
	&lt;li&gt;FacetsAccumulator with and without complements
	&lt;ul class="alternate" type="square"&gt;
		&lt;li&gt;With the 'default' TakmiSampleFixer&lt;/li&gt;
		&lt;li&gt;With a NOOP empty Sampler implementation that I will return by overriding the 'getSampleFixer' method in the Sampler that I will provide.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;MADQ with 1..5 selected facets&lt;/li&gt;
	&lt;li&gt;some other query that will return about 50% of the documents, also with 1..5 facets&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I currently have a nice 15M document set, I will use that as a basis. &lt;/p&gt;</comment>
                    <comment id="13668270" author="shaie" created="Tue, 28 May 2013 13:16:19 +0100">&lt;p&gt;yes that sounds good. If you don't want to apply the patch so you can use the Noop fixer, that's fine too. I think the main goal is to see whether the complements that kicked in were in the way.&lt;/p&gt;</comment>
                    <comment id="13668281" author="shaie" created="Tue, 28 May 2013 13:47:42 +0100">&lt;p&gt;Patch adds CHANGES entry as well as makes SampleFixer and TakmiSampleFixer public. I think this is ready but let's wait for Rob's input.&lt;/p&gt;</comment>
                    <comment id="13669082" author="robau" created="Wed, 29 May 2013 09:01:27 +0100">&lt;p&gt;Time in ms.&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt; 
        MADQ				75% hits			
        Complements	DISABLE Com.	Complements	DISABLE complements	
#facets Takmi 	Noop	Takmi	Noop	Takmi	Noop	Takmi	Noop
1         999	433	1024	393	1239	541	969	432
2        2292	388	2877	512	2379	609	2489	457
3        2501	219	3228	413	2477	569	2590	434
4        3589	224	5052	392	3372	562	4093	503
5        4764	247	6863	493	4356	577	5103	533
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt; 					
SamplingParams sampleParams = new SamplingParams();									
sampleParams.setMaxSampleSize( 5000 );									
sampleParams.setMinSampleSize( 5000 );									
sampleParams.setSamplingThreshold( 75000 );									
sampleParams.setOversampleFactor( 1.0d );									
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; </comment>
                    <comment id="13669085" author="shaie" created="Wed, 29 May 2013 09:14:29 +0100">&lt;p&gt;Thanks Rob. This shows that complements don't affect the performance much, and Takmi is the main issue. This is good!&lt;br/&gt;
And also, Noop is stable with the number of growing facet requests, which is expected because it doesn't do any more work, while Takmi gets worse as more requests are added.&lt;br/&gt;
Actually, you use overSampleFactor=1, which is a bit optimistic for Takmi. Usually we use 2. That would show an even worse running time.&lt;/p&gt;

&lt;p&gt;W.r.t running the test, do you loop through the number of requests, or start a new JVM for each testcase? Do you do "warmup" runs to exclude their results before the actual measure? This won't change the fact that Takmi is slower than Noop, just perhaps explain why Noop w/ 5 requests is faster than 1 (which makes no sense, I take it it's an OS cache, or no warmup run).&lt;/p&gt;

&lt;p&gt;Anyway, I think this proves we need to make the default fixer null (which is equivalent to noop). I'll go ahead and commit the changes.&lt;/p&gt;</comment>
                    <comment id="13669090" author="shaie" created="Wed, 29 May 2013 09:29:19 +0100">&lt;p&gt;Committed to trunk and 4x. Thanks Rob for reporting and taking the time to test this and Gilad for the fix!&lt;/p&gt;</comment>
                    <comment id="13669164" author="robau" created="Wed, 29 May 2013 12:36:22 +0100">&lt;p&gt;Thank you too &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Some more test-details:&lt;/p&gt;

&lt;p&gt;Each 'column' of 5 facets is done in a new JVM. Each individual cell is 4 searches, the first is disregarded, the three left are averaged. &lt;/p&gt;

&lt;p&gt;For the &lt;tt&gt;SampingParams&lt;/tt&gt;, I reduced the numbers from the defaults to speed up testing.&lt;/p&gt;</comment>
                    <comment id="13669226" author="shaie" created="Wed, 29 May 2013 14:20:42 +0100">&lt;p&gt;Thanks. I usually take the minimum, not average, since technically it's the fastest we could get. Also, discarding only one run is not always enough, since it may take the OS cache more time to warm up. But anyway, the numbers are clear. Thanks for doing this Rob!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12585027" name="LUCENE-5015.patch" size="21686" author="shaie" created="Tue, 28 May 2013 13:47:42 +0100"/>
                    <attachment id="12584861" name="LUCENE-5015.patch" size="19588" author="gilad" created="Sun, 26 May 2013 10:15:32 +0100"/>
                    <attachment id="12584855" name="LUCENE-5015.patch" size="21133" author="gilad" created="Sun, 26 May 2013 08:40:55 +0100"/>
                    <attachment id="12584644" name="LUCENE-5015.patch" size="18282" author="gilad" created="Fri, 24 May 2013 06:37:15 +0100"/>
                    <attachment id="12584545" name="LUCENE-5015.patch" size="18373" author="gilad" created="Thu, 23 May 2013 19:39:39 +0100"/>
                    <attachment id="12584544" name="LUCENE-5015.patch" size="17983" author="gilad" created="Thu, 23 May 2013 19:36:33 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>6.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 23 May 2013 08:18:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329398</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>329380</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5007] smokeTestRelease.py should be able to pass cmdline test args to 'ant test', e.g. "-Dtests.jettyConnector=Socket"; also, "ant nightly-smoke" should be able to pass these args to smokeTestRelease.py</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5007</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/SOLR-4189" title="Fix hanging threads on FreeBSD in JettySolrRunner by making connector configureable through sysprop"&gt;&lt;del&gt;SOLR-4189&lt;/del&gt;&lt;/a&gt; added sensitivity to sysprop &lt;tt&gt;tests.jettyConnector&lt;/tt&gt; to allow setting test mode Jetty to use Socket connector instead of the default SelectChannel connector.&lt;/p&gt;

&lt;p&gt;New module lucene/replicator is running into the same problem, failing 100% of the time when running under 'ant nightly-smoke' on ASF Jenkins on FreeBSD.&lt;/p&gt;

&lt;p&gt;At present there's no way from smokeTestRelease.py, or from "ant nightly-smoke", to pass through this sysprop (or any other).&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rcmuir" class="user-hover" rel="rcmuir"&gt;Robert Muir&lt;/a&gt; wrote on dev@l.o.a about one of the replicator module's failures on FreeBSD:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a jenkins setup/test harness issue.&lt;/p&gt;

&lt;p&gt;there needs to be a way for the jetty connector sysprop to be passed&lt;br/&gt;
all the way thru to ant test running from the smoketester.&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment/>
            <key id="12648325">LUCENE-5007</key>
            <summary>smokeTestRelease.py should be able to pass cmdline test args to 'ant test', e.g. "-Dtests.jettyConnector=Socket"; also, "ant nightly-smoke" should be able to pass these args to smokeTestRelease.py</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="steve_rowe">Steve Rowe</assignee>
                                <reporter username="steve_rowe">Steve Rowe</reporter>
                        <labels>
                    </labels>
                <created>Sun, 19 May 2013 01:29:30 +0100</created>
                <updated>Wed, 22 May 2013 16:27:09 +0100</updated>
                    <resolved>Mon, 20 May 2013 17:58:29 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13661448" author="steve_rowe" created="Sun, 19 May 2013 01:40:10 +0100">&lt;p&gt;Patch for branch_4x, adding new optional named arg/value pair &lt;tt&gt;-testArgs "stuff"&lt;/tt&gt; to &lt;tt&gt;smokeTestRelease.py&lt;/tt&gt;, which gets passed verbatim to all &lt;tt&gt;ant test&lt;/tt&gt; invocations.  &lt;tt&gt;ant nightly-smoke&lt;/tt&gt; will pass-through sysprop value &lt;tt&gt;smokeTestRelease.testArgs&lt;/tt&gt; as &lt;tt&gt;smokeTestRelease.py&lt;/tt&gt;'s &lt;tt&gt;-testArgs&lt;/tt&gt; arg value.&lt;/p&gt;</comment>
                    <comment id="13661451" author="steve_rowe" created="Sun, 19 May 2013 01:55:54 +0100">&lt;p&gt;Trunk patch.&lt;/p&gt;

&lt;p&gt;In branch_4x, fixed commented-out java6 Solr &lt;tt&gt;ant test&lt;/tt&gt; invocation to pass &lt;tt&gt;testArgs&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;ant nightly-smoke&lt;/tt&gt; works for me under trunk and branch_4x, both with and without &lt;tt&gt;-DsmokeTestRelease.testArgs='-Dtests.verbose=false -Dtests.locale=ru_RU'&lt;/tt&gt; (those are the actual sysprops I used, but of course others should work as well).&lt;/p&gt;

&lt;p&gt;I think it's ready to go.&lt;/p&gt;</comment>
                    <comment id="13662150" author="steve_rowe" created="Mon, 20 May 2013 17:58:29 +0100">&lt;p&gt;Committed:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;trunk: r1484524&lt;/li&gt;
	&lt;li&gt;branch_4x: r1484525&lt;/li&gt;
&lt;/ul&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12583746" name="LUCENE-5007-branch_4x.patch" size="8109" author="steve_rowe" created="Sun, 19 May 2013 01:55:54 +0100"/>
                    <attachment id="12583745" name="LUCENE-5007-branch_4x.patch" size="7647" author="steve_rowe" created="Sun, 19 May 2013 01:40:10 +0100"/>
                    <attachment id="12583747" name="LUCENE-5007-trunk.patch" size="7023" author="steve_rowe" created="Sun, 19 May 2013 01:55:54 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>328693</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>328675</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-5002] Deadlock in DocumentsWriterFlushControl</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-5002</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;We have an obvious deadlock between a "MaybeRefreshIndexJob" thread&lt;br/&gt;
calling ReferenceManager.maybeRefresh(ReferenceManager.java:204) and a&lt;br/&gt;
"RebuildIndexJob" thread calling&lt;br/&gt;
IndexWriter.deleteAll(IndexWriter.java:2065).&lt;/p&gt;

&lt;p&gt;Lucene wants to flush in the "MaybeRefreshIndexJob" thread trying to intrinsically lock the IndexWriter instance at &lt;tt&gt;DocumentsWriterPerThread.java:563&lt;/tt&gt; before notifyAll()ing the flush. &lt;/p&gt;

&lt;p&gt;Simultaneously the "RebuildIndexJob" thread who already intrinsically locked the IndexWriter instance at IndexWriter#deleteAll wait()s at &lt;tt&gt;DocumentsWriterFlushControl.java:245&lt;/tt&gt; for the flush forever causing a deadlock.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-quote"&gt;"MaybeRefreshIndexJob &lt;span class="code-object"&gt;Thread&lt;/span&gt; - 2"&lt;/span&gt; daemon prio=10 tid=0x00007f8fe4006000 nid=0x1ac2 waiting &lt;span class="code-keyword"&gt;for&lt;/span&gt; monitor entry [0x00007f8fa7bf7000]
   java.lang.&lt;span class="code-object"&gt;Thread&lt;/span&gt;.State: BLOCKED (on object monitor)
	at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2223)
	- waiting to lock &amp;lt;0x00000000f1c00438&amp;gt; (a org.apache.lucene.index.IndexWriter)
	at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:563)
	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:533)
	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:422)
	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:559)
	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:365)
	- locked &amp;lt;0x00000000f1c007d0&amp;gt; (a java.lang.&lt;span class="code-object"&gt;Object&lt;/span&gt;)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:270)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:245)
	at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:235)
	at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:170)
	at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:118)
	at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)
	at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:155)
	at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:204)
	at jobs.MaybeRefreshIndexJob.timeout(MaybeRefreshIndexJob.java:47)

&lt;span class="code-quote"&gt;"RebuildIndexJob &lt;span class="code-object"&gt;Thread&lt;/span&gt; - 1"&lt;/span&gt; prio=10 tid=0x00007f903000a000 nid=0x1a38 in &lt;span class="code-object"&gt;Object&lt;/span&gt;.wait() [0x00007f9037dd6000]
   java.lang.&lt;span class="code-object"&gt;Thread&lt;/span&gt;.State: WAITING (on object monitor)
	at java.lang.&lt;span class="code-object"&gt;Object&lt;/span&gt;.wait(Native Method)
	- waiting on &amp;lt;0x00000000f1c0c240&amp;gt; (a org.apache.lucene.index.DocumentsWriterFlushControl)
	at java.lang.&lt;span class="code-object"&gt;Object&lt;/span&gt;.wait(&lt;span class="code-object"&gt;Object&lt;/span&gt;.java:503)
	at org.apache.lucene.index.DocumentsWriterFlushControl.waitForFlush(DocumentsWriterFlushControl.java:245)
	- locked &amp;lt;0x00000000f1c0c240&amp;gt; (a org.apache.lucene.index.DocumentsWriterFlushControl)
	at org.apache.lucene.index.DocumentsWriter.abort(DocumentsWriter.java:235)
	- locked &amp;lt;0x00000000f1c05370&amp;gt; (a org.apache.lucene.index.DocumentsWriter)
	at org.apache.lucene.index.IndexWriter.deleteAll(IndexWriter.java:2065)
	- locked &amp;lt;0x00000000f1c00438&amp;gt; (a org.apache.lucene.index.IndexWriter)
	at jobs.RebuildIndexJob.buildIndex(RebuildIndexJob.java:102)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;OpenJDK 64-Bit Server VM (23.7-b01 mixed mode)&lt;br/&gt;
Linux Ubuntu Server 12.04 LTS 64-Bit&lt;/p&gt;</environment>
            <key id="12647896">LUCENE-5002</key>
            <summary>Deadlock in DocumentsWriterFlushControl</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="sergiusz.urbaniak">Sergiusz Urbaniak</reporter>
                        <labels>
                    </labels>
                <created>Thu, 16 May 2013 08:41:56 +0100</created>
                <updated>Tue, 21 May 2013 12:03:16 +0100</updated>
                    <resolved>Tue, 21 May 2013 12:03:16 +0100</resolved>
                            <version>4.3</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                <fixVersion>4.3.1</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="13659343" author="sergiusz.urbaniak" created="Thu, 16 May 2013 08:59:14 +0100">&lt;p&gt;Implementation note: we (obviously) use the same IndexWriter instance across all threads.&lt;/p&gt;</comment>
                    <comment id="13659361" author="simonw" created="Thu, 16 May 2013 09:40:06 +0100">&lt;p&gt;here is a patch that has a test that hangs. Pretty straight forward though. Yet, the problem is that we are locking on the index writer in DWPT. Or on the other hand there are too many synch blocks in IW to make it safe to call into IW from DWPT. &lt;/p&gt;

&lt;p&gt;I need to look into that more closely to figure out how to fix that.&lt;/p&gt;</comment>
                    <comment id="13659407" author="thetaphi" created="Thu, 16 May 2013 11:13:21 +0100">&lt;blockquote&gt;&lt;p&gt;Yet, the problem is that we are locking on the index writer in DWPT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My personal horror scenario! The worst thing you can do is to also externally synchronize on IW. This also causes deadlocks. We should maybe open an issue to fix the synchronization in IW and make it simplier, especially with using ju.concurrent.Lock implementations.&lt;/p&gt;</comment>
                    <comment id="13659508" author="sergiusz.urbaniak" created="Thu, 16 May 2013 14:21:18 +0100">&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Thanks for the quick feedback! As long as the sync issues on IW are unresolved we declare IW instances as &lt;b&gt;not thread-safe&lt;/b&gt; for our development and synchronize access to it externally (of course as mentioned in the docs not on the IW instance itself).&lt;/p&gt;</comment>
                    <comment id="13660559" author="mikemccand" created="Fri, 17 May 2013 12:15:21 +0100">&lt;p&gt;I think we should address this for 4.3.1?&lt;/p&gt;</comment>
                    <comment id="13660586" author="simonw" created="Fri, 17 May 2013 12:43:13 +0100">&lt;p&gt;Ok so I tried to make this work for an entire day and bottom line is that once I move the DocumentsWriter#abort() out of the sync block my test still fails all over the place. Yet, it's not hanging but concurrent access to IW while IW#deleteAll() is called is entirely broken IMO. I don't even know where to start, here is a small wrapup of the failures I saw:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;asserts are tripped in global field map since we clear and concurrently index (remember indexing is non-blocking)&lt;/li&gt;
	&lt;li&gt;concurrent commits fail with fiel not found exception (even if we fully sync) seems like some state in IW is not cleared&lt;/li&gt;
	&lt;li&gt;updatePendingMerges fails with FNF when merges are updated concurrently.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To begin with I doubt that the semantics of IW#deleteAll() are correct today if you are accessing the IW concurrently. I mean we basically dropping everything and don't maintain any happens before relationship here at all, delete all files that are not referenced in any seg info wipe all the global field infos etc. We should address this properly.&lt;/p&gt;

&lt;p&gt;I agree that we have to fix this until 4.3.1!&lt;/p&gt;

&lt;p&gt;Yet, Serguiuz  do you see any FileNotFoundExceptions or anything when you concurrently call deleteAll? I mean this seems entirely broken to me at this point. I suggest you to use deleteQuery(new MatchAllDocsQuery()) for now and not lock globally. &lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13660590" author="simonw" created="Fri, 17 May 2013 12:46:15 +0100">&lt;p&gt;here is a patch that fixes this issue and adds some asserts that make sure we don't wait while holding the IW lock. Yet, this is a pretty drastic step in the patch since I need to stop the world for this to be an operation that works correct in a concurrent world. I think it's ok for us do to a stop the world here but we really need to beast this patch.&lt;/p&gt;</comment>
                    <comment id="13660592" author="mikemccand" created="Fri, 17 May 2013 12:46:40 +0100">&lt;p&gt;One (app level) workaround here is to not call deleteAll in one thread, while other threads are still indexing.&lt;/p&gt;

&lt;p&gt;But we still have to fix the deadlock.  I think a simple fix in deleteAll is to move the sync down after docWriter.abort(), so we don't hold IW's intrinsic lock while calling docWriter.abort().  I tried this, but it leads to an AssertionError:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;Caused by: java.lang.AssertionError
	at __randomizedtesting.SeedInfo.seed([60429B79D72112B5]:0)
	at org.apache.lucene.index.FieldInfos$Builder.addOrUpdateInternal(FieldInfos.java:284)
	at org.apache.lucene.index.FieldInfos$Builder.addOrUpdate(FieldInfos.java:266)
	at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:211)
	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:256)
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376)
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1508)
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1183)
	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:152)
	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:114)
	at org.apache.lucene.index.TestIndexWriterDelete$1.run(TestIndexWriterDelete.java:331)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Separately we really need to overhaul / simplify IW/DW/BD's locking!&lt;/p&gt;</comment>
                    <comment id="13660655" author="mikemccand" created="Fri, 17 May 2013 13:01:00 +0100">&lt;p&gt;The patch is terrifying looking yet seems necessary, and on beasting seems to work (but: we have to either not use docValues, or prevent Lucene3x codec).&lt;/p&gt;

&lt;p&gt;I think stop-the-world is perfectly fine here: it's not like apps are calling deleteAll 1000s of times per second.&lt;/p&gt;

&lt;p&gt;I even think it'd be fine if concurrent indexing threads hit strange exceptions, and we document that you should not use other methods while deleteAll is invoked in one thread, as long as we can guarantee this never leads to index corruption.  This is just like you can call IW.close from one thread while other threads are still indexing but those other threads can hit strange exceptions.&lt;/p&gt;

&lt;p&gt;I'll open a new issue to somehow simplify IW's sync... it's a mess now.&lt;/p&gt;</comment>
                    <comment id="13660658" author="mikemccand" created="Fri, 17 May 2013 13:04:49 +0100">&lt;p&gt;OK I opened &lt;a href="https://issues.apache.org/jira/browse/LUCENE-5006" title="Simplify / understand IndexWriter/DocumentsWriter synchronization"&gt;LUCENE-5006&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="13660669" author="sergiusz.urbaniak" created="Fri, 17 May 2013 13:19:00 +0100">&lt;p&gt;Simon,&lt;/p&gt;

&lt;p&gt;No FileNotFoundExceptions what so ever. The stack trace above is "complete" except the crappy ejb stack forrest which is not relevant.&lt;/p&gt;

&lt;p&gt;Again thanks for the quick reaction, we'll use deleteQuery(new MatchAllDocsQuery()) instead and omit the global lock.&lt;/p&gt;</comment>
                    <comment id="13662852" author="simonw" created="Tue, 21 May 2013 10:59:14 +0100">&lt;p&gt;Mike, I want to commit this patch and let is bake in a bit on trunk and 4x, any objections? I will remove the DV use in 4x on trunk this is not a problem.&lt;/p&gt;</comment>
                    <comment id="13662857" author="mikemccand" created="Tue, 21 May 2013 11:09:56 +0100">&lt;p&gt;+1 to commit&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12583645" name="LUCENE-5002.patch" size="13160" author="simonw" created="Fri, 17 May 2013 12:46:15 +0100"/>
                    <attachment id="12583452" name="LUCENE-5002_test.patch" size="3384" author="simonw" created="Thu, 16 May 2013 09:40:06 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 16 May 2013 08:40:06 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>328264</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>328246</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4999] Lucene test (testCambridgeMA) fails when JVM 64-bit does not use memory compression</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4999</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When I ran the Lucene (4.2.1/4.3) test suite with IBM Java I get the following error:&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt; Suite: org.apache.lucene.search.postingshighlight.TestPostingsHighlighter&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;   2&amp;gt; NOTE: reproduce with: ant test  -Dtestcase=TestPostingsHighlighter -Dtests.method=testCambridgeMA -Dtests.seed=571E16AEAF72C9F9 -Dtests.s&lt;br/&gt;
low=true -Dtests.locale=mt_MT -Dtests.timezone=Pacific/Kiritimati -Dtests.file.encoding=UTF-8&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt; ERROR   0.71s J2 | TestPostingsHighlighter.testCambridgeMA &amp;lt;&amp;lt;&amp;lt;&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt; Throwable #1: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 37&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at __randomizedtesting.SeedInfo.seed(&lt;span class="error"&gt;&amp;#91;571E16AEAF72C9F9:D60B7505C1DC91F8&amp;#93;&lt;/span&gt;:0)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.Passage.addMatch(Passage.java:53)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightDoc(PostingsHighlighter.java:547)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightField(PostingsHighlighter.java:425)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:364)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlightFields(PostingsHighlighter.java:268)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.PostingsHighlighter.highlight(PostingsHighlighter.java:198)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at org.apache.lucene.search.postingshighlight.TestPostingsHighlighter.testCambridgeMA(TestPostingsHighlighter.java:373)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt;    at java.lang.Thread.run(Thread.java:738)&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;   2&amp;gt; NOTE: test params are: codec=FastDecompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=FA&lt;br/&gt;
ST_DECOMPRESSION, chunkSize=386), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=FAST_DECOMPRESSION, chunkSize=386)), sim=RandomSimilarityProv&lt;br/&gt;
ider(queryNorm=false,coord=yes): &lt;/p&gt;
{body=DFR I(n)Z(0.3), title=DFR I(F)Z(0.3), id=DFR I(n)2}
&lt;p&gt;, locale=mt_MT, timezone=Pacific/Kiritimati&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;   2&amp;gt; NOTE: Linux 2.6.32-279.el6.x86_64 amd64/IBM Corporation 1.6.0 (64-bit)/cpus=4,threads=1,free=10783032,total=24030208&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;   2&amp;gt; NOTE: All tests run in this JVM: [FieldQueryTest, FieldPhraseListTest, SimpleFragListBuilderTest, FieldTermStackTest, OffsetLimitTokenFil&lt;br/&gt;
terTest, TokenSourcesTest, TestPostingsHighlighter]&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt; Completed on J2 in 2.46s, 23 tests, 1 error &amp;lt;&amp;lt;&amp;lt; FAILURES!&lt;/p&gt;


&lt;p&gt;This error is not seen with Oracle Java.&lt;br/&gt;
A Google search showed that this error has already occurred in community builds and the solution proposed was disable the IBM Java in the community tests.&lt;/p&gt;

&lt;p&gt;I took a look in the code and found that the root of the problem is due to the assignment of the variable "referenceSize" in RamUsageEstimator.java:&lt;/p&gt;

&lt;p&gt;    // get object reference size by getting scale factor of Object[] arrays:&lt;br/&gt;
    try &lt;/p&gt;
{
      final Method arrayIndexScaleM = unsafeClass.getMethod("arrayIndexScale", Class.class);
      referenceSize = ((Number) arrayIndexScaleM.invoke(theUnsafe, Object[].class)).intValue();
      supportedFeatures.add(JvmFeature.OBJECT_REFERENCE_SIZE);
    }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
      // ignore.
    }


&lt;p&gt;The Java Object reference size for arrays have 8 bytes in 64-bit machines (Oracle or IBM) and can be reduced to 4 bytes (like 32-bit JVMs) using Compressed References and Compressed Ordinary Object Pointers (OOPs).&lt;/p&gt;

&lt;p&gt;This options seems to be enabled by default in Oracle Java when the heap size is under 32GB, but is not in IBM Java.&lt;/p&gt;

&lt;p&gt;As workaround, when testing with IBM JVM I can pass the options "-Xcompressedrefs" or "-XX:+UseCompressedOops" to Junit.&lt;/p&gt;

&lt;p&gt;Similarly, you can reproduce the error if you pass the option "-XX:-UseCompressedOops" when testing with Oracle Java.&lt;/p&gt;


&lt;p&gt;The bug is in oversize method of ArrayUtil.java. It does nothing when the object reference size (bytesPerElement) is 8.&lt;/p&gt;
</description>
                <environment>&lt;p&gt;Red Hat 6.3&lt;br/&gt;
IBM Java 6 - SR13&lt;br/&gt;
Ant 1.9.0&lt;/p&gt;</environment>
            <key id="12647435">LUCENE-4999</key>
            <summary>Lucene test (testCambridgeMA) fails when JVM 64-bit does not use memory compression</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="3">Duplicate</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="rodrigotrujillo">Rodrigo Trujillo</reporter>
                        <labels>
                    </labels>
                <created>Tue, 14 May 2013 03:46:12 +0100</created>
                <updated>Sat, 1 Jun 2013 13:23:39 +0100</updated>
                    <resolved>Sat, 1 Jun 2013 13:23:39 +0100</resolved>
                            <version>4.3</version>
                <version>4.2.1</version>
                                <fixVersion>4.4</fixVersion>
                <fixVersion>4.3.1</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13656667" author="rcmuir" created="Tue, 14 May 2013 04:03:13 +0100">&lt;p&gt;Yes, this at least was a bug in the highlighter. However its currently fixed in SVN (will be in 4.3.1):&lt;/p&gt;

&lt;p&gt;The datastructure expected parallel arrays and did not grow them consistently as you noticed.&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;* LUCENE-4948: Fixed ArrayIndexOutOfBoundsException in PostingsHighlighter
  if you had a 64-bit JVM without compressed OOPS: IBM J9, or Oracle with
  large heap/explicitly disabled.  (Mike McCandless, Uwe Schindler, Robert Muir)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;Similarly, you can reproduce the error if you pass the option "-XX:-UseCompressedOops" when testing with Oracle Java.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Uwe Schindler did exactly that to our jenkins instances: it randomizes this variable so that this class of bugs will be found when using Oracle too, and won't be brushed aside as a JVM issue in the future.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310000">
                <name>Duplicate</name>
                                <outwardlinks description="duplicates">
                            <issuelink>
            <issuekey id="12643964">LUCENE-4948</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 14 May 2013 03:03:13 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327803</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327785</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4998] be more precise about IOContext for reads</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4998</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Set the context as &lt;tt&gt;IOContext.READ&lt;/tt&gt; / &lt;tt&gt;IOContext.READONCE&lt;/tt&gt; where applicable&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Motivation:&lt;/p&gt;

&lt;p&gt;Custom &lt;tt&gt;PostingsFormat&lt;/tt&gt; may want to check the context on &lt;tt&gt;SegmentReadState&lt;/tt&gt; and branch differently, but for this to work properly the context has to be specified correctly up the stack.&lt;/p&gt;

&lt;p&gt;For example, &lt;tt&gt;DirectPostingsFormat&lt;/tt&gt; only loads postings into memory if the &lt;tt&gt;context != MERGE&lt;/tt&gt;. However a better condition would be &lt;tt&gt;context == Context.READ &amp;amp;&amp;amp; !context.readOnce&lt;/tt&gt;.&lt;/p&gt;</description>
                <environment/>
            <key id="12647323">LUCENE-4998</key>
            <summary>be more precise about IOContext for reads</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="shikhar">Shikhar Bhushan</reporter>
                        <labels>
                    </labels>
                <created>Mon, 13 May 2013 17:11:21 +0100</created>
                <updated>Mon, 13 May 2013 17:12:35 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                    <attachment id="12582943" name="LUCENE-4998.patch" size="4120" author="shikhar" created="Mon, 13 May 2013 17:12:35 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327691</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327673</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4997] Internal test framework's tests are sensitive to previous test failures and tests.failfast.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4997</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description/>
                <environment/>
            <key id="12647219">LUCENE-4997</key>
            <summary>Internal test framework's tests are sensitive to previous test failures and tests.failfast.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="dweiss">Dawid Weiss</assignee>
                                <reporter username="dweiss">Dawid Weiss</reporter>
                        <labels>
                    </labels>
                <created>Sun, 12 May 2013 19:15:45 +0100</created>
                <updated>Sun, 12 May 2013 20:48:31 +0100</updated>
                    <resolved>Sun, 12 May 2013 20:48:31 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13655621" author="commit-tag-bot" created="Sun, 12 May 2013 20:44:29 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; dweiss&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481634" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481634&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4997" title="Internal test framework&amp;#39;s tests are sensitive to previous test failures and tests.failfast."&gt;&lt;del&gt;LUCENE-4997&lt;/del&gt;&lt;/a&gt;: Internal test framework's tests are sensitive to previous test failures and tests.failfast.&lt;/p&gt;</comment>
                    <comment id="13655623" author="commit-tag-bot" created="Sun, 12 May 2013 20:48:24 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; dweiss&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481636" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481636&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4997" title="Internal test framework&amp;#39;s tests are sensitive to previous test failures and tests.failfast."&gt;&lt;del&gt;LUCENE-4997&lt;/del&gt;&lt;/a&gt;: Internal test framework's tests are sensitive to previous test failures and tests.failfast.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 12 May 2013 19:44:29 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327588</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327570</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4996] DocInverterPerField to log which field throws exceptions</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4996</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;One of ours fields seems to have a problem that didn't result in an exception before. It seems one of my filters doesn't deal with posIncAttr properly but Lucene did not log which of my numerous fields was the problem.&lt;/p&gt;

&lt;p&gt;This patch includes the field name in the exception.&lt;/p&gt;</description>
                <environment/>
            <key id="12647010">LUCENE-4996</key>
            <summary>DocInverterPerField to log which field throws exceptions</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="markus17">Markus Jelsma</reporter>
                        <labels>
                    </labels>
                <created>Fri, 10 May 2013 14:44:42 +0100</created>
                <updated>Fri, 10 May 2013 17:32:27 +0100</updated>
                    <resolved>Fri, 10 May 2013 17:32:27 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13654557" author="rcmuir" created="Fri, 10 May 2013 16:50:57 +0100">&lt;p&gt;+1. Thanks Markus. I'll take this one.&lt;/p&gt;</comment>
                    <comment id="13654579" author="commit-tag-bot" created="Fri, 10 May 2013 17:29:21 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; rmuir&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481080" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481080&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4996" title="DocInverterPerField to log which field throws exceptions"&gt;&lt;del&gt;LUCENE-4996&lt;/del&gt;&lt;/a&gt;: Include field name in all DocInverter exceptions&lt;/p&gt;</comment>
                    <comment id="13654583" author="commit-tag-bot" created="Fri, 10 May 2013 17:32:20 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; rmuir&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481081" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481081&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4996" title="DocInverterPerField to log which field throws exceptions"&gt;&lt;del&gt;LUCENE-4996&lt;/del&gt;&lt;/a&gt;: Include field name in all DocInverter exceptions&lt;/p&gt;</comment>
                    <comment id="13654584" author="rcmuir" created="Fri, 10 May 2013 17:32:27 +0100">&lt;p&gt;Thanks again!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12582621" name="LUCENE-4996-trunk.patch" size="2111" author="markus17" created="Fri, 10 May 2013 14:45:39 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 10 May 2013 15:50:57 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327379</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327361</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4992] ArrayOutOfBoundsException in BooleanScorer2</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4992</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Seeing following exception in BooleanScorer2 in our production system:&lt;br/&gt;
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 2147483647&lt;br/&gt;
	at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:312)&lt;br/&gt;
	at org.apache.lucene.queries.CustomScoreQuery$CustomScorer.score(CustomScoreQuery.java:324)&lt;br/&gt;
	at org.apache.lucene.search.DisjunctionMaxScorer.score(DisjunctionMaxScorer.java:84)&lt;br/&gt;
	at org.apache.lucene.search.TopScoreDocCollector$InOrderTopScoreDocCollector.collect(TopScoreDocCollector.java:47)&lt;br/&gt;
	at org.apache.lucene.search.Scorer.score(Scorer.java:64)&lt;br/&gt;
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:605)&lt;br/&gt;
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:482)&lt;br/&gt;
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:438)&lt;br/&gt;
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)&lt;br/&gt;
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)&lt;/p&gt;
</description>
                <environment/>
            <key id="12646841">LUCENE-4992</key>
            <summary>ArrayOutOfBoundsException in BooleanScorer2</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="john.wang@gmail.com">John Wang</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 May 2013 20:22:23 +0100</created>
                <updated>Wed, 22 May 2013 04:38:38 +0100</updated>
                    <resolved>Wed, 22 May 2013 04:38:38 +0100</resolved>
                            <version>4.1</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13653074" author="john.wang@gmail.com" created="Thu, 9 May 2013 20:23:06 +0100">&lt;p&gt;in DisjunctionSumScorer, line 96:&lt;/p&gt;

&lt;p&gt;private void afterNext() throws IOException &lt;/p&gt;
{
final Scorer sub = subScorers[0];
doc = sub.docID();
if (doc == NO_MORE_DOCS)
{ nrMatchers = Integer.MAX_VALUE; // stop looping &amp;lt;--- this is set }

&lt;p&gt;else&lt;/p&gt;
{ score = sub.score(); nrMatchers = 1; countMatches(1); countMatches(2); }

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;and in BooleanScorer2, line 167:&lt;br/&gt;
coordinator.nrMatchers += super.nrMatchers;&lt;/p&gt;

&lt;p&gt;and then it breaks on line 312.&lt;/p&gt;

&lt;p&gt;Attached is a patch. &lt;/p&gt;</comment>
                    <comment id="13653079" author="rcmuir" created="Thu, 9 May 2013 20:27:16 +0100">&lt;p&gt;Do you have a test John? its not clear that the maybe the problem is calling score() on a scorer that already returned NO_MORE_DOCS (which is undefined)&lt;/p&gt;</comment>
                    <comment id="13653083" author="john.wang@gmail.com" created="Thu, 9 May 2013 20:34:01 +0100">&lt;p&gt;Hi Rob:&lt;/p&gt;

&lt;p&gt;   I do have a test that reproduces this and with the patch it is fixed. (I also made sure all tests pass in lucene)&lt;/p&gt;

&lt;p&gt;   Unfortunately this is very difficult to reproduce. I was only able to reproduce it with our in house query builder and data corpus.&lt;/p&gt;

&lt;p&gt;-John&lt;/p&gt;</comment>
                    <comment id="13653084" author="john.wang@gmail.com" created="Thu, 9 May 2013 20:34:40 +0100">&lt;p&gt;re: its not clear that the maybe the problem is calling score() on a scorer that already returned NO_MORE_DOCS (which is undefined)&lt;/p&gt;

&lt;p&gt;I stepped thru my test program and that is exactly what is happening.&lt;/p&gt;</comment>
                    <comment id="13653096" author="john.wang@gmail.com" created="Thu, 9 May 2013 20:45:49 +0100">&lt;p&gt;Rob, I dug into the code a little more, seems like the better fix would be in CustomScoreQuery, line 309:&lt;/p&gt;

&lt;p&gt;valSrcScorer.advance(doc);&lt;/p&gt;

&lt;p&gt;what I am finding is that valSrcScorer impl when advance is called, can set the inner docid to NO_MORE_DOCS but that is not retrieved, and instead the doc returned previously from subQueryScorer is used.&lt;/p&gt;

&lt;p&gt;By changing the line to:&lt;br/&gt;
doc = valSrcScorer.advance(doc);&lt;/p&gt;

&lt;p&gt;also fixes the problem. I am not sure if that is the right thing to do though.&lt;/p&gt;

&lt;p&gt;-John&lt;/p&gt;</comment>
                    <comment id="13653099" author="john.wang@gmail.com" created="Thu, 9 May 2013 20:47:18 +0100">&lt;p&gt;Forgot to mention, in my test, the valSrcScorer is an instance of BooleanScorer2.&lt;/p&gt;

&lt;p&gt;-John&lt;/p&gt;</comment>
                    <comment id="13653107" author="rcmuir" created="Thu, 9 May 2013 20:51:27 +0100">&lt;p&gt;I'll review CustomScoreQuery and see if its doing the right thing: its definitely possible its doing the wrong thing. Thanks for reporting this.&lt;/p&gt;</comment>
                    <comment id="13654749" author="rcmuir" created="Fri, 10 May 2013 20:13:51 +0100">&lt;p&gt;John I see: so I think the whole design of this thing doesnt work today for your use case (where valSrcScorer is BooleanScorer2).&lt;/p&gt;

&lt;p&gt;It seems implicit in this CustomScoreQuery's implementation that the value source scorers will match all documents, yet nothing about the API enforces this, instead it takes arbitrary Query (but won't actually work correctly today with arbitrary queries!):&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;  &lt;span class="code-keyword"&gt;public&lt;/span&gt; CustomScoreQuery(Query subQuery, Query... scoringQueries) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I feel like one of three things should happen:&lt;br/&gt;
1. Change behavior of CustomScoreQuery to act more conjunction-like as you suggest. Though this means this query would be doing a significantly different thing than its current javadocs describe.&lt;br/&gt;
2. Keep the behavior of today, except add NO_MORE_DOCS checks. But this can be confusing too, e.g. what would vScores&lt;span class="error"&gt;&amp;#91;i&amp;#93;&lt;/span&gt; contain for an exhausted valSrcScorer? &lt;br/&gt;
2. Current behavior, but scoringQueries changed to &lt;b&gt;FunctionQuery&lt;/b&gt; to be type-safe.&lt;/p&gt;

&lt;p&gt;Any ideas?&lt;/p&gt;</comment>
                    <comment id="13654822" author="john.wang@gmail.com" created="Fri, 10 May 2013 21:53:19 +0100">&lt;p&gt;Thanks Rob for following up!&lt;br/&gt;
I think the right thing to do would be the third option, that is the least intrusive while keeping the functionality consistent. And you are absolutely right, by doing the hacks I am suggesting would provide unexpected behavior, which would be worse.&lt;br/&gt;
I will follow up on my end to make sure we would be ok with this change.&lt;/p&gt;

&lt;p&gt;-John&lt;/p&gt;</comment>
                    <comment id="13654837" author="rcmuir" created="Fri, 10 May 2013 22:07:54 +0100">&lt;p&gt;Another nice thing about the last approach (actually, you can do it today to fix your bug), is that you can always pass an arbitrary query anyway:&lt;/p&gt;

&lt;p&gt;new FunctionQuery(new QueryValueSource(MyBooleanQueryOrWhatever, 5.0f))&lt;/p&gt;

&lt;p&gt;The QueryValueSource has that additional argument that lets you specify what the score (in this example: 5.0f) should be for documents that don't match, so it removes any ambiguity.&lt;/p&gt;</comment>
                    <comment id="13654857" author="john.wang@gmail.com" created="Fri, 10 May 2013 22:21:11 +0100">&lt;p&gt;Oh! that's nice!&lt;br/&gt;
We will just do that!&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                    <comment id="13654940" author="john.wang@gmail.com" created="Fri, 10 May 2013 23:31:45 +0100">&lt;p&gt;Hey Rob:&lt;/p&gt;

&lt;p&gt;    Just verified your suggestion works in our env.&lt;/p&gt;

&lt;p&gt;    Feel free to resolve this.&lt;/p&gt;

&lt;p&gt;-john&lt;/p&gt;</comment>
                    <comment id="13655320" author="rcmuir" created="Sat, 11 May 2013 18:29:44 +0100">&lt;p&gt;Here's a patch to change the signature to FunctionQuery to prevent the trap.&lt;/p&gt;</comment>
                    <comment id="13655463" author="john.wang@gmail.com" created="Sun, 12 May 2013 05:25:59 +0100">&lt;p&gt;Thanks Rob for the patch!&lt;br/&gt;
This makes the api much safer.&lt;/p&gt;

&lt;p&gt;I am inclined to think option 2) above with the NO_MORE_DOCS check is a still good thing to do for BooleanScorer2 (unrelated to CustomScoreQuery).&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;

&lt;p&gt;=John&lt;/p&gt;</comment>
                    <comment id="13655473" author="rcmuir" created="Sun, 12 May 2013 06:47:48 +0100">&lt;p&gt;I dont think we should add this check (because only broken consumers would trigger it).&lt;/p&gt;

&lt;p&gt;Maybe an assert is ok though.&lt;/p&gt;</comment>
                    <comment id="13655478" author="john.wang@gmail.com" created="Sun, 12 May 2013 07:25:36 +0100">&lt;p&gt;makes senses, agreed.&lt;/p&gt;</comment>
                    <comment id="13663726" author="commit-tag-bot" created="Wed, 22 May 2013 04:35:22 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; rmuir&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1485040" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1485040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4992" title="ArrayOutOfBoundsException in BooleanScorer2"&gt;&lt;del&gt;LUCENE-4992&lt;/del&gt;&lt;/a&gt;: CustomScoreQuery does not work with arbitrary queries: scoringQueries must match every document&lt;/p&gt;</comment>
                    <comment id="13663727" author="commit-tag-bot" created="Wed, 22 May 2013 04:37:59 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; rmuir&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1485041" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1485041&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4992" title="ArrayOutOfBoundsException in BooleanScorer2"&gt;&lt;del&gt;LUCENE-4992&lt;/del&gt;&lt;/a&gt;: CustomScoreQuery does not work with arbitrary queries: scoringQueries must match every document&lt;/p&gt;</comment>
                    <comment id="13663728" author="rcmuir" created="Wed, 22 May 2013 04:38:38 +0100">&lt;p&gt;I committed this: thanks for reporting this John!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12582797" name="LUCENE-4992.patch" size="1604" author="rcmuir" created="Sat, 11 May 2013 18:29:44 +0100"/>
                    <attachment id="12582496" name="patch.diff" size="681" author="john.wang@gmail.com" created="Thu, 9 May 2013 20:23:31 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 9 May 2013 19:27:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327210</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327192</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4990] Improve MockDirectoryWrapper.sync</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4990</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently MockDirWrapper always calls delegate.sync() with a comment that we can relax this to not wear out the hardware for tests. The issue, as discussed on this thread &lt;a href="http://lucene.markmail.org/thread/eozdsbdahzhjvizj" class="external-link"&gt;http://lucene.markmail.org/thread/eozdsbdahzhjvizj&lt;/a&gt;, is related to NRTCachingDirectory and RateLimiter. The improvements I'd like to make under this issue are:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Call delgeate.sync() if:
	&lt;ul&gt;
		&lt;li&gt;rarely()&lt;/li&gt;
		&lt;li&gt;delegate is NRTCachingDir&lt;/li&gt;
		&lt;li&gt;delegate is RateLimitedDirWrapper and its delegate is NRTCachingDir&lt;/li&gt;
		&lt;li&gt;delegate is TrackingDirWrapper and its delegate is NRTCachingDir&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Also, today the method either fails to sync all files or succeeds. Rather, we can improve this to randomly throw IOE on each file.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Any other Directories that can cause issues when sync() isn't called?&lt;/p&gt;</description>
                <environment/>
            <key id="12646749">LUCENE-4990</key>
            <summary>Improve MockDirectoryWrapper.sync</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 May 2013 05:46:47 +0100</created>
                <updated>Thu, 9 May 2013 21:34:49 +0100</updated>
                    <resolved>Thu, 9 May 2013 21:34:49 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/test-framework</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13652743" author="shaie" created="Thu, 9 May 2013 06:03:57 +0100">&lt;p&gt;Patch with the mentioned fixes. Note that now I don't remove all names from unsynced files, only the ones that successfully synced.&lt;/p&gt;</comment>
                    <comment id="13652851" author="mikemccand" created="Thu, 9 May 2013 11:25:32 +0100">&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;But, I rather liked the comment "corrumpt..." &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Also, can you factor out that large instanceof if into a separate method, eg named "mustSync" or something, with a comment explaining that certain dir impls require sync'ing?  If we add future wrapping dir impls we need to update this method ...&lt;/p&gt;</comment>
                    <comment id="13652931" author="shaie" created="Thu, 9 May 2013 14:05:37 +0100">&lt;p&gt;Patch factors out a mustSync() method.&lt;/p&gt;</comment>
                    <comment id="13652965" author="mikemccand" created="Thu, 9 May 2013 14:59:34 +0100">&lt;p&gt;+1, thanks Shai!&lt;/p&gt;</comment>
                    <comment id="13653051" author="rcmuir" created="Thu, 9 May 2013 17:31:15 +0100">&lt;p&gt;Thanks for tackling this!&lt;/p&gt;

&lt;p&gt;as far as mustSync(), maybe it should be a loop?&lt;br/&gt;
something like:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;while&lt;/span&gt; (delegate &lt;span class="code-keyword"&gt;instanceof&lt;/span&gt; RateLimited || delegate &lt;span class="code-keyword"&gt;instanceof&lt;/span&gt; Tracking) {
  delegate = xxx.getDelegate();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It just isnt obvious from our test code if we create complex situations like Tracking(RateLimited(NRT))), and i know that when tracking was first added, it didnt immediately cause tons of tests to fail, unfortunately.&lt;/p&gt;</comment>
                    <comment id="13653080" author="shaie" created="Thu, 9 May 2013 20:29:56 +0100">&lt;p&gt;Good idea Rob! Patch checks delegate in a loop. The loop is rather weird though, would have been easier if both dirs implemented a DelegateDirectory (or FilterDirectory), but that's what we have for now.&lt;/p&gt;</comment>
                    <comment id="13653119" author="rcmuir" created="Thu, 9 May 2013 21:07:43 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13653145" author="commit-tag-bot" created="Thu, 9 May 2013 21:26:54 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; shaie&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1480761" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1480761&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4990" title="Improve MockDirectoryWrapper.sync"&gt;&lt;del&gt;LUCENE-4990&lt;/del&gt;&lt;/a&gt;: Improve MockDirectoryWrapper.sync&lt;/p&gt;</comment>
                    <comment id="13653158" author="commit-tag-bot" created="Thu, 9 May 2013 21:34:40 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; shaie&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1480764" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1480764&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4990" title="Improve MockDirectoryWrapper.sync"&gt;&lt;del&gt;LUCENE-4990&lt;/del&gt;&lt;/a&gt;: Improve MockDirectoryWrapper.sync&lt;/p&gt;</comment>
                    <comment id="13653159" author="shaie" created="Thu, 9 May 2013 21:34:49 +0100">&lt;p&gt;Thanks for the review guys. Committed to trunk and 4x.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12582499" name="LUCENE-4990.patch" size="4362" author="shaie" created="Thu, 9 May 2013 20:29:56 +0100"/>
                    <attachment id="12582464" name="LUCENE-4990.patch" size="4293" author="shaie" created="Thu, 9 May 2013 14:05:37 +0100"/>
                    <attachment id="12582424" name="LUCENE-4990.patch" size="3715" author="shaie" created="Thu, 9 May 2013 06:03:57 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 9 May 2013 10:25:32 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327119</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>327101</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4987] Test framework may fail internally under J9 (some serious JVM exclusive-section issue).</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4987</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This was reported by Shai. The runner failed with an exception:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
[junit4:junit4] Caused by: java.util.NoSuchElementException
[junit4:junit4]         at java.util.ArrayDeque.removeFirst(ArrayDeque.java:289)
[junit4:junit4]         at java.util.ArrayDeque.pop(ArrayDeque.java:518)
[junit4:junit4]         at com.carrotsearch.ant.tasks.junit4.JUnit4$1.onSlaveIdle(JUnit4.java:809)
[junit4:junit4]         ... 17 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is that this is impossible because the code around JUnit4.java:809 looks like this:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
     &lt;span class="code-keyword"&gt;final&lt;/span&gt; Deque&amp;lt;&lt;span class="code-object"&gt;String&lt;/span&gt;&amp;gt; stealingQueue = &lt;span class="code-keyword"&gt;new&lt;/span&gt; ArrayDeque&amp;lt;&lt;span class="code-object"&gt;String&lt;/span&gt;&amp;gt;(...);
     aggregatedBus.register(&lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;Object&lt;/span&gt;() {
        @Subscribe
        &lt;span class="code-keyword"&gt;public&lt;/span&gt; void onSlaveIdle(SlaveIdle slave) {
          &lt;span class="code-keyword"&gt;if&lt;/span&gt; (stealingQueue.isEmpty()) {
            ...
          } &lt;span class="code-keyword"&gt;else&lt;/span&gt; {
            &lt;span class="code-object"&gt;String&lt;/span&gt; suiteName = stealingQueue.pop();
            ...
          }
        }
      });
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the contract on Guava's EventBus states that:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
 * &amp;lt;p&amp;gt;The EventBus guarantees that it will not call a handler method from
 * multiple threads simultaneously, unless the method explicitly allows it by
 * bearing the {@link AllowConcurrentEvents} annotation.  If &lt;span class="code-keyword"&gt;this&lt;/span&gt; annotation is
 * not present, handler methods need not worry about being reentrant, unless
 * also called from outside the EventBus
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I wrote a simple snippet of code that does it in a loop and indeed, two threads can appear in the critical section at once. This is not reproducible on Hotspot and only appears to be the problem on J9/1.7/Windows (J9 1.6 works fine).&lt;/p&gt;

&lt;p&gt;I'll provide a workaround in the runner (an explicit monitor seems to be working) but this is some serious J9 issue.&lt;/p&gt;
</description>
                <environment/>
            <key id="12646609">LUCENE-4987</key>
            <summary>Test framework may fail internally under J9 (some serious JVM exclusive-section issue).</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="dweiss">Dawid Weiss</assignee>
                                <reporter username="dweiss">Dawid Weiss</reporter>
                        <labels>
                    </labels>
                <created>Wed, 8 May 2013 14:14:34 +0100</created>
                <updated>Thu, 9 May 2013 11:25:57 +0100</updated>
                    <resolved>Wed, 8 May 2013 21:52:28 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13651872" author="dweiss" created="Wed, 8 May 2013 14:16:49 +0100">&lt;p&gt;Compile and run under J9 (and hotspot):&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
javac -cp guava-14.0.1.jar;junit-4.10.jar;randomizedtesting-runner-2.0.9.jar J9SanityCheck.java
java -cp guava-14.0.1.jar;junit-4.10.jar;randomizedtesting-runner-2.0.9.jar;. J9SanityCheck
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13651882" author="dweiss" created="Wed, 8 May 2013 14:29:35 +0100">&lt;p&gt;I think I found the reason. J9 seems to be optimizing away the following code:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;  @Override &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-keyword"&gt;synchronized&lt;/span&gt; void handleEvent(&lt;span class="code-object"&gt;Object&lt;/span&gt; event)
      &lt;span class="code-keyword"&gt;throws&lt;/span&gt; InvocationTargetException {
    &lt;span class="code-keyword"&gt;super&lt;/span&gt;.handleEvent(event);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;this method is removed entirely as the stack trace from the 'wtf' exception shows:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
	at java.lang.reflect.Method.invoke(Method.java:613)
	at com.google.common.eventbus.EventHandler.handleEvent(EventHandler.java:74)
	at com.google.common.eventbus.EventBus.dispatch(EventBus.java:314)
	at com.google.common.eventbus.EventBus.dispatchQueuedEvents(EventBus.java:296)
	at com.google.common.eventbus.EventBus.post(EventBus.java:267)
	at com.carrotsearch.ant.tasks.junit4.it.TestJ9SanityCheck$2.call(TestJ9SanityCheck.java:75)
	at com.carrotsearch.ant.tasks.junit4.it.TestJ9SanityCheck$2.call(TestJ9SanityCheck.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:345)
	at java.util.concurrent.FutureTask.run(FutureTask.java:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1121)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)
	at java.lang.&lt;span class="code-object"&gt;Thread&lt;/span&gt;.run(&lt;span class="code-object"&gt;Thread&lt;/span&gt;.java:777)
Caused by: java.lang.RuntimeException: Wtf? two threads in a handler: &lt;span class="code-object"&gt;Thread&lt;/span&gt;[pool-100-thread-1,5,TGRP-TestJ9SanityCheck] and &lt;span class="code-object"&gt;Thread&lt;/span&gt;[pool-100-thread-2,5,TGRP-TestJ9SanityCheck]
	at com.carrotsearch.ant.tasks.junit4.it.TestJ9SanityCheck$1.onSlaveIdle(TestJ9SanityCheck.java:52)
	... 14 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The eventhandler type is definitely SynchronizedEventHandler and on Hotspot you do get a synchronized indirection layer (a call under a synchronized monitor), whereas on J9 this becomes a full race condition.&lt;/p&gt;
</comment>
                    <comment id="13651888" author="shaie" created="Wed, 8 May 2013 14:39:18 +0100">&lt;p&gt;Thanks Dawid. I filed a PMR with the J9 team. Will post back when they respond/resolve the issue.&lt;/p&gt;</comment>
                    <comment id="13652268" author="dweiss" created="Wed, 8 May 2013 20:56:15 +0100">&lt;p&gt;Yeah, I'm confident it's this. I've replaced the synchronized method flag with an explicit monitor and it works then.&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;  @Override &lt;span class="code-keyword"&gt;public&lt;/span&gt; void handleEvent(&lt;span class="code-object"&gt;Object&lt;/span&gt; event)
      &lt;span class="code-keyword"&gt;throws&lt;/span&gt; InvocationTargetException {
      &lt;span class="code-keyword"&gt;synchronized&lt;/span&gt; (&lt;span class="code-keyword"&gt;this&lt;/span&gt;) {
          &lt;span class="code-keyword"&gt;super&lt;/span&gt;.handleEvent(event);
      }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So it's a JVM issue.&lt;/p&gt;</comment>
                    <comment id="13652346" author="dweiss" created="Wed, 8 May 2013 21:51:45 +0100">&lt;p&gt;I've released and pushed rr 2.0.10 to maven repositories with a workaround for J9's bug.&lt;/p&gt;</comment>
                    <comment id="13652852" author="thetaphi" created="Thu, 9 May 2013 11:25:57 +0100">&lt;p&gt;I added this bug to the JVM bugs wiki page: &lt;a href="http://wiki.apache.org/lucene-java/JavaBugs" class="external-link"&gt;http://wiki.apache.org/lucene-java/JavaBugs&lt;/a&gt;&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12582291" name="j9.zip" size="2324535" author="dweiss" created="Wed, 8 May 2013 14:16:49 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 8 May 2013 13:39:18 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326979</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326961</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4985] Make it easier to mix different kinds of FacetRequests</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4985</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4980" title="Can&amp;#39;t use DrillSideways with both RangeFacetRequest and non-RangeFacetRequest"&gt;&lt;del&gt;LUCENE-4980&lt;/del&gt;&lt;/a&gt;, where we added a strange class called RangeFacetsAccumulatorWrapper, which takes an incoming FSP, splits out the FacetRequests into range and non-range, delegates to two accumulators for each set, and then zips the results back together in order.&lt;/p&gt;

&lt;p&gt;Somehow we should generalize this class and make it work with SortedSetDocValuesAccumulator as well.&lt;/p&gt;</description>
                <environment/>
            <key id="12646480">LUCENE-4985</key>
            <summary>Make it easier to mix different kinds of FacetRequests</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 7 May 2013 20:06:12 +0100</created>
                <updated>Tue, 7 May 2013 20:06:12 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326850</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326832</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4982] Make MockIndexOutputWrapper check disk full on copyBytes</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4982</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;While working on the consistency test for Replicator (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4975" title="Add Replication module to Lucene"&gt;&lt;del&gt;LUCENE-4975&lt;/del&gt;&lt;/a&gt;), I noticed that I don't trip disk-full exceptions and tracked it down to MockIndexOutputWrapper.copyBytes not doing these checks like writeBytes. I'd like to add this check.&lt;/p&gt;</description>
                <environment/>
            <key id="12646176">LUCENE-4982</key>
            <summary>Make MockIndexOutputWrapper check disk full on copyBytes</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Mon, 6 May 2013 09:25:34 +0100</created>
                <updated>Tue, 7 May 2013 06:54:30 +0100</updated>
                    <resolved>Tue, 7 May 2013 06:54:30 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/test-framework</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13649601" author="shaie" created="Mon, 6 May 2013 09:56:41 +0100">&lt;p&gt;Patch adds a test to TestMockDirWrapper and factors out checkDiskFull method in MockIOWrapper. The signature is a bit ugly, but that's needed because checkDiskFull copies the remaining bytes, and writeBytes copies from an array while copyBytes from DataInput. I don't think it's the end of the world, but if anyone has an idea how to do it better...&lt;/p&gt;

&lt;p&gt;I ran core tests and they passed (actually only 3 tests under core set dir.maxSize).&lt;/p&gt;</comment>
                    <comment id="13649654" author="mikemccand" created="Mon, 6 May 2013 12:12:09 +0100">&lt;p&gt;+1, good catch.&lt;/p&gt;

&lt;p&gt;Who tests the tester!&lt;/p&gt;</comment>
                    <comment id="13649702" author="rcmuir" created="Mon, 6 May 2013 13:45:00 +0100">&lt;p&gt;Its not clear to me if with the patch we will double-count against disk full if copyBytes calls writeBytes behind the scenes...&lt;/p&gt;

&lt;p&gt;Maybe we can make the test have a max size of 2 bytes and copyBytes twice to it just so this is obvious?&lt;/p&gt;</comment>
                    <comment id="13649711" author="shaie" created="Mon, 6 May 2013 13:48:46 +0100">&lt;p&gt;I can modify the test sure. But the problem is that copyBytes doesn't call writeBytes, otherwise I would have tripped it. I.e., we call delegate.copyBytes, which internally may call &lt;b&gt;its&lt;/b&gt; writeBytes, but not MockIO.writeBytes.&lt;/p&gt;</comment>
                    <comment id="13649719" author="shaie" created="Mon, 6 May 2013 13:59:35 +0100">&lt;p&gt;I modified the test to set maxSize=2 and then write 2 bytes in two calls. The first should succeed, the second fail. However, even the first fails and now I don't know if it's a bug in the test or MockIO.checkDiskFull(). The latter (copy of the original code) does &lt;tt&gt;freeSpace &amp;lt;= len&lt;/tt&gt; &amp;#8211; is this ok? I mean, if I have room for 2 bytes and the caller asks to write 2 bytes, should we really fail on diskFull?&lt;/p&gt;</comment>
                    <comment id="13649784" author="shaie" created="Mon, 6 May 2013 15:59:58 +0100">&lt;p&gt;I changed the check to &lt;tt&gt;freeSpace &amp;lt; len&lt;/tt&gt;, but then the test failed to trip disk-full the second time, unless I call out.flush() in between. Debugging tells me that RAMOutputStream sets RAMFile.length only on flush(), therefore even if I attempt to write a 2K byte[] (with maxSize=2), the test doesn't fail.&lt;/p&gt;

&lt;p&gt;Seems like getRecomputedActualSizeInBytes is not very accurate. It only returns the size of the flushed files (even for FSDir). This may be ok, dunno. It just felt wrong for RAMDirectory, since there is no real buffering happening.&lt;/p&gt;

&lt;p&gt;Anyway, I guess we'll have to live with that. Disk-full is anyway a best effort, so in this test, I'll just call flush(). In real tests that want to trip disk-full, usually indexing happens and therefore files get flushed, and the size measure is closer.&lt;/p&gt;</comment>
                    <comment id="13650009" author="shaie" created="Mon, 6 May 2013 20:20:42 +0100">&lt;p&gt;I thought about this some more and I realize that getComputedActualSizeInBytes works as expected. checkDiskFull should only trip if the Directory size has reached the limit, and it cannot tell how many bytes are pending in a buffer. The test would fail not only w/ RAMDirectory, but also a Directory which buffers writes (which I believe all our directories do), and therefore flush() is important for the test.&lt;/p&gt;

&lt;p&gt;So to summarize the changes in this issue:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Added checkDiskFull to MockIOWrapper so it can trip writeBytes and copyBytes.&lt;/li&gt;
	&lt;li&gt;Changed checkDiskFull to do &lt;tt&gt;freeSpace &amp;lt; len&lt;/tt&gt; because &lt;tt&gt;freeSpace == len&lt;/tt&gt; is still valid.&lt;/li&gt;
	&lt;li&gt;Added a test&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I plan to commit this tomorrow.&lt;/p&gt;</comment>
                    <comment id="13650539" author="shaie" created="Tue, 7 May 2013 06:54:30 +0100">&lt;p&gt;Committed to trunk and 4x&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581895" name="LUCENE-4982.patch" size="6476" author="shaie" created="Mon, 6 May 2013 15:59:58 +0100"/>
                    <attachment id="12581881" name="LUCENE-4982.patch" size="6074" author="shaie" created="Mon, 6 May 2013 13:59:35 +0100"/>
                    <attachment id="12581860" name="LUCENE-4982.patch" size="5951" author="shaie" created="Mon, 6 May 2013 09:56:41 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 6 May 2013 11:12:09 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326546</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326528</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4980] Can't use DrillSideways with both RangeFacetRequest and non-RangeFacetRequest</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4980</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I tried to combine these two and there were several issues:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;It's ... really tricky to manage the two different&lt;br/&gt;
    FacetAccumulators across that N FacetCollectors that DrillSideways&lt;br/&gt;
    creates ... to fix this I added a new MultiFacetsAccumulator that&lt;br/&gt;
    switches for you.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;There was still one place in DS/DDQ that wasn't properly handling&lt;br/&gt;
    a non-Term drill-down.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;There was a bug in the "collector method" for DrillSideways&lt;br/&gt;
    whereby if a given segment had no hits, it was skipped, which is&lt;br/&gt;
    incorrect because it must still be visited to tally up the&lt;br/&gt;
    sideways counts.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Separately I noticed that DrillSideways was doing too much work:&lt;br/&gt;
    it would count up drill-down counts &lt;b&gt;and&lt;/b&gt; drill-sideways counts&lt;br/&gt;
    against the same dim (but then discard the drill-down counts in&lt;br/&gt;
    the end).&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12646138">LUCENE-4980</key>
            <summary>Can't use DrillSideways with both RangeFacetRequest and non-RangeFacetRequest</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 5 May 2013 23:11:05 +0100</created>
                <updated>Tue, 7 May 2013 20:09:47 +0100</updated>
                    <resolved>Tue, 7 May 2013 20:09:47 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13649450" author="mikemccand" created="Sun, 5 May 2013 23:11:49 +0100">&lt;p&gt;Patch w/ test case + fixes.&lt;/p&gt;</comment>
                    <comment id="13649638" author="shaie" created="Mon, 6 May 2013 11:20:23 +0100">&lt;p&gt;I was confused by the name MultiFacetsAccumulator as I thought it takes something like a Map&amp;lt;FacetRequest,FacetsAccumulator&amp;gt;, but I see that it only distinguishes RangeAccumulator from others. So I'm worried about someone gets confused about the name and use it incorrectly. I don't have a better name in mind though ... RangeAndRegularFacetsAccumulator?&lt;/p&gt;

&lt;p&gt;What if RangeAccumulator did that under the covers? I.e. instead of rejecting non-RangeFacetRequest, it created FA over all such requests? Multi is quite simple though, so I like it .. maybe FacetAccumulatorRangeWrapper? I think as long as we keep the word Range in the name, it's less likely users will get confused.&lt;/p&gt;

&lt;p&gt;Minor comments about the class: (a) can you rename 'a' and 'ra'? (b) why do you need to hold onto fspOrig? Is it because FA.searchParams isn't final?&lt;/p&gt;</comment>
                    <comment id="13649661" author="mikemccand" created="Mon, 6 May 2013 12:22:21 +0100">&lt;blockquote&gt;&lt;p&gt;What if RangeAccumulator did that under the covers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well ... I have a TODO to also support SortedSetDocValuesAccumulator.  So I'm not quite sure what to name it / where to put it.&lt;/p&gt;

&lt;p&gt;Another option here is to commit this class only under src/test ... it's technically only needed right now by the test case to expose the bugs ... but then I'm using the class in the Jira search app, because I need to use DrillSideways with range and non-range facets, and without it things get very messy.  So we need to fix something here, but we can do it in a separate issue after fixing these bugs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Minor comments about the class: (a) can you rename 'a' and 'ra'? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Will do ...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(b) why do you need to hold onto fspOrig? Is it because FA.searchParams isn't final?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I need fspOrig in accumulator() to un-collate the wrapped List&amp;lt;FacetResult&amp;gt; back in the same order as the original requests ...&lt;/p&gt;</comment>
                    <comment id="13650968" author="mikemccand" created="Tue, 7 May 2013 16:33:24 +0100">&lt;p&gt;New patch, folding in Shai's feedback.&lt;/p&gt;

&lt;p&gt;I just renamed MFA -&amp;gt; RangeFacetsAccumulatorWrapper for now ... in a followon issue we can generalize this better (eg to include SortedSetDV facets too).&lt;/p&gt;</comment>
                    <comment id="13651063" author="shaie" created="Tue, 7 May 2013 18:04:43 +0100">&lt;p&gt;Thanks Mike. I think the TODO is a bit trickier than what it writes ... you cannot decide to create SortedSetDVAccumulator based on a FacetRequest. App needs to create it only if it indexed facets with SSDVFacetFields, right? This might be possible if we had an index schema, of somehow expressed that information in FacetIndexingParams.&lt;/p&gt;

&lt;p&gt;So it's up to you to remove the TODO, but I don't think you can actually handle it, currently, without app specifically telling you that it's ok.&lt;/p&gt;</comment>
                    <comment id="13651179" author="mikemccand" created="Tue, 7 May 2013 19:54:28 +0100">&lt;blockquote&gt;&lt;p&gt;I think the TODO is a bit trickier than what it writes ... you cannot decide to create SortedSetDVAccumulator based on a FacetRequest.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm true.  Maybe we need a separate SSDVFacetRequest?  Not sure ...&lt;/p&gt;

&lt;p&gt;I don't think I'll remove the TODO: I think it's still important that (somehow) we are able to send an FSP requiring different FacetsAccumulators and something figures out which ones to instantiate, and then collates the FacetResults back in order, somehow ... I will fixup the TODO to reflect its difficulty though &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12582109" name="LUCENE-4980.patch" size="23237" author="mikemccand" created="Tue, 7 May 2013 16:33:24 +0100"/>
                    <attachment id="12581836" name="LUCENE-4980.patch" size="23052" author="mikemccand" created="Sun, 5 May 2013 23:11:49 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 6 May 2013 10:20:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326508</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326490</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4979] LiveFieldValues should accept any ReferenceManager</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4979</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today it requires ReferenceManager&amp;lt;IndexSearcher&amp;gt; but it doesn't rely on that at all (it just forwards that IndexSearcher to the subclass's lookup method).&lt;/p&gt;</description>
                <environment/>
            <key id="12646135">LUCENE-4979</key>
            <summary>LiveFieldValues should accept any ReferenceManager</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 5 May 2013 22:20:20 +0100</created>
                <updated>Tue, 7 May 2013 17:24:18 +0100</updated>
                    <resolved>Tue, 7 May 2013 17:24:18 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13649433" author="mikemccand" created="Sun, 5 May 2013 22:21:43 +0100">&lt;p&gt;Simple patch ...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581832" name="LUCENE-4979.patch" size="2633" author="mikemccand" created="Sun, 5 May 2013 22:21:43 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326505</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326487</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4976] PersistentSnapshotDeletionPolicy should save to a single file</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4976</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today it creates a single-document Lucene index, and calls commit() after each snapshot/release.&lt;/p&gt;

&lt;p&gt;I think we can just use a single file instead, and remove Closeable.&lt;/p&gt;</description>
                <environment/>
            <key id="12645840">LUCENE-4976</key>
            <summary>PersistentSnapshotDeletionPolicy should save to a single file</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Thu, 2 May 2013 18:41:55 +0100</created>
                <updated>Sun, 5 May 2013 22:08:21 +0100</updated>
                    <resolved>Fri, 3 May 2013 12:20:13 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13647747" author="mikemccand" created="Thu, 2 May 2013 18:46:05 +0100">&lt;p&gt;patch.&lt;/p&gt;</comment>
                    <comment id="13647878" author="shaie" created="Thu, 2 May 2013 21:42:34 +0100">&lt;p&gt;looks good!&lt;/p&gt;</comment>
                    <comment id="13648024" author="mikemccand" created="Fri, 3 May 2013 00:24:47 +0100">&lt;p&gt;New patch, added a test case, and fixed PSDP to detect if you try to snapshot/release when it's not being used by an IW ... I think it's ready.&lt;/p&gt;</comment>
                    <comment id="13648150" author="shaie" created="Fri, 3 May 2013 04:31:05 +0100">&lt;p&gt;+1!&lt;/p&gt;</comment>
                    <comment id="13648328" author="commit-tag-bot" created="Fri, 3 May 2013 12:13:22 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478726" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478726&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4976" title="PersistentSnapshotDeletionPolicy should save to a single file"&gt;&lt;del&gt;LUCENE-4976&lt;/del&gt;&lt;/a&gt;: use single file to hold PersistentSnapshotDeletionPolicy state on disk&lt;/p&gt;</comment>
                    <comment id="13648332" author="commit-tag-bot" created="Fri, 3 May 2013 12:20:03 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478730" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478730&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4976" title="PersistentSnapshotDeletionPolicy should save to a single file"&gt;&lt;del&gt;LUCENE-4976&lt;/del&gt;&lt;/a&gt;: use single file to hold PersistentSnapshotDeletionPolicy state on disk&lt;/p&gt;</comment>
                    <comment id="13648530" author="commit-tag-bot" created="Fri, 3 May 2013 17:08:21 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478854" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478854&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4976" title="PersistentSnapshotDeletionPolicy should save to a single file"&gt;&lt;del&gt;LUCENE-4976&lt;/del&gt;&lt;/a&gt;: fix Solr IndexDeletionPolicy impls to handle empty commits onInit&lt;/p&gt;</comment>
                    <comment id="13648535" author="commit-tag-bot" created="Fri, 3 May 2013 17:12:24 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478855" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478855&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4976" title="PersistentSnapshotDeletionPolicy should save to a single file"&gt;&lt;del&gt;LUCENE-4976&lt;/del&gt;&lt;/a&gt;: fix Solr IndexDeletionPolicy impls to handle empty commits onInit&lt;/p&gt;</comment>
                    <comment id="13649428" author="commit-tag-bot" created="Sun, 5 May 2013 22:06:13 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479394" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479394&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4976" title="PersistentSnapshotDeletionPolicy should save to a single file"&gt;&lt;del&gt;LUCENE-4976&lt;/del&gt;&lt;/a&gt;: add missing sync / delete old save files&lt;/p&gt;</comment>
                    <comment id="13649430" author="commit-tag-bot" created="Sun, 5 May 2013 22:08:21 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479395" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479395&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4976" title="PersistentSnapshotDeletionPolicy should save to a single file"&gt;&lt;del&gt;LUCENE-4976&lt;/del&gt;&lt;/a&gt;: add missing sync / delete old save files&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581637" name="LUCENE-4976.patch" size="30471" author="mikemccand" created="Fri, 3 May 2013 00:24:47 +0100"/>
                    <attachment id="12581555" name="LUCENE-4976.patch" size="17850" author="mikemccand" created="Thu, 2 May 2013 18:46:05 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 May 2013 20:42:34 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326211</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326193</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4975] Add Replication module to Lucene</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4975</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I wrote a replication module which I think will be useful to Lucene users who want to replicate their indexes for e.g high-availability, taking hot backups etc.&lt;/p&gt;

&lt;p&gt;I will upload a patch soon where I'll describe in general how it works.&lt;/p&gt;</description>
                <environment/>
            <key id="12645826">LUCENE-4975</key>
            <summary>Add Replication module to Lucene</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Thu, 2 May 2013 16:35:31 +0100</created>
                <updated>Thu, 23 May 2013 14:35:06 +0100</updated>
                    <resolved>Mon, 13 May 2013 13:56:55 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="13647655" author="shaie" created="Thu, 2 May 2013 17:08:05 +0100">&lt;p&gt;So here's an overview how the Replicator works (it's also document under oal.replicator.package.html):&lt;/p&gt;

&lt;p&gt;At a high-level, producers (e.g. indexer) publish Revisions, and consumers update to the latest Revision available. Like SVN, if a client is on rev1 and the server has rev4, the next update request will upgrade the client to rev4, skipping all intermediate revisions.&lt;/p&gt;

&lt;p&gt;The Replicator offers two implementations at the moment: LocalReplicator to be used by at the server side and HttpReplicator to be used by clients to e.g. update over HTTP. In the future, we may want to add other Replicator implementations, e.g. rsync, torrent... for HTTP, the package also provides a ReplicationService which acts on the Http servlet request/response following some API specification. In that sense, the HttpReplicator expects a certain HTTP impl on the server side, so ReplicationService helps you by implementation that API. The reason it's not a servlet is so that you can plug it into your application servlet freely.&lt;/p&gt;

&lt;p&gt;A Revision is basically a list of files and sources. For example, IndexRevision contains the list of files in an IndexCommit (and only one source), while IndexAndTaxonomyRevision contains the list of files from both IndexCommits with corresponding sources (index/taxonomy). When the server publishes either of these two revision, the IndexCommits are snapshotted so that files aren't deleted, and the Replicator serves file requests (by clients) from the Revision. The Revision is also responsible for releasing itself &amp;#8211; this is done automatically by the Replicator which releases a revision when it's no longer needed (i.e. there's a new one already) and there are no clients that currently replicate its files.&lt;/p&gt;

&lt;p&gt;On the client side, the package offers a ReplicationClient class which can be invoked either manually, or start its update-thread to periodically check for updates. The client is given a ReplicationHandler (two matching implementations: IndexReplicationHandler and IndexAndTaxonomyReplicationHandler) which is responsible to act on the replicated files. The client first obtains all needed files (i.e. those that the new Revision offers, and the client is still missing), and after they were all successfully copied over, the handler is invoked. Both handlers copy the files from their temporary location to the index directories, fsync them and kiss the index such that unused files are deleted. You can provide each handler a Callable which is invoked after the index has been safely and successfully updated, so you can e.g. searcherManager.maybeReopen().&lt;/p&gt;

&lt;p&gt;Here's a general code example that explains how to work with the Replicator:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-comment"&gt;// ++++++++++++++ SERVER SIDE ++++++++++++++ // 
&lt;/span&gt;IndexWriter publishWriter; &lt;span class="code-comment"&gt;// the writer used &lt;span class="code-keyword"&gt;for&lt;/span&gt; indexing
&lt;/span&gt;Replicator replicator = &lt;span class="code-keyword"&gt;new&lt;/span&gt; LocalReplicator();
replicator.publish(&lt;span class="code-keyword"&gt;new&lt;/span&gt; IndexRevision(publishWriter));

&lt;span class="code-comment"&gt;// ++++++++++++++ CLIENT SIDE ++++++++++++++ // 
&lt;/span&gt;&lt;span class="code-comment"&gt;// either LocalReplictor, or HttpReplicator &lt;span class="code-keyword"&gt;if&lt;/span&gt; client and server are on different nodes
&lt;/span&gt;Replicator replicator;

&lt;span class="code-comment"&gt;// callback invoked after handler finished handling the revision and e.g. can reopen the reader.
&lt;/span&gt;Callable&amp;amp;lt;&lt;span class="code-object"&gt;Boolean&lt;/span&gt;&amp;amp;gt; callback = &lt;span class="code-keyword"&gt;null&lt;/span&gt;; &lt;span class="code-comment"&gt;// can also be &lt;span class="code-keyword"&gt;null&lt;/span&gt; &lt;span class="code-keyword"&gt;if&lt;/span&gt; no callback is needed
&lt;/span&gt;ReplicationHandler handler = &lt;span class="code-keyword"&gt;new&lt;/span&gt; IndexReplicationHandler(indexDir, callback);
SourceDirectoryFactory factory = &lt;span class="code-keyword"&gt;new&lt;/span&gt; PerSessionDirectoryFactory(workDir);
ReplicationClient client = &lt;span class="code-keyword"&gt;new&lt;/span&gt; ReplicationClient(replicator, handler, factory);

&lt;span class="code-comment"&gt;// invoke client manually
&lt;/span&gt;client.updateNow();
	
&lt;span class="code-comment"&gt;// or, periodically
&lt;/span&gt;client.startUpdateThread(100); &lt;span class="code-comment"&gt;// check &lt;span class="code-keyword"&gt;for&lt;/span&gt; update every 100 milliseconds&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The package of course comes with unit tests, though I'm sure there's room for improvement (there always is!).&lt;/p&gt;</comment>
                    <comment id="13647658" author="shaie" created="Thu, 2 May 2013 17:11:13 +0100">&lt;p&gt;Patch contains the Replicator code + all needed stuff to make it a valid module (ivy, maven, licenses etc.). A big portion of the patch is due to the ivy/maven/licenses things, so if you focus on the code, it's not that big.&lt;/p&gt;

&lt;p&gt;I ran precommit and it fails with this cryptic error:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;-documentation-lint:
     [echo] checking for broken html...
    [jtidy] Checking for broken html (such as invalid tags)...
   [delete] Deleting directory D:\dev\lucene\lucene-replicator\lucene\build\jtidy_tmp
     [echo] Checking for broken links...
     [exec] Traceback (most recent call last):
     [exec]   File "D:\dev\lucene\lucene-replicator\dev-tools/scripts/checkJavadocLinks.py", line 260, in
     [exec] Crawl/parse...
     [exec]  &amp;lt;module&amp;gt;
     [exec]     if checkAll(sys.argv[1]):
     [exec]   File "D:\dev\lucene\lucene-replicator\dev-tools/scripts/checkJavadocLinks.py", line 160, in checkAll
     [exec]     allFiles[fullPath] = parse(fullPath, open('%s/%s' % (root, f), encoding='UTF-8').read())
     [exec]   File "D:\dev\lucene\lucene-replicator\dev-tools/scripts/checkJavadocLinks.py", line 110, in parse
     [exec]     parser.feed(html)
     [exec]   File "/usr/lib/python3.2/html/parser.py", line 142, in feed
     [exec]     self.goahead(0)
     [exec]   File "/usr/lib/python3.2/html/parser.py", line 188, in goahead
     [exec]     k = self.parse_endtag(i)
     [exec]   File "/usr/lib/python3.2/html/parser.py", line 454, in parse_endtag
     [exec]     self.handle_endtag(elem.lower())
     [exec]   File "D:\dev\lucene\lucene-replicator\dev-tools/scripts/checkJavadocLinks.py", line 92, in handle_endtag
     [exec]     raise RuntimeError('%s %s:%s: saw &amp;lt;/%s&amp;gt; but expected &amp;lt;/%s&amp;gt;' % (self.baseURL, self.getpos()[0], self.getpos()[1], tag, self.stack[-1]))
     [exec] RuntimeError: file:///build/docs/core/allclasses-frame.html 680:0: saw &amp;lt;/body&amp;gt; but expected &amp;lt;/div&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Does anyone know what this is? I searched for &amp;lt;div&amp;gt; and I don't have any. Also, if I run 'ant documentation' from top-level, it passes. I ran this w/ Oracle 1.7. I did find a closing &amp;lt;/body&amp;gt; tag with no opening tag under the jetty license, but I don't think it's related?&lt;/p&gt;

&lt;p&gt;Otherwise, maven check passes, and tests compile and run successfully.&lt;/p&gt;</comment>
                    <comment id="13647908" author="shaie" created="Thu, 2 May 2013 22:12:09 +0100">&lt;p&gt;Patch updates IndexRevision and IndexAndTaxonomyRevision following the recent changes to SDP (not requiring snapshot ID). I also noticed I put grouping in pom.template rather than facet - fixed.&lt;/p&gt;

&lt;p&gt;As for this weird error I got, I noticed that the offending files under test-framework had an extra line none of the other modules seemed to have: &amp;lt;div role="navigation" title="All Classes"&amp;gt;. I have no idea how it got there. So I ran 'ant clean documentation' and it passed! I will run precommit tomorrow.&lt;/p&gt;</comment>
                    <comment id="13648155" author="shaie" created="Fri, 3 May 2013 04:43:40 +0100">&lt;p&gt;Patch adds @lucene.experimental to all classes. I also excluded all the licesne files from the patch, so that it's smaller and easier to review.&lt;/p&gt;</comment>
                    <comment id="13649298" author="shaie" created="Sun, 5 May 2013 09:00:42 +0100">&lt;p&gt;Added testConsistencyOnException to test the client and handlers' behavior when they encounter exceptions (I use MockDirWrapp diskFull and randomIOE to simulate that).&lt;/p&gt;

&lt;p&gt;I think this module is basically ready. I.e. it comes with tests, javadocs and pretty much does what it was written to do. I'm sure there's room for improvement, but I don't think this should hold off the commit. So unless there are any objections, I intend to commit in by Tuesday. If people want to do a thorough review, I don't mind waiting with the commit, but just drop a comment on the issue to let me know.&lt;/p&gt;</comment>
                    <comment id="13649457" author="mikemccand" created="Mon, 6 May 2013 00:18:46 +0100">&lt;p&gt;+1 to commit and iterate from here on... this new module looks very nice!&lt;/p&gt;

&lt;p&gt;I like the new testConsistencyOnException ... maybe also call MDW.setRandomIOExceptionRateOnOpen?  This will additionally randomly throw exceptions from openInput/createOutput.&lt;/p&gt;</comment>
                    <comment id="13649564" author="teofili" created="Mon, 6 May 2013 08:37:25 +0100">&lt;p&gt;+1 for committing it&lt;/p&gt;</comment>
                    <comment id="13649613" author="jpountz" created="Mon, 6 May 2013 10:27:06 +0100">&lt;p&gt;+1 to commit too.&lt;/p&gt;

&lt;p&gt;Looking at the code, there seems to be specialized implementations for faceting because of the need to replicate the taxonomy indexes too, so I was wondering that maybe this facet-specific code should be under lucene/facets rather than lucene/replicator so that lucene/replicator doesn't need to depend on all modules that have specific replication needs. (I'm not sure what the best option is yet, this can be addressed afterwards.)&lt;/p&gt;</comment>
                    <comment id="13649631" author="shaie" created="Mon, 6 May 2013 10:46:34 +0100">&lt;p&gt;I've been wondering about that too, but chose to keep the facet replication code under replicator for few reasons:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;A Revision contains files from multiple sources, and the taxonomy index is partly responsible for that. And ReplicationClient respects that &amp;#8211; so I guess it's not entirely true that the Replicator is unaware of taxonomy (even though it would still work if I pulled the taxonomy stuff out of it).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think it makes less sense to require lucene-replicator.jar for every faceted search app which makes use of lucene-facet.jar. The key reason is that replicator requires few additional jars such as httpclient, httpcore, jetty, servlet-api. Requiring lucene-facet.jar seems less painful to me, than requiring every faceted search app out there to include all these jars even if it doesn't want to do replication.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I like to keep things local to the module. There are many similarities between IndexAndTaxoRevision to IndexRevision (likewise for their handlers and tests). Therefore whenever I made change to one, I knew I should go make a similar change to the other.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;All in all, I guess arguments can be made both ways, but I prefer for the now to keep things local to the replicator module. Even in the future, I would imagine that if we added support for replicating a suggester files, then it would make sense to put a dependency between replicator and suggester, rather than the other way around.&lt;/p&gt;</comment>
                    <comment id="13649634" author="shaie" created="Mon, 6 May 2013 10:52:47 +0100">&lt;blockquote&gt;&lt;p&gt;maybe also call MDW.setRandomIOExceptionRateOnOpen&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks Mike! I added that and a slew of problems surfaced, most of them in the test, but I improved the handlers' implementation to cleanup after themselves if e.g. a copy or sync to the handlerDir failed. While this wasn't a bug, it leaves the target index directory clean.&lt;/p&gt;

&lt;p&gt;There's one nocommit which bugs me though &amp;#8211; I had to add dir.setPreventDoubleWrite(false) because when the handler fails during copying of say _2.fdt to the index dir, the file is deleted from the indexDir, and the client re-attemts to upgrade. At this point, MDW complains that _2.fdt was already written to, even though I deleted it.&lt;/p&gt;

&lt;p&gt;Adding this setPrevent was the only way I could make MDW happy, but I don't like it since I do want to catch errors in the handler/client if they e.g. attempt to copy over an existing file.&lt;/p&gt;

&lt;p&gt;Maybe we can make MDW respond somehow to delete()? I know that has bad implications on its own, e.g. code which deletes and then accidentally recreates files with older names ... any ideas?&lt;/p&gt;</comment>
                    <comment id="13649635" author="jpountz" created="Mon, 6 May 2013 10:55:03 +0100">&lt;p&gt;Good points, you convinced me. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13649643" author="rcmuir" created="Mon, 6 May 2013 11:37:01 +0100">&lt;blockquote&gt;
&lt;p&gt;Even in the future, I would imagine that if we added support for replicating a suggester files, then it would make sense to put a dependency between replicator and suggester, rather than the other way around.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wait: how does this make sense?!&lt;/p&gt;

&lt;p&gt;It should be the other way around: if suggester has a sidecar it needs special logic for replication. &lt;/p&gt;

&lt;p&gt;It does not need faceting.&lt;/p&gt;</comment>
                    <comment id="13649650" author="shaie" created="Mon, 6 May 2013 12:04:48 +0100">&lt;p&gt;As I said, arguments can be made both ways ... I don't know what's the best way here. I can see your point, but I don't feel good about having facet depend on replicator. I see Replicator as a higher-level service that besides providing the replication framework, also comes pre-built for replicating Lucene stuff. I don't mind seeing it grow to accommodate other Revision types in the future. For example, IndexAndTaxonomyRevision is just an example for replicating multiple indexes together. It can easily be duplicated to replicate few indexes at once, e.g. a MultiIndexRevision. Where would that object be? Cannot be in core, so why should IndexAndTaxo be in facet?&lt;/p&gt;</comment>
                    <comment id="13649655" author="jpountz" created="Mon, 6 May 2013 12:13:22 +0100">&lt;p&gt;Then maybe we could have sub-modules for specific replication strategies? lucene/replicator would only know how to handle raw indexes, while lucene/replicator/facets or lucene/replicator/suggest would implement custom logic?&lt;/p&gt;

&lt;p&gt;This way lucene/facet wouldn't need to pull all lucene/replicator transitive dependencies, and lucene/replicator wouldn't depend on any lucene module but lucene/core.&lt;/p&gt;</comment>
                    <comment id="13649658" author="rcmuir" created="Mon, 6 May 2013 12:19:46 +0100">&lt;p&gt;I still haven't had a change to look at the patch: but it sounds like some work needs to be done here to prevent dll hell.&lt;/p&gt;

&lt;p&gt;having replicator depend upon all sidecar modules is a no-go.&lt;/p&gt;

&lt;p&gt;it sounds like an interface is missing.&lt;/p&gt;</comment>
                    <comment id="13649674" author="shaie" created="Mon, 6 May 2013 12:46:32 +0100">&lt;p&gt;Ok, so there are 3 options I see: (1) have Replicator depend on Facet (and in the future on other modules), (2) have Facet depend on Replicator and (3) move Revision and ReplicationHandler (interfaces) someplace else, core or a new module we call 'commons' and Replicator and Facet depend on it. Tests though will need to depend on replicator though, since they need ReplicationClient.&lt;/p&gt;

&lt;p&gt;BTW, the jetty dependencies are tests only, but I don't know how to make ivy resolve the dependencies just for tests. The only thing replicator depends on is servlet-api, for ReplicationService and httpclient for ReplicationClient. I think these need to remain in the module ...&lt;/p&gt;

&lt;p&gt;If we made Facet depend on Replicator (I'm not totally against it), would that require you to have lucene-replicator.jar on the classpath, even if you don't use replication? If not, then perhaps this dependency isn't so bad ... it's just a compile-time dependency. Tests will still need to depend on replicator for runtime, but that's ok I think.&lt;/p&gt;</comment>
                    <comment id="13649727" author="jpountz" created="Mon, 6 May 2013 14:09:12 +0100">&lt;blockquote&gt;&lt;p&gt;Then maybe we could have sub-modules for specific replication strategies?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;To make my point a little clearer, I was suggesting something pretty much like the analysis module: analyzers that require additional dependencies (such as icu or morfologik) are in their own sub-module so that you don't need to pull the ICU or Morfologik JARs if you just want to use LetterTokenizer (which is in lucene/analysis/common).&lt;/p&gt;

&lt;p&gt;Likewise, we could have the interface and the logic to replicate simple (no sidecar data) indexes in lucene/replicator/common and have sub-modules for facet (lucene/replicator/facet) or suggesters (lucene/replicator/suggesters).&lt;/p&gt;

&lt;p&gt;This may look overkill but at least this would help us keep dependencies clean between modules.&lt;/p&gt;</comment>
                    <comment id="13649731" author="shaie" created="Mon, 6 May 2013 14:13:24 +0100">&lt;p&gt;I think that's not a bad idea! replicator/common will include the interfaces (Revision and ReplicationHandler) + the framework impl and also IndexRevision/Handler. replicator/facet will include the taxonomy parts and depend on replicator/common and facet.&lt;/p&gt;

&lt;p&gt;I can also move the facet related code under oal.replicator.facet and then suppress the Lucene3x codec for just these tests.&lt;/p&gt;

&lt;p&gt;If others agree, I'll make the changes (mostly build.xml changes).&lt;/p&gt;</comment>
                    <comment id="13650731" author="shaie" created="Tue, 7 May 2013 12:27:47 +0100">&lt;p&gt;I'm having reservations about creating a replicator/facet module which contains 2 classes ... Maybe we should proceed with the code as-is, and then refactor if it creates a problem, or the module grows? Perhaps the breakout won't be to replicator/common and replicator/facet but to replicator/infra (or common) and replicator/extras which will serve like a catchall for other modules too (e.g. facet, suggest).&lt;/p&gt;

&lt;p&gt;Another way is to break out replicator to common and framework/infra/impl such that common contains only whatever other modules require to compile against (i.e Revision, ReplicationHandler, maybe Replicator). Then we can add the facet replication code to facet/ with a dependency on replicator/common.&lt;/p&gt;

&lt;p&gt;But really, I think we should just get it in and start to work with it, have deeper reviews and refactor as we go.&lt;/p&gt;</comment>
                    <comment id="13651759" author="shaie" created="Wed, 8 May 2013 11:04:23 +0100">&lt;p&gt;Patch fixes a bug in IndexReplicationHandler (still need to fix in IndexAndTaxonomy) and adds some nocommits which I want to take care before I commit it.&lt;/p&gt;

&lt;p&gt;However, I hit a new test failure, which reproduces with the following command &lt;tt&gt;ant test -Dtestcase=IndexReplicationClientTest -Dtests.method=testConsistencyOnExceptions -Dtests.seed=EAF5294292642F1:6EE70BB59A9FC3CA&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The error is weird. I ran the test w/ -Dtests.verbose=true and here's the troubling parts from the log:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;ReplicationThread-index: MockDirectoryWrapper: now throw random exception during open file=segments_a
java.lang.Throwable
	at org.apache.lucene.store.MockDirectoryWrapper.maybeThrowIOExceptionOnOpen(MockDirectoryWrapper.java:364)
	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:522)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:281)
	at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:340)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:668)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:515)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:343)
	at org.apache.lucene.index.IndexWriter.&amp;lt;init&amp;gt;(IndexWriter.java:682)
	at org.apache.lucene.replicator.IndexReplicationHandler.revisionReady(IndexReplicationHandler.java:208)
	at org.apache.lucene.replicator.ReplicationClient.doUpdate(ReplicationClient.java:248)
	at org.apache.lucene.replicator.ReplicationClient.access$1(ReplicationClient.java:188)
	at org.apache.lucene.replicator.ReplicationClient$ReplicationThread.run(ReplicationClient.java:76)
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: init: current segments file is "segments_9"; deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@117da39a
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: init: load commit "segments_9"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: init: load commit "segments_a"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: now checkpoint "_0(5.0):C1 _1(5.0):C1 _2(5.0):c1 _3(5.0):c1 _4(5.0):c1 _5(5.0):c1 _6(5.0):c1 _7(5.0):c1 _8(5.0):c1" [9 segments ; isCommit = false]
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: 0 msec to checkpoint
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: deleteCommits: now decRef commit "segments_9"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: delete "segments_9"
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: init: create=false

....

IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: startCommit(): start
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: startCommit index=_0(5.0):C1 _1(5.0):C1 _2(5.0):c1 _3(5.0):c1 _4(5.0):c1 _5(5.0):c1 _6(5.0):c1 _7(5.0):c1 _8(5.0):c1 changeCount=1
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: done all syncs: [_2.si, _7.si, _5.cfs, _1.fnm, _4.cfs, _8.si, _4.cfe, _5.cfe, _0.si, _0.fnm, _6.cfe, _8.cfs, _3.cfs, _4.si, _7.cfe, _2.cfs, _5.si, _6.cfs, _1.fdx, _8.cfe, _1.fdt, _1.si, _7.cfs, _0.fdx, _3.si, _6.si, _3.cfe, _2.cfe, _0.fdt]
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: commit: pendingCommit != null
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: commit: wrote segments file "segments_a"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: now checkpoint "_0(5.0):C1 _1(5.0):C1 _2(5.0):c1 _3(5.0):c1 _4(5.0):c1 _5(5.0):c1 _6(5.0):c1 _7(5.0):c1 _8(5.0):c1" [9 segments ; isCommit = true]
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: deleteCommits: now decRef commit "segments_a"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: delete "_9.cfe"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: delete "_9.cfs"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: delete "_9.si"
IFD 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: 0 msec to checkpoint
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: commit: done
IW 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: at close: _0(5.0):C1 _1(5.0):C1 _2(5.0):c1 _3(5.0):c1 _4(5.0):c1 _5(5.0):c1 _6(5.0):c1 _7(5.0):c1 _8(5.0):c1
IndexReplicationHandler 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: updateHandlerState(): currentVersion=a currentRevisionFiles={index=[Lorg.apache.lucene.replicator.RevisionFile;@9bc2e26e}
IndexReplicationHandler 0 [Wed May 08 22:47:46 WST 2013; ReplicationThread-index]: {version=9}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I debug traced it and here's what I think is happening:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;MDW throws FNFE for segments_a on sis.read(dir), therefore the read SegmentInfos sees segments_9 as the current good commit. IW's segmentInfos.commitData stores version=9, which corresponds to segments_9.&lt;/li&gt;
	&lt;li&gt;IFD lists the files in the Directory, and finds both segments_a and segments_9 and through a series of calls, deletes segments_9 and keeps segments_a, since it is newer.&lt;/li&gt;
	&lt;li&gt;IW ctor, line 719, increments changeCount, since IFD.startingCommitDeleted is true &amp;#8211; which happens b/c IFD is initialized with segments_9, but finds segments_a and therefore deletes it.&lt;/li&gt;
	&lt;li&gt;IW then makes a commit, with the commit data from segments_9 ("version=9"), to a new commit point generation 10 (a in hex).&lt;/li&gt;
	&lt;li&gt;The Replicator's latest version is gen=10, the handler reads gen=10 from the index, but with the wrong commitData, and therefore the test fails.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I still want to review all this again, to double-check my understanding, but it looks like something bad happening between IW and IFD. At least from the perspective of the replicator, the index shouldn't "go forward" by new IW().close().&lt;/p&gt;

&lt;p&gt;If I modify the handler to do:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
IndexWriter writer = &lt;span class="code-keyword"&gt;new&lt;/span&gt; IndexWriter();
writer.deleteUnusedFiles();
writer.rollback();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The test passes. But is this the right solution &amp;#8211; i.e. guarantee that IW never commits? Or is this a bug in IW?&lt;/p&gt;</comment>
                    <comment id="13651762" author="shaie" created="Wed, 8 May 2013 11:25:44 +0100">&lt;p&gt;Ok some more insights ... I think this additional chain of events occurs:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;IW's ctor reads gen 9, and SegmentInfos.getSegmentsFileName returns segments_9.&lt;/li&gt;
	&lt;li&gt;IFD then successfully reads both segments_9 and segments_a, ending up w/ two commit points.&lt;/li&gt;
	&lt;li&gt;IFD sorts them and passes to IndexDeletionPolicy (KeepLastCommit) which deletes segments_9 and keeps segments_a&lt;/li&gt;
	&lt;li&gt;IFD marks that startingCommitDeleted, as it is&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And here's what I still don't understand &amp;#8211; for some reason, IW creates a new commit point, segments_a, with the commitData from segments_9. Still need to dig into that.&lt;/p&gt;

&lt;p&gt;In the meanwhile, I made the above change to the hanlder (to rollback(), not close()), and 430 iterations passed. Not sure if that's the right way to go ... if IW.close() didn't commit ... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13651786" author="rcmuir" created="Wed, 8 May 2013 11:54:45 +0100">&lt;p&gt;I'm trying really hard not to say anything sarcastic here &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13651839" author="shaie" created="Wed, 8 May 2013 13:26:15 +0100">&lt;blockquote&gt;&lt;p&gt;I'm trying really hard not to say anything sarcastic here&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Heh, I expected something from you &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;

&lt;p&gt;I chatted about this with Mike and he confirmed my reasoning. This is very slim chance, and usually indicates a truly bad (or crazy) IO subsystem (i.e. not like MDW throwing random IOEs on opening the same file over and over again). I think perhaps this can be solved in IW by having it refer to the latest commit point read by IFD and not what it read. This seems safe to me, but perhaps an overkill. Anyway, it belongs in a different issue.&lt;/p&gt;

&lt;p&gt;What also happened here is that IW overwrote segments_a (violating write-once policy), which MDW didn't catch because for replicator tests I need to turn off preventDoubleWrite. Mike also says that IW doesn't guarantee write-once if it hits exceptions ...&lt;/p&gt;

&lt;p&gt;So I think the safest solution is to deleteUnused() + rollback(), as anyway the handler must ensure that commits are not created by this "kiss".&lt;/p&gt;

&lt;p&gt;I will resolve the remaining nocommits and post a new patch.&lt;/p&gt;</comment>
                    <comment id="13651847" author="rcmuir" created="Wed, 8 May 2013 13:31:38 +0100">&lt;blockquote&gt;
&lt;p&gt;I chatted about this with Mike and he confirmed my reasoning. This is very slim chance, and usually indicates a truly bad (or crazy) IO subsystem (i.e. not like MDW throwing random IOEs on opening the same file over and over again). I think perhaps this can be solved in IW by having it refer to the latest commit point read by IFD and not what it read. This seems safe to me, but perhaps an overkill. Anyway, it belongs in a different issue.&lt;/p&gt;

&lt;p&gt;What also happened here is that IW overwrote segments_a (violating write-once policy), which MDW didn't catch because for replicator tests I need to turn off preventDoubleWrite. Mike also says that IW doesn't guarantee write-once if it hits exceptions ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, i disagree and am against this.&lt;/p&gt;

&lt;p&gt;The bug is that IndexWriter.close() waits for merges and commits. Lets quit kidding ourselves ok?&lt;/p&gt;</comment>
                    <comment id="13651850" author="shaie" created="Wed, 8 May 2013 13:36:43 +0100">&lt;blockquote&gt;&lt;p&gt;The bug is that IndexWriter.close() waits for merges and commits. Lets quit kidding ourselves ok?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure I agree .. the bug is real, and if somebody did new IW().commit().close() without making any change, he might be surprised about that commit too, and also IW would overwrite an existing file. The only thing "close-not-committing" would solve in this case is that I won't need to call rollback(), but close(). The bug won't disappear.&lt;/p&gt;</comment>
                    <comment id="13653562" author="shaie" created="Fri, 10 May 2013 06:46:36 +0100">&lt;p&gt;Patch resolves all remaining noocmmits. I made both IndexReplicationHandler and IndexAndTaxonomyReplicationHandler more resilient to errors but carefully reverting any changes made to the target indexes on errors. Also ensure that segments_N files are written last.&lt;/p&gt;

&lt;p&gt;I also added code to write segments.gen. To do that I factored out SegmentInfos.writeSegmentsGen which takes a Directory and generation. It is now used by SIS.finishCommit and the handlers.&lt;/p&gt;

&lt;p&gt;Would appreciate if someone can review the handlers, and whether they can be written in a more resilient ways.&lt;/p&gt;

&lt;p&gt;I want to beast IndexReplicationClientTest and IndexAndTaxonomyReplictionClientTest checkConsistencyOnExceptions before committing. If anyone's interested to help, just fire &lt;tt&gt;ant test -Dtestcase=IndexReplicationClientTest -Dtestsm.method=testConsistencyOnExceptions -Dtests.iters=1000&lt;/tt&gt; (and same for IndexAndTaxonomyReplicationClientTest) and let me know the seeds that failed.&lt;/p&gt;</comment>
                    <comment id="13653896" author="shaie" created="Fri, 10 May 2013 11:33:20 +0100">&lt;p&gt;I ran both tests w/ tests.iters=1000 and they passed. This gives me more confidence about the robustness of these two handlers. Still, other machines can dig up "special" seeds &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;</comment>
                    <comment id="13654399" author="shaie" created="Fri, 10 May 2013 12:58:52 +0100">&lt;p&gt;Mike found a seed which tripped a test bug. Fixed it and on the way made the test less sensitive to time changes (i.e. it waited up to 20 seconds for the index to get updated, otherwise failed) and added some other improvements (to the test only).&lt;/p&gt;

&lt;p&gt;Let's search for more seeds &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;</comment>
                    <comment id="13654404" author="shaie" created="Fri, 10 May 2013 13:03:53 +0100">&lt;p&gt;Forgot to include one change &amp;#8211; handleUpdateEx should fail if the exception it hits is not IOE. Previous patch called super.handle(), which only logged it. But I think it's fair to say that the test shouldn't hit any exception, and terminate the client if it does.&lt;/p&gt;</comment>
                    <comment id="13654829" author="mikemccand" created="Fri, 10 May 2013 22:05:11 +0100">&lt;p&gt;+1, I looked at the replication handlers and they look great!&lt;/p&gt;

&lt;p&gt;I wonder if we could factor out touchIndex to a static method and share from IndexReplicationHandler and IndexAndTaxoReplicationHandler?&lt;/p&gt;</comment>
                    <comment id="13655183" author="shaie" created="Sat, 11 May 2013 08:10:45 +0100">&lt;p&gt;Good idea Mike. I factored out touchIndex, cleanupFilesOnFailure and copyFiles to static utilities on IndexReplHandler.&lt;/p&gt;

&lt;p&gt;Maybe instead of the static utilities, we can have an abstract IRH with two impls: SingleIndexReplicationHandler and IndexAndTaxoReplHandler. It can provide the utility methods as protected, plus implement the general framework for index replication, using abstract callbacks that are implemented by the concrete handlers. But perhaps we should do it later.&lt;/p&gt;</comment>
                    <comment id="13655472" author="shaie" created="Sun, 12 May 2013 06:45:30 +0100">&lt;p&gt;Mike tripped a seed which was reproducible only on Linux which uncovered a bug in the handlers &amp;#8211; writing the segments.gen file should be done only after updateHandlerState() is called, and from what the handler says is its last state. Otherwise, we could write segments.gen w/ gen=7, but "kissing" the index, or updating the state (reading commits) trips an error, the handler is reset back to the last revision it knows is good, and we end up w/ a segments.gen file from the future. This then causes DirReader.open to fail w/ FNFE looking for a futuristic segments_N file.&lt;/p&gt;

&lt;p&gt;MockDirWrapper is a nasty bitch. It should be called WhackoIOSubSystemDirectory! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Patch improves the handlers + make the tests more resilient so they don't enter an infinite loop (that's part of what Mike hit).&lt;/p&gt;

&lt;p&gt;I'm beasting more. Please review the handlers' segments.gen writing logic.&lt;/p&gt;</comment>
                    <comment id="13655543" author="shaie" created="Sun, 12 May 2013 15:13:24 +0100">&lt;p&gt;New patch changes how handlers work:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Beasting found a seed which uncovered a major problem with their current operation. They were trying to be "too honest" with the index and e.g. revert/delete upon any exception that occurred.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Thanks to MDW, Mike and I decided to keep the handlers simple &amp;#8211; if a handler successfully copies + syncs the revision files, then this is considered the "new revision".&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Kissing the index is now done not through IndexWriter, but rather deleting all files not referenced by last commit.
	&lt;ul&gt;
		&lt;li&gt;That cleanup is a best-effort only ... if it fails, it just logs this information and not act on it. Cleanup can happen later too.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;That means that if you have a really nasty crazy IO system (like MDW sometimes acts), the Replicator is not the one that's going to care about it. The app will hit those weird errors in other places too, e.g. when it tries to refresh SearcherManager or perform search.
	&lt;ul&gt;
		&lt;li&gt;These errors are not caused by the Replicator or bad handler operation. I.e. if the handler successfully called fsync(), yet the IO system decides to fail later ... that's really not the handler's problem.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Therefore the handlers are now simpler, don't use IW (and the crazy need to rollback()), and once files were successfully copied + sync'd, no more exceptions can occur by the handler (except callback may fail, but that's ok).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I also removed the timeout behavior the test employed &amp;#8211; now that ReplicationClient has isUpdateThreadAlive(), assertHandlerRevision loops as long as the client is alive. If there's a serious bug, test-framework will terminate the test after 2 hours ...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;ReplicationClient.startUpdateThread is nicer &amp;#8211; allows starting the thread if updateThread != null, but !isAlive.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now beasting this patch.&lt;/p&gt;</comment>
                    <comment id="13655632" author="shaie" created="Sun, 12 May 2013 21:15:15 +0100">&lt;p&gt;Patch adds instance InfoStream members instead of relying on the static default. Beasted 10K+ iterations for both IndexReplicationClientTest and IndexAndTaxonomyReplicationClientTest, all pass.&lt;/p&gt;

&lt;p&gt;I think it's ready. I plan to commit it tomorrow.&lt;/p&gt;</comment>
                    <comment id="13655915" author="commit-tag-bot" created="Mon, 13 May 2013 12:57:25 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; shaie&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481804" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1481804&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4975" title="Add Replication module to Lucene"&gt;&lt;del&gt;LUCENE-4975&lt;/del&gt;&lt;/a&gt;: Add Replication module to Lucene&lt;/p&gt;</comment>
                    <comment id="13655938" author="shaie" created="Mon, 13 May 2013 13:56:55 +0100">&lt;p&gt;Committed to trunk and 4x. Thanks Mike!&lt;/p&gt;</comment>
                    <comment id="13665156" author="shyamvs1001" created="Thu, 23 May 2013 14:16:11 +0100">&lt;p&gt;Shair Erera,&lt;br/&gt;
If I want to try out this feature, how and where should I start? I'm planning to try out in a Master + 2 slaves lucene(integrted with hibernate) setup.&lt;/p&gt;</comment>
                    <comment id="13665171" author="shaie" created="Thu, 23 May 2013 14:35:06 +0100">&lt;p&gt;Shyam, checkout this blog post (&lt;a href="http://shaierera.blogspot.com/2013/05/the-replicator.html" class="external-link"&gt;http://shaierera.blogspot.com/2013/05/the-replicator.html&lt;/a&gt;) which explains how the Replicator works and includes some example code. The javadocs also contain example docs. If you run into any issues, don't hesitate to email java-user@lucene.apache.org.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12582864" name="LUCENE-4975.patch" size="301373" author="shaie" created="Sun, 12 May 2013 21:15:15 +0100"/>
                    <attachment id="12582839" name="LUCENE-4975.patch" size="299988" author="shaie" created="Sun, 12 May 2013 15:13:24 +0100"/>
                    <attachment id="12582821" name="LUCENE-4975.patch" size="302527" author="shaie" created="Sun, 12 May 2013 06:45:30 +0100"/>
                    <attachment id="12582773" name="LUCENE-4975.patch" size="298125" author="shaie" created="Sat, 11 May 2013 08:10:45 +0100"/>
                    <attachment id="12582607" name="LUCENE-4975.patch" size="299381" author="shaie" created="Fri, 10 May 2013 13:03:53 +0100"/>
                    <attachment id="12582606" name="LUCENE-4975.patch" size="299235" author="shaie" created="Fri, 10 May 2013 12:58:52 +0100"/>
                    <attachment id="12582578" name="LUCENE-4975.patch" size="297903" author="shaie" created="Fri, 10 May 2013 06:46:36 +0100"/>
                    <attachment id="12582276" name="LUCENE-4975.patch" size="293017" author="shaie" created="Wed, 8 May 2013 11:04:23 +0100"/>
                    <attachment id="12581867" name="LUCENE-4975.patch" size="288048" author="shaie" created="Mon, 6 May 2013 10:52:47 +0100"/>
                    <attachment id="12581822" name="LUCENE-4975.patch" size="282698" author="shaie" created="Sun, 5 May 2013 09:00:42 +0100"/>
                    <attachment id="12581657" name="LUCENE-4975.patch" size="181459" author="shaie" created="Fri, 3 May 2013 04:43:40 +0100"/>
                    <attachment id="12581606" name="LUCENE-4975.patch" size="273875" author="shaie" created="Thu, 2 May 2013 22:12:09 +0100"/>
                    <attachment id="12581543" name="LUCENE-4975.patch" size="275043" author="shaie" created="Thu, 2 May 2013 17:11:13 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 5 May 2013 23:18:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326197</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326179</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4973] SnapshotDeletionPolicy should not require an id</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4973</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The id is unnecessary and just adds complexity: SDP can just return&lt;br/&gt;
the IndexCommit, and when you want to release you pass back the&lt;br/&gt;
IndexCommit.  PersistentSDP can expose release(long gen).&lt;/p&gt;</description>
                <environment/>
            <key id="12645689">LUCENE-4973</key>
            <summary>SnapshotDeletionPolicy should not require an id</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 May 2013 20:45:25 +0100</created>
                <updated>Thu, 2 May 2013 18:21:57 +0100</updated>
                    <resolved>Thu, 2 May 2013 18:21:57 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13646868" author="mikemccand" created="Wed, 1 May 2013 20:46:39 +0100">&lt;p&gt;Patch ... SDP is simpler now (I also removed some methods, eg&lt;br/&gt;
getSnapshot, snapshotExists).  I think separately we should fix&lt;br/&gt;
PersistentSDP to store its state in a single file ... separate issue.&lt;/p&gt;</comment>
                    <comment id="13646880" author="shaie" created="Wed, 1 May 2013 21:10:23 +0100">&lt;p&gt;+1. I will review the patch tomorrow, but I think pSDP needs to have a getIC(gen) because if the apps persists the gens, it needs a way to obtain their IC instance.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think separately we should fix PersistentSDP to store its state in a single file ... separate issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1! would be great if this can be in the index directory, so that app doesn't need to make up locations. but let's discuss that separately.&lt;/p&gt;</comment>
                    <comment id="13646898" author="mikemccand" created="Wed, 1 May 2013 21:36:46 +0100">&lt;p&gt;New patch, adding SDP.getIndexCommit(long gen).&lt;/p&gt;</comment>
                    <comment id="13647350" author="shaie" created="Thu, 2 May 2013 07:41:01 +0100">&lt;p&gt;Looks good! Comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;assertTrue(snapshot == sdp.getIndexCommit(snapshot.getGeneration()));&lt;/tt&gt; &amp;#8211; can you change to assertSame? They should be equivalent, only assertSame produces nicer error message.
	&lt;ul&gt;
		&lt;li&gt;Same for &lt;tt&gt;assertTrue(s1 == s2); // should be the same instance&lt;/tt&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;SDP.snapshot javadoc:
	&lt;ul&gt;
		&lt;li&gt;"using the same ID parameter"; I guess it's a copy-paste error from the previous SDP version?&lt;/li&gt;
		&lt;li&gt;IllegalStateException still refers to ID&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;releaseGen: can we relax it to not fail if someone calls release on an unsnpshotted gen? I guess someone can workaround that by calling sdp.getIC(gen) and if not null call release. But I think it's not necessarily an error to release an unsnapshotted gen, e.g. code which releases from different places. No strong feelings about it though, so it's your call.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Persistent &amp;#8211; do you want to change SNAPSHOT_ID to SNAPSHOT_GENS?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13647435" author="mikemccand" created="Thu, 2 May 2013 11:54:45 +0100">&lt;p&gt;Thanks Shai!  New patch ... I also noticed that onInit (to build the gen -&amp;gt; IC map) was in pSDP but should be in SDP so I moved it up, and I fixed a few other leftover ids in the javadocs ... I think it's ready.&lt;/p&gt;</comment>
                    <comment id="13647712" author="commit-tag-bot" created="Thu, 2 May 2013 18:16:09 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478452" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478452&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4973" title="SnapshotDeletionPolicy should not require an id"&gt;&lt;del&gt;LUCENE-4973&lt;/del&gt;&lt;/a&gt;: Persistent/SnapshotDeletionPolicy no longer require a unique id&lt;/p&gt;</comment>
                    <comment id="13647716" author="commit-tag-bot" created="Thu, 2 May 2013 18:21:48 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478459" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478459&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4973" title="SnapshotDeletionPolicy should not require an id"&gt;&lt;del&gt;LUCENE-4973&lt;/del&gt;&lt;/a&gt;: Persistent/SnapshotDeletionPolicy no longer require a unique id&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581503" name="LUCENE-4973.patch" size="43841" author="mikemccand" created="Thu, 2 May 2013 11:54:45 +0100"/>
                    <attachment id="12581410" name="LUCENE-4973.patch" size="43168" author="mikemccand" created="Wed, 1 May 2013 21:36:46 +0100"/>
                    <attachment id="12581402" name="LUCENE-4973.patch" size="41296" author="mikemccand" created="Wed, 1 May 2013 20:46:39 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 1 May 2013 20:10:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326062</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326044</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4972] DirectoryTaxonomyWriter makes a commit even if no changes were made</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4972</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Now that IndexWriter allows committing changes even if the only change is setCommitData, DirectoryTaxonomyWriter creates empty commits because whenever you call commit/close, it sets as commitData the indexEpoch, thereby creating unnecessary commit points.&lt;/p&gt;

&lt;p&gt;I think that DirTaxoWriter should track if the index is dirty ... or preferably get that from IndexWriter (i.e. getChangeCount or something). I'll create a test case exposing the bug and then fix DTW.&lt;/p&gt;</description>
                <environment/>
            <key id="12645682">LUCENE-4972</key>
            <summary>DirectoryTaxonomyWriter makes a commit even if no changes were made</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="shaie">Shai Erera</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 May 2013 19:59:09 +0100</created>
                <updated>Fri, 3 May 2013 04:59:18 +0100</updated>
                    <resolved>Fri, 3 May 2013 04:59:18 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13647334" author="shaie" created="Thu, 2 May 2013 07:12:07 +0100">&lt;p&gt;Patch adds isDirty flag to DTW and tests. I think it's ready, but would appreciate a review.&lt;/p&gt;</comment>
                    <comment id="13647423" author="mikemccand" created="Thu, 2 May 2013 11:47:18 +0100">&lt;p&gt;I wonder if instead of isDirty we could instead call IW.getCommitData() and check if the epoch hasn't changed?&lt;/p&gt;</comment>
                    <comment id="13647846" author="shaie" created="Thu, 2 May 2013 21:00:21 +0100">&lt;p&gt;Patch removes isDirty and compares the epoch from indexWriter.commitData.&lt;/p&gt;</comment>
                    <comment id="13647854" author="mikemccand" created="Thu, 2 May 2013 21:11:04 +0100">&lt;p&gt;+1, thanks Shai.&lt;/p&gt;</comment>
                    <comment id="13648161" author="commit-tag-bot" created="Fri, 3 May 2013 04:52:52 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; shaie&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478638" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478638&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4972" title="DirectoryTaxonomyWriter makes a commit even if no changes were made"&gt;&lt;del&gt;LUCENE-4972&lt;/del&gt;&lt;/a&gt;: DirectoryTaxonomyWriter makes a commit even if no changes were made&lt;/p&gt;</comment>
                    <comment id="13648166" author="commit-tag-bot" created="Fri, 3 May 2013 04:58:57 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; shaie&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478640" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478640&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4972" title="DirectoryTaxonomyWriter makes a commit even if no changes were made"&gt;&lt;del&gt;LUCENE-4972&lt;/del&gt;&lt;/a&gt;: DirectoryTaxonomyWriter makes a commit even if no changes were made&lt;/p&gt;</comment>
                    <comment id="13648168" author="shaie" created="Fri, 3 May 2013 04:59:18 +0100">&lt;p&gt;Committed to trunk and 4x. Thanks Mike!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581594" name="LUCENE-4972.patch" size="5280" author="shaie" created="Thu, 2 May 2013 21:00:21 +0100"/>
                    <attachment id="12581487" name="LUCENE-4972.patch" size="5598" author="shaie" created="Thu, 2 May 2013 07:12:07 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 May 2013 10:47:18 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326055</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326037</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4971] NPE in AnalyzingSuggester</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4971</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Setting maxGraphExpansions &amp;gt; 0 with a lot of expansions (e.g. due to synonyms).&lt;br/&gt;
Set&amp;lt;IntsRef&amp;gt; paths = toFiniteStrings(surfaceForm, ts2a);&lt;br/&gt;
paths may be null, so maxAnalyzedPathsForOneInput = Math.max(maxAnalyzedPathsForOneInput, paths.size()) may end with NPE&lt;/p&gt;</description>
                <environment>&lt;p&gt;Windows 7&lt;/p&gt;</environment>
            <key id="12645631">LUCENE-4971</key>
            <summary>NPE in AnalyzingSuggester</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="alexeyk">Alexey Kudinov</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 May 2013 14:36:11 +0100</created>
                <updated>Wed, 1 May 2013 21:00:06 +0100</updated>
                                    <version>4.1</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13646878" author="mikemccand" created="Wed, 1 May 2013 21:00:06 +0100">&lt;p&gt;Patch with a test case showing the NPE.&lt;/p&gt;

&lt;p&gt;The question is how to fix it ... SpecialOperations.getFiniteStrings today returns null when it hits the limit, yet, it has accumulated the first N strings it saw at that point, so maybe we should change it to return the first N so that we have something to build?&lt;/p&gt;

&lt;p&gt;Or alternatively, AnalyzingSuggester could just skip a given input if it created too many strings.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581403" name="LUCENE-4971.patch" size="1654" author="mikemccand" created="Wed, 1 May 2013 21:00:06 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 1 May 2013 20:00:06 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>326004</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325986</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4967] Absorb NRTManager entirely into a separate reopen thread class</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4967</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I think NRTManager can be drastically simplified by moving all of its&lt;br/&gt;
logic into a new reopen thread class.  All logic for waiting for a&lt;br/&gt;
specific generation and reopening at different rates would live in&lt;br/&gt;
this class.&lt;/p&gt;

&lt;p&gt;This would fully decouple the "wait for generation X to be visible"&lt;br/&gt;
from which particular ReferenceManager impl you're using, which would&lt;br/&gt;
make it possible to use the controlled consistency approach of&lt;br/&gt;
NRTManager with any managers (e.g. SearcherTaxonomyManager).&lt;/p&gt;</description>
                <environment/>
            <key id="12645114">LUCENE-4967</key>
            <summary>Absorb NRTManager entirely into a separate reopen thread class</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 29 Apr 2013 00:09:59 +0100</created>
                <updated>Thu, 2 May 2013 18:06:54 +0100</updated>
                    <resolved>Thu, 2 May 2013 18:06:54 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13644164" author="mikemccand" created="Mon, 29 Apr 2013 00:11:54 +0100">&lt;p&gt;Patch, I think it's ready.&lt;/p&gt;</comment>
                    <comment id="13647685" author="commit-tag-bot" created="Thu, 2 May 2013 17:44:39 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478438" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478438&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4967" title="Absorb NRTManager entirely into a separate reopen thread class"&gt;&lt;del&gt;LUCENE-4967&lt;/del&gt;&lt;/a&gt;: move NRTManager entirely into a reopen thread so it can interact with any ReferenceManager&lt;/p&gt;</comment>
                    <comment id="13647705" author="commit-tag-bot" created="Thu, 2 May 2013 18:06:11 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478446" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478446&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4967" title="Absorb NRTManager entirely into a separate reopen thread class"&gt;&lt;del&gt;LUCENE-4967&lt;/del&gt;&lt;/a&gt;: move NRTManager entirely into a reopen thread so it can interact with any ReferenceManager&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580914" name="LUCENE-4967.patch" size="65350" author="mikemccand" created="Mon, 29 Apr 2013 00:11:54 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 May 2013 16:44:39 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325487</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325469</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4966] Add CachingWrapperFilter.sizeInBytes()</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4966</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I think it's useful to be able to check how much RAM a given CWF is using ...&lt;/p&gt;</description>
                <environment/>
            <key id="12645105">LUCENE-4966</key>
            <summary>Add CachingWrapperFilter.sizeInBytes()</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 28 Apr 2013 23:03:03 +0100</created>
                <updated>Mon, 29 Apr 2013 22:23:42 +0100</updated>
                    <resolved>Mon, 29 Apr 2013 22:23:42 +0100</resolved>
                            <version>5.0</version>
                <version>4.4</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13644151" author="mikemccand" created="Sun, 28 Apr 2013 23:04:03 +0100">&lt;p&gt;Patch.&lt;/p&gt;</comment>
                    <comment id="13644512" author="jpountz" created="Mon, 29 Apr 2013 15:23:23 +0100">&lt;p&gt;+1 I wish we had such methods for the terms index, norms/doc values, stored fields/term vectors index, etc. too in order to get a better understanding of how Lucene uses memory. &lt;/p&gt;</comment>
                    <comment id="13644523" author="simonw" created="Mon, 29 Apr 2013 15:37:05 +0100">&lt;blockquote&gt;&lt;p&gt;+1 I wish we had such methods for the terms index, norms/doc values, stored fields/term vectors index, etc. too in order to get a better understanding of how Lucene uses memory.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 we should make them all consistent here as well. &lt;/p&gt;</comment>
                    <comment id="13644875" author="commit-tag-bot" created="Mon, 29 Apr 2013 22:22:24 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477348" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477348&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4966" title="Add CachingWrapperFilter.sizeInBytes()"&gt;&lt;del&gt;LUCENE-4966&lt;/del&gt;&lt;/a&gt;: add CachingWrapperFilter.sizeInBytes&lt;/p&gt;</comment>
                    <comment id="13644877" author="commit-tag-bot" created="Mon, 29 Apr 2013 22:23:12 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477349" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477349&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4966" title="Add CachingWrapperFilter.sizeInBytes()"&gt;&lt;del&gt;LUCENE-4966&lt;/del&gt;&lt;/a&gt;: add CachingWrapperFilter.sizeInBytes&lt;/p&gt;</comment>
                    <comment id="13644878" author="mikemccand" created="Mon, 29 Apr 2013 22:23:42 +0100">&lt;p&gt;Committed ... we can open new issues for the other data structures that consume RAM ...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580909" name="LUCENE-4966.patch" size="1990" author="mikemccand" created="Sun, 28 Apr 2013 23:04:03 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 29 Apr 2013 14:23:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325478</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325460</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4965] Add dynamic numeric range faceting</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4965</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The facet module today requires the app to compute the hierarchy&lt;br/&gt;
at index time, eg a timestamp field might use a year/month/day&lt;br/&gt;
hierarchy.&lt;/p&gt;

&lt;p&gt;While this gives great performance, since it minimizes the search-time&lt;br/&gt;
computation, sometimes it's unfortunately useful/necessary to do things entirely at&lt;br/&gt;
search time, like Solr does.&lt;/p&gt;

&lt;p&gt;E.g. I'm playing with a prototype Lucene search for Jira issues&lt;br/&gt;
and I'd like to add a drill down+sideways for "Updated in past day,&lt;br/&gt;
2 days, week, month" etc.  But because time is constantly advancing,&lt;br/&gt;
doing this at index time is a not easy ...&lt;/p&gt;</description>
                <environment/>
            <key id="12645050">LUCENE-4965</key>
            <summary>Add dynamic numeric range faceting</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sat, 27 Apr 2013 22:21:38 +0100</created>
                <updated>Tue, 30 Apr 2013 12:22:10 +0100</updated>
                    <resolved>Tue, 30 Apr 2013 12:22:10 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13643801" author="mikemccand" created="Sat, 27 Apr 2013 22:23:16 +0100">&lt;p&gt;Patch, just a start but the test passes &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; It uses a NumericDV field&lt;br/&gt;
and just does the obvious approach (load value for each hit and find&lt;br/&gt;
ranges that value falls within).  Once &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4964" title="Allow custom drill-down sub-queries"&gt;&lt;del&gt;LUCENE-4964&lt;/del&gt;&lt;/a&gt; is in I'll test the&lt;br/&gt;
integration with DrillDown/SidewaysQuery...&lt;/p&gt;</comment>
                    <comment id="13643912" author="shaie" created="Sun, 28 Apr 2013 06:47:46 +0100">&lt;p&gt;Looks great! Few comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I think the Accumulator should not take FSP, just the ndv field, for various reasons:
	&lt;ul&gt;
		&lt;li&gt;It will eliminate the need for the checks about the validity of FSP&lt;/li&gt;
		&lt;li&gt;When I read the test, it seemed awkward to me that you need to create a FSP and CountingFR, since 'field' is not a facet at all...&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;If you change FacetsAccumulator to not initialize a FacetArrays, you can safely pass null for taxonomy and FacetArrays&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think the only reason you override getAggregator is because FacetsCollector.create() calls it to determine if doc scores are needed?
	&lt;ul&gt;
		&lt;li&gt;Maybe we can have FacetsAccumulator.requiresDocScore() &amp;#8211; the default will call getAggregator().requireDocScore() and you will just return false? Then FC will use the accumulator to determine that.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Regarding the "nocommit int/float/double too", I think this class should be called DynamicLongRangeAccumulator since it can only handle longs. To handle int/float/double, you will need to override accumulate() entirely to pull a different DV and find the range.
	&lt;ul&gt;
		&lt;li&gt;If you make this class abstract, you can have a utility method which converts the ranges to FacetResultNodes.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;if (r.count &amp;gt; 0)&lt;/tt&gt; &amp;#8211; I think that's wrong? We should return all requested ranges, some may be with count=0. Just like we return in normal faceted search a FacetResult per FacetRequest.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Instead of addLongRange, can the ctor take an &lt;tt&gt;IndexReader&lt;/tt&gt; and &lt;tt&gt;LongRange...&lt;/tt&gt;? The ranges need to be final (there's no sense adding a new range after accumulate), and also, I think that the start/endInclusive logic may not be that simple when you come to handle a FloatRange.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13644028" author="mikemccand" created="Sun, 28 Apr 2013 15:25:16 +0100">&lt;p&gt;New patch incorporating Shai's feedback ...&lt;/p&gt;</comment>
                    <comment id="13644029" author="mikemccand" created="Sun, 28 Apr 2013 15:30:39 +0100">&lt;p&gt;Woops, trying again ...&lt;/p&gt;</comment>
                    <comment id="13644032" author="shaie" created="Sun, 28 Apr 2013 15:52:27 +0100">&lt;p&gt;Thanks Mike. Looks good! Few more comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The nocommit on default aggregator &amp;#8211; I think you can override and throw Unsupported?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;+1 to move to o.a.l.facet.dynamic, or oal.facet.range.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DynamicRange:
	&lt;ul&gt;
		&lt;li&gt;thanks for introducing accept(), but I don't think the way it's implemented can support other impls, such as float/double.&lt;/li&gt;
		&lt;li&gt;Perhapd instead DynamicRange should be abstract, with impls for LongRange, IntRange, FloatRange and DoubleRange, which can then implement accept properly.
		&lt;ul&gt;
			&lt;li&gt;It won't have min/max, but can keep label.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;Also, that will allow you to add type-safety to DynamicRangeFacetRequest. I.e. now someone can pass int and float ranges in the same request, which makes no sense. But if you make it 'typed', one would need to specify the type at construction, and cannot make mistakes.
		&lt;ul&gt;
			&lt;li&gt;DRFR would then be defined with &lt;tt&gt;&amp;lt;T extends DynamicRange&amp;gt;&lt;/tt&gt; and its ctor would take &lt;tt&gt;T...&lt;/tt&gt;.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think it's ok if the request is called DynamicRange without numerics because someone can write a DynamicRange impl not on numeric fields. He'll then need to also write an accumulator though, but as for the request, it's still about DynamicRange implementations ... in short, I'm +0 if it stays like that or renamed &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13644036" author="mikemccand" created="Sun, 28 Apr 2013 16:14:21 +0100">&lt;p&gt;Patch, just moving things under oal.facet.range.&lt;/p&gt;

&lt;p&gt;I'm ... not sure how to do the generics to support float/double/int/long, without just making separate Accumulator (eg Int/Float/Double/LongRangeAccumulator), since I need to use native types for each case.  &lt;/p&gt;

&lt;p&gt;Also, I think ideally (maybe not on the first commit), we should use an interval tree to find overlapping ranges for each value ... so somehow we'd need to build this from all of the provided ranges up front.&lt;/p&gt;</comment>
                    <comment id="13644067" author="mikemccand" created="Sun, 28 Apr 2013 18:35:07 +0100">&lt;p&gt;New patch, breaking out separate .newLong/Float/DoubleRange.&lt;/p&gt;</comment>
                    <comment id="13644101" author="shaie" created="Sun, 28 Apr 2013 20:30:41 +0100">&lt;p&gt;Comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Range is not really extendable (private ctor) &amp;#8211; is that intentional? I guess there's not much point extending it ...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Any reason why not take primitive values and let the user define e.g Long.MAX_VALUE as an unlimited upper bound? I'm not a big fan of auto-boxing, and passing null is as good to me as passing MAX_VAL. User has to make a decision anyway, so might as well be explicit about it.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Accumulate() iterates at the outer loop on matchingDocs, and inner loop on ranges. I remember while writing FacetsAccumulator that luceneutil was happier with the other way (matchingDocs inner). Maybe test?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Shouldn't &lt;tt&gt;if (ranges.ranges.accept(v))&lt;/tt&gt; break if there's a match?
	&lt;ul&gt;
		&lt;li&gt;While at it, maybe RangeSet should sort the ranges by their minimum value? Not sure if asymptotically this will matter...&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I don't think it matters much, but maybe allocate &lt;tt&gt;new ArrayList&amp;lt;FacetResultNode&amp;gt;()&lt;/tt&gt; with &lt;tt&gt;ranges.ranges.length&lt;/tt&gt;?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think this we're almost ready!&lt;/p&gt;</comment>
                    <comment id="13644156" author="mikemccand" created="Sun, 28 Apr 2013 23:21:41 +0100">&lt;blockquote&gt;&lt;p&gt;Range is not really extendable (private ctor) – is that intentional? I guess there's not much point extending it ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suppose if an app had some custom encoding (into the long value in NumericDVField) then it would need a custom impl?  I'll make it protected...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Any reason why not take primitive values and let the user define e.g Long.MAX_VALUE as an unlimited upper bound? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was trying to mimic NumericRangeQuery, but I agree: let's just take primitives.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Accumulate() iterates at the outer loop on matchingDocs, and inner loop on ranges. I remember while writing FacetsAccumulator that luceneutil was happier with the other way (matchingDocs inner). Maybe test?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahhh right I forgot about that &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  I'll just switch it to inner loop on matchingDocs... I'm not quite set up to perf test this yet &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Shouldn't if (ranges.ranges.accept(v)) break if there's a match&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think not?  Ie overlapping ranges should be allowed?  For Jira search I'd like to have "Past day", "Past 2 days", etc.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;While at it, maybe RangeSet should sort the ranges by their minimum value? Not sure if asymptotically this will matter...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we need to use an interval tree here (I put a TODO) ... it's a little tricky to do that since each Range has its own private min/max ... but I think we can do this in a followon issue.&lt;/p&gt;</comment>
                    <comment id="13644157" author="mikemccand" created="Sun, 28 Apr 2013 23:23:06 +0100">&lt;p&gt;Patch w/ last round of fixes...&lt;/p&gt;</comment>
                    <comment id="13644279" author="shaie" created="Mon, 29 Apr 2013 06:34:01 +0100">&lt;blockquote&gt;&lt;p&gt;Ie overlapping ranges should be allowed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh, you're right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;it's a little tricky to do that since each Range has its own private min/max&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe Range can implement Comparable, and you build the tree using it, like we build PQ? In practice though, I wonder how much more efficient a tree would be vs simply sorting and iterating until a range is bigger than value? We're talking probably very few ranges no?&lt;/p&gt;

&lt;p&gt;BTW, I think Range would need to implement Comparable as well as compareTo(long) since a value may not fall into the first (sorted order) range (e.g. range.accept() returns false), but you'll need to continue looking until range.compareTo(long) &amp;gt; 0 (i.e. range.minValue &amp;gt; value).&lt;/p&gt;

&lt;p&gt;Also, maybe instead of FRN we should return a RangeFRN which contains the range? That way, someone can extract the min/max values of the range without parsing the label by casting to the range added? Hmm, but then you'll need to make the range impls public, but maybe that's not bad? Instead of Range.newLongRange, someone will just do &lt;tt&gt;new LongRange()&lt;/tt&gt;?&lt;/p&gt;

&lt;p&gt;I see you decided not to go with generics? In that case, maybe document that you are expected to pass the same Range type? Although, why not make RangeFacetRequest generic and prevent this pitfall?&lt;/p&gt;</comment>
                    <comment id="13644402" author="mikemccand" created="Mon, 29 Apr 2013 11:30:08 +0100">&lt;p&gt;New patch, adds generics to make sure the Range instances are the same class, and switches to simple ctors instead of .newXXXRange.&lt;/p&gt;

&lt;p&gt;I also added RangeFRN ... I only use it for the "sub nodes" and not the root node.&lt;/p&gt;

&lt;p&gt;I don't think we should try to do the sorting here ... I think that's too simplistic ... we should do an interval tree (but on a new issue).&lt;/p&gt;

&lt;p&gt;Eg the simplistic sorting approach can easily fail to speed things up if, say, most of my values are &amp;gt; max(range.max) (this will be the case with the Jira search) and you had sorted "smallest to biggest".&lt;/p&gt;</comment>
                    <comment id="13644419" author="shaie" created="Mon, 29 Apr 2013 12:03:51 +0100">&lt;p&gt;I agree we should optimize separately (and preferably after you have luceneutil set up so we can measure how important it really is).&lt;/p&gt;

&lt;p&gt;Thanks for adding generics.&lt;/p&gt;

&lt;p&gt;+1 to commit!&lt;/p&gt;</comment>
                    <comment id="13644900" author="mikemccand" created="Mon, 29 Apr 2013 22:33:38 +0100">&lt;p&gt;New patch, breaking out Long/Float/DoubleRange as standalone sources, adding Test/RangeFacetsExample, and adding randomized tests in TestRangeAccumulator ...&lt;/p&gt;</comment>
                    <comment id="13645375" author="shaie" created="Tue, 30 Apr 2013 09:04:24 +0100">&lt;p&gt;Awesome! And thanks for adding the examples!&lt;/p&gt;

&lt;p&gt;In TestRangeAccumulator:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;You don't use LongRange.accept (for expected counts) b/c that's what you test right? maybe just drop a one liner?&lt;/li&gt;
	&lt;li&gt;Also, does the test really need to run atLeast(100) iters? I would think one iteration is enough, if we let Jenkins periodic builds do the iters for us (plus, someone can run with -Dtests.iters or -Dtests.dups)&lt;/li&gt;
	&lt;li&gt;These apply to all 4 random test methods.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In RangeAccumulator:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I think you can remove "nocommit add to example"?&lt;/li&gt;
	&lt;li&gt;There are some leftover commented out lines (members and ctor)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In the example:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;new Date().getTime() can be just System.currentTimeMillis()?&lt;/li&gt;
	&lt;li&gt;Also, do we really need to index 1000 documents to demo? I ask because it's harder to debug 1000 docs &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I'm +1 to commit, no need to upload a new patch from my side.&lt;/p&gt;</comment>
                    <comment id="13645459" author="mikemccand" created="Tue, 30 Apr 2013 11:56:50 +0100">
&lt;blockquote&gt;&lt;p&gt;You don't use LongRange.accept (for expected counts) b/c that's what you test right? maybe just drop a one liner?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I'll add a comment.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, does the test really need to run atLeast(100) iters? I would think one iteration is enough, if we let Jenkins periodic builds do the iters for us (plus, someone can run with -Dtests.iters or -Dtests.dups)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well it's an efficiency question I guess (how many jenkins iters will&lt;br/&gt;
it take to find a bug) ... I have beasted it and it's passing for me&lt;br/&gt;
&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  I'll drop it to 10 ... the test is plenty fast.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think you can remove "nocommit add to example"?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;There are some leftover commented out lines (members and ctor)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Woops, fixed!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the example:&lt;br/&gt;
new Date().getTime() can be just System.currentTimeMillis()?&lt;br/&gt;
Also, do we really need to index 1000 documents to demo? I ask because it's harder to debug 1000 docs &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good, I'll switch to currentTimeMillis, and drop it to 100 docs.&lt;/p&gt;

&lt;p&gt;Thanks Shai.&lt;/p&gt;</comment>
                    <comment id="13645462" author="shaie" created="Tue, 30 Apr 2013 12:02:19 +0100">&lt;p&gt;Great thanks. +1!&lt;/p&gt;</comment>
                    <comment id="13645474" author="commit-tag-bot" created="Tue, 30 Apr 2013 12:17:34 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477560" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477560&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4965" title="Add dynamic numeric range faceting"&gt;&lt;del&gt;LUCENE-4965&lt;/del&gt;&lt;/a&gt;: add dynamic numeric range faceting&lt;/p&gt;</comment>
                    <comment id="13645476" author="commit-tag-bot" created="Tue, 30 Apr 2013 12:20:15 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477562" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477562&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4965" title="Add dynamic numeric range faceting"&gt;&lt;del&gt;LUCENE-4965&lt;/del&gt;&lt;/a&gt;: add dynamic numeric range faceting&lt;/p&gt;</comment>
                    <comment id="13645477" author="commit-tag-bot" created="Tue, 30 Apr 2013 12:20:29 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477563" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4965" title="Add dynamic numeric range faceting"&gt;&lt;del&gt;LUCENE-4965&lt;/del&gt;&lt;/a&gt;: add dynamic numeric range faceting&lt;/p&gt;</comment>
                    <comment id="13645478" author="commit-tag-bot" created="Tue, 30 Apr 2013 12:21:46 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477564" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477564&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4965" title="Add dynamic numeric range faceting"&gt;&lt;del&gt;LUCENE-4965&lt;/del&gt;&lt;/a&gt;: add dynamic numeric range faceting&lt;/p&gt;</comment>
                    <comment id="13645479" author="mikemccand" created="Tue, 30 Apr 2013 12:22:10 +0100">&lt;p&gt;Thanks Shai!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581043" name="LUCENE-4965.patch" size="50080" author="mikemccand" created="Mon, 29 Apr 2013 22:33:38 +0100"/>
                    <attachment id="12580958" name="LUCENE-4965.patch" size="25447" author="mikemccand" created="Mon, 29 Apr 2013 11:30:08 +0100"/>
                    <attachment id="12580913" name="LUCENE-4965.patch" size="24106" author="mikemccand" created="Sun, 28 Apr 2013 23:23:06 +0100"/>
                    <attachment id="12580896" name="LUCENE-4965.patch" size="23380" author="mikemccand" created="Sun, 28 Apr 2013 18:35:07 +0100"/>
                    <attachment id="12580889" name="LUCENE-4965.patch" size="15364" author="mikemccand" created="Sun, 28 Apr 2013 16:14:20 +0100"/>
                    <attachment id="12580886" name="LUCENE-4965.patch" size="15234" author="mikemccand" created="Sun, 28 Apr 2013 15:30:39 +0100"/>
                    <attachment id="12580885" name="LUCENE-4965.patch" size="8912" author="mikemccand" created="Sun, 28 Apr 2013 15:25:16 +0100"/>
                    <attachment id="12580848" name="LUCENE-4965.patch" size="8912" author="mikemccand" created="Sat, 27 Apr 2013 22:23:16 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 28 Apr 2013 05:47:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325423</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325405</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4964] Allow custom drill-down sub-queries</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4964</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today the facet module indexes a term for each facet added to a&lt;br/&gt;
document, and DrillDown/SidewaysQuery assume this by creating a&lt;br/&gt;
TermQuery, or OR of TermQuery, for each dimension the app drills down&lt;br/&gt;
on.&lt;/p&gt;

&lt;p&gt;I think we should relax this and allow an &lt;span class="error"&gt;&amp;#91;expert&amp;#93;&lt;/span&gt; arbitrary query to&lt;br/&gt;
drill down on a given dimension ... e.g., this can enable future&lt;br/&gt;
dynamic faceting methods, or custom app drill-down methods.&lt;/p&gt;

&lt;p&gt;It's easy for DrillDownQuery to do this, but requires generalization&lt;br/&gt;
in DrillSideways, basically just reviving the first approach on&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4748" title="Add DrillSideways helper class to Lucene facets module"&gt;&lt;del&gt;LUCENE-4748&lt;/del&gt;&lt;/a&gt;.  This approach is somewhat slower, but more general&lt;br/&gt;
... it will keep using the current method as an optimization when it&lt;br/&gt;
applies.&lt;/p&gt;

&lt;p&gt;This should also fix the possible performance regression from&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4952" title="DrillSideways should expose &amp;quot;scoreSubDocsAtOnce&amp;quot; control"&gt;&lt;del&gt;LUCENE-4952&lt;/del&gt;&lt;/a&gt; when scoreSubDocsAtOnce is true, by using the&lt;br/&gt;
MinShouldMatchSumScorer in that case.&lt;/p&gt;</description>
                <environment/>
            <key id="12645049">LUCENE-4964</key>
            <summary>Allow custom drill-down sub-queries</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sat, 27 Apr 2013 21:43:11 +0100</created>
                <updated>Mon, 29 Apr 2013 21:17:04 +0100</updated>
                    <resolved>Mon, 29 Apr 2013 21:17:04 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/facet</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13643792" author="mikemccand" created="Sat, 27 Apr 2013 21:44:55 +0100">&lt;p&gt;Patch ... I think it's ready.&lt;/p&gt;</comment>
                    <comment id="13644812" author="commit-tag-bot" created="Mon, 29 Apr 2013 21:12:11 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477315" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477315&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4964" title="Allow custom drill-down sub-queries"&gt;&lt;del&gt;LUCENE-4964&lt;/del&gt;&lt;/a&gt;: allow custom per-dimension drill-down queries&lt;/p&gt;</comment>
                    <comment id="13644815" author="commit-tag-bot" created="Mon, 29 Apr 2013 21:15:39 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477316" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1477316&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4964" title="Allow custom drill-down sub-queries"&gt;&lt;del&gt;LUCENE-4964&lt;/del&gt;&lt;/a&gt;: allow custom per-dimension drill-down queries&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580844" name="LUCENE-4964.patch" size="26962" author="mikemccand" created="Sat, 27 Apr 2013 21:44:55 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 29 Apr 2013 20:12:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325422</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325404</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4963] Deprecate broken TokenFilter constructors</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4963</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We have some TokenFilters which are only broken with specific options. This includes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;TrimFilter when updateOffsets=true&lt;/li&gt;
	&lt;li&gt;StopFilter, JapanesePartOfSpeechStopFilter, KeepWordFilter, LengthFilter, TypeTokenFilter when enablePositionIncrements=false&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think we should deprecate these behaviors in 4.4 and remove them in trunk.&lt;/p&gt;</description>
                <environment/>
            <key id="12645011">LUCENE-4963</key>
            <summary>Deprecate broken TokenFilter constructors</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="jpountz">Adrien Grand</reporter>
                        <labels>
                    </labels>
                <created>Sat, 27 Apr 2013 10:28:01 +0100</created>
                <updated>Sat, 4 May 2013 21:27:30 +0100</updated>
                    <resolved>Sat, 4 May 2013 21:27:30 +0100</resolved>
                                            <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13643617" author="thetaphi" created="Sat, 27 Apr 2013 10:40:56 +0100">&lt;p&gt;Hi,&lt;br/&gt;
+1 to deprecate the constuctors taking those arguments and make sure the alternatives already implement the correct behaviour.&lt;br/&gt;
Unfortunately we may not be able to completely remove that code in 5.x (we can remove the ctors, yes), because matchVersion sometimes implies the wrong behaviour. We nuked almost all of them, we should just check this.&lt;/p&gt;</comment>
                    <comment id="13645061" author="jpountz" created="Tue, 30 Apr 2013 01:06:36 +0100">&lt;p&gt;Thanks Uwe for the advice. Here is a first patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Deprecate constructors that expose broken options and make them throw an IllegalArgumentException when the lucene match version is &amp;gt;= 4.4&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Remove the same constructors from TestRandomChains' exclusion list.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Since enablePositionIncrements=true was used by the Analyzing and Fuzzy suggesters to ignore position holes, I had to make it an option in the suggesters themselves instead of the token streams.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;More documentation in the oal.analysis package: PositionLengthAttribute and guidelines on writing non-corrupt token streams.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13645330" author="thetaphi" created="Tue, 30 Apr 2013 07:56:42 +0100">&lt;p&gt;Hi Adrien,&lt;br/&gt;
thanks for doing the work! I think on the first look its fine! I will now verify the changes for each TokenFilter... The patch is for 4.x? In trunk you will remove the deprecations in a second step to enable merging without extra work?&lt;/p&gt;</comment>
                    <comment id="13645378" author="jpountz" created="Tue, 30 Apr 2013 09:10:19 +0100">&lt;p&gt;Hi Uwe, thanks for doing the review! The patch applies to trunk and I plan to remove deprecations in a second step. Is it OK with you?&lt;/p&gt;</comment>
                    <comment id="13648525" author="thetaphi" created="Fri, 3 May 2013 17:04:53 +0100">&lt;p&gt;Yeah, I would do it the same way. First deprecate and commit to both branches. Then remove stuff in trunk only.&lt;/p&gt;</comment>
                    <comment id="13648559" author="jpountz" created="Fri, 3 May 2013 17:56:42 +0100">&lt;p&gt;I'll commit this soon unless someone objects.&lt;/p&gt;</comment>
                    <comment id="13649130" author="commit-tag-bot" created="Sat, 4 May 2013 19:18:10 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479148" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479148&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4963" title="Deprecate broken TokenFilter constructors"&gt;&lt;del&gt;LUCENE-4963&lt;/del&gt;&lt;/a&gt;: Deprecate broken TokenFilter options.&lt;/p&gt;</comment>
                    <comment id="13649133" author="commit-tag-bot" created="Sat, 4 May 2013 19:33:40 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479151" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479151&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4963" title="Deprecate broken TokenFilter constructors"&gt;&lt;del&gt;LUCENE-4963&lt;/del&gt;&lt;/a&gt;: Deprecate broken TokenFilter options (merged from r1479148).&lt;/p&gt;</comment>
                    <comment id="13649159" author="commit-tag-bot" created="Sat, 4 May 2013 21:27:16 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479171" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1479171&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4963" title="Deprecate broken TokenFilter constructors"&gt;&lt;del&gt;LUCENE-4963&lt;/del&gt;&lt;/a&gt;: Completely remove deprecated options in 5.0.&lt;/p&gt;</comment>
                    <comment id="13649160" author="jpountz" created="Sat, 4 May 2013 21:27:30 +0100">&lt;p&gt;Thank you Uwe!&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                                <inwardlinks description="is part of">
                            <issuelink>
            <issuekey id="12624823">LUCENE-4641</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12581078" name="LUCENE-4963.patch" size="129992" author="jpountz" created="Tue, 30 Apr 2013 01:06:36 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 27 Apr 2013 09:40:56 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325384</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325366</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4961] Filters should return null if they don't accept documents</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4961</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today we document that Filter#getDocIdSet can return null if it doesn't accept documents. Infact in the code we sometimes return null and sometimes return DocIdSet.EMPTY_DOCIDSET. Conceptually there is nothing wrong with that but apparently we are not applying our optimisations accordingly ie. some parts of the code check for the EMPTY_DOCIDSET and all check for null. this is also a source of potential bugs like in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4940" title="ToParentBlockJoinQuery throws exception on empty parent filter DocIdSet"&gt;&lt;del&gt;LUCENE-4940&lt;/del&gt;&lt;/a&gt; and I think there are still problems in the ToChildBlock query.&lt;/p&gt;

&lt;p&gt;Anyways, I think we should be consistent here about when to apply the optimisations and for the sake of caching in CachingWrapperFilter we should make the EMPTY_DOCIDSET and impl detail.&lt;/p&gt;</description>
                <environment/>
            <key id="12644791">LUCENE-4961</key>
            <summary>Filters should return null if they don't accept documents</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 26 Apr 2013 09:45:35 +0100</created>
                <updated>Fri, 26 Apr 2013 15:58:41 +0100</updated>
                    <resolved>Fri, 26 Apr 2013 15:11:27 +0100</resolved>
                            <version>4.3</version>
                <version>4.2.1</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13642688" author="simonw" created="Fri, 26 Apr 2013 09:46:56 +0100">&lt;p&gt;here is a first patch&lt;/p&gt;</comment>
                    <comment id="13642735" author="mikemccand" created="Fri, 26 Apr 2013 11:50:03 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13642839" author="commit-tag-bot" created="Fri, 26 Apr 2013 14:30:08 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; simonw&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476185" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476185&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4961" title="Filters should return null if they don&amp;#39;t accept documents"&gt;&lt;del&gt;LUCENE-4961&lt;/del&gt;&lt;/a&gt;: Filters should return null if they don't accept documents&lt;/p&gt;</comment>
                    <comment id="13642868" author="commit-tag-bot" created="Fri, 26 Apr 2013 15:07:12 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; simonw&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476210" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476210&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4961" title="Filters should return null if they don&amp;#39;t accept documents"&gt;&lt;del&gt;LUCENE-4961&lt;/del&gt;&lt;/a&gt;: Filters should return null if they don't accept documents&lt;/p&gt;</comment>
                    <comment id="13642929" author="thetaphi" created="Fri, 26 Apr 2013 15:58:41 +0100">&lt;p&gt;Thanks! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Nice patch!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580678" name="LUCENE-4961.patch" size="32529" author="simonw" created="Fri, 26 Apr 2013 09:46:56 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 26 Apr 2013 10:50:03 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325165</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325147</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4960] Require minimum ivy version</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4960</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Someone on solr-user ran into a problem while trying to run 'ant idea' so they could work on Solr in their IDE.  &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=steve_rowe" class="user-hover" rel="steve_rowe"&gt;Steve Rowe&lt;/a&gt; indicated that this is probably due to &lt;a href="https://issues.apache.org/jira/browse/IVY-1194" title="Downloads from maven repository fail when using transparent HTTP proxies"&gt;&lt;del&gt;IVY-1194&lt;/del&gt;&lt;/a&gt;, requiring an ivy jar upgrade.&lt;/p&gt;

&lt;p&gt;The build system should check for a minimum ivy version, just like it does with ant.  The absolute minimum we require appears to be 2.2.0, but do we want to make it 2.3.0 due to &lt;a href="https://issues.apache.org/jira/browse/IVY-1388" title="*.lck files created by &amp;quot;artifact-lock&amp;quot; lock strategy are not cleaned up if ivy quits abruptly"&gt;&lt;del&gt;IVY-1388&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;I'm not sure how to go about checking the ivy version.  Checking the ant version is easy because it's ant itself that does the checking.&lt;/p&gt;

&lt;p&gt;There might be other component versions that should be checked too.&lt;/p&gt;</description>
                <environment/>
            <key id="12644646">LUCENE-4960</key>
            <summary>Require minimum ivy version</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="elyograg">Shawn Heisey</reporter>
                        <labels>
                    </labels>
                <created>Thu, 25 Apr 2013 19:16:42 +0100</created>
                <updated>Thu, 25 Apr 2013 19:16:42 +0100</updated>
                                    <version>4.2.1</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325020</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>325002</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4955] NGramTokenFilter increments positions for each gram</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4955</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;NGramTokenFilter increments positions for each gram rather for the actual token which can lead to rather funny problems especially with highlighting. if this filter should be used for highlighting is a different story but today this seems to be a common practice in many situations to highlight sub-term matches.&lt;/p&gt;

&lt;p&gt;I have a test for highlighting that uses ngram failing with a StringIOOB since tokens are sorted by position which causes offsets to be mixed up due to ngram token filter.&lt;/p&gt;</description>
                <environment/>
            <key id="12644385">LUCENE-4955</key>
            <summary>NGramTokenFilter increments positions for each gram</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Wed, 24 Apr 2013 17:40:42 +0100</created>
                <updated>Sun, 12 May 2013 02:01:27 +0100</updated>
                    <resolved>Fri, 26 Apr 2013 15:34:39 +0100</resolved>
                            <version>4.3</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13640636" author="simonw" created="Wed, 24 Apr 2013 17:48:22 +0100">&lt;p&gt;here is a patch including the deprecation and version support (I bet somebody relies on this behaviour) ...for trunk I will remove the deprecated methods but I added it here to make sure I don't miss it. &lt;/p&gt;

&lt;p&gt;the second patch is a test for the highlighter that produces a SIOOB without the patch but succeeds with the positions patch.&lt;/p&gt;</comment>
                    <comment id="13640657" author="jkrupan" created="Wed, 24 Apr 2013 18:07:46 +0100">&lt;p&gt;I think that ngram filter and edge-ngram filter are rather different cases.&lt;/p&gt;

&lt;p&gt;With edge-ngram it is abundantly clear that all of the edge ngrams "stack up" at the same position (at least for "front" edge ngrams!). But embedded ngrams seem more like a stretching out of the token, from one token to a sequence of tokens. Actually, it is k overlayed sequences, where k = maxGramSize minus minGramSize plus 1.&lt;/p&gt;

&lt;p&gt;I think the solution should be to have a "mode" which indicates whether the "intent" is merely variations (sub-tokens) for the token at the same position vs. a stretching the token into a sequence of tokens. Maybe call it "expansionMode": "stack" vs. "sequence".&lt;/p&gt;

&lt;p&gt;But even for the latter, I would definitely recommend that each of the k sequences should restart the position at the original token position.&lt;/p&gt;</comment>
                    <comment id="13641567" author="jpountz" created="Thu, 25 Apr 2013 09:29:16 +0100">&lt;p&gt;Given that offsets can't go backwards and that tokens in the same position must have the same start offset, I think that the only way to get NGramTokenFilter out of TestRandomChains' exclusion list (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4641" title="Fix analyzer bugs documented in TestRandomChains"&gt;LUCENE-4641&lt;/a&gt;) is to fix position increments (this issue), change the order tokens are emitted in (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3920" title="ngram tokenizer/filters create nonsense offsets if followed by a word combiner"&gt;&lt;del&gt;LUCENE-3920&lt;/del&gt;&lt;/a&gt;) and stop modifying offsets? I know some people rely on the current behavior but I think it's more important to get this filter out of TestRandomChains' exclusions since it causes highlighting bugs and makes the term vectors files unnecessary larger.&lt;/p&gt;</comment>
                    <comment id="13641704" author="rcmuir" created="Thu, 25 Apr 2013 13:01:51 +0100">&lt;p&gt;+1 Adrien. these analysis components should either be fixed or removed.&lt;/p&gt;

&lt;p&gt;We can speed up the process now by changing IndexWriter to reject this kinda bogus shit. We shouldnt be putting broken data into e.g. term vectors. That should encourage the fixing process.&lt;/p&gt;</comment>
                    <comment id="13641705" author="simonw" created="Thu, 25 Apr 2013 13:03:51 +0100">&lt;blockquote&gt;&lt;p&gt;We can speed up the process now by changing IndexWriter to reject this kinda bogus shit. We shouldnt be putting broken data into e.g. term vectors. That should encourage the fixing process.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I updated the highlighter test and added analysis-common as a test dependency such that this can be run with ant.&lt;/p&gt;</comment>
                    <comment id="13641706" author="jpountz" created="Thu, 25 Apr 2013 13:05:16 +0100">&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I'll work on fixing NGramTokenizer and NGramTokenFilter.&lt;/p&gt;</comment>
                    <comment id="13641709" author="rcmuir" created="Thu, 25 Apr 2013 13:06:57 +0100">&lt;p&gt;I don't think we should add analysis-common as a test dependency to the highlighter. I worked pretty hard to clean all this up with e.g. mocktokenizer so we didnt have dependency hell. It also keeps our tests clean.&lt;/p&gt;</comment>
                    <comment id="13641710" author="simonw" created="Thu, 25 Apr 2013 13:10:45 +0100">&lt;p&gt;robert I agree, I added this as sep. patch to make sure that whatever we commit here we can at least test that the ngram filter doesn't throw an IOOB anymore. I just wanted to make it easier to run the test.&lt;/p&gt;</comment>
                    <comment id="13642424" author="jpountz" created="Fri, 26 Apr 2013 01:21:23 +0100">&lt;p&gt;I tried to iterate on Simon's patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;NGramTokenFilter doesn't modify offsets and emits all n-grams of a single term at the same position&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;NGramTokenizer uses a sliding window.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;NGramTokenizer and NGramTokenFilter removed from TestRandomChains exclusions.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It was very hard to add the compatibility version support to NGramTokenizer so there are now two distinct classes and the factory picks the right one depending on the Lucene match version.&lt;/p&gt;

&lt;p&gt;Simon's highlighting test now fails because the highlighted content is different, but not because of a broken token stream.&lt;/p&gt;</comment>
                    <comment id="13642744" author="commit-tag-bot" created="Fri, 26 Apr 2013 12:03:28 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476135" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476135&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4955" title="NGramTokenFilter increments positions for each gram"&gt;&lt;del&gt;LUCENE-4955&lt;/del&gt;&lt;/a&gt;: Fix NGramTokenizer and NGramTokenFilter, and remove them from TestRandomChains' exclusion list.&lt;/p&gt;</comment>
                    <comment id="13642772" author="commit-tag-bot" created="Fri, 26 Apr 2013 13:14:44 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476159" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1476159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4955" title="NGramTokenFilter increments positions for each gram"&gt;&lt;del&gt;LUCENE-4955&lt;/del&gt;&lt;/a&gt;: Fix NGramTokenizer and NGramTokenFilter, and remove them from TestRandomChains' exclusion list (merged from r1476135).&lt;/p&gt;

&lt;p&gt;In addition to the trunk changes, I had to fix SlowSynonymFilterFactory to reset the token stream before consuming it.&lt;/p&gt;</comment>
                    <comment id="13654967" author="steve_rowe" created="Fri, 10 May 2013 23:49:14 +0100">&lt;p&gt;If there are no objections, I'd like to backport this to 4.3.1.&lt;/p&gt;</comment>
                    <comment id="13655346" author="simonw" created="Sat, 11 May 2013 20:32:31 +0100">&lt;p&gt;Steve, I don't think we should do this unless we add a Version.LUCENE_4_3_1. The problem here is that this filter would introduce a totally different behaviour compared to the 4.3 version that we released. If you build your index with 4.3 you would need to reindex if you switch to 4.3.1 unless we add the version so we can fallback on the old behaviour. Makes sense?&lt;/p&gt;</comment>
                    <comment id="13655415" author="steve_rowe" created="Sun, 12 May 2013 02:00:20 +0100">&lt;blockquote&gt;
&lt;p&gt;Steve, I don't think we should do this unless we add a Version.LUCENE_4_3_1. The problem here is that this filter would introduce a totally different behaviour compared to the 4.3 version that we released. If you build your index with 4.3 you would need to reindex if you switch to 4.3.1 unless we add the version so we can fallback on the old behaviour. Makes sense?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, Simon.  I'll remove the lucene-4.3.1-candidate label.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12548050">LUCENE-3920</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12548050">LUCENE-3920</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12580521" name="highlighter-test.patch" size="4196" author="simonw" created="Thu, 25 Apr 2013 13:03:51 +0100"/>
                    <attachment id="12580305" name="highlighter-test.patch" size="3394" author="simonw" created="Wed, 24 Apr 2013 17:48:22 +0100"/>
                    <attachment id="12580631" name="LUCENE-4955.patch" size="42684" author="jpountz" created="Fri, 26 Apr 2013 01:21:23 +0100"/>
                    <attachment id="12580304" name="LUCENE-4955.patch" size="12088" author="simonw" created="Wed, 24 Apr 2013 17:48:22 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 24 Apr 2013 17:07:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324759</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324741</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4954] LuceneTestFramework fails to catch temporary FieldCache insanity</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4954</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Ever since we added readerClosedListeners to evict FieldCache entries, LTC will no longer detect insanity as long as the test closes all readers leading to insanity ...&lt;/p&gt;

&lt;p&gt;So this has weakened our testing of catching accidental insanity producing code.&lt;/p&gt;

&lt;p&gt;To fix this I think we could tap into FieldCacheImpl.setInfoStream ... and ensure the test didn't print anything to it.&lt;/p&gt;

&lt;p&gt;This was a spinoff from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4953" title="readerClosedListener is not invoked for ParallelCompositeReader&amp;#39;s leaves"&gt;&lt;del&gt;LUCENE-4953&lt;/del&gt;&lt;/a&gt;, where that test (AllGroupHeadsCollectorTest) is always producing insanity, but then because of a bug the FC eviction wasn't working right, and LTC then detected the insanity.&lt;/p&gt;</description>
                <environment/>
            <key id="12644362">LUCENE-4954</key>
            <summary>LuceneTestFramework fails to catch temporary FieldCache insanity</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Wed, 24 Apr 2013 15:29:35 +0100</created>
                <updated>Wed, 24 Apr 2013 15:29:35 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324736</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324718</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4952] DrillSideways should expose "scoreSubDocsAtOnce" control</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4952</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I hit this when running a ToParentBlockJoinCollector/Query under&lt;br/&gt;
DrillSideways ... the problem is ToParentBlockJoinCollector.collect&lt;br/&gt;
expects that all sub-scorers are positioned on the docID being&lt;br/&gt;
collected, but DrillSideways sometimes scores with a in-order&lt;br/&gt;
BooleanScorer-like scorer that advances each sub-scorer in chunks&lt;br/&gt;
... this breaks ToParentBlockJoinCollector.&lt;/p&gt;

&lt;p&gt;This is the same issue as &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2686" title="DisjunctionSumScorer should not call .score on sub scorers until consumer calls .score"&gt;&lt;del&gt;LUCENE-2686&lt;/del&gt;&lt;/a&gt;, where apps that want to peek at&lt;br/&gt;
the sub-scorers from their collector need those sub-scorers to all be&lt;br/&gt;
"on" the current docID being collected...&lt;/p&gt;

&lt;p&gt;One way to "fix" this would be to switch based on&lt;br/&gt;
Collector.acceptsDocsOutOfOrder() ... but that's really a hack ... it&lt;br/&gt;
only "works" for BooleanQuery because BooleanScorer is an out-of-order&lt;br/&gt;
scorer (well and because we fixed all BS2s to keep sub-scorers&lt;br/&gt;
positioned on the doc being collected in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3505" title="BooleanScorer2.freq() doesnt work unless you call score() first."&gt;&lt;del&gt;LUCENE-3505&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But if for example we added MUST clauses back into BooleanScorer&lt;br/&gt;
(which I think we should!) then it could easily score those queries&lt;br/&gt;
in-order.  Really we need another boolean (scoreSubDocsAtOnce or&lt;br/&gt;
something) to Weight.scorer... but that's a big change...&lt;/p&gt;

&lt;p&gt;I think for this issue I'll just add an expert protected method to&lt;br/&gt;
DrillSideways that returns this boolean, and an app could subclass to&lt;br/&gt;
override.  Apps that "know" they are using queries/collectors like&lt;br/&gt;
ToParentBlockJoinQuery/Collector must subclass and override&lt;br/&gt;
... DrillSideways already has other expert methods that you subclass &amp;amp;&lt;br/&gt;
override.&lt;/p&gt;</description>
                <environment/>
            <key id="12644119">LUCENE-4952</key>
            <summary>DrillSideways should expose "scoreSubDocsAtOnce" control</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 23 Apr 2013 14:43:18 +0100</created>
                <updated>Wed, 24 Apr 2013 23:17:41 +0100</updated>
                    <resolved>Wed, 24 Apr 2013 23:17:41 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13639209" author="mikemccand" created="Tue, 23 Apr 2013 17:39:49 +0100">&lt;p&gt;Patch, I think it's ready.&lt;/p&gt;

&lt;p&gt;I added expert protected method scoreSubDocsAtOnce, and fixed&lt;br/&gt;
TestDrillSideways.testRandom to sometimes return true here.&lt;/p&gt;

&lt;p&gt;I added an AssertingSubDocsAtOnceCollector that then verifies that all&lt;br/&gt;
sub-scorers are positioned on the same docID being collected ...&lt;/p&gt;</comment>
                    <comment id="13641047" author="commit-tag-bot" created="Wed, 24 Apr 2013 23:14:51 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471732" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471732&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4952" title="DrillSideways should expose &amp;quot;scoreSubDocsAtOnce&amp;quot; control"&gt;&lt;del&gt;LUCENE-4952&lt;/del&gt;&lt;/a&gt;: add method to force DrillSideways to keep all sub-scorers on the doc being collected&lt;/p&gt;</comment>
                    <comment id="13641051" author="commit-tag-bot" created="Wed, 24 Apr 2013 23:16:40 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471733" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471733&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4952" title="DrillSideways should expose &amp;quot;scoreSubDocsAtOnce&amp;quot; control"&gt;&lt;del&gt;LUCENE-4952&lt;/del&gt;&lt;/a&gt;: put CHANGES entry in the right place&lt;/p&gt;</comment>
                    <comment id="13641053" author="commit-tag-bot" created="Wed, 24 Apr 2013 23:17:28 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471735" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471735&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4952" title="DrillSideways should expose &amp;quot;scoreSubDocsAtOnce&amp;quot; control"&gt;&lt;del&gt;LUCENE-4952&lt;/del&gt;&lt;/a&gt;: add method to force DrillSideways to keep all sub-scorers on the doc being collected&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580068" name="LUCENE-4952.patch" size="10990" author="mikemccand" created="Tue, 23 Apr 2013 17:39:49 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 24 Apr 2013 22:14:51 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324493</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324474</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4951] DrillSidewaysScorer should use Scorer.cost instead of its own heuristic</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4951</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today it does the "first docID" trick to guess the cost of the&lt;br/&gt;
baseQuery, which is silly now that we have cost API.&lt;/p&gt;</description>
                <environment/>
            <key id="12644099">LUCENE-4951</key>
            <summary>DrillSidewaysScorer should use Scorer.cost instead of its own heuristic</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 23 Apr 2013 12:40:52 +0100</created>
                <updated>Wed, 24 Apr 2013 23:23:50 +0100</updated>
                    <resolved>Wed, 24 Apr 2013 22:47:33 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13638974" author="mikemccand" created="Tue, 23 Apr 2013 12:46:30 +0100">&lt;p&gt;Patch.&lt;/p&gt;</comment>
                    <comment id="13638975" author="mikemccand" created="Tue, 23 Apr 2013 12:47:33 +0100">&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff
       HighTermHardOrDD1        2.54      (2.6%)        1.25      (2.4%)  -50.9% ( -54% -  -47%)
       HighTermEasyOrDD2       18.02      (1.0%)        9.76      (0.8%)  -45.9% ( -47% -  -44%)
       HighTermHardOrDD2        2.42      (2.6%)        1.32      (1.6%)  -45.4% ( -48% -  -42%)
         HighTermHardDD1        3.13      (2.4%)        1.85      (1.7%)  -40.8% ( -43% -  -37%)
        MedTermEasyOrDD2       37.69      (1.6%)       22.31      (1.4%)  -40.8% ( -43% -  -38%)
        MedTermEasyOrDD1       29.78      (1.9%)       18.27      (1.4%)  -38.7% ( -41% -  -36%)
         HighTermEasyDD1        5.86      (2.5%)        3.73      (3.0%)  -36.3% ( -40% -  -31%)
       HighTermEasyOrDD1        6.37      (2.5%)        4.06      (1.5%)  -36.2% ( -39% -  -33%)
          MedTermEasyDD1       32.05      (1.9%)       21.38      (2.7%)  -33.3% ( -37% -  -29%)
        MedTermHardOrDD1       11.73      (2.9%)        7.84      (2.7%)  -33.2% ( -37% -  -28%)
        HighTermMixedDD2        4.50      (2.4%)        3.02      (1.3%)  -32.9% ( -35% -  -30%)
         HighTermHardDD2        1.98      (2.6%)        1.34      (1.5%)  -32.5% ( -35% -  -29%)
        LowTermEasyOrDD1       90.29      (0.9%)       60.98      (1.6%)  -32.5% ( -34% -  -30%)
        MedTermHardOrDD2        7.19      (2.9%)        5.05      (1.7%)  -29.8% ( -33% -  -25%)
         HighTermEasyDD2       27.17      (0.9%)       19.70      (0.9%)  -27.5% ( -29% -  -25%)
          LowTermEasyDD1      101.22      (0.8%)       80.80      (1.6%)  -20.2% ( -22% -  -17%)
          MedTermHardDD1       12.26      (2.8%)        9.82      (2.0%)  -19.9% ( -24% -  -15%)
          MedTermEasyDD2       55.60      (1.4%)       45.55      (1.2%)  -18.1% ( -20% -  -15%)
          LowTermHardDD2       29.13      (0.7%)       25.56      (1.2%)  -12.3% ( -14% -  -10%)
          MedTermHardDD2        7.21      (3.0%)        6.47      (1.7%)  -10.2% ( -14% -   -5%)
         MedTermMixedDD2       14.49      (4.4%)       13.52      (1.5%)   -6.7% ( -12% -    0%)
          LowTermEasyDD2      145.99      (3.3%)      148.04      (1.8%)    1.4% (  -3% -    6%)
        LowTermEasyOrDD2       65.37      (2.9%)       69.22      (2.1%)    5.9% (   0% -   11%)
        LowTermHardOrDD1       20.85      (4.8%)       27.88      (2.9%)   33.7% (  24% -   43%)
          LowTermHardDD1       22.05      (4.8%)       33.55      (2.0%)   52.2% (  43% -   61%)
        LowTermHardOrDD2       12.70      (5.3%)       19.86      (2.2%)   56.3% (  46% -   67%)
         LowTermMixedDD2       23.87      (6.5%)       47.12      (3.0%)   97.4% (  82% -  114%)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After the patch:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff
       HighTermHardOrDD1        2.56      (3.1%)        1.25      (1.9%)  -51.1% ( -54% -  -47%)
        MedTermEasyOrDD2       45.66      (3.4%)       22.35      (0.9%)  -51.0% ( -53% -  -48%)
       HighTermHardOrDD2        2.43      (3.1%)        1.31      (1.8%)  -46.0% ( -49% -  -42%)
        MedTermHardOrDD1       14.41      (2.2%)        7.86      (1.7%)  -45.4% ( -48% -  -42%)
        MedTermHardOrDD2        8.63      (1.8%)        5.04      (1.5%)  -41.6% ( -44% -  -39%)
         HighTermHardDD1        3.15      (3.0%)        1.86      (1.2%)  -40.8% ( -43% -  -37%)
        MedTermEasyOrDD1       30.07      (2.7%)       18.28      (1.2%)  -39.2% ( -41% -  -36%)
       HighTermEasyOrDD1        6.46      (3.3%)        4.07      (1.4%)  -37.1% ( -40% -  -33%)
         HighTermEasyDD1        5.95      (3.5%)        3.76      (1.3%)  -36.9% ( -40% -  -33%)
          MedTermHardDD1       15.24      (2.1%)        9.89      (1.2%)  -35.1% ( -37% -  -32%)
        HighTermMixedDD2        4.57      (3.1%)        3.02      (1.0%)  -34.0% ( -36% -  -30%)
          MedTermEasyDD1       32.38      (2.8%)       21.51      (1.2%)  -33.6% ( -36% -  -30%)
         HighTermHardDD2        1.99      (3.0%)        1.34      (1.3%)  -32.8% ( -36% -  -29%)
        LowTermEasyOrDD1       90.54      (1.1%)       61.22      (1.1%)  -32.4% ( -34% -  -30%)
        LowTermEasyOrDD2      101.27      (4.1%)       69.42      (1.1%)  -31.5% ( -35% -  -27%)
       HighTermEasyOrDD2       14.14      (4.5%)        9.70      (1.4%)  -31.4% ( -35% -  -26%)
        LowTermHardOrDD1       38.76      (1.2%)       27.93      (1.2%)  -28.0% ( -29% -  -25%)
         MedTermMixedDD2       18.71      (1.3%)       13.52      (1.0%)  -27.7% ( -29% -  -25%)
          MedTermHardDD2        8.84      (1.9%)        6.48      (1.2%)  -26.6% ( -29% -  -23%)
          MedTermEasyDD2       61.20      (4.3%)       45.55      (1.2%)  -25.6% ( -29% -  -21%)
        LowTermHardOrDD2       26.55      (0.9%)       19.86      (1.3%)  -25.2% ( -27% -  -23%)
          LowTermEasyDD2      187.68      (1.2%)      148.29      (1.3%)  -21.0% ( -23% -  -18%)
          LowTermEasyDD1      101.97      (1.5%)       80.92      (0.7%)  -20.6% ( -22% -  -18%)
          LowTermHardDD1       41.60      (0.8%)       33.70      (1.1%)  -19.0% ( -20% -  -17%)
          LowTermHardDD2       29.39      (1.0%)       25.58      (1.2%)  -13.0% ( -15% -  -10%)
         LowTermMixedDD2       53.89      (0.9%)       47.31      (1.2%)  -12.2% ( -14% -  -10%)
         HighTermEasyDD2       18.77      (5.3%)       19.72      (1.3%)    5.0% (  -1% -   12%)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ie, using cost API lets DrillSidewaysScorer do a better job picking&lt;br/&gt;
which scorer impl to use ...&lt;/p&gt;</comment>
                    <comment id="13640998" author="commit-tag-bot" created="Wed, 24 Apr 2013 22:43:59 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471705" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471705&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4951" title="DrillSidewaysScorer should use Scorer.cost instead of its own heuristic"&gt;&lt;del&gt;LUCENE-4951&lt;/del&gt;&lt;/a&gt;: DrillSideways now uses Scorer.cost() to decide which scorer impl to use&lt;/p&gt;</comment>
                    <comment id="13641000" author="commit-tag-bot" created="Wed, 24 Apr 2013 22:45:15 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471707" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471707&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4951" title="DrillSidewaysScorer should use Scorer.cost instead of its own heuristic"&gt;&lt;del&gt;LUCENE-4951&lt;/del&gt;&lt;/a&gt;: DrillSideways now uses Scorer.cost() to decide which scorer impl to use&lt;/p&gt;</comment>
                    <comment id="13641064" author="commit-tag-bot" created="Wed, 24 Apr 2013 23:20:23 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471738" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471738&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4951" title="DrillSidewaysScorer should use Scorer.cost instead of its own heuristic"&gt;&lt;del&gt;LUCENE-4951&lt;/del&gt;&lt;/a&gt;: cutover another freq -&amp;gt; cost&lt;/p&gt;</comment>
                    <comment id="13641066" author="commit-tag-bot" created="Wed, 24 Apr 2013 23:23:50 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; mikemccand&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471741" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471741&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4951" title="DrillSidewaysScorer should use Scorer.cost instead of its own heuristic"&gt;&lt;del&gt;LUCENE-4951&lt;/del&gt;&lt;/a&gt;: cutover another freq -&amp;gt; cost&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580027" name="LUCENE-4951.patch" size="1572" author="mikemccand" created="Tue, 23 Apr 2013 12:46:30 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 24 Apr 2013 21:43:59 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324473</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324454</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4950] AssertingIndexSearcher isn't wrapping the Collector to AssertingCollector</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4950</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description/>
                <environment/>
            <key id="12644007">LUCENE-4950</key>
            <summary>AssertingIndexSearcher isn't wrapping the Collector to AssertingCollector</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 22 Apr 2013 22:49:22 +0100</created>
                <updated>Tue, 23 Apr 2013 16:06:18 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13638475" author="mikemccand" created="Mon, 22 Apr 2013 22:53:10 +0100">&lt;p&gt;Current patch ... but some tests still failing, e.g. this fun one:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;[junit4:junit4]   2&amp;gt; NOTE: reproduce with: ant test  -Dtestcase=TestNumericRangeQuery32 -Dtests.method=testOneMatchQuery -Dtests.seed=CFA55692AE54C388 -Dtests.slow=true -Dtests.locale=sr_BA_#Latn -Dtests.timezone=US/Alaska -Dtests.file.encoding=ISO-8859-1
[junit4:junit4] FAILURE 0.08s | TestNumericRangeQuery32.testOneMatchQuery &amp;lt;&amp;lt;&amp;lt;
[junit4:junit4]    &amp;gt; Throwable #1: java.lang.AssertionError: class org.apache.lucene.search.ConstantScoreQuery$ConstantScorer: invalid initial doc id: 5
[junit4:junit4]    &amp;gt; 	at __randomizedtesting.SeedInfo.seed([CFA55692AE54C388:C04896C672DCE5CB]:0)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.index.AssertingAtomicReader$AssertingDocsEnum.&amp;lt;init&amp;gt;(AssertingAtomicReader.java:240)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.AssertingScorer.&amp;lt;init&amp;gt;(AssertingScorer.java:90)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.AssertingScorer.getAssertingScorer(AssertingScorer.java:71)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.AssertingCollector.setScorer(AssertingCollector.java:48)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer$1.setScorer(ConstantScoreQuery.java:220)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.Scorer.score(Scorer.java:85)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:254)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.AssertingScorer.score(AssertingScorer.java:125)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:616)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.AssertingIndexSearcher.search(AssertingIndexSearcher.java:93)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:480)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:436)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:273)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:261)
[junit4:junit4]    &amp;gt; 	at org.apache.lucene.search.TestNumericRangeQuery32.testOneMatchQuery(TestNumericRangeQuery32.java:215)
[junit4:junit4]    &amp;gt; 	at java.lang.Thread.run(Thread.java:722)
[junit4:junit4]   2&amp;gt; NOTE: test params are: codec=Lucene42: {field4=PostingsFormat(name=TestBloomFilteredLucene41Postings), field2147483647=PostingsFormat(name=TestBloomFilteredLucene41Postings), field2=PostingsFormat(name=SimpleText), ascfield8=MockVariableIntBlock(baseBlockSize=109), field8=PostingsFormat(name=TestBloomFilteredLucene41Postings), ascfield2=PostingsFormat(name=Memory doPackFST= false), ascfield4=MockVariableIntBlock(baseBlockSize=109)}, docValues:{}, sim=DefaultSimilarity, locale=sr_BA_#Latn, timezone=US/Alaska
[junit4:junit4]   2&amp;gt; NOTE: Linux 3.5.0-23-generic amd64/Oracle Corporation 1.7.0_06 (64-bit)/cpus=8,threads=1,free=210356704,total=247463936
[junit4:junit4]   2&amp;gt; NOTE: All tests run in this JVM: [TestNumericRangeQuery32]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think it's because of the sneakiness that ConstantScoreQuery does to enable out-of-order scoring ...&lt;/p&gt;</comment>
                    <comment id="13638487" author="mikemccand" created="Mon, 22 Apr 2013 23:03:01 +0100">&lt;p&gt;It looks like all the core test failures are due to ConstantScoreQuery ... but I'm at a loss how to fix it!&lt;/p&gt;</comment>
                    <comment id="13639098" author="rcmuir" created="Tue, 23 Apr 2013 15:22:23 +0100">&lt;p&gt;How about this:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;Index: lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java	(revision 1470976)
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java	(working copy)
@@ -87,10 +87,10 @@
     this.in = in;
     this.topScorer = topScorer;
     this.inOrder = inOrder;
-    this.docsEnumIn = new AssertingAtomicReader.AssertingDocsEnum(in, topScorer == TopScorer.NO);
     this.canCallNextDoc = topScorer != TopScorer.YES // not a top scorer
       || !SCORE_COLLECTOR_RANGE.isOverriddenAsOf(in.getClass()) // the default impl relies upon nextDoc()
       || !SCORE_COLLECTOR.isOverriddenAsOf(in.getClass()); // the default impl relies upon nextDoc()
+    this.docsEnumIn = new AssertingAtomicReader.AssertingDocsEnum(in, canCallNextDoc);
   }
 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13639111" author="mikemccand" created="Tue, 23 Apr 2013 15:39:09 +0100">&lt;p&gt;Hmm I'm still seeing exceptions in TestNumericRangeQuery32 even with that fix ...&lt;/p&gt;

&lt;p&gt;Maybe there's a real bug in NumericRangeQuery or ConstantScoreQuery?&lt;/p&gt;</comment>
                    <comment id="13639121" author="thetaphi" created="Tue, 23 Apr 2013 15:45:18 +0100">&lt;p&gt;Huch?&lt;br/&gt;
The sneakiness in ConstantScoreQuery could cause this. I have to rethink whats happening there. The idea is to enable the inner scorer's out of order. The sneakiness is there to allow out-of order scoring when wrapping BS1, otherwise CSQ would strip the scores but with the cost of turning it in-order.&lt;br/&gt;
NRQ is out of scope here, because query is already rewritten.&lt;/p&gt;</comment>
                    <comment id="13639138" author="rcmuir" created="Tue, 23 Apr 2013 16:06:18 +0100">&lt;p&gt;I think the bug is in ConstantScoreQuery. it wraps in setScorer() but also wraps in score().&lt;/p&gt;

&lt;p&gt;Maybe it would be easier, if it had two implementations: one where its a Scorer+OutOfOrder, and another more general DISI+InOrder. this could remove the extra wrapping.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12579912" name="LUCENE-4950.patch" size="1922" author="mikemccand" created="Mon, 22 Apr 2013 22:53:10 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 23 Apr 2013 14:22:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324381</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324362</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4946] Refactor SorterTemplate</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4946</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When working on TimSort (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4839" title="Sorter API: Use TimSort to sort doc IDs and postings lists"&gt;&lt;del&gt;LUCENE-4839&lt;/del&gt;&lt;/a&gt;), I was a little frustrated of not being able to add galloping support because it would have required to add new primitive operations in addition to compare and swap.&lt;/p&gt;

&lt;p&gt;I started working on a prototype that uses inheritance to allow some sorting algorithms to rely on additional primitive operations. You can have a look at &lt;a href="https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts" class="external-link"&gt;https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts&lt;/a&gt; (but beware it is a prototype and still misses proper documentation and good tests).&lt;/p&gt;

&lt;p&gt;I think it would offer several advantages:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;no more need to implement setPivot and comparePivot when using in-place merge sort or insertion sort,&lt;/li&gt;
	&lt;li&gt;the ability to use faster stable sorting algorithms at the cost of some memory overhead (our in-place merge sort is very slow),&lt;/li&gt;
	&lt;li&gt;the ability to implement properly algorithms that are useful on specific datasets but require different primitive operations (such as TimSort for partially-sorted data).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If you are interested in comparing these implementations with Arrays.sort, there is a Benchmark class in src/examples.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</description>
                <environment/>
            <key id="12643843">LUCENE-4946</key>
            <summary>Refactor SorterTemplate</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="5" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.png">Trivial</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="jpountz">Adrien Grand</reporter>
                        <labels>
                    </labels>
                <created>Sun, 21 Apr 2013 21:46:46 +0100</created>
                <updated>Fri, 3 May 2013 16:54:25 +0100</updated>
                    <resolved>Fri, 3 May 2013 16:54:25 +0100</resolved>
                                            <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13637646" author="dweiss" created="Sun, 21 Apr 2013 22:15:19 +0100">&lt;p&gt;Looks cool to me!&lt;/p&gt;</comment>
                    <comment id="13647931" author="jpountz" created="Thu, 2 May 2013 22:45:45 +0100">&lt;p&gt;This patch contains one base class Sorter and 3 implementations:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;IntroSorter (improved quicksort like we had before but I think the name is better since it makes it clear that the worst case complexity is O(n ln&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.gif" height="19" width="19" align="absmiddle" alt="" border="0"/&gt;) instead of O(n^2) as with traditional quicksort&lt;/li&gt;
	&lt;li&gt;InPlaceMergeSort, the merge sort we had before.&lt;/li&gt;
	&lt;li&gt;TimSort, an improved version of the previous implementation that can gallop to make sorting even faster on partially-sorted data.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;One major difference is that the end offsets are now exclusive. I tend to find it less confusing since you would now call &lt;tt&gt;sort(0, array.length)&lt;/tt&gt; instead of &lt;tt&gt;sort(0, array.length - 1)&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Please let me know if you would like to review the patch!&lt;/p&gt;</comment>
                    <comment id="13647953" author="jpountz" created="Thu, 2 May 2013 22:58:21 +0100">&lt;p&gt;Add missing @lucene.internal.&lt;/p&gt;</comment>
                    <comment id="13648001" author="thetaphi" created="Thu, 2 May 2013 23:55:30 +0100">&lt;p&gt;Hi Adrien,&lt;br/&gt;
thansk for the refactoring. The history of the SorterTemplate class going back to CGLIB is long and this is a really good idea. Its also useful for other projects, so its maybe a good idea to make a Apache Commons projects out of it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I scanned the patch, looks good. The from...to semantics are better now for the user. I think the original implementation used inclusive end because most implementations on the web were based on this. For me it always looked wrong, but I did not want to change it.&lt;/p&gt;

&lt;p&gt;I found some code duplication: To me it looks like ArrayUtil has a private re-implementation of ArrayIntroSorter which is a top-level class in oal.util. Could ArrayUtil not simply use that public impl instead? I know there are 2 implementations with Comparators and without comparators, just an idea! Maybe add a static final singleton NaturalComparator&amp;lt;T extends Comparable&amp;lt;? super T&amp;gt;&amp;gt; that calls compareTo, so we dont need 2 implementations.&lt;/p&gt;

&lt;p&gt;I also like that you used timsort at places were the lists are already sorted in the common case (like Automatons).&lt;/p&gt;</comment>
                    <comment id="13648005" author="thetaphi" created="Thu, 2 May 2013 23:57:48 +0100">&lt;p&gt;We should remove the following from NOTICE.txt:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The class org.apache.lucene.util.SorterTemplate was inspired by CGLIB's class&lt;br/&gt;
with the same name. The implementation part is mainly done using pre-existing&lt;br/&gt;
Lucene sorting code. In-place stable mergesort was borrowed from CGLIB,&lt;br/&gt;
which is Apache-licensed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The new code has no similarity anymore to the original code - its a complete reimplementation. Only the "pattern" stayed alive (you have abstract class, where you have to implement the compare and swap ops).&lt;/p&gt;</comment>
                    <comment id="13648227" author="dweiss" created="Fri, 3 May 2013 07:49:31 +0100">&lt;blockquote&gt;&lt;p&gt;I think the original implementation used inclusive end because most implementations on the web were based on this. For me it always looked wrong, but I did not want to change it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I admit I am on the 'inclusive' side of things. To me sort(0..5) means sort elements between indexes 0 and 5, simple. There is also a side-effect of making it exclusive &amp;#8211; you can't sort the full array because an exclusive index on any end would overflow into negative values. I guess it's really a matter of taste in most cases. Perhaps the best way to change it would be to give (startIndex, elementsCount) which still reads (0, array.length) in most cases and does not have the problems mentioned above...&lt;/p&gt;</comment>
                    <comment id="13648271" author="jpountz" created="Fri, 3 May 2013 09:55:03 +0100">&lt;blockquote&gt;&lt;p&gt;Its also useful for other projects, so its maybe a good idea to make a Apache Commons projects out of it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why not. Or maybe use an already existing commons project such as commons collections? I'll dig that...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I found some code duplication&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'll fix that. The reason is that I modified ArrayUtil and CollectionUtil which have their own private Sorter implementations and then I added tests which required me to have concrete implementations in src/test. I'll merge them.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We should remove the following from NOTICE.txt&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'll fix that too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Perhaps the best way to change it would be to give (startIndex, elementsCount) which still reads (0, array.length) in most cases and does not have the problems mentioned above...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have no strong opinion about that. I think the reason I like the (from,to) option better is that List.subList and Arrays.copyOfRange have the same arguments. For example someone who wants to sort a sub-list with the JDK would do &lt;tt&gt;Collections.sort(list.subList(from,to))&lt;/tt&gt;. So I think it'd be nice to make directly translatable to &lt;tt&gt;new InPlaceMergeSorter() { compare/swap }.sort(from, to)&lt;/tt&gt;.&lt;/p&gt;</comment>
                    <comment id="13648358" author="jpountz" created="Fri, 3 May 2013 13:19:52 +0100">&lt;p&gt;New Patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;no more code duplication between ArrayUtil and the test classes&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;ArrayUtil exposes a NATURAL_COMPARATOR to sort arrays based on the natural order (for objects that implement Comparable)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Removed references to CGlib in the NOTICE.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13648373" author="thetaphi" created="Fri, 3 May 2013 13:49:37 +0100">&lt;p&gt;+1, looks good.&lt;/p&gt;

&lt;p&gt;About the from/to issue: The whole JDK collections API used from and exclusive to, so I agree with Adrien, we should do it in the same way. The overflow issue is no real issue as the meximum array size is limited, too &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; new byte&lt;span class="error"&gt;&amp;#91;Integer.MAX_VALUE&amp;#93;&lt;/span&gt; does not work on most JDKs.&lt;/p&gt;</comment>
                    <comment id="13648378" author="dweiss" created="Fri, 3 May 2013 13:56:50 +0100">&lt;p&gt;I still think inclusive ranges are more logical &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. For JDK subList and others the argument probably was that specifying inclusive zero-elements range becomes problematic with inclusive values.... so there's always something. I'm not objecting to choosing the "exclusive" option either, I'm just saying both options have their pros and cons.&lt;/p&gt;</comment>
                    <comment id="13648403" author="jpountz" created="Fri, 3 May 2013 14:32:31 +0100">&lt;blockquote&gt;&lt;p&gt;make a Apache Commons projects out of it&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I just left an email on their dev@ mailing-list to get their opinion about it: &lt;a href="http://markmail.org/message/if5cgarhavzuy45j" class="external-link"&gt;http://markmail.org/message/if5cgarhavzuy45j&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="13648405" author="commit-tag-bot" created="Fri, 3 May 2013 14:37:49 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478785" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478785&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4946" title="Refactor SorterTemplate"&gt;&lt;del&gt;LUCENE-4946&lt;/del&gt;&lt;/a&gt;: Refactor SorterTemplate (now Sorter).&lt;/p&gt;</comment>
                    <comment id="13648433" author="commit-tag-bot" created="Fri, 3 May 2013 15:11:17 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478801" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478801&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4946" title="Refactor SorterTemplate"&gt;&lt;del&gt;LUCENE-4946&lt;/del&gt;&lt;/a&gt;: Re-add the random-access checks that have been lost during refactoring.&lt;/p&gt;</comment>
                    <comment id="13648436" author="commit-tag-bot" created="Fri, 3 May 2013 15:15:19 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478802" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1478802&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4946" title="Refactor SorterTemplate"&gt;&lt;del&gt;LUCENE-4946&lt;/del&gt;&lt;/a&gt;: Refactor SorterTemplate (now Sorter) (merged from r1478785 and r1478801).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581687" name="LUCENE-4946.patch" size="132987" author="jpountz" created="Fri, 3 May 2013 13:19:52 +0100"/>
                    <attachment id="12581615" name="LUCENE-4946.patch" size="133049" author="jpountz" created="Thu, 2 May 2013 22:58:21 +0100"/>
                    <attachment id="12581611" name="LUCENE-4946.patch" size="132964" author="jpountz" created="Thu, 2 May 2013 22:45:45 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 21 Apr 2013 21:15:19 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324217</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324198</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4945] CustomScoreQuery's should have getters for its fields</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4945</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It would be convenient if CustomScoreQuery provided access to all its fields, not just isStrict().  Today I found that my subclass of it was forced to store redundant fields just to access the state.&lt;/p&gt;</description>
                <environment/>
            <key id="12643757">LUCENE-4945</key>
            <summary>CustomScoreQuery's should have getters for its fields</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="dsmiley">David Smiley</assignee>
                                <reporter username="dsmiley">David Smiley</reporter>
                        <labels>
                    </labels>
                <created>Sat, 20 Apr 2013 04:15:56 +0100</created>
                <updated>Tue, 23 Apr 2013 06:24:17 +0100</updated>
                    <resolved>Tue, 23 Apr 2013 06:24:17 +0100</resolved>
                                            <fixVersion>4.4</fixVersion>
                                <component>modules/join</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13637098" author="dsmiley" created="Sat, 20 Apr 2013 04:20:44 +0100">&lt;p&gt;I'll commit sometime within a few days.  Low risk thing, obviously.&lt;/p&gt;</comment>
                    <comment id="13638791" author="commit-tag-bot" created="Tue, 23 Apr 2013 06:21:15 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; dsmiley&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470816" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470816&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4945" title="CustomScoreQuery&amp;#39;s should have getters for its fields"&gt;&lt;del&gt;LUCENE-4945&lt;/del&gt;&lt;/a&gt;: added some getters to CustomScoreQuery&lt;/p&gt;</comment>
                    <comment id="13638794" author="commit-tag-bot" created="Tue, 23 Apr 2013 06:22:54 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; dsmiley&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470817" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470817&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4945" title="CustomScoreQuery&amp;#39;s should have getters for its fields"&gt;&lt;del&gt;LUCENE-4945&lt;/del&gt;&lt;/a&gt;: added some getters to CustomScoreQuery&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12579658" name="LUCENE-4945_CustomScoreQuery_getters.patch" size="909" author="dsmiley" created="Sat, 20 Apr 2013 04:20:44 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 23 Apr 2013 05:21:15 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324131</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>324112</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4943] remove 'Changes to Backwards Compatibility Policy' from lucene/CHANGES.txt</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4943</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;CHANGES.txt is useful to summarize the changes in a release. &lt;/p&gt;

&lt;p&gt;However its expected that a lot of changes will impact the APIs, this currently hurts the quality of CHANGES.txt because it leads to a significant portion of changes (whether they be bugs, features, whatever) being grouped under this one title.&lt;/p&gt;

&lt;p&gt;It also leads to descriptions of CHANGES being unnecessarily verbose.&lt;/p&gt;

&lt;p&gt;I think it makes CHANGES confusing and overwhelming, and it would be better to have a simpler 'upgrading' section with practical information on what you actually need to do (like Solr's CHANGES.txt).&lt;/p&gt;</description>
                <environment/>
            <key id="12643581">LUCENE-4943</key>
            <summary>remove 'Changes to Backwards Compatibility Policy' from lucene/CHANGES.txt</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Apr 2013 14:08:09 +0100</created>
                <updated>Fri, 19 Apr 2013 19:00:12 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13636350" author="thetaphi" created="Fri, 19 Apr 2013 14:14:57 +0100">&lt;p&gt;+1 to have a better upgrading section (and the original issues just spread around the other sections like API changes, new Features,...), but we have it somehow already: MIGRATE.txt (currently it only contains migration to 4.0 (or 5.0 on trunk).&lt;/p&gt;</comment>
                    <comment id="13636352" author="rcmuir" created="Fri, 19 Apr 2013 14:17:32 +0100">&lt;p&gt;I thought about MIGRATE.txt too, but I see its use for 'major releases'. Maybe we dont even need it any more long term and it was mainly just needed for 4.0 (hopefully we can get releases out faster so we dont need things like this).&lt;/p&gt;

&lt;p&gt;One advantage of the "Upgrading" section is that it would make the Lucene and Solr CHANGES files more consistent, and I think the changes2html parser would already work with it?&lt;/p&gt;</comment>
                    <comment id="13636375" author="thetaphi" created="Fri, 19 Apr 2013 14:46:47 +0100">&lt;p&gt;In my opinion, we should separate the migration/update guide completely from the changes.txt (also in solr).&lt;/p&gt;

&lt;p&gt;From the "usability" standpoint on the documentation homepage (index.xsl, the page generated by ant process-webpages) should have a separate link to this guide (like the MIGRATE.txt guide). By this a user gets a nice HTML-formatted guide.&lt;/p&gt;

&lt;p&gt;And the Markdown parser has more features to make nice HTML than changes2html.&lt;/p&gt;</comment>
                    <comment id="13636668" author="elyograg" created="Fri, 19 Apr 2013 19:00:12 +0100">&lt;blockquote&gt;&lt;p&gt;In my opinion, we should separate the migration/update guide completely from the changes.txt (also in solr).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Small note: The general open source convention seems to be INSTALL.txt and UPGRADE.txt ... when I attempted to help my boss upgrade his Lucene project from 3.x to 4.x, I was completely unaware of MIGRATE.txt, because I wasn't looking for that file.  Admittedly I was being shortsighted and had a PEBCAK moment when a helpful member of the dev list pointed me at the file, but I wonder how many other users miss it because of the name.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 19 Apr 2013 13:14:57 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>323955</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>323936</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4936] docvalues date compression</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4936</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;DocValues fields can be very wasteful if you are storing dates (like solr's TrieDateField does if you enable docvalues) and don't actually need all the precision: e.g. "date-only" fields like date of birth with no time component, time fields without milliseconds precision, and so on.&lt;/p&gt;

&lt;p&gt;Ideally we'd compute GCD of all the values to save space (numberOfTrailingZeros is not really enough here), but i think we should at least look for values like 86400000, 3600000, and 1000 to be practical.&lt;/p&gt;</description>
                <environment/>
            <key id="12642717">LUCENE-4936</key>
            <summary>docvalues date compression</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Tue, 16 Apr 2013 13:42:24 +0100</created>
                <updated>Mon, 29 Apr 2013 19:07:52 +0100</updated>
                    <resolved>Mon, 29 Apr 2013 19:07:52 +0100</resolved>
                                            <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13632801" author="rcmuir" created="Tue, 16 Apr 2013 13:43:22 +0100">&lt;p&gt;here's my hack patch... I think we should do something for DiskDV too though... and of course add tests &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13635270" author="jpountz" created="Thu, 18 Apr 2013 16:58:40 +0100">&lt;p&gt;Patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Adds MathUtil.gcd(long, long)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Adds "GCD compression" to Lucene42, Disk and CheapBastard.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Improves BaseDocValuesFormatTest which almost only tested "TABLE_COMPRESSED" with Lucene42DVF&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;No more attempts to compress storage when the values are known to be dense, such as SORTED ords.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I measured how slower doc values indexing is with these new checks, and it is completely unnoticeable with random or dense values since the GCD quickly reaches 1. When the GCD is larger, it only made indexing 2% slower (every doc has a single field which is a NumericDocValuesField). So I think it's fine.&lt;/p&gt;</comment>
                    <comment id="13635310" author="rcmuir" created="Thu, 18 Apr 2013 17:30:04 +0100">&lt;p&gt;Looks great! I'm glad you were able to make this fast.&lt;/p&gt;

&lt;p&gt;A few ideas:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I like the switch with corruption-check on DiskDV. Can we easily integrate this into Lucene42?&lt;/li&gt;
	&lt;li&gt;Can we update the file format docs (we attempt to describe the numerics strategies succinctly here)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I can do a more thorough review and some additional testing later, but this looks awesome.&lt;/p&gt;

&lt;p&gt;Later we should think about a place (maybe in codec file format docs, maybe even NumericDocValuesField?) to add some practical general guidelines to users, that might not otherwise be intuitive: Stuff like if you are putting Dates in NumericDV, zero out portions you dont care about (e.g. milliseconds, time, etc) to save space, indexing as UTC will be a little more efficient than with local offset, etc.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Improves BaseDocValuesFormatTest which almost only tested "TABLE_COMPRESSED" with Lucene42DVF&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah this is a good catch! We should also maybe open an issue to review DiskDV and try to make it more efficient. Optimizations like TABLE_COMPRESSED don't exist there I think: it could be handy if someone wants e.g. smallfloat scoring factor. Its nice this patch provides back compat for DiskDV but its not totally necessary in the future, if we want to review and rewrite it. In general that codec was just done very quickly and hasn't seen much benchmarking or anything: could use some work.&lt;/p&gt;</comment>
                    <comment id="13635329" author="rcmuir" created="Thu, 18 Apr 2013 17:44:01 +0100">&lt;blockquote&gt;
&lt;p&gt;indexing as UTC will be a little more efficient than with local offset, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We could probably solve issues like that too (maybe in something like cheap-bastard codec), if we did a first pass to compute min/max&lt;br/&gt;
and then did GCD only on the deltas from min... right?&lt;/p&gt;</comment>
                    <comment id="13635374" author="thetaphi" created="Thu, 18 Apr 2013 18:25:22 +0100">&lt;blockquote&gt;&lt;p&gt;indexing as UTC will be a little more efficient than with local offset, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you use a NumericField and store the long epoch in it, it is UTC as no localization involved.&lt;/p&gt;</comment>
                    <comment id="13635406" author="rcmuir" created="Thu, 18 Apr 2013 18:43:59 +0100">&lt;p&gt;But NumericField is totally unrelated to docvalues!&lt;/p&gt;

&lt;p&gt;Besides, delta+GCD has other applications than just GMT offset, e.g. solr's Currencyfield (at least in the US, people love prices like 199, 299, 399...): in that case it would save 9 bits per value where it would do nothing with the current patch.&lt;/p&gt;

&lt;p&gt;I'm not arguing the extra pass should be the in the default codec, i just said it might be interesting for cheap-bastard or something.&lt;/p&gt;</comment>
                    <comment id="13635423" author="rcmuir" created="Thu, 18 Apr 2013 18:51:36 +0100">&lt;p&gt;Sorry i was thinking about car prices when i said 9bpv, but you get the drift &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13635444" author="thetaphi" created="Thu, 18 Apr 2013 19:00:32 +0100">&lt;blockquote&gt;&lt;p&gt;But NumericField is totally unrelated to docvalues!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats clear. I just said, if you use a LONG docvalues field and store the long epoch its always timezone-less. That was what I wanted to say. This applies to Solr, too.&lt;/p&gt;</comment>
                    <comment id="13636361" author="jpountz" created="Fri, 19 Apr 2013 14:29:32 +0100">&lt;p&gt;New patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Computes the GCD based on deltas in order to be able to compress non-UTC dates.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Adds support for TABLE_COMPRESSED to DiskDVF.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Adds tests that ensure that these new compression methods are actually used whenever applicable.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Adds a quick description of the compression method to Lucene42DVF javadocs.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13636390" author="thetaphi" created="Fri, 19 Apr 2013 15:00:18 +0100">&lt;p&gt;This is really cool! I did not yet completely reviewed it, but i like that it is included in default codec, so user does not need to take care.&lt;/p&gt;</comment>
                    <comment id="13636406" author="jpountz" created="Fri, 19 Apr 2013 15:15:29 +0100">&lt;p&gt;Thank you Uwe! Unfortunately, I just figured out that the patch is broken when v - minValue overflows (in Consumer.addNumericField). I need to think about a way to fix it...&lt;/p&gt;</comment>
                    <comment id="13636488" author="jpountz" created="Fri, 19 Apr 2013 16:30:47 +0100">&lt;p&gt;Here is a work-around for the issue: the consumer stops trying to perform GCD compression as soon as it encounters a value outside the [ -MAX_VALUE/2 , MAX_VALE/2 ] range. This prevents overflows from happening and I can't think of a reasonable use-case that would benefit from GCD compression and have values outside of this range?&lt;/p&gt;</comment>
                    <comment id="13636500" author="rcmuir" created="Fri, 19 Apr 2013 16:41:07 +0100">&lt;p&gt;Yeah, i think those are ridiculously large numbers &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I'm gonna try to help review and play with the patch. I think this is great though.&lt;/p&gt;</comment>
                    <comment id="13636503" author="jpountz" created="Fri, 19 Apr 2013 16:44:47 +0100">&lt;p&gt;Thank you Robert, I'd love to have a review to make sure the patch is correct, especially for MathUtil.gcd and the DVConsumer.addNumericField logic.&lt;/p&gt;</comment>
                    <comment id="13636508" author="rcmuir" created="Fri, 19 Apr 2013 16:51:43 +0100">&lt;p&gt;First thing that sticks out is maybe to remove the extra pass? Even though it just pulls&lt;br/&gt;
the first value...&lt;/p&gt;

&lt;p&gt;For the DiskDV codec i feel its simple, just remove the loop and look for 'count == 0'.&lt;br/&gt;
For Lucene42, its probably best to just add 'count' for the same reason?&lt;/p&gt;

&lt;p&gt;But if it makes things more confusing, maybe just leave it the way it is. Its a little tricky either way &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13636537" author="jpountz" created="Fri, 19 Apr 2013 17:14:20 +0100">&lt;p&gt;Simple ideas are often the best ones, the new patch has a single loop! Thanks Robert!&lt;/p&gt;</comment>
                    <comment id="13636556" author="thetaphi" created="Fri, 19 Apr 2013 17:27:00 +0100">&lt;p&gt;Yeah the duplicate loop was strange much better now.&lt;/p&gt;

&lt;p&gt;One small thing:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-keyword"&gt;if&lt;/span&gt; (v &amp;lt; - &lt;span class="code-object"&gt;Long&lt;/span&gt;.MAX_VALUE / 2 || v &amp;gt; &lt;span class="code-object"&gt;Long&lt;/span&gt;.MAX_VALUE / 2) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;should maybe changed to, looks more logical to me...&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-keyword"&gt;if&lt;/span&gt; (v &amp;lt; &lt;span class="code-object"&gt;Long&lt;/span&gt;.MIN_VALUE / 2 || v &amp;gt; &lt;span class="code-object"&gt;Long&lt;/span&gt;.MAX_VALUE / 2) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13637252" author="rcmuir" created="Sat, 20 Apr 2013 14:51:04 +0100">&lt;p&gt;Same patch: I just uploaded a test.&lt;/p&gt;

&lt;p&gt;For DiskDVProducer, I think the compression tables and so on should be structured to be in the metadata section rather than the data section, to minimize the per-thread instance overhead.&lt;/p&gt;

&lt;p&gt;today its:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    &lt;span class="code-keyword"&gt;final&lt;/span&gt; IndexInput data = &lt;span class="code-keyword"&gt;this&lt;/span&gt;.data.clone();
    data.seek(entry.offset);

    &lt;span class="code-keyword"&gt;final&lt;/span&gt; BlockPackedReader reader = &lt;span class="code-keyword"&gt;new&lt;/span&gt; BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, &lt;span class="code-keyword"&gt;true&lt;/span&gt;);
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; &lt;span class="code-keyword"&gt;new&lt;/span&gt; LongNumericDocValues() {
      @Override
      &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;long&lt;/span&gt; get(&lt;span class="code-object"&gt;long&lt;/span&gt; id) {
        &lt;span class="code-keyword"&gt;return&lt;/span&gt; reader.get(id);
      }
    };
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I'll try to cut this stuff over...&lt;/p&gt;</comment>
                    <comment id="13637273" author="rcmuir" created="Sat, 20 Apr 2013 15:50:17 +0100">&lt;p&gt;Additionally in all cases we can move the corruption check to where we read the metadata. this means we detect problems earlier, rather than when someone asks for the field the first time. I'll do this too.&lt;/p&gt;</comment>
                    <comment id="13637281" author="rcmuir" created="Sat, 20 Apr 2013 16:17:31 +0100">&lt;p&gt;OK: patch with these changes: we write all this stuff in the metadata for diskdv (to reduce overhead), and also stronger/earlier checks on startup (lucene42, too). in getNumeric() i changed it to AssertionError since we already checked it at open time.&lt;/p&gt;

&lt;p&gt;additionally for all corrumption checks, i ensured we did:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (something) {
  &lt;span class="code-keyword"&gt;throw&lt;/span&gt; &lt;span class="code-keyword"&gt;new&lt;/span&gt; CorruptIndexException(&lt;span class="code-quote"&gt;"some message, input="&lt;/span&gt; + meta);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;passing the indexinput (meta/data) is very useful, in case someone hits the problem, they get a file name.&lt;/p&gt;

&lt;p&gt;I'll do some more review later...&lt;/p&gt;</comment>
                    <comment id="13637285" author="rcmuir" created="Sat, 20 Apr 2013 16:25:08 +0100">&lt;p&gt;I will generate backwards indexes from 4.2.0 now and commit them to branch_4x/trunk.&lt;/p&gt;

&lt;p&gt;It hurts nothing even if we don't commit this issue &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13637287" author="rcmuir" created="Sat, 20 Apr 2013 16:28:10 +0100">&lt;p&gt;Oh... i checked, i already did this. so when we commit this one, we can just generate 4.4 indexes.&lt;/p&gt;

&lt;p&gt;I think i generated these whenever we changed the format initially so we detect problems earlier than later...&lt;/p&gt;</comment>
                    <comment id="13637628" author="jpountz" created="Sun, 21 Apr 2013 20:37:12 +0100">&lt;p&gt;+1 to the proposed changes!&lt;/p&gt;

&lt;p&gt;Here is an updated patch that fixes the DVProducer constructors to open the data file and check the header in a try/finally block (so that data files are closed even if the header check fails).&lt;/p&gt;</comment>
                    <comment id="13638005" author="rcmuir" created="Mon, 22 Apr 2013 14:47:51 +0100">&lt;p&gt;+1: nice catch adrien.&lt;/p&gt;

&lt;p&gt;Why do we have this logic for TABLE_COMPRESSED in diskdv consumer?&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (uniqueValues != &lt;span class="code-keyword"&gt;null&lt;/span&gt;
        &amp;amp;&amp;amp; ((maxValue - minValue) &amp;lt; 0L || (maxValue - minValue) &amp;gt; 256)
        &amp;amp;&amp;amp; count &amp;lt;= &lt;span class="code-object"&gt;Integer&lt;/span&gt;.MAX_VALUE) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Shouldn't this just be:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (uniqueValues != &lt;span class="code-keyword"&gt;null&lt;/span&gt; &amp;amp;&amp;amp; count &amp;lt;= &lt;span class="code-object"&gt;Integer&lt;/span&gt;.MAX_VALUE) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We only care about the number of unique values, but in this case it does not matter what they actually are.&lt;/p&gt;</comment>
                    <comment id="13638090" author="jpountz" created="Mon, 22 Apr 2013 16:14:40 +0100">&lt;p&gt;I guess the point was to avoid one level of indirection in case all values can be stored using a single byte. Maybe "(maxValue - minValue) &amp;gt; 256" should be replaced with "(maxValue - minValue) &amp;gt;= uniqueValues.size()"? This would ensure that table compression isn't used if values are alreadu dense?&lt;/p&gt;</comment>
                    <comment id="13638102" author="rcmuir" created="Mon, 22 Apr 2013 16:29:01 +0100">&lt;p&gt;I see. In this case should we just take bitsRequired on both sides?&lt;/p&gt;</comment>
                    <comment id="13638114" author="jpountz" created="Mon, 22 Apr 2013 16:41:44 +0100">&lt;p&gt;One advantage of DELTA_COMPRESSED is that it uses different numbers of bits per value per block. Even if max-min=200, it could still happen that most blocks only require 6 or 7 bits per value. If there are many blocks, this could save substantial disk/memory.&lt;/p&gt;</comment>
                    <comment id="13638115" author="rcmuir" created="Mon, 22 Apr 2013 16:45:04 +0100">&lt;p&gt;Only in the case of outliers... but TABLE could do this too, if it sorted its table by increasing frequency of occurrence.&lt;/p&gt;</comment>
                    <comment id="13638117" author="jpountz" created="Mon, 22 Apr 2013 16:45:26 +0100">&lt;blockquote&gt;&lt;p&gt;In this case should we just take bitsRequired on both sides?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, this makes sense !&lt;/p&gt;</comment>
                    <comment id="13638118" author="rcmuir" created="Mon, 22 Apr 2013 16:45:51 +0100">&lt;p&gt;(decreasing rather)&lt;/p&gt;</comment>
                    <comment id="13638270" author="rcmuir" created="Mon, 22 Apr 2013 19:19:22 +0100">&lt;p&gt;OK i added the bitsRequired check in this patch... I think we are good to go here.&lt;/p&gt;

&lt;p&gt;+1 to commit&lt;/p&gt;</comment>
                    <comment id="13639053" author="commit-tag-bot" created="Tue, 23 Apr 2013 14:38:59 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470948" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470948&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4936" title="docvalues date compression"&gt;&lt;del&gt;LUCENE-4936&lt;/del&gt;&lt;/a&gt;: Improve numeric doc values compression (for dates especially).&lt;/p&gt;</comment>
                    <comment id="13639058" author="commit-tag-bot" created="Tue, 23 Apr 2013 14:44:47 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; jpountz&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470953" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1470953&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4936" title="docvalues date compression"&gt;&lt;del&gt;LUCENE-4936&lt;/del&gt;&lt;/a&gt;: Improve numeric doc values compression (for dates especially).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12579869" name="LUCENE-4936.patch" size="53804" author="rcmuir" created="Mon, 22 Apr 2013 19:19:22 +0100"/>
                    <attachment id="12579744" name="LUCENE-4936.patch" size="53762" author="jpountz" created="Sun, 21 Apr 2013 20:37:12 +0100"/>
                    <attachment id="12579679" name="LUCENE-4936.patch" size="51858" author="rcmuir" created="Sat, 20 Apr 2013 16:17:31 +0100"/>
                    <attachment id="12579675" name="LUCENE-4936.patch" size="50027" author="rcmuir" created="Sat, 20 Apr 2013 14:51:04 +0100"/>
                    <attachment id="12579565" name="LUCENE-4936.patch" size="48810" author="jpountz" created="Fri, 19 Apr 2013 17:14:20 +0100"/>
                    <attachment id="12579554" name="LUCENE-4936.patch" size="48975" author="jpountz" created="Fri, 19 Apr 2013 16:30:47 +0100"/>
                    <attachment id="12579538" name="LUCENE-4936.patch" size="48272" author="jpountz" created="Fri, 19 Apr 2013 14:29:32 +0100"/>
                    <attachment id="12579348" name="LUCENE-4936.patch" size="30572" author="jpountz" created="Thu, 18 Apr 2013 16:58:40 +0100"/>
                    <attachment id="12578935" name="LUCENE-4936.patch" size="4144" author="rcmuir" created="Tue, 16 Apr 2013 13:43:22 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>9.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 18 Apr 2013 15:58:40 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>323138</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>323119</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4901] TestIndexWriterOnJRECrash should work on any JRE vendor via Runtime.halt()</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4901</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I successfully compiled Lucene 4.2 with IBM.&lt;br/&gt;
Then ran unit tests with the nightly option set to "true"&lt;/p&gt;

&lt;p&gt;The test case TestIndexWriterOnJRECrash was skipped returning "IBM Corporation JRE not supported":&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt; Suite: org.apache.lucene.index.TestIndexWriterOnJRECrash&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt; IGNOR/A 0.28s | TestIndexWriterOnJRECrash.testNRTThreads&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt;    &amp;gt; Assumption #1: IBM Corporation JRE not supported.&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;junit4:junit4&amp;#93;&lt;/span&gt; Completed in 0.68s, 1 test, 1 skipped&lt;/p&gt;
</description>
                <environment>&lt;p&gt;Red Hat EL 6.3&lt;br/&gt;
IBM Java 1.6.0&lt;br/&gt;
ANT 1.9.0&lt;/p&gt;</environment>
            <key id="12640554">LUCENE-4901</key>
            <summary>TestIndexWriterOnJRECrash should work on any JRE vendor via Runtime.halt()</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="dweiss">Dawid Weiss</assignee>
                                <reporter username="rodrigotrujillo">Rodrigo Trujillo</reporter>
                        <labels>
                    </labels>
                <created>Wed, 3 Apr 2013 19:03:40 +0100</created>
                <updated>Wed, 15 May 2013 11:11:05 +0100</updated>
                    <resolved>Wed, 15 May 2013 11:11:05 +0100</resolved>
                                            <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13621126" author="rodrigotrujillo" created="Wed, 3 Apr 2013 19:07:03 +0100">&lt;p&gt;Test if IBM JRE is being used.&lt;br/&gt;
Avoid TestIndexWriterOnJRECrash be skipped&lt;/p&gt;</comment>
                    <comment id="13621127" author="dweiss" created="Wed, 3 Apr 2013 19:07:04 +0100">&lt;p&gt;I think I can actually handle this one &amp;#8211; I have JRE crash code for all major vendors (never thought I'd say that...). Let me see.&lt;/p&gt;</comment>
                    <comment id="13621131" author="rodrigotrujillo" created="Wed, 3 Apr 2013 19:10:58 +0100">&lt;p&gt;Hi Dawid,&lt;br/&gt;
just attached a patch.&lt;/p&gt;

&lt;p&gt;It worked fine in my tests.&lt;/p&gt;

&lt;p&gt;I wonder whether it could be applied to trunk too&lt;/p&gt;</comment>
                    <comment id="13621132" author="rcmuir" created="Wed, 3 Apr 2013 19:11:39 +0100">&lt;p&gt;I remember the current crash code worked for IBM, but there was some reason we disabled it.&lt;/p&gt;

&lt;p&gt;I am having trouble remembering the reason why: I think it was something crazy, like IBM being pretty verbose when it crashes and filling up your home directory with huge logs? I'm not sure about that though...&lt;/p&gt;</comment>
                    <comment id="13621136" author="dweiss" created="Wed, 3 Apr 2013 19:16:02 +0100">&lt;p&gt;IBM definitely dumps a &lt;b&gt;lot&lt;/b&gt; of stuff when it crashes. But isn't build/ cwd cleaned up on each build? I run such tests for randomizedtesting so if needed I can crash anything (J9, jrockit, hotspot). The way I do it is I link to a small snippet of native code (there are binaries for all platforms) which dereferences a null pointer. This core-dumps the JVM sort of by-design.&lt;/p&gt;</comment>
                    <comment id="13621138" author="rcmuir" created="Wed, 3 Apr 2013 19:16:32 +0100">&lt;p&gt;According to SVN logs, when i disabled this it was because:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;Log:
can't reliably crash recent IBM J9 jvms this way
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And i did this because occasionally, instead of crashing, it would cause the jvm to hang instead.&lt;/p&gt;

&lt;p&gt;so we need a better crash code for this JVM &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13621141" author="thetaphi" created="Wed, 3 Apr 2013 19:21:39 +0100">&lt;p&gt;This remembers me to upgrade IBM J9 on Policeman Jenkins and try Jenkins tests again. Currently the Lucene tests does not successfully pass with IBM J9 for every random seed, because IBM J9 has too many bugs (miscompiled bytecode/loops/...).&lt;/p&gt;</comment>
                    <comment id="13621142" author="dweiss" created="Wed, 3 Apr 2013 19:22:21 +0100">&lt;p&gt;It crashed for me on every build, always, on everything I've tried. Perhaps if it traps and there are some native threads elsewhere it can hang, don't know. &lt;/p&gt;</comment>
                    <comment id="13621145" author="dweiss" created="Wed, 3 Apr 2013 19:23:56 +0100">&lt;p&gt;Wait... I'm not saying to dereference an NPE via Unsafe &amp;#8211; this, indeed, doesn't always work. I'm talking about using a snippet of native code to do it (and this is bulletproof).&lt;/p&gt;</comment>
                    <comment id="13621147" author="rcmuir" created="Wed, 3 Apr 2013 19:27:32 +0100">&lt;p&gt;Right, but i'm not sure its worth it to bring in native code just for this test (its cheating, and would complicate a lot of things in the build for this one guy).&lt;/p&gt;

&lt;p&gt;i'm also unhappy with the test currently, because it doesn't even detect the bugs it was written to find (like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4738" title="Killed JVM when first commit was running will generate a corrupted index"&gt;&lt;del&gt;LUCENE-4738&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="13621149" author="rcmuir" created="Wed, 3 Apr 2013 19:29:19 +0100">&lt;p&gt;Maybe the 'alternative' crash should be just that... grab its own PID and fork off a process to kill -9 itself?&lt;/p&gt;</comment>
                    <comment id="13621151" author="dweiss" created="Wed, 3 Apr 2013 19:31:38 +0100">&lt;p&gt;The native bits are tiny and could be unpacked dynamically. But you're right &amp;#8211; it's a pain. I don't know if there's a kill -9 equivalent on Windows, otherwise you're right &amp;#8211; it'd be a nice solution. &lt;/p&gt;

&lt;p&gt;(Just remember it's impossible to get the PID of a forked process from Java below Java 8 &amp;#8211; you need to use the workarounds we use, pass the ID from the child back to the parent etc. Uglier than native bits I think).&lt;/p&gt;</comment>
                    <comment id="13621262" author="thetaphi" created="Wed, 3 Apr 2013 20:54:19 +0100">&lt;p&gt;I am not sure: Instead of crashing, maybe using Runtime.halt() from a parallel thread after some timeout? Runtime.halt() kills the JVM like kill -9 and does not run any shutdown hooks or let GC finish its work? Open files are closed in any case, because the kernel does this when the process exits (also on Unsafe's SIGSEGV).&lt;/p&gt;</comment>
                    <comment id="13621276" author="rcmuir" created="Wed, 3 Apr 2013 21:03:45 +0100">&lt;p&gt;If we change the test to do this, we need to give it a backdoor for the security manager i think.&lt;/p&gt;</comment>
                    <comment id="13621288" author="thetaphi" created="Wed, 3 Apr 2013 21:16:06 +0100">&lt;p&gt;This is a child JVM of the test, it has no security manager (at least it needs none)?&lt;/p&gt;</comment>
                    <comment id="13621862" author="dweiss" created="Thu, 4 Apr 2013 07:46:06 +0100">&lt;p&gt;Yep, system.halt will work here too. I just like it when it really breaks without the possibility to clean up properly &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; You get to see what each jvm dumps etc.&lt;/p&gt;

&lt;p&gt;But seriously, a halt from a background thread should indeed do fine and is portable. I did encounter very rare cases of halt() &lt;b&gt;not&lt;/b&gt; exiting under very low memory conditions but I don't think this is the case here.&lt;/p&gt;
</comment>
                    <comment id="13655627" author="dweiss" created="Sun, 12 May 2013 20:59:45 +0100">&lt;p&gt;I've removed the vendor-check assumption entirely and call Runtime.halt() instead of messing with Unsafe/ zero pointer dereference.&lt;/p&gt;

&lt;p&gt;Take a look, if there are no objections I'll commit it in.&lt;/p&gt;</comment>
                    <comment id="13656561" author="mikemccand" created="Tue, 14 May 2013 01:02:46 +0100">&lt;p&gt;Is Runtime.halt too "nice"?  Ie is there any chance it will do any cleanup at all?  (The javadocs seem to indicate no but still....).&lt;/p&gt;

&lt;p&gt;I like the crashing version because it's most "accurate" to what we are trying to test here ... ie rather than asking the JVM to shoot itself, we do the shooting.&lt;/p&gt;</comment>
                    <comment id="13656866" author="dweiss" created="Tue, 14 May 2013 08:08:19 +0100">&lt;p&gt;I think Runtime.halt is a very similar thing &amp;#8211; it (should) stop the threads immediately without any finalization blocks/ propagation of ThreadDeathException or otherwise. Any open file handles will be closed by the operating system on a "real" crash so I don't think this makes any difference. &lt;/p&gt;

&lt;p&gt;I do have native code that crashes J9 and JRockit too but this has side effects &amp;#8211; J9 dumps a &lt;b&gt;lot&lt;/b&gt; of debugging state information, for example. Right now we're testing just a subset of JVMs, if the tradeoff is to increase the coverage on other JVMs I think it's worth it to use Runtime.halt().&lt;/p&gt;</comment>
                    <comment id="13656931" author="mikemccand" created="Tue, 14 May 2013 10:53:18 +0100">&lt;p&gt;Maybe we could use our gun on the JVMs where it works / doesn't have heavy side effects, and Runtime.halt on all others?&lt;/p&gt;</comment>
                    <comment id="13656957" author="dweiss" created="Tue, 14 May 2013 11:51:39 +0100">&lt;p&gt;Sure.&lt;/p&gt;</comment>
                    <comment id="13658215" author="dweiss" created="Wed, 15 May 2013 11:10:03 +0100">&lt;p&gt;A version with runtime.halt() fallback for JVMs on which unsafe npe doesn't crash.&lt;/p&gt;

&lt;p&gt;I also fixed the stream pumping from subprocess; previously it could have hung if something was printed to stderr (due to blocked pipe).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12583307" name="LUCENE-4901.patch" size="5093" author="dweiss" created="Wed, 15 May 2013 11:10:03 +0100"/>
                    <attachment id="12582862" name="LUCENE-4901.patch" size="1728" author="dweiss" created="Sun, 12 May 2013 20:59:45 +0100"/>
                    <attachment id="12576822" name="test-IBM-java-vendor.patch" size="906" author="rodrigotrujillo" created="Wed, 3 Apr 2013 19:07:03 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 3 Apr 2013 18:07:04 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>321020</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>321001</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4872] BooleanWeight should decide how to execute minNrShouldMatch</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4872</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4571" title="speedup disjunction with minShouldMatch "&gt;&lt;del&gt;LUCENE-4571&lt;/del&gt;&lt;/a&gt; adds a dedicated document-at-time scorer for minNrShouldMatch which can use advance() behind the scenes. &lt;/p&gt;

&lt;p&gt;In cases where you have some really common terms and some rare ones this can be a huge performance improvement.&lt;/p&gt;

&lt;p&gt;On the other hand BooleanScorer might still be faster in some cases.&lt;/p&gt;

&lt;p&gt;We should think about what the logic should be here: one simple thing to do is to always use the new scorer when minShouldMatch is set: thats where i'm leaning. &lt;/p&gt;

&lt;p&gt;But maybe we could have a smarter heuristic too, perhaps based on cost()&lt;/p&gt;</description>
                <environment/>
            <key id="12638509">LUCENE-4872</key>
            <summary>BooleanWeight should decide how to execute minNrShouldMatch</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                    <parent id="12617445">LUCENE-4571</parent>
                        <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Fri, 22 Mar 2013 14:06:28 +0000</created>
                <updated>Fri, 10 May 2013 00:05:05 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="13610312" author="rcmuir" created="Fri, 22 Mar 2013 14:14:43 +0000">&lt;p&gt;To really do this right I think we need a better tasks file for luceneutil probably too.&lt;/p&gt;</comment>
                    <comment id="13612228" author="mikemccand" created="Sun, 24 Mar 2013 20:30:29 +0000">&lt;p&gt;I generated some random biggish minShouldMatch queries (attached), and then ran luceneutil perf test where base=trunk (always use BS2 if minShouldMatch &amp;gt; 1) and comp= always use BS1:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff
      20Terms15High20MSM      273.33      (2.5%)        0.87      (0.0%)  -99.7% ( -99% -  -99%)
      25Terms16High25MSM      328.44      (3.1%)        1.41      (0.0%)  -99.6% ( -99% -  -99%)
       15Terms7High15MSM      378.55      (2.5%)        1.66      (0.0%)  -99.6% ( -99% -  -99%)
      15Terms12High15MSM      437.86      (3.1%)        1.97      (0.0%)  -99.6% ( -99% -  -99%)
       25Terms8High25MSM      304.31      (3.0%)        1.46      (0.0%)  -99.5% ( -99% -  -99%)
       10Terms7High10MSM      560.31      (2.2%)        3.35      (0.0%)  -99.4% ( -99% -  -99%)
      15Terms11High15MSM      529.42      (3.0%)        3.98      (0.0%)  -99.2% ( -99% -  -99%)
      20Terms11High19MSM      214.71      (4.7%)        2.22      (0.0%)  -99.0% ( -99% -  -98%)
      20Terms15High19MSM      138.97      (5.7%)        1.46      (0.1%)  -98.9% ( -99% -  -98%)
      20Terms10High19MSM      377.77      (3.3%)        4.59      (0.1%)  -98.8% ( -98% -  -98%)
      25Terms10High24MSM      101.62      (5.7%)        1.31      (0.1%)  -98.7% ( -98% -  -98%)
       20Terms8High19MSM      108.52      (5.9%)        1.55      (0.1%)  -98.6% ( -98% -  -98%)
         5Terms2High5MSM      383.99      (1.4%)        5.48      (0.1%)  -98.6% ( -98% -  -98%)
         5Terms3High5MSM      355.19      (1.7%)        5.14      (0.1%)  -98.6% ( -98% -  -98%)
      25Terms14High24MSM      234.80      (3.8%)        3.43      (0.1%)  -98.5% ( -98% -  -98%)
       10Terms4High10MSM      802.64      (2.4%)       11.76      (0.1%)  -98.5% ( -98% -  -98%)
      15Terms11High14MSM      252.84      (5.0%)        4.07      (0.1%)  -98.4% ( -98% -  -98%)
       15Terms8High14MSM      286.52      (4.8%)        4.94      (0.1%)  -98.3% ( -98% -  -98%)
      25Terms16High22MSM       83.95      (5.6%)        1.46      (0.1%)  -98.3% ( -98% -  -98%)
      15Terms10High13MSM      200.36      (4.9%)        4.72      (0.1%)  -97.6% ( -97% -  -97%)
       25Terms2High25MSM      400.74      (2.9%)        9.84      (0.1%)  -97.5% ( -97% -  -97%)
       10Terms1High10MSM      443.03      (1.9%)       11.48      (0.1%)  -97.4% ( -97% -  -97%)
       25Terms7High23MSM      119.41      (4.9%)        3.38      (0.1%)  -97.2% ( -97% -  -96%)
         5Terms1High5MSM      791.03      (1.5%)       22.66      (0.1%)  -97.1% ( -97% -  -96%)
       25Terms6High22MSM       65.43      (6.5%)        2.16      (0.2%)  -96.7% ( -97% -  -96%)
       15Terms8High11MSM       49.38      (7.8%)        1.67      (0.2%)  -96.6% ( -97% -  -96%)
       15Terms7High12MSM       87.96      (5.9%)        3.55      (0.2%)  -96.0% ( -96% -  -95%)
      25Terms10High18MSM       55.03      (6.6%)        2.30      (0.2%)  -95.8% ( -96% -  -95%)
        10Terms4High8MSM      133.95      (5.9%)        5.67      (0.2%)  -95.8% ( -96% -  -95%)
        10Terms5High7MSM       62.52      (7.0%)        2.71      (0.2%)  -95.7% ( -96% -  -95%)
        10Terms3High8MSM       76.74      (6.8%)        3.51      (0.2%)  -95.4% ( -95% -  -94%)
       15Terms7High11MSM       73.30      (6.3%)        3.43      (0.2%)  -95.3% ( -95% -  -94%)
         5Terms4High5MSM      310.05      (2.4%)       14.54      (0.2%)  -95.3% ( -95% -  -94%)
       15Terms3High13MSM      256.05      (4.5%)       12.36      (0.2%)  -95.2% ( -95% -  -94%)
       25Terms9High19MSM      103.25      (5.3%)        5.02      (0.2%)  -95.1% ( -95% -  -94%)
       25Terms6High18MSM       36.26      (8.2%)        2.17      (0.3%)  -94.0% ( -94% -  -93%)
      25Terms14High15MSM       27.13      (5.4%)        1.70      (0.3%)  -93.7% ( -94% -  -93%)
        15Terms4High9MSM       43.14      (8.0%)        2.70      (0.3%)  -93.7% ( -94% -  -92%)
         5Terms2High4MSM       82.68      (7.1%)        5.65      (0.3%)  -93.2% ( -93% -  -92%)
        15Terms6High9MSM       55.67      (7.0%)        4.01      (0.3%)  -92.8% ( -93% -  -91%)
       15Terms3High12MSM       62.05      (7.0%)        4.51      (0.3%)  -92.7% ( -93% -  -91%)
       20Terms0High20MSM      597.72      (1.7%)       43.89      (0.2%)  -92.7% ( -92% -  -92%)
        10Terms3High6MSM       56.10      (7.5%)        4.13      (0.3%)  -92.6% ( -93% -  -91%)
       20Terms6High13MSM       55.80      (6.8%)        4.14      (0.3%)  -92.6% ( -93% -  -91%)
        10Terms4High6MSM       72.64      (6.5%)        5.95      (0.4%)  -91.8% ( -92% -  -90%)
       20Terms6High12MSM       51.01      (6.9%)        4.28      (0.4%)  -91.6% ( -92% -  -90%)
       20Terms8High10MSM       17.80      (7.4%)        1.53      (0.4%)  -91.4% ( -92% -  -90%)
       20Terms5High14MSM       87.56      (5.5%)        7.74      (0.4%)  -91.2% ( -92% -  -90%)
       15Terms2High13MSM      119.82      (5.5%)       11.57      (0.4%)  -90.3% ( -91% -  -89%)
       15Terms0High15MSM      548.28      (2.9%)       56.01      (0.3%)  -89.8% ( -90% -  -89%)
       20Terms3High15MSM      112.66      (5.2%)       11.93      (0.5%)  -89.4% ( -90% -  -88%)
      20Terms17High17MSM       23.13      (8.8%)        2.55      (0.5%)  -89.0% ( -90% -  -87%)
       20Terms3High13MSM       35.37      (8.4%)        4.09      (0.5%)  -88.4% ( -89% -  -86%)
       15Terms1High13MSM      204.25      (4.7%)       26.27      (0.5%)  -87.1% ( -88% -  -85%)
       25Terms9High10MSM       33.16      (5.3%)        4.59      (0.6%)  -86.2% ( -87% -  -84%)
       15Terms2High11MSM       89.91      (5.8%)       12.49      (0.6%)  -86.1% ( -87% -  -84%)
         5Terms3High4MSM       30.54      (3.8%)        4.35      (0.7%)  -85.8% ( -86% -  -84%)
         5Terms0High5MSM      843.58      (1.8%)      121.72      (0.6%)  -85.6% ( -86% -  -84%)
         5Terms1High4MSM      292.58      (5.9%)       45.87      (0.7%)  -84.3% ( -85% -  -82%)
        10Terms4High5MSM       37.67      (4.9%)        5.94      (0.8%)  -84.2% ( -85% -  -82%)
      15Terms12High12MSM       11.43      (9.4%)        1.90      (0.8%)  -83.4% ( -85% -  -80%)
        15Terms3High6MSM       43.32      (7.4%)        7.50      (0.8%)  -82.7% ( -84% -  -80%)
      15Terms10High10MSM       12.00      (8.4%)        2.14      (0.8%)  -82.2% ( -84% -  -79%)
        10Terms2High5MSM       41.81      (8.2%)        7.54      (0.9%)  -82.0% ( -84% -  -79%)
        15Terms2High7MSM       31.31      (8.4%)        5.83      (0.9%)  -81.4% ( -83% -  -78%)
        20Terms3High9MSM       61.79      (6.0%)       12.05      (0.8%)  -80.5% ( -82% -  -78%)
        10Terms1High6MSM       55.03      (7.8%)       11.26      (1.0%)  -79.5% ( -81% -  -76%)
         5Terms1High3MSM       73.74      (6.4%)       15.42      (1.0%)  -79.1% ( -81% -  -76%)
        15Terms2High6MSM       44.95      (7.2%)       10.28      (1.1%)  -77.1% ( -79% -  -74%)
         5Terms2High3MSM       24.52      (3.8%)        5.78      (1.1%)  -76.4% ( -78% -  -74%)
       20Terms1High13MSM       58.74      (6.4%)       16.13      (1.2%)  -72.5% ( -75% -  -69%)
        25Terms4High7MSM       37.41      (6.6%)       10.40      (1.2%)  -72.2% ( -75% -  -68%)
        15Terms1High9MSM       60.48      (6.8%)       17.85      (1.3%)  -70.5% ( -73% -  -66%)
        15Terms1High5MSM       22.75      (8.5%)        7.22      (1.5%)  -68.3% ( -72% -  -63%)
        20Terms2High8MSM       18.82      (8.2%)        6.33      (1.5%)  -66.4% ( -70% -  -61%)
       20Terms1High11MSM       50.55      (7.0%)       19.27      (1.6%)  -61.9% ( -65% -  -57%)
        10Terms6High6MSM        4.99      (7.3%)        2.02      (2.0%)  -59.4% ( -64% -  -53%)
         5Terms1High2MSM       73.14      (3.2%)       31.07      (2.1%)  -57.5% ( -60% -  -53%)
        20Terms4High5MSM        8.08      (5.7%)        3.47      (2.0%)  -57.0% ( -61% -  -52%)
        10Terms2High3MSM       14.55      (5.1%)        6.40      (2.1%)  -56.0% ( -60% -  -51%)
       15Terms0High11MSM      177.51      (5.1%)       78.53      (1.4%)  -55.8% ( -59% -  -51%)
        15Terms9High8MSM        9.83      (9.1%)        4.72      (2.3%)  -51.9% ( -58% -  -44%)
      20Terms13High11MSM        7.18      (9.0%)        3.51      (2.4%)  -51.1% ( -57% -  -43%)
        10Terms0High8MSM      142.40      (5.4%)       78.02      (1.9%)  -45.2% ( -49% -  -39%)
        10Terms4High4MSM       19.21      (8.9%)       11.12      (2.8%)  -42.1% ( -49% -  -33%)
        15Terms1High4MSM       19.08      (8.4%)       11.44      (2.8%)  -40.0% ( -47% -  -31%)
         5Terms0High4MSM      207.03      (5.9%)      124.76      (2.5%)  -39.7% ( -45% -  -33%)
        15Terms7High6MSM        5.02      (8.7%)        3.14      (3.0%)  -37.5% ( -45% -  -28%)
      20Terms15High12MSM        1.44      (7.1%)        0.90      (3.6%)  -37.2% ( -44% -  -28%)
        10Terms9High7MSM        8.39      (9.1%)        5.40      (3.1%)  -35.7% ( -43% -  -25%)
      15Terms13High10MSM        2.70      (8.5%)        1.75      (3.0%)  -35.2% ( -43% -  -25%)
       20Terms0High14MSM       42.75      (8.0%)       28.29      (2.4%)  -33.8% ( -40% -  -25%)
      25Terms21High16MSM        1.68      (7.6%)        1.12      (3.4%)  -33.4% ( -41% -  -24%)
        20Terms1High4MSM       45.87      (6.2%)       31.85      (2.6%)  -30.6% ( -37% -  -23%)
         5Terms4High4MSM        4.03      (8.6%)        2.93      (3.5%)  -27.2% ( -36% -  -16%)
         5Terms0High3MSM      211.23      (5.6%)      172.13      (3.4%)  -18.5% ( -26% -  -10%)
         5Terms3High3MSM        9.27      (9.8%)        7.81      (4.2%)  -15.7% ( -26% -   -1%)
      15Terms14High10MSM        1.11      (7.0%)        0.97      (5.0%)  -12.9% ( -23% -    0%)
      20Terms17High11MSM        2.96      (8.4%)        2.71      (4.3%)   -8.5% ( -19% -    4%)
        20Terms3High3MSM       14.55      (7.9%)       13.45      (4.1%)   -7.6% ( -18% -    4%)
        10Terms0High6MSM       78.35      (6.4%)       73.87      (3.3%)   -5.7% ( -14% -    4%)
        15Terms0High8MSM       56.41      (6.8%)       53.67      (3.1%)   -4.8% ( -13% -    5%)
      25Terms14High10MSM        0.95      (6.6%)        0.93      (5.4%)   -2.3% ( -13% -   10%)
       25Terms11High5MSM      623.50      (1.6%)      620.72      (1.4%)   -0.4% (  -3% -    2%)
       25Terms14High9MSM        2.99      (8.2%)        2.99      (4.7%)    0.1% ( -11% -   14%)
      20Terms17High10MSM        2.60      (8.3%)        2.67      (4.9%)    3.1% (  -9% -   17%)
        15Terms0High6MSM       70.68      (6.0%)       75.03      (3.6%)    6.2% (  -3% -   16%)
       20Terms13High9MSM        1.02      (6.9%)        1.08      (5.6%)    6.2% (  -5% -   20%)
      25Terms22High14MSM        1.02      (6.9%)        1.09      (5.7%)    6.8% (  -5% -   20%)
        15Terms5High4MSM        4.38      (8.4%)        4.69      (5.1%)    7.1% (  -5% -   22%)
        25Terms0High8MSM       42.57      (6.4%)       48.39      (3.1%)   13.7% (   3% -   24%)
        15Terms0High5MSM       64.99      (6.0%)       76.17      (4.0%)   17.2% (   6% -   28%)
       20Terms12High7MSM        1.59      (7.6%)        1.86      (5.5%)   17.3% (   3% -   32%)
       15Terms14High9MSM        0.78      (6.6%)        0.93      (6.9%)   20.2% (   6% -   36%)
        10Terms8High5MSM        5.14      (9.0%)        6.21      (5.9%)   20.9% (   5% -   39%)
      25Terms20High13MSM        0.58      (5.9%)        0.71      (7.5%)   21.6% (   7% -   37%)
         5Terms4High3MSM        8.61      (9.8%)       10.60      (6.1%)   23.2% (   6% -   43%)
         5Terms2High2MSM       11.50     (10.1%)       14.73      (6.2%)   28.1% (  10% -   49%)
        10Terms0High4MSM       36.94      (8.7%)       47.68      (4.8%)   29.1% (  14% -   46%)
      25Terms22High12MSM        0.82      (6.5%)        1.13      (7.1%)   38.3% (  23% -   55%)
        10Terms4High3MSM        3.38      (8.6%)        4.68      (6.6%)   38.7% (  21% -   58%)
        10Terms7High4MSM        1.25      (7.4%)        1.75      (6.6%)   40.5% (  24% -   58%)
         5Terms0High2MSM       60.10      (8.1%)       88.43      (5.2%)   47.1% (  31% -   65%)
        10Terms0High2MSM       47.57      (7.6%)       72.23      (5.1%)   51.9% (  36% -   69%)
        15Terms5High3MSM        2.52      (8.2%)        3.96      (7.4%)   56.8% (  38% -   78%)
         5Terms3High2MSM        4.25      (9.7%)        6.67      (7.5%)   57.0% (  36% -   82%)
       15Terms11High5MSM        2.63      (8.0%)        4.31      (7.7%)   63.7% (  44% -   86%)
        20Terms9High5MSM        0.87      (6.4%)        1.45      (8.7%)   67.7% (  49% -   88%)
        15Terms9High5MSM        0.82      (6.7%)        1.38      (8.1%)   68.6% (  50% -   89%)
         5Terms4High2MSM        5.49     (10.0%)       10.06      (8.9%)   83.4% (  58% -  113%)
      25Terms23High12MSM        0.35      (5.4%)        0.65     (11.6%)   84.2% (  63% -  106%)
        10Terms8High3MSM        1.29      (7.7%)        2.61      (9.6%)  102.5% (  79% -  129%)
        10Terms5High2MSM        5.07      (9.2%)       10.46      (9.8%)  106.3% (  79% -  138%)
        10Terms9High3MSM        1.12      (7.5%)        2.35      (9.8%)  110.8% (  87% -  138%)
        15Terms8High2MSM        2.56      (8.1%)        5.66     (10.0%)  121.5% (  95% -  151%)
        10Terms9High2MSM        2.07      (7.9%)        4.71     (10.4%)  127.5% ( 101% -  158%)
        15Terms9High2MSM        0.98      (6.7%)        2.26     (10.1%)  131.5% ( 107% -  158%)
       20Terms17High6MSM        0.31      (5.4%)        0.79     (14.8%)  151.9% ( 125% -  181%)
       25Terms12High2MSM        0.68      (5.8%)        1.73     (11.4%)  154.3% ( 129% -  182%)
       25Terms23High7MSM        0.40      (5.4%)        1.02     (13.3%)  155.3% ( 129% -  184%)
       25Terms22High5MSM        0.81      (6.2%)        2.12     (12.1%)  159.8% ( 133% -  190%)
       25Terms24High5MSM        0.32      (5.2%)        0.96     (15.7%)  203.4% ( 173% -  236%)
       25Terms22High2MSM        0.31      (4.6%)        0.97     (15.1%)  215.5% ( 187% -  246%)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each query draws from low and high freq terms, and eg 25Terms22High14MSM means there are 25 terms int he query, 22 of which are high freq, and minShouldMatch is 14.&lt;/p&gt;

&lt;p&gt;Net/net the lower the minShouldMatch, especially vs the number of high-freq terms, the better BS1 is.  But BS2 kicks butt otherwise!&lt;/p&gt;</comment>
                    <comment id="13612467" author="spo" created="Mon, 25 Mar 2013 08:34:29 +0000">&lt;p&gt;Thanks, Mike, this behaves as expected. Now we have a sense of what trade-off we'd be going for if we agree on the current model, it is still a hard decision though, entailing questions like:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Does it matter that queries that are anyway slow got 2-3 times slower?&lt;/li&gt;
	&lt;li&gt;Are those queries representative to what users do?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;A few suggestions for a better model which maybe goes beyond the scope of this ticket:&lt;/p&gt;

&lt;p&gt;A very conservative usage rule for MSMSumScorer would be to use it only if the constraint is at least one higher than the number of high-freq terms, then it will always "kick butt" and we'd get most bang of this scorer without having slow-downs. But we'd miss out on many cases where it would be faster and those might be the ones that are used in practice by users, and it is not clear (to me&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; what 'high-freq' means. If at all, this should be seen relative to the highest-freq subclause.&lt;/p&gt;

&lt;p&gt;More generally, it seems to me the problem we're trying to solve here is identical to computing a cost. If the cost returned by Scorers correlates with execution time, then we could simply call the cost() method on BS and MSMSumScorer and use MSMSumScorer if it is significantly below the former (assuming there are no side-effects in doing these calls). So we'd defer the problem to the individual Scorers, which splits the problem up into smaller subproblems and the Scorers know themselves best about their implementation and behavior.&lt;/p&gt;

&lt;p&gt;To make accurate decisions, we probably have to extend the cost-API to return more detailed information to base decision rules on, e.g. upper bound, lower bound (to be able to make conservative/speculative decisions) and estimate the number of returned docs &lt;b&gt;and&lt;/b&gt; runtime-correlated cost (in some unit). For instance, MSMSumScorer's overall cost depends on both of the latter and can be split up into the following 2 stages:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Candidate generation = heap-based merge of clause subset, i.e. the same as for DisjSumScorer, but on a clause subset:&lt;br/&gt;
time to generate all docs from subScorer: correlates with sum over costs of #clauses-(mm-1) least-costly subScorers&lt;br/&gt;
Number of candidates: &lt;span class="error"&gt;&amp;#91;max(...), min(sum(...), maxdoc)&amp;#93;&lt;/span&gt;, where ... can be either an upper bound, lower bound or an estimate in between of the #candidates returned by the #clauses-(mm-1) subScorers&lt;br/&gt;
Even for TermScorer, the definition of these two measures are not identical due to the min(..., maxdoc).&lt;/li&gt;
	&lt;li&gt;Full scoring of candidates:&lt;br/&gt;
time to advance() and decode postings: (mm-1) * # candidates&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The costs would still have to be weighted by the relative overhead of 1) heap-merging, 2) advance() + early-stopping; not sure, if constants are enough here.&lt;/p&gt;

&lt;p&gt;While the scope of this topic seems large (modelling all scorers), I currently don't see a simpler way to make this reliably work for arbitrarily structured queries, think of MSM(Disj(MSM(Conj(...),...),...),subtree2,...).&lt;/p&gt;</comment>
                    <comment id="13613035" author="mikemccand" created="Mon, 25 Mar 2013 20:01:06 +0000">&lt;p&gt;I really don't really know what the typical/common use cases are for&lt;br/&gt;
minShouldMatch.&lt;/p&gt;

&lt;p&gt;I agree we should err towards BS2, since it can be insanely faster&lt;br/&gt;
while BS1 can only be ~3X faster (on super-slow queries to begin&lt;br/&gt;
with), in this test anyway.&lt;/p&gt;

&lt;p&gt;A more accurate cost model for scorers would be awesome!  This could&lt;br/&gt;
be a general framework that we'd be able to use for various forms for&lt;br/&gt;
query optimizing (which we don't do today or do with heuristics), eg&lt;br/&gt;
things like whether to apply a filter (AND) high vs low, whether to&lt;br/&gt;
use BS1 or BS2 for pure conjunctions, when to split a PhraseQuery into&lt;br/&gt;
conjunction + position checking, flattening of nested boolean&lt;br/&gt;
queries, MultiTermQuery rewrite method, etc.  But probably we should&lt;br/&gt;
explore this on a new issue.&lt;/p&gt;</comment>
                    <comment id="13614371" author="spo" created="Tue, 26 Mar 2013 18:00:41 +0000">&lt;blockquote&gt;
&lt;p&gt;I really don't really know what the typical/common use cases are for minShouldMatch.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What about your own great work (&lt;a href="http://blog.mikemccandless.com/2013/02/drill-sideways-faceting-with-lucene.html" class="external-link"&gt;http://blog.mikemccandless.com/2013/02/drill-sideways-faceting-with-lucene.html&lt;/a&gt;) as a use-case to start with?&lt;br/&gt;
Maybe some consulting committers can also share some insight on how this is used in the wild.&lt;/p&gt;</comment>
                    <comment id="13614475" author="mikemccand" created="Tue, 26 Mar 2013 19:48:50 +0000">&lt;blockquote&gt;&lt;p&gt;What about your own great work (&lt;a href="http://blog.mikemccandless.com/2013/02/drill-sideways-faceting-with-lucene.html" class="external-link"&gt;http://blog.mikemccandless.com/2013/02/drill-sideways-faceting-with-lucene.html&lt;/a&gt;) as a use-case to start with?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thank Stefan &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  That's sort of a specialized (but very useful) use case I think ... and the minShouldMatch is always N-1.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe some consulting committers can also share some insight on how this is used in the wild.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, that'd be great to know!&lt;/p&gt;</comment>
                    <comment id="13614514" author="simonw" created="Tue, 26 Mar 2013 20:38:25 +0000">&lt;p&gt;I often use min_should_match in practice. Like one example is if you do search for titles or name like POI's or meta-data. Lets take youtube as an example you often get queries like "queen wembley live 1989" which was in-fact 1986 (at least the one I meant here) a pretty good pattern is to use some metric like 80% must match if &amp;gt;= 2 query terms etc. &lt;br/&gt;
Another good example is if you use shingles a query like "queen wembley live 1989" produces lots of terms and "wembley live" might be pretty common so you want to make sure that you are not returning stuff from other band but on the other hand a pure conjunction is not acceptable here either. &lt;/p&gt;

&lt;p&gt;hope that give some insight?&lt;/p&gt;</comment>
                    <comment id="13615006" author="eksdev" created="Wed, 27 Mar 2013 07:59:34 +0000">&lt;p&gt;the same pattern like Simon here, just having these terms wrapped in fuzzy/prefix query, often as dismax query. &lt;/p&gt;

&lt;p&gt;for example:&lt;br/&gt;
BQ(boo* OR hoo* OR whatever) with e.g. minShouldMatch = 2  &lt;/p&gt;

&lt;p&gt;So the only diff to Simon's case is that single boolean clauses are often more complicated then simple TermQuery &lt;/p&gt;</comment>
                    <comment id="13615951" author="rcmuir" created="Thu, 28 Mar 2013 01:19:44 +0000">&lt;blockquote&gt;
&lt;p&gt;I really don't really know what the typical/common use cases are for&lt;br/&gt;
minShouldMatch.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;One very practical thing is that solr queryparsers (probably elasticsearch has similar ones too?) such as dismax/edismax actually seem to be fully defined in terms of minShouldMatch (with the extremes being handled as OR and AND). &lt;/p&gt;

&lt;p&gt;I know Tom Burton-West has experimented with this some on chinese TREC data (he has some comments on &lt;a href="https://issues.apache.org/jira/browse/SOLR-3589" title="Edismax parser does not honor mm parameter if analyzer splits a token"&gt;&lt;del&gt;SOLR-3589&lt;/del&gt;&lt;/a&gt;), etc.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12575239" name="crazyMinShouldMatch.tasks" size="22900" author="mikemccand" created="Sun, 24 Mar 2013 20:30:29 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 24 Mar 2013 20:30:29 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>318990</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>318971</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4845] Add AnalyzingInfixSuggester</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4845</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Our current suggester impls do prefix matching of the incoming text&lt;br/&gt;
against all compiled suggestions, but in some cases it's useful to&lt;br/&gt;
allow infix matching.  E.g, Netflix does infix suggestions in their&lt;br/&gt;
search box.&lt;/p&gt;

&lt;p&gt;I did a straightforward impl, just using a normal Lucene index, and&lt;br/&gt;
using PostingsHighlighter to highlight matching tokens in the&lt;br/&gt;
suggestions.&lt;/p&gt;

&lt;p&gt;I think this likely only works well when your suggestions have a&lt;br/&gt;
strong prior ranking (weight input to build), eg Netflix knows&lt;br/&gt;
the popularity of movies.&lt;/p&gt;</description>
                <environment/>
            <key id="12637463">LUCENE-4845</key>
            <summary>Add AnalyzingInfixSuggester</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 17 Mar 2013 22:16:35 +0000</created>
                <updated>Sun, 26 May 2013 04:56:22 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/spellchecker</component>
                        <due/>
                    <votes>0</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="13604771" author="mikemccand" created="Sun, 17 Mar 2013 22:19:10 +0000">&lt;p&gt;Initial patch, lots of nocommits, only a basic test so far...&lt;/p&gt;</comment>
                    <comment id="13604772" author="mikemccand" created="Sun, 17 Mar 2013 22:20:35 +0000">&lt;p&gt;Screen shot showing suggestions for "hear".&lt;/p&gt;</comment>
                    <comment id="13604774" author="mikemccand" created="Sun, 17 Mar 2013 22:22:00 +0000">&lt;p&gt;This is an example of the infix suggestions: &lt;img src="https://issues.apache.org/jira/secure/attachment/12574091/12574091_infixSuggest.png" align="absmiddle" border="0" /&gt;&lt;/p&gt;</comment>
                    <comment id="13604777" author="rcmuir" created="Sun, 17 Mar 2013 22:33:33 +0000">&lt;p&gt;Wouldnt the straightforward impl be to put the suffixes of the suggestions into the FST?&lt;/p&gt;

&lt;p&gt;so for "this is a test" &lt;br/&gt;
you also add "is a test", "a test", ...&lt;/p&gt;

&lt;p&gt;I feel like this could be done with just a tokenfilter used only at build-time + analyzingsuggester, and would be more performant.&lt;/p&gt;</comment>
                    <comment id="13605768" author="mikemccand" created="Mon, 18 Mar 2013 22:51:42 +0000">&lt;p&gt;Another iteration:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I use SortingAtomicReader to sort all docs by weight (impact&lt;br/&gt;
    sorted postings), and then during the search I stop after&lt;br/&gt;
    collecting the first N docs.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I index leading ngrams up to a limit (default 4 characters) and&lt;br/&gt;
    use those instead of PrefixQuery when the last term is short.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I switched to a custom highlighter so prefix matches will always&lt;br/&gt;
    highlight correctly.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I tested on the FreeDB corpus (song titles) ... this is a pretty big&lt;br/&gt;
set of suggestions: 44.5 M songs across 3.2 M albums.  I pick a random&lt;br/&gt;
subset of the titles, and then test 2,4,6,8 length prefixes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;852 sec to build&lt;/li&gt;
	&lt;li&gt;3.7 GB index&lt;/li&gt;
	&lt;li&gt;Prefix 2: 50656.1 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 4: 1361.0 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 6: 7291.0 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 8: 5364.5 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 10: 4144.0 lookups/sec&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Eg AnalyzingSuggester (which doesn't highlight so it's not quite a&lt;br/&gt;
fair comparison):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;641 sec to build&lt;/li&gt;
	&lt;li&gt;2.1 GB FST&lt;/li&gt;
	&lt;li&gt;Prefix 2: 9719.3 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 4: 15750.2 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 6: 21491.4 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 8: 27453.4 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 10: 33168.4 lookups/sec&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So it's quite a bit slower than AnalyzingSuggester but I think it's&lt;br/&gt;
still plenty fast for most apps (this is perf for a single thread).&lt;/p&gt;</comment>
                    <comment id="13605769" author="mikemccand" created="Mon, 18 Mar 2013 22:53:43 +0000">&lt;blockquote&gt;&lt;p&gt;Wouldnt the straightforward impl be to put the suffixes of the suggestions into the FST?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think so ... but then I worry about the FST blowing up.  I guess if we limit how "deep" the infixing can work that would limit the FST size ... but I'd rather not have that limit.&lt;/p&gt;

&lt;p&gt;We should definitely try it ... it should be a lot faster.  I wonder how we could get highlighting working with AnalyzingSuggester.&lt;/p&gt;</comment>
                    <comment id="13605868" author="rcmuir" created="Tue, 19 Mar 2013 00:38:17 +0000">&lt;blockquote&gt;
&lt;p&gt;I think so ... but then I worry about the FST blowing up. I guess if we limit how "deep" the infixing can work that would limit the FST size ... but I'd rather not have that limit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But how is this any different than edge-ngrams up to a limit?&lt;/p&gt;

&lt;p&gt;With words of &amp;lt;= 4 chars, this suggester avoids the typical bad complexity you would get from an inverted index because the docids are pre-sorted in weight-order, so it can early terminate.&lt;/p&gt;

&lt;p&gt;But as soon as you type that 5th character: it can blow up. I'm not saying its likely, but can happen due to particulars of the content, for example if you had place names and you typed Shangh... and this prefix matches millions and millions of terms.&lt;/p&gt;</comment>
                    <comment id="13606088" author="rcmuir" created="Tue, 19 Mar 2013 05:40:49 +0000">&lt;p&gt;ok here's the start of a hack patch. my trivial test passes.&lt;/p&gt;

&lt;p&gt;the whole thing is a nocommit (and there is no depth bound for the infixing). &lt;/p&gt;

&lt;p&gt;Its also probably really slow and buggy &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13606089" author="rcmuir" created="Tue, 19 Mar 2013 05:41:07 +0000">&lt;p&gt;And this one is FuzzyAnalyzingInfixSuggester so you have to top that with your perf tests &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13606405" author="rcmuir" created="Tue, 19 Mar 2013 15:29:14 +0000">&lt;p&gt;This seems to not blow up for title-like fields:&lt;br/&gt;
I did a quick test of geonames (8.3M place names, just using ID as the weight)&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;AnalyzingSuggester: 117444563 bytes, 74887ms build time
InfixingSuggester: 302127665 bytes, 125895ms build time
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think realistically an N limit can work well here. After such a limit, the infixing is&lt;br/&gt;
pretty crazy anyway, and really infixing should "punish" the weight in some way since its&lt;br/&gt;
a very scary "edit" operation to do to the user.&lt;/p&gt;

&lt;p&gt;Plus you get optional fuzziness and real "phrasing" works too &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13607085" author="mikemccand" created="Wed, 20 Mar 2013 00:12:00 +0000">&lt;p&gt;I like this approach!  (Add epsilon transitions after the automaton is built).&lt;/p&gt;

&lt;p&gt;I managed to build the FreeDB suggest using this but ... it required a lot of RAM: it OOM'd at 14 GB heap but finished successfully at 20 GB heap.&lt;/p&gt;

&lt;p&gt;Took a longish time to build too, and made a biggish FST (more than 2X larger than the index):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;2466 sec to build&lt;/li&gt;
	&lt;li&gt;FST is 8.6 GB&lt;/li&gt;
	&lt;li&gt;Prefix 2: 2527.5 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 4: 1681.7 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 6: 1948.3 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 8: 2050.9 lookups/sec&lt;/li&gt;
	&lt;li&gt;Prefix 10: 2076.0 lookups/sec&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We should try the N prefix limit ... but I don't really like that.  Maybe we should just offer both approaches ...&lt;/p&gt;</comment>
                    <comment id="13607120" author="rcmuir" created="Wed, 20 Mar 2013 00:48:39 +0000">&lt;blockquote&gt;
&lt;p&gt;I managed to build the FreeDB suggest using this but ... it required a lot of RAM: it OOM'd at 14 GB heap but finished successfully at 20 GB heap.&lt;/p&gt;

&lt;p&gt;Took a longish time to build too, and made a biggish FST (more than 2X larger than the index):&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think its because your FreeDB has a lot more words than my place names?&lt;/p&gt;

&lt;p&gt;But really there must be a infixing limit for relevance reasons alone.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We should try the N prefix limit ... but I don't really like that. Maybe we should just offer both approaches ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why is it so bad, but the edge-ngrams limit ok?&lt;/p&gt;</comment>
                    <comment id="13607539" author="mikemccand" created="Wed, 20 Mar 2013 12:09:26 +0000">&lt;blockquote&gt;&lt;p&gt;I think its because your FreeDB has a lot more words than my place names?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think so.  Song titles are longer than place names &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But really there must be a infixing limit for relevance reasons alone.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the app can decide this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why is it so bad, but the edge-ngrams limit ok?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think either limit is OK!  In the ideal world we wouldn't require such limits due to performance/RAM issues.&lt;/p&gt;

&lt;p&gt;But no suggester is perfect, this is why we offer multiple options.  These two approaches have different tradeoffs...&lt;/p&gt;</comment>
                    <comment id="13607552" author="rcmuir" created="Wed, 20 Mar 2013 12:39:27 +0000">&lt;blockquote&gt;
&lt;p&gt;I don't think either limit is OK! In the ideal world we wouldn't require such limits due to performance/RAM issues.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You still misunderstand me. I dont want the limit for performance/RAM reasons. I want it for relevance reasons. It&lt;br/&gt;
just also gives better performance and memory for free. this is a really simple thing to do mike. Its a win/win&lt;/p&gt;

&lt;p&gt;On the other hand your edge-ngrams limit is completely different. When exceeded, it causes that suggester to work&lt;br/&gt;
in linear time!&lt;/p&gt;</comment>
                    <comment id="13663992" author="mikemccand" created="Wed, 22 May 2013 11:44:51 +0100">&lt;p&gt;Just checkpointing my current patch, with changes from &lt;a href="http://jirasearch.mikemccandless.com" class="external-link"&gt;http://jirasearch.mikemccandless.com&lt;/a&gt; ... still lots of nocommits ...&lt;/p&gt;</comment>
                    <comment id="13667207" author="shaie" created="Sun, 26 May 2013 04:56:22 +0100">&lt;p&gt;Oops, I hit something on the keyboard while reading the issue and it just assigned it to me &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12574091" name="infixSuggest.png" size="169546" author="mikemccand" created="Sun, 17 Mar 2013 22:20:35 +0000"/>
                    <attachment id="12584286" name="LUCENE-4845.patch" size="26922" author="mikemccand" created="Wed, 22 May 2013 11:44:51 +0100"/>
                    <attachment id="12574325" name="LUCENE-4845.patch" size="11828" author="rcmuir" created="Tue, 19 Mar 2013 05:40:49 +0000"/>
                    <attachment id="12574254" name="LUCENE-4845.patch" size="25769" author="mikemccand" created="Mon, 18 Mar 2013 22:51:42 +0000"/>
                    <attachment id="12574090" name="LUCENE-4845.patch" size="17470" author="mikemccand" created="Sun, 17 Mar 2013 22:19:10 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 17 Mar 2013 22:33:33 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>317959</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>317940</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4835] Raise maxClauseCount in BooleanQuery to Integer.MAX_VALUE</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4835</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Discussion on &lt;a href="https://issues.apache.org/jira/browse/SOLR-4586" title="Increase default maxBooleanClauses"&gt;SOLR-4586&lt;/a&gt; raised the idea of raising the limit on boolean clauses from 1024 to Integer.MAX_VALUE.  This should be a safe change.  It will change the nature of help requests from "Why can't I do 2000 clauses?" to "Why is my 5000-clause query slow?"&lt;/p&gt;</description>
                <environment/>
            <key id="12637266">LUCENE-4835</key>
            <summary>Raise maxClauseCount in BooleanQuery to Integer.MAX_VALUE</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="elyograg">Shawn Heisey</reporter>
                        <labels>
                    </labels>
                <created>Fri, 15 Mar 2013 20:52:40 +0000</created>
                <updated>Fri, 10 May 2013 00:05:05 +0100</updated>
                                    <version>4.2</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13603841" author="rcmuir" created="Fri, 15 Mar 2013 20:55:26 +0000">&lt;p&gt;I'm not sure it should be Integer.MAX_VALUE. you can't even create arrays this big with current jvms. this wouldn't be a safe change. it would change the natural of help requests from "why did i get TooManyClauses exception" to "why did i get super-strange exception: is this a bug?"&lt;/p&gt;</comment>
                    <comment id="13603847" author="markrmiller@gmail.com" created="Fri, 15 Mar 2013 21:02:42 +0000">&lt;p&gt;Yeah, that comes from me - every time I use that as an example of what I'm meaning, I get in trouble &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;What I meant by it is that there should be no limit, not necessarily that that should be the limit in code.&lt;/p&gt;</comment>
                    <comment id="13603850" author="markrmiller@gmail.com" created="Fri, 15 Mar 2013 21:05:31 +0000">&lt;blockquote&gt;&lt;p&gt;it would change the natural of help requests from "why did i get TooManyClauses exception" to "why did i get super-strange exception: is this a bug?"&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It would remove even more of those silly "why did I get TooManyClauses exception" questions that tons can still get at 1024 or whatever it is. How many people will be bitten with what you talking about? That many explicit bq's? Well that one in a million guy will bring his expcetion to the list and mention, oh, im doing 5 billion boolean clauses or whatever.&lt;/p&gt;

&lt;p&gt;This silly artificial limit hasnt even kept pace with hardware improvements over the years &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Not that it matters - it's arbitrary to begin with.&lt;/p&gt;</comment>
                    <comment id="13603852" author="rcmuir" created="Fri, 15 Mar 2013 21:09:02 +0000">&lt;p&gt;i was thinking about someone who has a bug in their code and accidentally keeps adding to the same BQ. I feel like i've done this writing tests before, probably multiple times. If instead my code seemed hung, only to finally get OOM or some ArrayStoreException or something wacky in some strange place, it would take me longer to realize my mistake. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13603862" author="markrmiller@gmail.com" created="Fri, 15 Mar 2013 21:14:28 +0000">&lt;p&gt;If that was really a concern - which for me, it wouldn't really be until I started seeing the reports - but even if it's your concern, a huge limit still would be much better. There is not really anything special about the fairly low current value.&lt;/p&gt;

&lt;p&gt;Im no so worried about this type of thing...you might think that you want every hit back no matter what and ask for like the top 1 million hits and require a huge pq and blow our your ram and oh how confusing...but people seem to get by without us throwing arbitrary exceptions.&lt;/p&gt;</comment>
                    <comment id="13603867" author="rcmuir" created="Fri, 15 Mar 2013 21:21:50 +0000">&lt;p&gt;Its not really a huge concern. I'd like for their still to be a limit, e.g. that i could set in lucenetestcase or even in solr to prevent tripping my own self, even if it has a different default value.&lt;/p&gt;</comment>
                    <comment id="13603876" author="markrmiller@gmail.com" created="Fri, 15 Mar 2013 21:27:40 +0000">&lt;p&gt;Alright, well thats fine with me - as long as the limit is not so silly low, I'll at least be happier. I'd be happiest to have the whole concept go away, but I'll compramise to just suffocate it a bit.&lt;/p&gt;</comment>
                    <comment id="13603879" author="elyograg" created="Fri, 15 Mar 2013 21:32:26 +0000">&lt;p&gt;Robert's concern didn't occur to me at all, because I reside on the Solr side of the fence.  One approach that might satisfy both sides: 1) Be conservative in Lucene by leaving the value at 1024 or increasing it to something that would stress modern hardware.  2) In Solr, explicitly set it to Integer.MAX_VALUE, or REALLY_BIG_NUMBER so it's less likely to cause overflow problems.  If REALLY_BIG_NUMBER is chosen, we'd probably have to leave maxBooleanClauses parsing in, but it could be reduced to a commented section in the example config and could be left out of most of the test configs.&lt;/p&gt;

&lt;p&gt;A Lucene user, because they are already in the 'custom code' realm, can increase the value if they need to, and Solr (which deals in query strings rather than complex objects) would effectively have the limit removed.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12637134">SOLR-4586</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 15 Mar 2013 20:55:26 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>317763</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>317744</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4823] Add a separate "registration" singleton for Lucene's SPI, so there is only one central instance to request rescanning of classpath (e.g. from Solr's ResourceLoader)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4823</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently there is no easy way to do a global "rescan/reload" of all of Lucene's SPIs in the right order. In solr there is a long list of reload instructions in the ResourceLoader. If somebody adds a new SPI type, you have to add it there.&lt;/p&gt;

&lt;p&gt;It would be good to java a central instance in oal.util that keeps track of all NamedSPILoaders and AnalysisSPILoaders (in the order they were instantiated), so you have one central entry point to trigger a reload.&lt;/p&gt;

&lt;p&gt;This issue will introduce:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;A singleton that makes reloading possible. The singleton keeps weak refs to all loaders (of any kind) in the order they were created.&lt;/li&gt;
	&lt;li&gt;NamedSPILoader and AnalysisSPILoader cleanup (unfortunately we need both instances, as they differ in the internals (one keeps classes, the other one instances). Both should implement a "reloadable" interface.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12636559">LUCENE-4823</key>
            <summary>Add a separate "registration" singleton for Lucene's SPI, so there is only one central instance to request rescanning of classpath (e.g. from Solr's ResourceLoader)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Tue, 12 Mar 2013 12:08:24 +0000</created>
                <updated>Fri, 10 May 2013 00:05:05 +0100</updated>
                                                    <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>317056</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>317037</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4813] Allow DirectSpellchecker to use totalTermFrequency rather than docFrequency</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4813</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;we have a bunch of new statistics in on our term dictionaries that we should make use of where it makes sense. For DirectSpellChecker totalTermFreq and sumTotalTermFreq might be better suited for spell correction on top of a fulltext index than docFreq and maxDoc&lt;/p&gt;</description>
                <environment/>
            <key id="12635615">LUCENE-4813</key>
            <summary>Allow DirectSpellchecker to use totalTermFrequency rather than docFrequency</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Wed, 6 Mar 2013 17:17:46 +0000</created>
                <updated>Fri, 10 May 2013 00:05:05 +0100</updated>
                                    <version>4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/spellchecker</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13594876" author="simonw" created="Wed, 6 Mar 2013 17:19:48 +0000">&lt;p&gt;here is an initial patch that adds this as the default yet optional statistics.&lt;/p&gt;</comment>
                    <comment id="13595958" author="simonw" created="Thu, 7 Mar 2013 15:19:24 +0000">&lt;p&gt;next iteration making the new statistic optional and experts can just pass them in if they want. This patch is bw compatible runtime wise while it breaks some apis since I change floats to double but I think that is a fair game here. I think its close / ready&lt;/p&gt;</comment>
                    <comment id="13597101" author="rcmuir" created="Fri, 8 Mar 2013 13:09:16 +0000">&lt;p&gt;Can we do without the FieldStatistics/DocFreqStatistics/etc and just change 'freq' to long?&lt;/p&gt;</comment>
                    <comment id="13597123" author="simonw" created="Fri, 8 Mar 2013 13:40:26 +0000">&lt;blockquote&gt;&lt;p&gt;Can we do without the FieldStatistics/DocFreqStatistics/etc and just change 'freq' to long?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I really appreciate the fact that this is an object that I can pass in for several reasons. First you can just plug in your own stats if you want to and it pulls a terms object only once that I can provide. In my usecase I call the same instance of DirectSpellChecker in the same request multiple times to generate candidates and that way I can just keep my Terms / TermsEnum instance reused which is a small but yet important cost IMO which can in my expert case help. For the users this that have used this class before nothing really changes unless you want to go to totalTermFreq as their stats but we can make this simple. We can also make these classes package private I am totally ok with this to hide this small complexity here from the average user but enable the expert user. API stays the same and if sumTotalTermFreq is available you also get it in the SuggestWord. I would not want to fork this entire code just for the sake of being able to reuse these statistics etc. if hiding this from the user is the problem then lets move to pkg private. if its just you "feeling" this is a too big of a change for the sake then I am not moving sorry.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12572544" name="LUCENE-4813.patch" size="38138" author="simonw" created="Thu, 7 Mar 2013 15:19:24 +0000"/>
                    <attachment id="12572349" name="LUCENE-4813.patch" size="31947" author="simonw" created="Wed, 6 Mar 2013 17:19:48 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 8 Mar 2013 13:09:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>316113</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>316094</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4803] DrillDownQuery should rewrite to FilteredQuery?</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4803</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today we rewrite to a query like +baseQuery +ConstantScoreQuery(boost=0.0 TermQuery(drillDownTerm)), but I'm not certain 0.0 boost is safe / doesn't actually change scores.&lt;/p&gt;

&lt;p&gt;We should also add a test to assert that scores are not changed by drill down.&lt;/p&gt;</description>
                <environment/>
            <key id="12634206">LUCENE-4803</key>
            <summary>DrillDownQuery should rewrite to FilteredQuery?</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 26 Feb 2013 17:29:23 +0000</created>
                <updated>Fri, 10 May 2013 00:05:05 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13587329" author="commit-tag-bot" created="Tue, 26 Feb 2013 18:14:18 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Michael McCandless&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1450320" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1450320&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4803" title="DrillDownQuery should rewrite to FilteredQuery?"&gt;LUCENE-4803&lt;/a&gt;: add test coverage&lt;/p&gt;</comment>
                    <comment id="13587330" author="mikemccand" created="Tue, 26 Feb 2013 18:14:31 +0000">&lt;p&gt;I committed test coverage, asserting that DDQ never alters the scores of the original query, and it seems to be passing ...&lt;/p&gt;

&lt;p&gt;I'll leave this open (but not work on it for now...) to explore whether we should use FilteredQuery instead.  Our Filter would not be random access (we'd just do QueryWrapperFilter(BQ(+dd1 +dd2 +dd3 ...)) ... not sure if we'd see perf changes with FilteredQuery.&lt;/p&gt;</comment>
                    <comment id="13587345" author="commit-tag-bot" created="Tue, 26 Feb 2013 18:20:23 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Michael McCandless&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1450321" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1450321&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4803" title="DrillDownQuery should rewrite to FilteredQuery?"&gt;LUCENE-4803&lt;/a&gt;: add test coverage&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 26 Feb 2013 18:14:18 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>314704</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>314685</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4766] Pattern token filter which emits a token for every capturing group</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4766</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The PatternTokenizer either functions by splitting on matches, or allows you to specify a single capture group.  This is insufficient for my needs. Quite often I want to capture multiple overlapping tokens in the same position.&lt;/p&gt;

&lt;p&gt;I've written a pattern token filter which accepts multiple patterns and emits tokens for every capturing group that is matched in any pattern.&lt;br/&gt;
Patterns are not anchored to the beginning and end of the string, so each pattern can produce multiple matches.&lt;/p&gt;

&lt;p&gt;For instance a pattern like :&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    &lt;span class="code-quote"&gt;"(([a-z]+)(\d*))"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when matched against: &lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    &lt;span class="code-quote"&gt;"abc123def456"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;would produce the tokens:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    abc123, abc, 123, def456, def, 456
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Multiple patterns can be applied, eg these patterns could be used for camelCase analysis:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    &lt;span class="code-quote"&gt;"([A-Z]{2,})"&lt;/span&gt;,
    &lt;span class="code-quote"&gt;"(?&amp;lt;![A-Z])([A-Z][a-z]+)"&lt;/span&gt;,
    &lt;span class="code-quote"&gt;"(?:^|\\b|(?&amp;lt;=[0-9_])|(?&amp;lt;=[A-Z]{2}))([a-z]+)"&lt;/span&gt;,
    &lt;span class="code-quote"&gt;"([0-9]+)"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When matched against the string "letsPartyLIKEits1999_dude", they would produce the tokens:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    lets, Party, LIKE, its, 1999, dude
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If no token is emitted, the original token is preserved. &lt;br/&gt;
If the preserveOriginal flag is true, it will output the full original token (ie "letsPartyLIKEits1999_dude") in addition to any matching tokens (but in this case, if a matching token is identical to the original, it will only emit one copy of the full token).&lt;/p&gt;

&lt;p&gt;Multiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand.&lt;/p&gt;

&lt;p&gt;This is my first Java code, so apologies if I'm doing something stupid.&lt;/p&gt;</description>
                <environment/>
            <key id="12631641">LUCENE-4766</key>
            <summary>Pattern token filter which emits a token for every capturing group</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="clintongormley">Clinton Gormley</reporter>
                        <labels>
                        <label>analysis</label>
                        <label>feature</label>
                        <label>lucene</label>
                    </labels>
                <created>Sun, 10 Feb 2013 12:31:38 +0000</created>
                <updated>Wed, 24 Apr 2013 11:33:21 +0100</updated>
                    <resolved>Wed, 24 Apr 2013 11:33:21 +0100</resolved>
                            <version>4.1</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13575427" author="clintongormley" created="Sun, 10 Feb 2013 12:35:40 +0000">&lt;p&gt;Patch implementing org.apache.lucene.analysis.pattern.PatternCaptureGroupTokenFilter and org.apache.lucene.analysis.pattern.TestPatternCaptureGroupTokenFilter&lt;/p&gt;</comment>
                    <comment id="13575714" author="simonw" created="Mon, 11 Feb 2013 10:01:18 +0000">&lt;p&gt;Hey Clinton, this looks very interesting and given this is your first java experience pretty impressive too. I am not sure how expensive this filter in practice will be but given that you can do stuff you can't do with any of the other filters I think folks just have to pay the price here. I like that all patterns operate on the same CharSequence and that you are setting offsets right. Cool stuff! &lt;/p&gt;</comment>
                    <comment id="13575718" author="jpountz" created="Mon, 11 Feb 2013 10:21:44 +0000">&lt;blockquote&gt;&lt;p&gt;I like that all patterns operate on the same CharSequence and that you are setting offsets right.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Does this filter need to set offsets? I'm worried that under certain circumstances filters that modify offsets might create inconsistent offset graphs (because they don't know what filters have been applied before, there is an exclusion list for filters that modify offsets in TestRandomChains).&lt;/p&gt;</comment>
                    <comment id="13575731" author="simonw" created="Mon, 11 Feb 2013 11:13:49 +0000">&lt;blockquote&gt;&lt;p&gt;Does this filter need to set offsets? I'm worried that under certain circumstances filters that modify offsets might create inconsistent offset graphs (because they don't know what filters have been applied before, there is an exclusion list for filters that modify offsets in TestRandomChains).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah I agree offsets are tricky here. I just wonder if we really should restrict our TF to not fix offsets? Kind of an odd thing though. What should a tokenfilter like this do instead?&lt;/p&gt;</comment>
                    <comment id="13575733" author="simonw" created="Mon, 11 Feb 2013 11:15:43 +0000">&lt;p&gt;I updated the patch and added a FilterFactory etc. to make tests pass. it still contains the offset fix and I personally don't really know what TF should do in those cases&lt;/p&gt;</comment>
                    <comment id="13575739" author="jpountz" created="Mon, 11 Feb 2013 11:29:03 +0000">&lt;blockquote&gt;&lt;p&gt;I just wonder if we really should restrict our TF to not fix offsets? Kind of an odd thing though. What should a tokenfilter like this do instead?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that for some examples, it makes sense not to fix offsets? In the case of the URL example (&lt;tt&gt;(https?://(&lt;span class="error"&gt;&amp;#91;a-zA-Z\-_0-9.&amp;#93;&lt;/span&gt;+))&lt;/tt&gt;), I think it makes sense to highlight the whole URL (including the leading http(s)://) even if the query term is just &lt;tt&gt;www.mysite.com&lt;/tt&gt;. On the other hand, it could be weird if the goal was to split a long CamelCase token (letsPartyLIKEits1999_dude), but maybe this should be done by a Tokenizer rather than a TokenFilter?&lt;/p&gt;

&lt;p&gt;(No strong feeling here, I'd just like to see if we can find a way to commit this patch without having to grow our TokenFilter exclusion list.)&lt;/p&gt;</comment>
                    <comment id="13575760" author="rcmuir" created="Mon, 11 Feb 2013 12:33:36 +0000">&lt;blockquote&gt;
&lt;p&gt;(No strong feeling here, I'd just like to see if we can find a way to commit this patch without having to grow our TokenFilter exclusion list.)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I dont think tokenfilters should change offsets in general. This is not possible to do correctly. In general if you are splitting and creating like this... its a tokenizer not a tokenfilter. And only a tokenizer can set offsets, because its the only one that has access to the charfilter correction data.&lt;/p&gt;

&lt;p&gt;besides, trying to be a token 'creator' over an incoming tokenstream graph is really hard to get right.&lt;/p&gt;

&lt;p&gt;So I would prefer if this filter either became a tokenizer or did not change offsets at all. Then we can probably get it committed without hassle.&lt;/p&gt;

&lt;p&gt;we cannot allow this exclusion list to grow. Its not an exclusion list that says 'its ok to add more broken filters'. Its a list of filters that will get deleted from lucene soon unless someone fixes them, because we have to stop indexwriter from writing invalid data into the term vectors here.&lt;/p&gt;

&lt;p&gt;Also the test should call checkRandomData() &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13575774" author="clintongormley" created="Mon, 11 Feb 2013 13:11:13 +0000">&lt;p&gt;Is it OK for a tokenizer to create multiple tokens in the same positions but with different offsets? eg &lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-quote"&gt;"foobar"&lt;/span&gt; -&amp;gt; [&lt;span class="code-quote"&gt;"foobar"&lt;/span&gt;,&lt;span class="code-quote"&gt;"foo"&lt;/span&gt;,&lt;span class="code-quote"&gt;"bar"&lt;/span&gt;] with positions [1,1,1] and startOffsets [0,0,3]?
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Having a look at the wordDelimiter token filter, with preserveOriginal set to true, it increments the position for each new offset, eg: &lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-quote"&gt;"fooBar"&lt;/span&gt; -&amp;gt; [&lt;span class="code-quote"&gt;"fooBar"&lt;/span&gt;,&lt;span class="code-quote"&gt;"foo"&lt;/span&gt;,&lt;span class="code-quote"&gt;"Bar"&lt;/span&gt;] with positions [1,1,2] and startOffsets [0,0,3].
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I'm asking because I'm not sure exactly how positions and offsets get used elsewhere, and so what the correct behaviour is. From my naive understanding, the wordDelimiter filter can produce spurious results with phrase searches, eg matching "fooBar Bar"&lt;/p&gt;</comment>
                    <comment id="13575776" author="rcmuir" created="Mon, 11 Feb 2013 13:24:19 +0000">&lt;p&gt;The positions are used for searching, offsets for highlighting.&lt;/p&gt;

&lt;p&gt;So you can (unfortunately) set the offsets to whatever you want, it wont affect searches. Instead it will only cause problems for highlighting. An example of this is: &lt;a href="https://issues.apache.org/jira/browse/SOLR-4137" class="external-link"&gt;https://issues.apache.org/jira/browse/SOLR-4137&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For a tokenfilter, it doesnt make sense to change offsets, because a tokenizer already broke the document into words and mapped them back to their original location in the document.&lt;/p&gt;

&lt;p&gt;If a tokenfilter REALLY needs to change offsets, then its a sign its subclassing the wrong analysis type and should be a tokenizer: because its trying to break the document into words, not just alter existing tokenization &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13575786" author="jpountz" created="Mon, 11 Feb 2013 13:35:25 +0000">&lt;blockquote&gt;&lt;p&gt;Is it OK for a tokenizer to create multiple tokens in the same positions but with different offsets?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Although it's not common, it is perfectly fine for a Tokenizer to generate multiple tokens in the same position.&lt;/p&gt;

&lt;p&gt;However, I think the correct way to tokenize your example would be:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;tokens: foo, foobar, bar
positions: 1, 1, 2
position lengths: 1, 2, 1
start offsets: 0, 0, 3
end offsets: 3, 6, 6
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I'm not sure WordDelimiterFilter is the best example to look at. I'm not familiar with it at all, but it's currently in the exclusion list for both positions and offsets (and is the culprit for &lt;a href="https://issues.apache.org/jira/browse/SOLR-4137" title="FastVectorHighlighter: StringIndexOutOfBoundsException in BaseFragmentsBuilder"&gt;&lt;del&gt;SOLR-4137&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="13575793" author="thetaphi" created="Mon, 11 Feb 2013 13:53:32 +0000">&lt;p&gt;WDF is one of those "bad" examples. WDF should be included in a custom Tokenizer. WDF is always used together with WhitespaceTokenizer, so it should be included into some  "WordDelimiterWhitespaceTokenizer".&lt;/p&gt;</comment>
                    <comment id="13575811" author="clintongormley" created="Mon, 11 Feb 2013 14:33:02 +0000">&lt;p&gt;OK, so I should redo this as a tokenizer, and set positionLengths correctly.&lt;/p&gt;

&lt;p&gt;One issue is that, because there are multiple patterns, the emitted tokens can overlap, eg:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;   &lt;span class="code-quote"&gt;"foobarbaz"&lt;/span&gt; -&amp;gt; foo, foobar, oba, bar, baz
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in which case I think I would need to emit:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;    positions:         1, 1, 2, 3, 5
    position lengths:  2, 4, 2, 2, 1
    start offsets:     0, 0, 0, 0, 0
    end offsets:       3, 6, 3, 3, 3
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is this correct? It's starting to look quite complex...&lt;/p&gt;</comment>
                    <comment id="13575813" author="simonw" created="Mon, 11 Feb 2013 14:39:12 +0000">&lt;p&gt;I tend to disagree that this should be a tokenizer. IMO a tokenizer should only split tokens in a stream fashion and should not emit tokens on the same position really. This is what token filters should do. It also clashes with reuseability since you can't really reuse tokenizers you have to decide which one you want. At some point you need to know what you are doing here really. I don't have a definite answer but there is currently no clean way to do what clinton wants to do IMO.&lt;/p&gt;</comment>
                    <comment id="13575889" author="elyograg" created="Mon, 11 Feb 2013 16:47:48 +0000">&lt;p&gt;I use WDF with ICUTokenizer.  ICUTokenizer is customized with an RBBI file for latin1 that only breaks on whitespace.&lt;/p&gt;</comment>
                    <comment id="13577469" author="jpountz" created="Wed, 13 Feb 2013 10:23:04 +0000">&lt;blockquote&gt;&lt;p&gt;I tend to disagree that this should be a tokenizer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe another option is to change this filter so that it doesn't change offsets? Let's imagine this filter is used to break &lt;tt&gt;TokenFilter&lt;/tt&gt; into &lt;tt&gt;Token&lt;/tt&gt;, &lt;tt&gt;TokenFilter&lt;/tt&gt; and &lt;tt&gt;Filter&lt;/tt&gt;, I think it's acceptable to highlight &lt;tt&gt;TokenFilter&lt;/tt&gt; as a whole when searching for "Token" or "Filter"?&lt;/p&gt;</comment>
                    <comment id="13577505" author="clintongormley" created="Wed, 13 Feb 2013 11:59:36 +0000">&lt;p&gt;New patch which preserves the offsets of the original token. Includes Simons patch to create the filter factory&lt;/p&gt;</comment>
                    <comment id="13577512" author="rcmuir" created="Wed, 13 Feb 2013 12:08:36 +0000">&lt;p&gt;The patch should add a test that uses checkRandomData.&lt;/p&gt;

&lt;p&gt;it would find bugs like this:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;charOffsetStart = offsetAttr.startOffset();
charOffsetEnd = charOffsetStart + spare.length;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;charOffsetEnd should be offsetAttr.endOffset(). If there is a charfilter, the current calculation will be incorrect.&lt;/p&gt;</comment>
                    <comment id="13577514" author="simonw" created="Wed, 13 Feb 2013 12:15:22 +0000">&lt;p&gt;Clinton, I think you can trash the offset attribute reference in there entirely just don't mess with them at all. Can you also call BaseTokenStreamTest#assertAnalyzesToReuse and BTST#checkRandomData in your tests please&lt;/p&gt;</comment>
                    <comment id="13577555" author="clintongormley" created="Wed, 13 Feb 2013 13:31:39 +0000">&lt;p&gt;The charOffsetEnd now uses the correct offsetAttr.endOffset() and, added a test using checkRandomData()&lt;/p&gt;</comment>
                    <comment id="13577556" author="thetaphi" created="Wed, 13 Feb 2013 13:31:43 +0000">&lt;blockquote&gt;&lt;p&gt;Clinton, I think you can trash the offset attribute reference in there entirely just don't mess with them at all.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's part of a bigger problem in the current code. The idea of this filter is to make from one input Token multiple output Tokens. To make this work correct, the &lt;b&gt;new&lt;/b&gt; output tokens must be produced based on the original token (means the filter must reset the new produced token to a clean state, otherwise it might happen that unrelated and unknown attributes stay alive with wrong values - especiall if later TokenFilter change attributes, e.g. a Synonymfilter is inserting more synonyms). The problem Clinton had was that he had to re-set the offset attribute (although he does not change it); but he missed possible other attributes on the stream he does not know about.&lt;/p&gt;

&lt;p&gt;If you look at other filters doing similar things like Synonymfilter, WDF, the way it has to work is like that:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;The first token emmitted is the "original one, maybe modified&lt;/li&gt;
	&lt;li&gt;All "inserted Tokens" are cloned from the original (first) token, use captureState/restoreState to do that. This will initialize the attribute source to the exact same token like the original (unmodified one). After you called restoreState, you can &lt;b&gt;modify&lt;/b&gt; the attribute (like term text) and setPositionIncrement(0). You can then leave the the offset (and other unknown attributes that may be on the token stream) unchanged - don't reference them at all.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13640260" author="simonw" created="Wed, 24 Apr 2013 10:02:27 +0100">&lt;p&gt;here is a new patch, updated to trunk and moved over to use capture&amp;amp;restore state. I think this one is ready, I will commit this today if nobody objects.&lt;/p&gt;</comment>
                    <comment id="13640262" author="simonw" created="Wed, 24 Apr 2013 10:05:11 +0100">&lt;p&gt;argh... I missed svn add... here is the right patch&lt;/p&gt;</comment>
                    <comment id="13640292" author="jpountz" created="Wed, 24 Apr 2013 10:36:21 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13640314" author="commit-tag-bot" created="Wed, 24 Apr 2013 11:21:06 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; simonw&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471347" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471347&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4766" title="Pattern token filter which emits a token for every capturing group"&gt;&lt;del&gt;LUCENE-4766&lt;/del&gt;&lt;/a&gt;: Added a PatternCaptureGroupTokenFilter that uses Java regexes to emit multiple tokens one for each capture group in one or more patterns&lt;/p&gt;</comment>
                    <comment id="13640318" author="commit-tag-bot" created="Wed, 24 Apr 2013 11:32:11 +0100">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; simonw&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471352" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1471352&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4766" title="Pattern token filter which emits a token for every capturing group"&gt;&lt;del&gt;LUCENE-4766&lt;/del&gt;&lt;/a&gt;: Added a PatternCaptureGroupTokenFilter that uses Java regexes to emit multiple tokens one for each capture group in one or more patterns&lt;/p&gt;</comment>
                    <comment id="13640320" author="simonw" created="Wed, 24 Apr 2013 11:33:21 +0100">&lt;p&gt;committed, thanks clinton&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12580262" name="LUCENE-4766.patch" size="31538" author="simonw" created="Wed, 24 Apr 2013 10:05:11 +0100"/>
                    <attachment id="12569189" name="LUCENE-4766.patch" size="30164" author="clintongormley" created="Wed, 13 Feb 2013 13:31:39 +0000"/>
                    <attachment id="12569182" name="LUCENE-4766.patch" size="29549" author="clintongormley" created="Wed, 13 Feb 2013 11:59:36 +0000"/>
                    <attachment id="12568810" name="LUCENE-4766.patch" size="30006" author="simonw" created="Mon, 11 Feb 2013 11:15:43 +0000"/>
                    <attachment id="12568746" name="LUCENE-4766.patch" size="23194" author="clintongormley" created="Sun, 10 Feb 2013 12:35:40 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 11 Feb 2013 10:01:18 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>312142</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>312124</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-4746] Create a move method in Directory.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4746</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I'd like to make a move method for directory.&lt;/p&gt;

&lt;p&gt;We already have a move for Solr in DirectoryFactory, but it seems it belongs at the directory level really.&lt;/p&gt;

&lt;p&gt;The default impl can do a copy and delete, but most implementations will be able to optimize to a rename.&lt;/p&gt;

&lt;p&gt;Besides the move we do for Solr (to move a replicated index into place), it would also be useful for another feature I'd like to add - the ability to merge an index with moves rather than copies. In some cases, you don't need/want to copy all the files and could just rename/move them. &lt;/p&gt;</description>
                <environment/>
            <key id="12630502">LUCENE-4746</key>
            <summary>Create a move method in Directory.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Sat, 2 Feb 2013 04:06:52 +0000</created>
                <updated>Fri, 10 May 2013 00:05:06 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13569552" author="mikemccand" created="Sat, 2 Feb 2013 13:57:17 +0000">&lt;p&gt;I'm not sure we should do this: I think Directory should have only he methods necessary for Lucene, and we should minimize those methods as much as possible.&lt;/p&gt;

&lt;p&gt;We used to have 'rename' and we removed it because it was problematic in some cases.&lt;/p&gt;</comment>
                    <comment id="13569560" author="markrmiller@gmail.com" created="Sat, 2 Feb 2013 14:35:03 +0000">&lt;blockquote&gt;&lt;p&gt;I think Directory should have only he methods necessary for Lucene&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is useful for Lucene if you want to copy but don't need to keep around the source file. Something very useful for addIndexes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We used to have 'rename' and we removed it because it was problematic in some cases.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is different than a rename method as described above and you don't describe how any of those problems would apply to a move method.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we should minimize those methods as much as possible&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's not really an argument against adding this feature.&lt;/p&gt;</comment>
                    <comment id="13569562" author="rcmuir" created="Sat, 2 Feb 2013 14:56:00 +0000">&lt;blockquote&gt;
&lt;p&gt;This is useful for Lucene if you want to copy but don't need to keep around the source file. Something very useful for addIndexes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For copy-free addIndexes(Directory...) we could improve FSDir's copy(Directory, String, String) impl to try to create a hard link, and the existing impl would work.&lt;/p&gt;

&lt;p&gt;But on windows i think by default links dont work unless you are administrator, and some other obscure filesystems probably dont support links either (but do support rename).&lt;/p&gt;

&lt;p&gt;And we'd need to be fully on java7 (i think?).&lt;/p&gt;

&lt;p&gt;So maybe we need a moveIndexes(Directory...) It could acquire the write lock on all the source Directories and just move instead of copy (otehrwise its like addIndexes).&lt;/p&gt;
</comment>
                    <comment id="13569566" author="markrmiller@gmail.com" created="Sat, 2 Feb 2013 15:14:01 +0000">&lt;blockquote&gt;&lt;p&gt;So maybe we need a moveIndexes(Directory...) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 - this is my motivation - moveIndexes &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13569573" author="rcmuir" created="Sat, 2 Feb 2013 15:52:41 +0000">&lt;p&gt;here's a prototype.&lt;/p&gt;

&lt;p&gt;My main concerns are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;3.0 formatted indexes with shared doc stores. We shouldnt support this. throwing exception when we see it makes the method more dangerous / doing an extra-pass to detect it = unacceptable (this is the kind of hairiness we must avoid).&lt;/li&gt;
	&lt;li&gt;I dont want a slow (but safe) addIndexes and a fast (but unsafe) moveIndexes. So if we do this, i want java7 as a minimum requirement so we can at least try to create a hard link in Directory.copy() to make addIndexes fast too.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To me, these both suggest trunk-only, but i'm open to other ideas. &lt;/p&gt;</comment>
                    <comment id="13569574" author="mikemccand" created="Sat, 2 Feb 2013 16:01:15 +0000">&lt;p&gt;OK, I agree IndexWriter.moveIndices would be a nice addition to&lt;br/&gt;
Lucene, and that we'll need to bring back Directory.rename/move to do&lt;br/&gt;
it efficiently.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;3.0 formatted indexes with shared doc stores. We shouldnt support this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I dont want a slow (but safe) addIndexes and a fast (but unsafe) moveIndexes. So if we do this, i want java7 as a minimum requirement so we can at least try to create a hard link in Directory.copy() to make addIndexes fast too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No matter what moveIndexes will be unsafe right?  Meaning if you hit an exc part way through then some or all of your source indices are corrupt ... the dest index should be fine though (and, should only reflect all or none of the additions I think).&lt;/p&gt;

&lt;p&gt;+1 for hard link via Dir.copy; that would be nice!  Should we just require Java 7 on trunk now...?&lt;/p&gt;</comment>
                    <comment id="13569575" author="rcmuir" created="Sat, 2 Feb 2013 16:03:16 +0000">&lt;p&gt;another tricky thing (at least i didnt think about it yet) is how to nuke all the commit points in the src dirs referring to the files we nuked &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;In general i dont think the patch works, i just wanted to show what API i was suggesting.&lt;/p&gt;</comment>
                    <comment id="13569576" author="rcmuir" created="Sat, 2 Feb 2013 16:06:37 +0000">&lt;blockquote&gt;
&lt;p&gt;No matter what moveIndexes will be unsafe right? Meaning if you hit an exc part way through then some or all of your source indices are corrupt ... the dest index should be fine though (and, should only reflect all or none of the additions I think).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah at least i think it has to be. even if added complex crap and tried to move files back etc, someone could run out of disk space during the process or all kinds of other possibilities.&lt;/p&gt;

&lt;p&gt;I think in general speeding up addIndexes to be more efficient (and still safe) is most attractive. But i know some systems cant support hard linking so i think it would be good to have a more generally fast method for them. Maybe there is a way it wouldnt have to be on indexwriter as well (but a tool in contrib), i dont know.&lt;/p&gt;</comment>
                    <comment id="13569577" author="markrmiller@gmail.com" created="Sat, 2 Feb 2013 16:06:38 +0000">&lt;blockquote&gt;&lt;p&gt;Should we just require Java 7 on trunk now...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 - security updates for Java 6 stop next month. Trunk to java 7 seems right on time...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;to me, these both suggest trunk-only&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't have a strong opinion this early. Initially, doesn't seem like a problem to me. &lt;/p&gt;</comment>
                    <comment id="13569581" author="thetaphi" created="Sat, 2 Feb 2013 16:25:50 +0000">&lt;p&gt;+1 for Java 7. I would open some issues to update syntax sugar and fix some reflection-based hacks (e.g. IOUtils.addSuppressed).&lt;/p&gt;</comment>
                    <comment id="13569582" author="rcmuir" created="Sat, 2 Feb 2013 16:26:38 +0000">&lt;p&gt;I will split out a new JIRA issue for this!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12567730" name="LUCENE-4746.patch" size="6398" author="rcmuir" created="Sat, 2 Feb 2013 15:52:41 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 2 Feb 2013 13:57:17 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>311001</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310981</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4740] Weak references cause extreme GC churn</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4740</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We are running a set of independent search machines, running our custom software using lucene as a search library. We recently upgraded from lucene 3.0.3 to 3.6.1 and noticed a severe degradation of performance.&lt;/p&gt;

&lt;p&gt;After doing some heap dump digging, it turns out the process is stalling because it's spending so much time in GC. We noticed about 212 million WeakReference, originating from WeakIdentityMap, originating from MMapIndexInput.&lt;/p&gt;

&lt;p&gt;Our problem completely went away after removing the clones weakhashmap from MMapIndexInput, and as a side-effect, disabling support for explictly unmapping the mmapped data.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Linux debian squeeze 64 bit, Oracle JDK 6, 32 GB RAM, 16 cores&lt;br/&gt;
-Xmx16G&lt;/p&gt;</environment>
            <key id="12630159">LUCENE-4740</key>
            <summary>Weak references cause extreme GC churn</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="krka">Kristofer Karlsson</reporter>
                        <labels>
                    </labels>
                <created>Thu, 31 Jan 2013 15:49:44 +0000</created>
                <updated>Fri, 10 May 2013 00:05:06 +0100</updated>
                                    <version>3.6.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13567734" author="krka" created="Thu, 31 Jan 2013 15:54:05 +0000">&lt;p&gt;I am not sure if this is a problem in practice, but there are also WeakIdentityMaps used in AttributeImpl, AttributeSource, VirtualMethod where the keys of the map are classes, which I don't imagine get allocated or collected a lot at all.&lt;/p&gt;

&lt;p&gt;I replaced those maps with regular ConcurrentHashMap&amp;lt;Class, X&amp;gt; without any negative impact.&lt;/p&gt;</comment>
                    <comment id="13567744" author="krka" created="Thu, 31 Jan 2013 16:01:15 +0000">&lt;p&gt;This seems to be somewhat addressed in trunk, with the inputs in &lt;a href="http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java" class="external-link"&gt;http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java&lt;/a&gt; removing themselves from the clones list on close(). Should that be backported to 3.x?&lt;/p&gt;</comment>
                    <comment id="13567751" author="thetaphi" created="Thu, 31 Jan 2013 16:09:21 +0000">&lt;blockquote&gt;&lt;p&gt;I replaced those maps with regular ConcurrentHashMap&amp;lt;Class, X&amp;gt; without any negative impact.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This kills the possibility of unloading web applications in application servers. Those maps don't produce many references.&lt;/p&gt;

&lt;p&gt;Which OS are you using? Somebody had a similar problem on Windows - but on Linux all was fine. Maybe some JVM version has a bug in reference queues... Can you give your exact version number?&lt;/p&gt;</comment>
                    <comment id="13567887" author="krka" created="Thu, 31 Jan 2013 18:10:00 +0000">&lt;p&gt;Yes, that change is probably not a good general solution, but it worked well for our usecase. It might be nice to have support for unloadable classes optional.&lt;/p&gt;

&lt;p&gt;I am using debian squeeze, 64 bit, Oracle JDK 6 with a 16 GB heap and four indexes, totalling 9 GB&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;java version "1.6.0_26"
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The 212M WeakReferences on the heap indicates that the referencequeues weren't functioning correctly.&lt;/p&gt;

&lt;p&gt;In any case, if the useUnmap is false, then it seems unnecessary to even add references to the clones to the map.&lt;/p&gt;</comment>
                    <comment id="13567914" author="rcmuir" created="Thu, 31 Jan 2013 18:36:34 +0000">&lt;blockquote&gt;
&lt;p&gt;In any case, if the useUnmap is false, then it seems unnecessary to even add references to the clones to the map.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, I think make it null in this case. it just gives the user options (with different tradeoffs).&lt;/p&gt;</comment>
                    <comment id="13567924" author="thetaphi" created="Thu, 31 Jan 2013 18:45:27 +0000">&lt;blockquote&gt;&lt;p&gt;Yes, that change is probably not a good general solution, but it worked well for our usecase. It might be nice to have support for unloadable classes optional.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As I said, a change in AttributeSource or VirtualMethod is not needed, the number of total per-JVM references there are in the number of 10s. This is perfectly fine code and nobody needs to change anything. No need for "optional" class unloading. &lt;b&gt;Not&lt;/b&gt; using weak references here would be a major design issue and a large leak.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In any case, if the useUnmap is false, then it seems unnecessary to even add references to the clones to the map.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Robert and me were discussing about that already, we can do that, this patch is easy. We can offer that as an option (the no-unmap option), with the backside of e.g. windows can no longer delete index files unless they are garbage collected and especially higher disk usage while indexing.&lt;/p&gt;

&lt;p&gt;I did some testing with various JDKs on windows 64 bit, using a loop that clones one indexinput over and over. This loop runs successful for hours without OOM, so there is no cleanup problem, ReferenceQueues are working correctly. With a heap size of 512 MB and this simple loop, the number of Weak references is between 5000 and 600,000. But indeed, there are some GC pauses (in JDK 6 and 7). The reason for this is: Weak referees are a little bit more "reachable" than unreachable objects, so GC let them survive for a longer time than unreachable ones. There is nothing we can do against that. The main problem in your case maybe the really large heap size: why do you need it?&lt;/p&gt;

&lt;p&gt;My second test was to close every cloned index input (trunk/4.x only, where the commit you mentioned was added by me one week ago), in that case the number of references was of course a static "1" &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; In this test, no GC pauses occurred and the test ran faster.&lt;/p&gt;

&lt;p&gt;In my final test I disabled the put() to the WeakIdentityMap completely, in that case it was again faster, but this was caused more by the complete non-existence of any locking or maintenance of the ConcurrentHashMap.&lt;/p&gt;

&lt;p&gt;The times for 300 million clones:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;With default Lucene 4.x/trunk, no close of clones &lt;em&gt;(Lucene never closes clones and thats almost impossible to add)&lt;/em&gt;: 200 secs, GC pauses&lt;/li&gt;
	&lt;li&gt;With closing clones: 65 secs&lt;/li&gt;
	&lt;li&gt;Without any map: 40 secs&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;(JDK 6u32, windows, 64 bit, server vm, default garbage collector)&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;  &lt;span class="code-comment"&gt;// &lt;span class="code-keyword"&gt;for&lt;/span&gt; &lt;span class="code-keyword"&gt;this&lt;/span&gt; test, make the clones map in ByteBufferIndexInput &lt;span class="code-keyword"&gt;public&lt;/span&gt;/&lt;span class="code-keyword"&gt;package&lt;/span&gt;-&lt;span class="code-keyword"&gt;private&lt;/span&gt;/...
&lt;/span&gt;  &lt;span class="code-keyword"&gt;public&lt;/span&gt; void testGC() &lt;span class="code-keyword"&gt;throws&lt;/span&gt; Exception {
    MMapDirectory mmapDir = &lt;span class="code-keyword"&gt;new&lt;/span&gt; MMapDirectory(_TestUtil.getTempDir(&lt;span class="code-quote"&gt;"testGC"&lt;/span&gt;));
    IndexOutput io = mmapDir.createOutput(&lt;span class="code-quote"&gt;"bytes"&lt;/span&gt;, newIOContext(random()));
    io.writeVInt(5);
    io.close();
    IndexInput ii = mmapDir.openInput(&lt;span class="code-quote"&gt;"bytes"&lt;/span&gt;, IOContext.DEFAULT);
    &lt;span class="code-object"&gt;int&lt;/span&gt; hash = 0;
    &lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; i = 0; i &amp;lt; 300*1024*1024; i++) {
      &lt;span class="code-keyword"&gt;final&lt;/span&gt; IndexInput clone = ii.clone();
      hash += &lt;span class="code-object"&gt;System&lt;/span&gt;.identityHashCode(clone);
      &lt;span class="code-keyword"&gt;if&lt;/span&gt; (i % (10*1024) == 0) {
        &lt;span class="code-object"&gt;System&lt;/span&gt;.out.println(&lt;span class="code-quote"&gt;"&lt;span class="code-object"&gt;Number&lt;/span&gt; of clones: "&lt;/span&gt; + ((ByteBufferIndexInput) ii).clones.size());
      }
      &lt;span class="code-comment"&gt;//clone.close();
&lt;/span&gt;    }
    ii.close();
    mmapDir.close();
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In any case, we can allow user to disable unmap, but we then have to keep the weak references to the clones when unmapping is enabled, unless we add close() of clones to Lucene everywhere...&lt;/p&gt;

&lt;p&gt;Some other ideas are: Reuse the ByteBufferIndexInput instances, so we dont need to recreate them all the time. I have no idea how to do that, because we have no close() to release those, which brings us back to that problem again.&lt;/p&gt;</comment>
                    <comment id="13568007" author="krka" created="Thu, 31 Jan 2013 19:32:41 +0000">&lt;p&gt;Agree that the weakreferences for classes is probably a very minor part of it, and very unlikely part of the problem here.&lt;/p&gt;

&lt;p&gt;The unmap option is nice, and you could make it less complicated by simply disallowing the option to be changed after the mmapdirectory has ever been cloned. In practice it will always be set immediately after construction.&lt;/p&gt;</comment>
                    <comment id="13568074" author="krka" created="Thu, 31 Jan 2013 20:31:18 +0000">&lt;p&gt;After doing some more thinking and micro-benchmarking, I think the problem occurs when you create clones at a faster rate than the GC can cope with.&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;public class WeakTest extends TestCase {
    private static final int CPUS = Runtime.getRuntime().availableProcessors();

    public void testFoo() throws Exception {
        final WeakIdentityMap&amp;lt;Object, Boolean&amp;gt; map = WeakIdentityMap.newConcurrentHashMap();
        ExecutorService executorService = Executors.newCachedThreadPool();

        for (int k = 0; k &amp;lt; CPUS; k++) {
            executorService.submit(new Runnable() {
                public void run() {
                    while (true) {
                        map.put(new Object(), Boolean.TRUE);
                    }
                }
            });
            executorService.submit(new Runnable() {
                public void run() {
                    while (true) {
                        System.gc();
                    }
                }
            });
        }
        while (true) {
            System.out.println("Map size: " + map.size());
            Thread.sleep(1000);
        }
    }

}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I tried running this with -Xmx100m. This makes the map grow indefinitely.&lt;br/&gt;
I know this is not a very reliable test, since the JVM only considers System.gc() a hint, but it definitely seems like it's not very good at freeing weak references during high load.&lt;/p&gt;

&lt;p&gt;If I add Thread.sleep(1) in the put-worker, I can make it grow slower, but it still seems to grow over time.&lt;/p&gt;

&lt;p&gt;(Running java 7 on my home computer)&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;java version "1.7.0_07"
Java(TM) SE Runtime Environment (build 1.7.0_07-b10)
Java HotSpot(TM) 64-Bit Server VM (build 23.3-b01, mixed mode)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13568605" author="thetaphi" created="Fri, 1 Feb 2013 09:44:15 +0000">&lt;p&gt;Attached you will find the patch, that disables tracking of clones if the unmapping is disabled. We dont need to make the setting in MMapDirectory unmodifiable, after changing it all IndexInputs created afterwards, the new setting is used. This does not differ from the previous behaviour or unmapping at all.&lt;/p&gt;

&lt;p&gt;In general, people should in any case set this setting after constrcution (we may add a ctor param, too).&lt;/p&gt;</comment>
                    <comment id="13568611" author="thetaphi" created="Fri, 1 Feb 2013 09:52:49 +0000">&lt;blockquote&gt;&lt;p&gt;After doing some more thinking and micro-benchmarking, I think the problem occurs when you create clones at a faster rate than the GC can cope with.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree that might be aproblem and you may be facing it. How mayn requests per second do you have on your server?&lt;/p&gt;

&lt;p&gt;This behaviour is Java's weak reference GC behaviour, it has nothing to do with WeakIdentityMap. The default WeakHashMap from JDK has the same problems.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Agree that the weakreferences for classes is probably a very minor part of it, and very unlikely part of the problem here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is very common, the JDK uses the same mechanism like in AttributeSource at several places. It is definitely not part of the problem.&lt;/p&gt;

&lt;p&gt;The problem here is the weak map that has a very high throughput of puts (every query produces at least one IndexInput clone, possibly more). The high throughput already lead to the change to WeakIdentityMap recently, because a synchronized WeakHashMap was not able to handle the large number of concurrent puts (Lucene 3.6.0 regression).&lt;/p&gt;

&lt;p&gt;I am currently thinking of making the whole thing work without weak references and instead have some "hard reference" from the clone to master (it is already there, MappedByteBuffer.duplicate() returns a duplicate buffer that has a reference to the master). The problem with this is, that you need a check on every access of the IndexInput if the buffer is still valid. If it is only some null check, we may add it, but its risky for performance too.&lt;/p&gt;

&lt;p&gt;My ide was that the master creates some boolean&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; and passes this boolen&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; array to all childs. When the master closes, it does set the b&lt;span class="error"&gt;&amp;#91;0&amp;#93;&lt;/span&gt; to false. All childs would do a check on b&lt;span class="error"&gt;&amp;#91;0&amp;#93;&lt;/span&gt;... Not sure how this affects performance.&lt;/p&gt;</comment>
                    <comment id="13568615" author="krka" created="Fri, 1 Feb 2013 09:58:47 +0000">&lt;p&gt;Looks good, but what happens if you start with having useUnmap = false, then creating a bunch of clones, and then setting it back to useUnmap = true?&lt;/p&gt;

&lt;p&gt;If I read the code correctly (which I am not certain of), closing the original input will then unmap the data and break all the existing clones.&lt;/p&gt;</comment>
                    <comment id="13568634" author="krka" created="Fri, 1 Feb 2013 10:17:02 +0000">&lt;blockquote&gt;&lt;p&gt;I agree that might be aproblem and you may be facing it. How mayn requests per second do you have on your server?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not that many - about 8000 per minute on yesterdays peak, which is about 133 per second. However, each requests leads to several complex lucene queries, though I don't have any numbers on the actual lucene query throughput.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This behaviour is Java's weak reference GC behaviour, it has nothing to do with WeakIdentityMap. The default WeakHashMap from JDK has the same problems.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;My ide was that the master creates some boolean&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; and passes this boolen&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; array to all childs. When the master closes, it does set the b&lt;span class="error"&gt;&amp;#91;0&amp;#93;&lt;/span&gt; to false. All childs would do a check on b&lt;span class="error"&gt;&amp;#91;0&amp;#93;&lt;/span&gt;... Not sure how this affects performance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I thought about this too, and I am not sure the performance penalty would be that problematic (but it would need to be measured). And if possibly, users of the inputs should avoid doing small individual byte gets, and instead try to consume chunks of bytes to avoid the overhead.&lt;/p&gt;

&lt;p&gt;I would prefer an AtomicBoolean since it uses a volatile field. As far as I know, you can't make contents of arrays volatile.&lt;br/&gt;
In any case, wouldn't it would possible to skip the whole master/slave relationship and make everyone equal, just sharing the closed state flag? Though then running close() on a clone would close everything, which is possibly not what you want to happen.&lt;/p&gt;</comment>
                    <comment id="13568635" author="thetaphi" created="Fri, 1 Feb 2013 10:20:13 +0000">&lt;blockquote&gt;&lt;p&gt;but what happens if you start with having useUnmap = false, then creating a bunch of clones, and then setting it back to useUnmap = true? If I read the code correctly (which I am not certain of), closing the original input will then unmap the data and break all the existing clones.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The settings are decoupled:&lt;br/&gt;
If you start with useUnmap = false, all IndexInputs created will have no weak map, so when they are closed, the clones are not touched.&lt;/p&gt;

&lt;p&gt;If you change the setting to true, the already existing indexinputs will not be tracked (as before), but new indexinputs will get a map and all of their clones will be freed correctly.&lt;/p&gt;

&lt;p&gt;The other special case: If you change the setting from true -&amp;gt; false, all existing IndexInputs will keep their maps and will be cleaned up on close (buffers set to null). But the cleanMapping() method will get a no-op, so they are correctly nulled, but no longer unmapped.&lt;/p&gt;

&lt;p&gt;In any case a SIGSEGV is prevented (as good as we can without locking).&lt;/p&gt;

&lt;p&gt;In general, nothing breaks if you change the setting later, but you should really do it only after construction.&lt;/p&gt;</comment>
                    <comment id="13568637" author="thetaphi" created="Fri, 1 Feb 2013 10:23:39 +0000">&lt;blockquote&gt;&lt;p&gt;I would prefer an AtomicBoolean since it uses a volatile field. As far as I know, you can't make contents of arrays volatile.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This kills performance. MMapIndexInput would be slower than SimpleFSIndexInput! This is why the array is used as a fake "reference" to a boolean.&lt;/p&gt;

&lt;p&gt;The current approach of unmapping the byte buffers and protecting for sigsegv by nulling them is not 100% safe. The JVM may still crash if another thread does not yet see the nulled buffer. But in &lt;b&gt;most&lt;/b&gt; cases the use will get a AlreadyClosedException and can fix his code before he goes into production and his JVM crashes suddenly.&lt;/p&gt;</comment>
                    <comment id="13568657" author="krka" created="Fri, 1 Feb 2013 10:53:17 +0000">&lt;blockquote&gt;&lt;p&gt;If you change the setting to true, the already existing indexinputs will not be tracked (as before), but new indexinputs will get a map and all of their clones will be freed correctly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, but the already existing indexinputs will have buffers pointing to the same bytebuffer, so if you close the master, you would get SIGSEGV in the clones, since the master can not forcibly close the clones.&lt;/p&gt;</comment>
                    <comment id="13568659" author="krka" created="Fri, 1 Feb 2013 10:58:00 +0000">&lt;p&gt;freeBuffers in MMapIndexInput only looks at MMapDirectory.useUnmap, which is the thing that may change, unlike the trackClones / clones which is fixed once the master has been created.&lt;/p&gt;

&lt;p&gt;I propose adding this to close():&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;if (clones != null) { freeBuffers(); }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13568665" author="thetaphi" created="Fri, 1 Feb 2013 11:01:11 +0000">&lt;blockquote&gt;&lt;p&gt;Right, but the already existing indexinputs will have buffers pointing to the same bytebuffer, so if you close the master, you would get SIGSEGV in the clones, since the master can not forcibly close the clones.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right that can happen, the fix is to make the freeBuffer method use the setting of the actual IndexInput, not the global one of MMapDirectory.&lt;/p&gt;</comment>
                    <comment id="13568666" author="thetaphi" created="Fri, 1 Feb 2013 11:02:39 +0000">&lt;blockquote&gt;&lt;p&gt;I propose adding this to close():&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;thats not good and makes reuse of ByteBufferIndexInput complicated. MMapDirectory have to take care in its overridden abstract method (by using the IndexInput's instance setting.&lt;/p&gt;</comment>
                    <comment id="13568671" author="thetaphi" created="Fri, 1 Feb 2013 11:13:38 +0000">&lt;p&gt;Patch that makes the unmapping behaviour consistent. The "unmap" setting is now maintained by each MMapIndexInput. The unmap method was also moved into the impl class (the previous implementation was a relict from older times)&lt;/p&gt;</comment>
                    <comment id="13568673" author="thetaphi" created="Fri, 1 Feb 2013 11:21:52 +0000">&lt;p&gt;Better patch using the getter method instead stupid MMapDirectory.this.useUnmap.&lt;/p&gt;</comment>
                    <comment id="13568716" author="krka" created="Fri, 1 Feb 2013 12:50:19 +0000">&lt;p&gt;That looks good to me. Will this only be applied to latest 4.x or also be backported to 3.x?&lt;/p&gt;</comment>
                    <comment id="13569496" author="commit-tag-bot" created="Sat, 2 Feb 2013 10:38:19 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Uwe Schindler&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1441726" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1441726&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4740" title="Weak references cause extreme GC churn"&gt;LUCENE-4740&lt;/a&gt;: Don't track clones of MMapIndexInput if unmapping is disabled. This reduces GC overhead.&lt;/p&gt;</comment>
                    <comment id="13569498" author="thetaphi" created="Sat, 2 Feb 2013 10:44:43 +0000">&lt;p&gt;I committed this to trunk and 4.x branch.&lt;/p&gt;

&lt;p&gt;It is not yet planned to release another 3.6.x version. Users should upgrade to Lucene 4.x (4.2 will contain this patch).&lt;/p&gt;</comment>
                    <comment id="13569499" author="commit-tag-bot" created="Sat, 2 Feb 2013 10:46:00 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Uwe Schindler&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1441727" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1441727&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Merged revision(s) 1441726 from lucene/dev/trunk:&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4740" title="Weak references cause extreme GC churn"&gt;LUCENE-4740&lt;/a&gt;: Don't track clones of MMapIndexInput if unmapping is disabled. This reduces GC overhead.&lt;/p&gt;</comment>
                    <comment id="13569500" author="thetaphi" created="Sat, 2 Feb 2013 10:46:11 +0000">&lt;p&gt;I keep this issue open:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;If there will be a 3.6.3 release, we can backport.&lt;/li&gt;
	&lt;li&gt;We should try/benchmark other solutions to tracking clones, as there is indeed an overhead in GC.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13569636" author="elyograg" created="Sat, 2 Feb 2013 20:29:08 +0000">&lt;p&gt;I've been watching the activity on this issue because I have occasional extreme GC pauses in Solr with an 8GB heap.  GC tuning has reduced them somewhat so that my load balancer hasn't marked the service offline in a few days, but I think that things still aren't ideal.&lt;/p&gt;

&lt;p&gt;I will admit that I find most of the comments baffling.  Therefore I have a simple question.  If I run Solr branch_4x with this patch applied, will I benefit?  I can see from the commit log that unmmapping must disabled to benefit, but I don't know if this is how Solr operates.&lt;/p&gt;</comment>
                    <comment id="13569652" author="krka" created="Sat, 2 Feb 2013 22:09:48 +0000">&lt;p&gt;I just had an idea for a potential optimization:&lt;br/&gt;
Use a timerthread/executor/whatever to delay the unmap operation for X seconds after marking it as closed.&lt;br/&gt;
This should give the clones enough time to figure out that it should fail nicely instead of crash the jvm.&lt;/p&gt;

&lt;p&gt;You could even have the clones make sure to check the state of the master every X/2 seconds before performing any operation:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;byte readByte() {
  if (this.closed) {
    throw ...;
  }
  long currentTime = System.currentTimeMillis();
  if (currentTime - lastCheck &amp;gt;= X/2) {
    lastCheck = currentTime;
    if (master.isClosed() {
      this.close();
      throw ...;
    }
   }
  return curBuf.readByte();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not sure how much overhead this would mean in practice, but it would at least avoid synchronization most of the time.&lt;br/&gt;
An alternative is to schedule a periodic job for each clone, checking the master state and updating the clone state. That might lead to memory leaks unless weakreferences are used, so maybe that's not actually an improvement.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310000">
                <name>Duplicate</name>
                                                <inwardlinks description="is duplicated by">
                            <issuelink>
            <issuekey id="12630160">LUCENE-4741</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12630868">LUCENE-4755</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12567571" name="LUCENE-4740.patch" size="5885" author="thetaphi" created="Fri, 1 Feb 2013 11:21:52 +0000"/>
                    <attachment id="12567551" name="LUCENE-4740.patch" size="3299" author="thetaphi" created="Fri, 1 Feb 2013 09:44:15 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 31 Jan 2013 16:09:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310660</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310640</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4734] FastVectorHighlighter Overlapping Proximity Queries Do Not Highlight</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4734</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;If a proximity phrase query overlaps with any other query term it will not be highlighted.&lt;/p&gt;

&lt;p&gt;Example Text:  A B C D E F G&lt;/p&gt;

&lt;p&gt;Example Queries: &lt;/p&gt;

&lt;p&gt;"B E"~10 D&lt;br/&gt;
(D will be highlighted instead of "B C D E")&lt;/p&gt;

&lt;p&gt;"B E"~10 "C F"~10&lt;br/&gt;
(nothing will be highlighted)&lt;/p&gt;


&lt;p&gt;This can be traced to the FieldPhraseList constructor's inner while loop. From the first example query, the first TermInfo popped off the stack will be "B". The second TermInfo will be "D" which will not be found in the submap for "B E"~10 and will trigger a failed match.&lt;/p&gt;</description>
                <environment/>
            <key id="12629769">LUCENE-4734</key>
            <summary>FastVectorHighlighter Overlapping Proximity Queries Do Not Highlight</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rlauck">Ryan Lauck</reporter>
                        <labels>
                        <label>fastvectorhighlighter</label>
                        <label>highlighter</label>
                    </labels>
                <created>Tue, 29 Jan 2013 20:10:49 +0000</created>
                <updated>Fri, 10 May 2013 00:05:06 +0100</updated>
                                    <version>4.0</version>
                <version>4.1</version>
                <version>5.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/highlighter</component>
                        <due/>
                    <votes>1</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13565967" author="rlauck" created="Tue, 29 Jan 2013 23:34:48 +0000">&lt;p&gt;Added a patch with two test cases that reproduce the issue&lt;/p&gt;</comment>
                    <comment id="13567380" author="rlauck" created="Thu, 31 Jan 2013 05:38:25 +0000">&lt;p&gt;Tricky problem! I created a patch and modified my test cases (deleted the old test case patch).&lt;/p&gt;

&lt;p&gt;I'd appreciate any feedback, my solution seems durable and passes all highlighter test cases but I took a slightly different approach to finding the longest matching phrase.&lt;/p&gt;

&lt;p&gt;Also a bonus idea!&lt;br/&gt;
The current addIfNoOverlap method assumes that we would never want overlapping highlights and throws them out. A better approach might be to allow the user to provide a delegate that can add/modify overlapping WeightedPhraseInfo, some possible implementations could be:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;first only - current behavior&lt;/li&gt;
	&lt;li&gt;merge - creates a single WPI that covers all overlaps&lt;/li&gt;
	&lt;li&gt;split - gives priority to the first/existing WPI and add whatever is left overlaps&lt;/li&gt;
	&lt;li&gt;longest - gives priority to the longest WPI and drop any overlaps&lt;/li&gt;
	&lt;li&gt;include all - this use case solves my need to return only the offsets of all highlights in a document and perform the highlighting and overlap handling myself at a later stage.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13567818" author="rlauck" created="Thu, 31 Jan 2013 17:12:17 +0000">&lt;p&gt;I hope I'm not stepping on any toes here, but I realized my patch is similar to some of the work done in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4118" title="FastVectorHighlighter fail to highlight taking in input some proximity query."&gt;LUCENE-4118&lt;/a&gt;. My patch also solves the bug where repeated terms in a proximity query cause highlight matching to fail.&lt;/p&gt;

&lt;p&gt;I also took a different approach to handling reverse order matching on slop queries so that this patch could be a complete alternative to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4118" title="FastVectorHighlighter fail to highlight taking in input some proximity query."&gt;LUCENE-4118&lt;/a&gt;. I modified QueryPhraseMap.add to detect PhraseQuerys with slop and create a second mapping for the phrase terms in reverse order - this way no other code needs to change to handle proximity phrase terms appearing in reverse order.&lt;/p&gt;

&lt;p&gt;I added two simple test cases for both reverse ordering and repeated terms.&lt;/p&gt;</comment>
                    <comment id="13594842" author="rlauck" created="Wed, 6 Mar 2013 16:34:11 +0000">&lt;p&gt;Store the max possible slop on the QueryPhraseMap rather than the entire FieldQuery. This limits unnecessary matching when a PhraseQuery with a large slop is combined with other PhraseQuerys.&lt;/p&gt;

&lt;p&gt;Also, I added a fragment of slop recalculation code from WeightedSpanTermExtractor that handles PhraseQuerys with position gaps. The most common way this is encountered is by searching a phrase that contains stop words while using an analyzer that filters them.&lt;/p&gt;

&lt;p&gt;Also cleaned up the test cases a little, and added a few comments. &lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12559697">LUCENE-4118</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12572342" name="lucene-4734.patch" size="9468" author="rlauck" created="Wed, 6 Mar 2013 16:34:11 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310270</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310250</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4731] New ReplicatingDirectory mirrors index files to HDFS</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4731</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I've been working on a Directory implementation that mirrors the index files to HDFS (or other Hadoop supported FileSystem).&lt;/p&gt;

&lt;p&gt;A ReplicatingDirectory delegates all calls to an underlying Directory (supplied in the constructor). The only hooks are the deleteFile and sync calls. We submit deletes and replications to a single scheduler thread to keep things serializer. During a sync call, if "segments.gen" is seen in the list of files, we know a commit is finishing. After calling the deletage's sync method, we initialize an asynchronous replication as follows.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Read segments.gen (before leaving ReplicatingDirectory#sync), save the values for later&lt;/li&gt;
	&lt;li&gt;Get a list of local files from ReplicatingDirectory#listAll before leaving ReplicatingDirectory#sync&lt;/li&gt;
	&lt;li&gt;Submit replication task (DirectoryReplicator) to scheduler thread&lt;/li&gt;
	&lt;li&gt;Compare local files to remote files, determine which remote files get deleted, and which need to get copied&lt;/li&gt;
	&lt;li&gt;Submit a thread to copy each file (one thead per file)&lt;/li&gt;
	&lt;li&gt;Submit a thread to delete each file (one thead per file)&lt;/li&gt;
	&lt;li&gt;Submit a "finalizer" thread. This thread waits on the previous two batches of threads to finish. Once finished, this thread generates a new "segments.gen" remotely (using the version and generation number previously read in).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I have no idea where this would belong in the Lucene project, so i'll just attach the standalone class instead of a patch. It introduces dependencies on Hadoop core (and all the deps that brings with it).&lt;/p&gt;</description>
                <environment/>
            <key id="12629693">LUCENE-4731</key>
            <summary>New ReplicatingDirectory mirrors index files to HDFS</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mumrah">David Arthur</reporter>
                        <labels>
                    </labels>
                <created>Tue, 29 Jan 2013 14:37:34 +0000</created>
                <updated>Fri, 10 May 2013 00:05:06 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13565403" author="mumrah" created="Tue, 29 Jan 2013 14:38:31 +0000">&lt;p&gt;Attaching ReplicatingDirectory.java&lt;/p&gt;</comment>
                    <comment id="13565414" author="shaie" created="Tue, 29 Jan 2013 14:48:08 +0000">&lt;p&gt;Why do you need such a Directory implementation? HDFS already does replication (unless you turn it off), so I wonder what does that replication give you, that HDFS replication doesn't?&lt;/p&gt;</comment>
                    <comment id="13565418" author="mumrah" created="Tue, 29 Jan 2013 14:53:11 +0000">&lt;p&gt;The idea is to have a mirror of your index on a remote HDFS.&lt;/p&gt;

</comment>
                    <comment id="13565467" author="shaie" created="Tue, 29 Jan 2013 15:39:58 +0000">&lt;p&gt;Did you take a look at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2632" title="FilteringCodec, TeeCodec, TeeDirectory"&gt;LUCENE-2632&lt;/a&gt; (TeeDirectory)? I think it's similar to what you need? Perhaps you can compare the two?&lt;/p&gt;

&lt;p&gt;Hmmm .. but the approach you've taken here is different. While TeeDirectory mimics Unix's "tee" and forwards calls to two directories, ReplicationDirectory implements ... replication.&lt;/p&gt;

&lt;p&gt;I would not implement replication at the level of Directory, and rely on things like "when segments.gen is seen we know commit happened". It sounds too fragile of a protocol to me.&lt;/p&gt;

&lt;p&gt;Perhaps instead you can think of a replication module which lets a producer publish IndexCommits whenever it called commit(), and consumers can periodically poll the replicator for updates, giving it their current state. When an update is available, they do the replication? Or something along those lines? IndexCommits are much more "official" to rely on, than the fragile algorithm you describe. For example, You can use SnapshotDeletionPolicy to hold onto IndexCommits that are currently being replicated, which will prevent the deletion of their files. Whereas in your algorithm, if two commits are called close to each other, one thread could start a replication action, while the next commit will delete the files in the middle of copy, or just delete some of the files that haven't been copied yet.&lt;/p&gt;

&lt;p&gt;I think what we need in Lucene is a Replicator module &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;</comment>
                    <comment id="13566914" author="mumrah" created="Wed, 30 Jan 2013 21:19:43 +0000">&lt;p&gt;TeeDirectory was actually the inspiration for this. The primary difference is that I want to asynchronously copy the index files, rather than having two sync underlying Directories. The motivating use case for me is I want to run some Hadoop jobs that make use of my Lucene index, but I don't want to collocate Lucene and Hadoop (sounds like a recipe for bad performance all around). With this async strategy, commits will get to HDFS &lt;em&gt;eventually&lt;/em&gt;, and I don't really care how far behind the lag as, so long as I have a readable commit in HDFS.&lt;/p&gt;

&lt;p&gt;Also, regarding push vs pull, I'd rather push from Lucene to avoid having to deal with remote agents pulling.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Whereas in your algorithm, if two commits are called close to each other, one thread could start a replication action, while the next commit will delete the files in the middle of copy, or just delete some of the files that haven't been copied yet.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;"Replication actions" and "delete actions" are serialized by a single thread, so they will not be interleaved.&lt;/p&gt;</comment>
                    <comment id="13567396" author="shaie" created="Thu, 31 Jan 2013 06:16:19 +0000">&lt;p&gt;I see, so you only allow one commit at a time. That's not great either ... e.g. if the replicating thread copies a large index commit (due to merges or something), all other processes are stopped until it finishes. This makes indexing on Hadoop even more horrible (if such thing is possible &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;).&lt;/p&gt;

&lt;p&gt;You don't have to do pull requests, you can have an agent running on the Hadoop cluster (where MapReduce jobs are run) that will poll the index directory periodically and then push the files to HDFS. The difference is that it will:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Take a snapshot on the index, so those files that it copies won't get deleted for sure.&lt;/li&gt;
	&lt;li&gt;It does not block indexing operations. If it copies a large index commit, and few commits are made in parallel by the indexing process, then when the replication process finishes, it will copy a single index commit with all recent changes. That might even make it more efficient.&lt;/li&gt;
	&lt;li&gt;You don't rely on a fragile algorithm, e.g. the detection of segments.gen.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13567616" author="mumrah" created="Thu, 31 Jan 2013 13:22:09 +0000">&lt;blockquote&gt;&lt;p&gt;all other processes are stopped until it finishes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not exactly, just no other replication or delete events will happen&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Take a snapshot on the index, so those files that it copies won't get deleted for sure.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is that what the SnapshotDeletionPolicy does? This does sound more robust than watching for segments.gen - where can I see it in use? Is this what Solr uses for replication?&lt;/p&gt;

&lt;p&gt;What would be a recommended mechanism for receiving "push requests" from a remote agent? Does lucene have any kind of RPC server like Thrift built-in (I imagine not).&lt;/p&gt;</comment>
                    <comment id="13567621" author="shaie" created="Thu, 31 Jan 2013 13:30:58 +0000">&lt;blockquote&gt;&lt;p&gt;Not exactly, just no other replication or delete events will happen&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well in that case then you could run into troubles. I.e. imagine two threads, one doing commit() and one doing replication. The commit() thread could be much faster than the replication one. Therefore, it can do commit(#1), replication thread starts to replication that index commit. In the middle, the commit thread does commit(#2), which deletes some files of the previous commit (e.g. due to segment merging), and the replication thread will be left with a corrupt commit ...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is that what the SnapshotDeletionPolicy does&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. You can see how it's used in the tests. Also, here's a thread from the user list with an example code: &lt;a href="http://markmail.org/message/3novogsi6vcgarur" class="external-link"&gt;http://markmail.org/message/3novogsi6vcgarur&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am not sure if Solr uses it, but I think it does. I mean .. it's the "safe" way to replicate/backup your index.&lt;/p&gt;

&lt;p&gt;Lucene doesn't have an RPC server built-in .. I wrote a simple Servlet that responds to some REST API to invoke replication ...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12566981" name="ReplicatingDirectory.java" size="12161" author="mumrah" created="Tue, 29 Jan 2013 14:38:31 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 29 Jan 2013 14:48:08 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310194</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>310174</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4722] Can we move SortField.Type.SCORE/DOC to singleton SortField instances instead...?</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4722</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It's ... weird that you can do eg new SortField("myfield", SortField.Type.SCORE).&lt;/p&gt;

&lt;p&gt;We already have dedicated SortField.FIELD_SCORE and FIELD_DOC ... so I think apps should use those and never make a new SortField for them?&lt;/p&gt;</description>
                <environment/>
            <key id="12629334">LUCENE-4722</key>
            <summary>Can we move SortField.Type.SCORE/DOC to singleton SortField instances instead...?</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Fri, 25 Jan 2013 22:07:57 +0000</created>
                <updated>Fri, 10 May 2013 00:05:07 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13563802" author="rcmuir" created="Sun, 27 Jan 2013 13:01:37 +0000">&lt;p&gt;the fact there is a ".Type.CUSTOM" tells us this entire design is wrong!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 27 Jan 2013 13:01:37 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>309639</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>298484</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4688] Reuse TermsEnum in BlockTreeTermsReader</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4688</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Opening a TermsEnum comes with a significant cost at this point if done frequently like primary key lookups or if many segments are present. Currently we don't reuse it at all and create a lot of objects even if the enum is just used for a single seekExact (ie. TermQuery). Stressing the Terms#iterator(reuse) call shows significant gains with reuse...&lt;/p&gt;</description>
                <environment/>
            <key id="12627842">LUCENE-4688</key>
            <summary>Reuse TermsEnum in BlockTreeTermsReader</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Wed, 16 Jan 2013 15:30:18 +0000</created>
                <updated>Fri, 10 May 2013 00:05:07 +0100</updated>
                                    <version>4.0</version>
                <version>4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13555123" author="simonw" created="Wed, 16 Jan 2013 15:37:32 +0000">&lt;p&gt;here is an initial patch including my small benchmark that shows a pretty significant impact of reuse. &lt;/p&gt;

&lt;p&gt;the benchmark indexes 2 Million super small docs and checks for each doc if the ID has already been indexed. I use NRT manager to reopen the reader every second. &lt;/p&gt;

&lt;p&gt;the results are pretty significant IMO: &lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;start benchmark
run with reuse
Run took: 24 seconds with reuse terms enum = [true]
run without reuse
Run took: 34 seconds with reuse terms enum = [false]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;while all tests pass with that patch I really wanna ask somebody (mike? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; ) with more knowledge about the BlockTreeTermsReader to look at this patch!! &lt;/p&gt;

&lt;p&gt;I also run benchmarks with lucene util but didn't see any real gains with this change so far.&lt;/p&gt;</comment>
                    <comment id="13555126" author="mikemccand" created="Wed, 16 Jan 2013 15:39:27 +0000">&lt;p&gt;Awesome!  I'll look at the patch.&lt;/p&gt;

&lt;p&gt;Reuse is important w/ BlockTree's TermsEnum ...&lt;/p&gt;</comment>
                    <comment id="13555164" author="rcmuir" created="Wed, 16 Jan 2013 16:11:59 +0000">&lt;p&gt;Can we break this patch up... particularly i think we should look at the multitermquery API as a separate issue from BlockTree's impl.&lt;/p&gt;</comment>
                    <comment id="13555201" author="mikemccand" created="Wed, 16 Jan 2013 16:56:33 +0000">&lt;p&gt;I think it's interesting/powerful to enable across-segment reuse: none&lt;br/&gt;
of our other reuse APIs (DocsEnum, D&amp;amp;PEnum) can do that.&lt;/p&gt;

&lt;p&gt;But I'm not sure we should do it: to take full advantage of it&lt;br/&gt;
requires API changes (like the MTQ.getTermsEnum change) ... we'd have&lt;br/&gt;
to do something similar to Weight/Scorer to share the D/&amp;amp;PEnum across&lt;br/&gt;
segments.&lt;/p&gt;

&lt;p&gt;The patch itself is spooky: this BlockTree code is hairy, and I'm not&lt;br/&gt;
sure that the reset() isn't going to cause subtle corner-case bugs.&lt;br/&gt;
(Separately: we need to simplify this code: it's unapproachable now).&lt;/p&gt;

&lt;p&gt;The benchmark gain is impressive, but, we are talking about 10 seconds&lt;br/&gt;
over 2M docs right? So 5 micro-seconds (.005 msec) per document?  In a&lt;br/&gt;
more realistic scenario (indexing more "normal" docs) surely this is a&lt;br/&gt;
minor part of the time ...&lt;/p&gt;

&lt;p&gt;The app can always reuse itself per-segment today ... I think reuse is&lt;br/&gt;
rather expert so it's OK to offer that as the way to reuse?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12565130" name="LUCENE-4688.patch" size="163300" author="simonw" created="Wed, 16 Jan 2013 15:37:32 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 16 Jan 2013 15:39:27 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>304632</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>252460</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4638] If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4638</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The ClosedByInterruptException will prevent the index from being unlocked in close. If you try and close again, the call will hang. If you are using native locks and try to open a new IndexWriter, it will fail to get the lock. If you try IW#forceUnlock, it wont work because the not fully closed IW will still have the lock.&lt;/p&gt;

&lt;p&gt;ideas:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;On ClosedByInterruptException, IW should continue trying to close what it can and unlock the index? Generally I have see the exception trigger in commitInternal.&lt;/li&gt;
	&lt;li&gt;We should add a non static forceUnlock to IW that lets you remove the lock and start a new IW?&lt;/li&gt;
	&lt;li&gt;We should make the lock protected so IW sub classes could unlock the index in advanced use cases?&lt;/li&gt;
	&lt;li&gt;others?&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12624660">LUCENE-4638</key>
            <summary>If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Wed, 19 Dec 2012 16:13:22 +0000</created>
                <updated>Fri, 10 May 2013 00:05:07 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13536196" author="mikemccand" created="Wed, 19 Dec 2012 17:55:41 +0000">&lt;p&gt;Hmm why does the 2nd call to close hang?  Do you have the original exc?&lt;/p&gt;

&lt;p&gt;IW.rollback() should do a "better job" closing and releasing the lock, and in general on getting an exception from IW.close I think it's the only real recourse you have (ie, it's hard to know what docs you lost due to that exception).&lt;/p&gt;

&lt;p&gt;Also, I think &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4246" title="Fix IndexWriter.close() to not commit or wait for pending merges"&gt;LUCENE-4246&lt;/a&gt; (IW.close should "just close", not wait for merges, commit, etc.) would improve this situation because then close would reliably release the lock.&lt;/p&gt;</comment>
                    <comment id="13536207" author="markrmiller@gmail.com" created="Wed, 19 Dec 2012 18:04:24 +0000">&lt;p&gt;I should be able to find what it was hanging on, but a lot of logs to look back through. I can probably reproduce more easily instead when I get home tonight. If I remember right, it was trying to open an mmap input or something and if I remember right, it was just blocking. I'll reproduce and report the exact details.&lt;/p&gt;</comment>
                    <comment id="13538484" author="markrmiller@gmail.com" created="Fri, 21 Dec 2012 22:35:24 +0000">&lt;p&gt;I have not had a chance to duplicate the hang yet - using fullmetaljenkins to work on some other bugs. I really could use a resolution to this though.&lt;/p&gt;

&lt;p&gt;Currently, the advice for cleaning up after an IndexWriter in the javadoc is broken with native locks. You can't necessarily call close twice and you can't unlock using the static unlock method.&lt;/p&gt;

&lt;p&gt;Here is a patch that provides a way for users to use the unlock in a finally pattern that is safe for native locks.&lt;/p&gt;

&lt;p&gt;It adds a forceUnlock method to IndexWriter that is not static.&lt;/p&gt;</comment>
                    <comment id="13538486" author="markrmiller@gmail.com" created="Fri, 21 Dec 2012 22:37:37 +0000">&lt;p&gt;Probably want to put a != null check around the writeLock.&lt;/p&gt;</comment>
                    <comment id="13538621" author="mikemccand" created="Sat, 22 Dec 2012 01:20:21 +0000">&lt;p&gt;I don't think we should rush a fix here.&lt;/p&gt;

&lt;p&gt;Let's see if rollback would have fixed it (and really javadocs should state that as the "recovery" if you hit exc during close), and let's understand what was hanging in the 2nd call to IW.close.&lt;/p&gt;

&lt;p&gt;I think a new forceUnluck method in IndexWriter is too dangerous because the IndexWriter technically is still open so the app can continue to do ops after releasing the lock.&lt;/p&gt;</comment>
                    <comment id="13538632" author="markrmiller@gmail.com" created="Sat, 22 Dec 2012 01:30:23 +0000">&lt;blockquote&gt;&lt;p&gt;I think a new forceUnluck method in IndexWriter is too dangerous&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It's the same as the current static unlock method and javadocd the same.&lt;/p&gt;

&lt;p&gt;I'm okay with it not being in Lucene though - I figure users would like to avoid this bug as well, but simply making the lock factory protected exposes it in an advanced enough way that it couldnt be considered dangerous. That would let me gid rid of this bug as well.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Let's see if rollback would have fixed it &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'll try that.&lt;/p&gt;</comment>
                    <comment id="13538724" author="mikemccand" created="Sat, 22 Dec 2012 10:50:49 +0000">&lt;blockquote&gt;&lt;p&gt;simply making the lock factory protected exposes it in an advanced enough way that it couldnt be considered dangerous. That would let me gid rid of this bug as well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that's a good solution for this issue?&lt;/p&gt;

&lt;p&gt;It would still be nice to know if rollback resolves it (it's supposed to!), and why the 2nd IW.close() hangs (which is &lt;b&gt;weird&lt;/b&gt;).&lt;/p&gt;</comment>
                    <comment id="13538811" author="rcmuir" created="Sat, 22 Dec 2012 13:31:17 +0000">&lt;p&gt;if thats what gets committed, please keep the issue open in that case.&lt;/p&gt;

&lt;p&gt;This kind of behavior in close is outright buggy. its because its doing too much.&lt;/p&gt;</comment>
                    <comment id="13538812" author="rcmuir" created="Sat, 22 Dec 2012 13:36:50 +0000">&lt;p&gt;Also i would view it as a temporary solution, like until we have time to unfuck close() to not do so much.&lt;/p&gt;

&lt;p&gt;I dont care that the issue is controversial. Its time to bring this to a head. I'm good at that.&lt;/p&gt;</comment>
                    <comment id="13538813" author="rcmuir" created="Sat, 22 Dec 2012 13:38:00 +0000">&lt;p&gt;just a start so there is no wimpy solution committed permanently because close() does too much. I dont want 4.1 released with that solution.&lt;/p&gt;</comment>
                    <comment id="13539129" author="commit-tag-bot" created="Mon, 24 Dec 2012 00:16:10 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425561" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425561&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4638" title="If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks."&gt;LUCENE-4638&lt;/a&gt;, &lt;a href="https://issues.apache.org/jira/browse/SOLR-3180" title="ChaosMonkey test failures"&gt;SOLR-3180&lt;/a&gt;: try using the IW's writeLock to unlock&lt;/p&gt;</comment>
                    <comment id="13539136" author="commit-tag-bot" created="Mon, 24 Dec 2012 00:29:04 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425563" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425563&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4638" title="If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks."&gt;LUCENE-4638&lt;/a&gt;, &lt;a href="https://issues.apache.org/jira/browse/SOLR-3180" title="ChaosMonkey test failures"&gt;SOLR-3180&lt;/a&gt;: try using the IW's writeLock to unlock&lt;/p&gt;</comment>
                    <comment id="13539151" author="commit-tag-bot" created="Mon, 24 Dec 2012 02:12:11 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425574" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4638" title="If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks."&gt;LUCENE-4638&lt;/a&gt;, &lt;a href="https://issues.apache.org/jira/browse/SOLR-3180" title="ChaosMonkey test failures"&gt;SOLR-3180&lt;/a&gt;: revert for now (try using the IW's writeLock to unlock)&lt;/p&gt;</comment>
                    <comment id="13539154" author="commit-tag-bot" created="Mon, 24 Dec 2012 02:18:24 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Mark Robert Miller&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425576" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1425576&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4638" title="If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks."&gt;LUCENE-4638&lt;/a&gt;, &lt;a href="https://issues.apache.org/jira/browse/SOLR-3180" title="ChaosMonkey test failures"&gt;SOLR-3180&lt;/a&gt;: revert for now (try using the IW's writeLock to unlock)&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12599843">LUCENE-4246</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12562165" name="LUCENE-4638.patch" size="2094" author="markrmiller@gmail.com" created="Fri, 21 Dec 2012 22:35:30 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 19 Dec 2012 17:55:41 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>300487</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>244158</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4630] add a system property to allow testing of suspicious stuff</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4630</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;there are times when people want to add assumptions in test to prevent confusing/false failures in certain situations (eg: known bugs in JVM X, known incompatibilities between lucene feature Z and filesystem Y, etc...)&lt;/p&gt;

&lt;p&gt;By default we want these situations to be "skiped" in tests with clear messages so that it's clear to end users trying out releases that these tests can't be run for specific sitautions.&lt;/p&gt;

&lt;p&gt;But at the same time we need a way for developers to be able to try running these tests anyway so we know if/when the underliyng problem is resolved.&lt;/p&gt;

&lt;p&gt;i propose we add a "tests.suspicious.shit" system property, which defaults to "false" in the javacode, but can be set at runtime to "true"&lt;/p&gt;

&lt;p&gt;assumptions about things like incompatibilities with OSs, JVM vendors, JVM versions, filesystems, etc.. can all be dependent on this system propery.&lt;/p&gt;

</description>
                <environment/>
            <key id="12623855">LUCENE-4630</key>
            <summary>add a system property to allow testing of suspicious stuff</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="hossman">Hoss Man</reporter>
                        <labels>
                    </labels>
                <created>Thu, 13 Dec 2012 23:12:35 +0000</created>
                <updated>Fri, 10 May 2013 00:05:07 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13531635" author="thetaphi" created="Thu, 13 Dec 2012 23:40:08 +0000">&lt;p&gt;I am fine with such a system property and also some code to warn user of several incompatible combinations, but I want to be able to run tests to find the problems behind the issue.&lt;/p&gt;

&lt;p&gt;In my opinion, we should really warn users also on Solr startup, if they have jRockit (this JVM only works with Lucene if you pass -XnoOpt) or J9 (fails with Lucene 4.0+), so they don't corrumpt their index. Please note: Policeman Jenkins (before it was shot by some Generics Drug Dealer) was running JRockit with this JVM option.&lt;/p&gt;</comment>
                    <comment id="13532155" author="dweiss" created="Fri, 14 Dec 2012 08:08:22 +0000">&lt;p&gt;Why does it need to be a system property, Hoss? The test group annotations can be enabled/disabled via system properties and they also do display messages on assumption-ignored tests &amp;#8211; wouldn't this be enough to cover your use case?&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;@SuspiciousJ9Shit
@SuspiciousJRockitShit
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The only problem I see is that these need to be provided statically &amp;#8211; if you need to detect them at runtime then I'd either need to change the code of the runner or we'd need to switch to assumptions inside a rule, for example.&lt;/p&gt;</comment>
                    <comment id="13532273" author="commit-tag-bot" created="Fri, 14 Dec 2012 12:34:42 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;trunk commit&amp;#93;&lt;/span&gt; Uwe Schindler&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1421818" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1421818&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4630" title="add a system property to allow testing of suspicious stuff"&gt;LUCENE-4630&lt;/a&gt; what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!&lt;/p&gt;</comment>
                    <comment id="13532276" author="commit-tag-bot" created="Fri, 14 Dec 2012 12:38:13 +0000">&lt;p&gt;&lt;span class="error"&gt;&amp;#91;branch_4x commit&amp;#93;&lt;/span&gt; Uwe Schindler&lt;br/&gt;
&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1421819" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1421819&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Merged revision(s) 1421818 from lucene/dev/trunk:&lt;br/&gt;
Revert the revert of the revert. I hope Robert Muir will followup and we can agree that this patch is not really a good idea and we should discuss on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4630" title="add a system property to allow testing of suspicious stuff"&gt;LUCENE-4630&lt;/a&gt; what to do. I know that Mike is already beasting J9, so we would need an option to disable this AssumptionFailedEx. Thanks in advance!&lt;/p&gt;</comment>
                    <comment id="13532541" author="hossman" created="Fri, 14 Dec 2012 18:41:10 +0000">
&lt;blockquote&gt;&lt;p&gt;In my opinion, we should really warn users also on Solr startup, if they have...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is a great idea &amp;#8211; i've spun off &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4631" title="Add utility class for getting information about suspicious system concerns at runtime"&gt;LUCENE-4631&lt;/a&gt; to track that since it's kind of broader then if/how to allow people to run tests even under suspicious circumstances (but whatever we add there can probably be leveraged here)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why does it need to be a system property, Hoss?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No reason ... it was just the first thing i thought of that seemed really generic. Anyplace we might normally write...&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-object"&gt;boolean&lt;/span&gt; someBoolean = ...something interesting...;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (someBoolean) {
  &lt;span class="code-keyword"&gt;throw&lt;/span&gt; &lt;span class="code-keyword"&gt;new&lt;/span&gt; AssumptionViolatedException(&lt;span class="code-quote"&gt;"...why your system is suspicious..."&lt;/span&gt;)
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...could be replaced with...&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-object"&gt;boolean&lt;/span&gt; someBoolean = ...something interesting...;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (someBoolean || &lt;span class="code-object"&gt;Boolean&lt;/span&gt;.getBoolean(&lt;span class="code-quote"&gt;"tests.suspicious.shit"&lt;/span&gt;)) {
  &lt;span class="code-keyword"&gt;throw&lt;/span&gt; &lt;span class="code-keyword"&gt;new&lt;/span&gt; AssumptionViolatedException(&lt;span class="code-quote"&gt;"...why your system is suspicious..."&lt;/span&gt;)
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...of the top of my head, i wasn't sure if an annotation would be as easy to use (particularly when you might mix and match with other test groups)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;@SuspiciousJ9Shit&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My suggestion was to try and keep it &lt;em&gt;reaallly&lt;/em&gt; generic ... so that with one "option" active you say "i'm a developer who is asking for trouble, try everything even if you think it's not valid on my system.  Making people know that their particular os/jvm/filesystem/jvm-opt-combos are suspicious therefore they need to explicitly ask for test group X and test group Y test group Z seems like it would make it overly hard for people to try everything.&lt;/p&gt;

&lt;p&gt;Perhaps the ideal case would be specific annotations like you describe, which could be used as test groups for people who want to go out of their way to test specific suspicious stuff (ie: "there is a new J9 JVM, does it still have these problems?") but then have a feature in the runner that by default those groups are skipped with a clear AssumptionViolatedException("...what is suspicious about your setup ...") but if you set "-Dtest.suspicious.shit=true" then instead the runner will run those tests anyway, but wrap any failures/exceptions it gets in another "FailureUnderSuspiciousCircumstances" exception whose getMessage() would contain info about what assumption would have normally prevented that test from if you hadn't gone out of your way to run it.&lt;/p&gt;

&lt;p&gt;what do you think?&lt;/p&gt;
</comment>
                    <comment id="13532578" author="dweiss" created="Fri, 14 Dec 2012 19:36:24 +0000">&lt;blockquote&gt;&lt;p&gt;but if you set "-Dtest.suspicious.shit=true" then instead the runner will run those tests anyway, &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This you can do already; test groups can be turned on and off by overriding their assigned system property, no problem with that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but wrap any failures/exceptions it gets in another "FailureUnderSuspiciousCircumstances" exception whose getMessage() would contain info about what assumption would have normally prevented that test from if you hadn't gone out of your way to run it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Even this description makes me feel dizzy... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I get your idea but I don't know how to implement it in a sensible way. It &lt;em&gt;could&lt;/em&gt; be a rule that would intercept failures, check for groups annotations and then rethrow... but I honestly don't think many people would find it useful (or understand the principle under which it operates).&lt;/p&gt;

&lt;p&gt;Dawid&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12624022">LUCENE-4631</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 13 Dec 2012 23:40:08 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>297577</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>235552</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4583] StraightBytesDocValuesField fails if bytes &gt; 32k</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4583</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I didn't observe any limitations on the size of a bytes based DocValues field value in the docs.  It appears that the limit is 32k, although I didn't get any friendly error telling me that was the limit.  32k is kind of small IMO; I suspect this limit is unintended and as such is a bug.    The following test fails:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; void testBigDocValue() &lt;span class="code-keyword"&gt;throws&lt;/span&gt; IOException {
    Directory dir = newDirectory();
    IndexWriter writer = &lt;span class="code-keyword"&gt;new&lt;/span&gt; IndexWriter(dir, writerConfig(&lt;span class="code-keyword"&gt;false&lt;/span&gt;));

    Document doc = &lt;span class="code-keyword"&gt;new&lt;/span&gt; Document();
    BytesRef bytes = &lt;span class="code-keyword"&gt;new&lt;/span&gt; BytesRef((4+4)*4097);&lt;span class="code-comment"&gt;//4096 works
&lt;/span&gt;    bytes.length = bytes.bytes.length;&lt;span class="code-comment"&gt;//&lt;span class="code-object"&gt;byte&lt;/span&gt; data doesn't matter
&lt;/span&gt;    doc.add(&lt;span class="code-keyword"&gt;new&lt;/span&gt; StraightBytesDocValuesField(&lt;span class="code-quote"&gt;"dvField"&lt;/span&gt;, bytes));
    writer.addDocument(doc);
    writer.commit();
    writer.close();

    DirectoryReader reader = DirectoryReader.open(dir);
    DocValues docValues = MultiDocValues.getDocValues(reader, &lt;span class="code-quote"&gt;"dvField"&lt;/span&gt;);
    &lt;span class="code-comment"&gt;//FAILS IF BYTES IS BIG!
&lt;/span&gt;    docValues.getSource().getBytes(0, bytes);

    reader.close();
    dir.close();
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment/>
            <key id="12618435">LUCENE-4583</key>
            <summary>StraightBytesDocValuesField fails if bytes &gt; 32k</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="dsmiley">David Smiley</reporter>
                        <labels>
                    </labels>
                <created>Sat, 1 Dec 2012 16:09:57 +0000</created>
                <updated>Tue, 21 May 2013 15:02:28 +0100</updated>
                                    <version>4.0</version>
                <version>4.1</version>
                <version>5.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>9</watches>
                                                    <comments>
                    <comment id="13507998" author="yseeley@gmail.com" created="Sat, 1 Dec 2012 16:24:46 +0000">&lt;p&gt;I'm guessing that limit is due to the implementation with PagedBytes?&lt;br/&gt;
These limits were sensible when applied to indexed values, but obviously not so much to stored values (unless we decide that DocValues are only meant for smallish values and document the limit). &lt;/p&gt;</comment>
                    <comment id="13508284" author="rcmuir" created="Sun, 2 Dec 2012 15:01:00 +0000">&lt;p&gt;The most important thing: if this implementation (or if we decide dv itself) should be limited,&lt;br/&gt;
then it should check this at index-time and throw a useful exception.&lt;/p&gt;</comment>
                    <comment id="13508759" author="dsmiley" created="Mon, 3 Dec 2012 14:27:34 +0000">&lt;p&gt;FWIW the app the triggered this has a document requiring ~68k but there's a long tail down such that most documents only need 8 bytes.  As a hack, I could use multiple fields to break out of the 32k limit and concatenate each together (yuck).  It'd be great if this 32k limit wasn't there.&lt;/p&gt;</comment>
                    <comment id="13508761" author="rcmuir" created="Mon, 3 Dec 2012 14:29:59 +0000">&lt;p&gt;I'm not concerned if the limit is 10 bytes.&lt;/p&gt;

&lt;p&gt;if it is, then it is what it is.&lt;/p&gt;

&lt;p&gt;Its just important that IW throw exception at index-time when any such limit is exceeded.&lt;/p&gt;</comment>
                    <comment id="13508763" author="rcmuir" created="Mon, 3 Dec 2012 14:33:54 +0000">&lt;p&gt;The only correct bugfix here is a missing check.&lt;/p&gt;

&lt;p&gt;any discussion about extending limitations in the supported lengths belongs on another issue.&lt;/p&gt;</comment>
                    <comment id="13508910" author="yseeley@gmail.com" created="Mon, 3 Dec 2012 18:08:46 +0000">&lt;p&gt;Looking at the issue name and the problem that David ran into, this issue is certainly about more than a missing check during indexing.&lt;br/&gt;
Small hidden limits can still cause stuff to blow up in production - the user may not have thought to test anything above 32K.  Small limits need to be documented.&lt;/p&gt;

&lt;p&gt;Like David, I also suspect that the limit was unintended and represents a bug.&lt;br/&gt;
The question is on a practical level, how easy is it to raise the limit, and are there any negative consequences of doing so?  If it's not easy (or there are negative consequences), I think it's OK to leave it at 32K and document it as a current limitation.  Off the top of my head, I can't really think of use cases that would require more, but perhaps others might?&lt;/p&gt;

&lt;p&gt;Of course we should also fail early if someone tries to add a value above that limit.&lt;/p&gt;</comment>
                    <comment id="13508924" author="rcmuir" created="Mon, 3 Dec 2012 18:28:18 +0000">&lt;p&gt;Again, I really think adding the check and documenting current limits is what should happen here.&lt;/p&gt;

&lt;p&gt;Just like length of indexed terms are limited to 32k, its a bigger issue to try to deal with this (especially in the current DV situation).&lt;/p&gt;

&lt;p&gt;I think also if you are putting very large values in DV, you know its perfectly acceptable to require a custom codec for this kind of situation. the one we provide can be general purpose.&lt;/p&gt;

&lt;p&gt;I dont think we should try to cram in a huge change to the limits masqueraded as a bug... the bug is not recognizing the limit at index time (and of course documenting it in the codec, or indexwriter, depending on where it is (currently here i think its the codec).&lt;/p&gt;</comment>
                    <comment id="13508955" author="yseeley@gmail.com" created="Mon, 3 Dec 2012 19:22:18 +0000">&lt;blockquote&gt;&lt;p&gt;I dont think we should try to cram in a huge change to the limits masqueraded as a bug...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the limit was not intentional then it was certainly a bug (not just a missing check). Now we have to figure out what to do about it.&lt;/p&gt;

&lt;p&gt;This issue is about deciding what the limit should be (and 32K &lt;b&gt;may&lt;/b&gt; be fine, depending on the trade-offs, as I said), and then documenting and enforcing that limit.&lt;br/&gt;
For example your previous "I'm not concerned if the limit is 10 bytes." would get a -1 from me as "clearly not big enough... lets fix it".&lt;/p&gt;</comment>
                    <comment id="13508979" author="rcmuir" created="Mon, 3 Dec 2012 19:49:30 +0000">&lt;p&gt;Just like changing the length here gets a -1 from me. Pretty simple.&lt;/p&gt;</comment>
                    <comment id="13508988" author="yseeley@gmail.com" created="Mon, 3 Dec 2012 19:54:56 +0000">&lt;blockquote&gt;&lt;p&gt;Just like changing the length here gets a -1 from me. Pretty simple.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Regardless of how easy or hard it might be, regardless of what use cases are brought up (I was waiting to hear from David at least, since he hit it), regardless of what the trade-offs might be involved with changing an unintended/accidental limit?  That's just silly.&lt;/p&gt;

&lt;p&gt;It also doesn't make sense to -1 something "here" vs somewhere else.  +1s/-1s are for code changes, regardless of where they are discussed.&lt;/p&gt;</comment>
                    <comment id="13509044" author="barakatx2" created="Mon, 3 Dec 2012 21:03:33 +0000">&lt;p&gt;The limitation comes from PagedBytes. When PagedBytes is created it is given a number of bits to use per block. The blockSize is set to (1 &amp;lt;&amp;lt; blockBits). From what I've seen, classes that use PagedBytes usually pass in 15 as the blockBits. This leads to the 32768 byte limit.&lt;/p&gt;

&lt;p&gt;The fillSlice function of the PagedBytes.Reader will return a block of bytes that is either inside one block or overlapping two blocks. If you try to give it a length that is over the block size it will hit the out of bounds exception. For the project I am working on, we need more than 32k bytes for our DocValues. We need that much rarely, but we still need that much to keep the search functioning. I fixed this for our project by changing fillSlices to this:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://pastebin.com/raw.php?i=TCY8zjAi" class="external-link"&gt;http://pastebin.com/raw.php?i=TCY8zjAi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Test unit:&lt;br/&gt;
&lt;a href="http://pastebin.com/raw.php?i=Uy29BGGJ" class="external-link"&gt;http://pastebin.com/raw.php?i=Uy29BGGJ&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After placing this in our Solr instance, the search no longer crashes and returns the correct values when the document has a DocValues field more than 32k bytes. As far as I know there is no limit now. I haven't noticed a performance hit. It shouldn't really affect performance unless you have many of these large DocValues fields. Thank you to David for his help with this.&lt;/p&gt;

&lt;p&gt;Edit: This only works when start == 0. Seeing if I can fix it.&lt;/p&gt;</comment>
                    <comment id="13509117" author="yseeley@gmail.com" created="Mon, 3 Dec 2012 22:08:34 +0000">&lt;blockquote&gt;&lt;p&gt;I am working on, we need more than 32k bytes for our DocValues. We need that much rarely, but we still need that much to keep the search functioning.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks!  I suspected one generic use case would be "normally small, but hard to put an upper bound on".  That's great if the only issue really is PagedBytes.fillSlices()!  That definitely shouldn't have any performance impact since the first "if" will always be true in the common case.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Edit: This only works when start == 0. Seeing if I can fix it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A simple loop might be easiest to understand, rather than calculating with DIV and MOD?&lt;/p&gt;</comment>
                    <comment id="13509856" author="barakatx2" created="Tue, 4 Dec 2012 16:59:17 +0000">&lt;p&gt;I updated the code to work when start isn't zero. The code can still crash if you ask for a length that goes beyond the total size of the paged bytes, but I'm not sure how you guys like to prevent things like that. The code seems to be working fine with our Solr core so far. I am new to posting patches and writing test units in Java so please let me know if there is anything wrong with the code.&lt;/p&gt;</comment>
                    <comment id="13651926" author="barakatx2" created="Wed, 8 May 2013 15:20:21 +0100">&lt;p&gt;I recently switched from 4.1 to 4.3, and my patch needed to be updated because of the changes to DocValues. The problem was almost fixed for BinaryDocValues, but it just needed one little change. I've attached a patch that removes the BinaryDocValues exception when the length is over BYTE_BLOCK_SIZE (32k), fixes ByteBlockPool#readBytes:348, and changes the TestDocValuesIndexing#testTooLargeBytes test to check for accuracy.&lt;/p&gt;</comment>
                    <comment id="13651945" author="rcmuir" created="Wed, 8 May 2013 15:46:09 +0100">&lt;p&gt;I dont think we should just bump the limit like this.&lt;/p&gt;

&lt;p&gt;the patch is not safe: Some codecs rely upon this limit (e.g. they use 64k-size pagedbytes and other things).&lt;/p&gt;

&lt;p&gt;But in general anyway I'm not sure what the real use cases are for storing &amp;gt; 32kb inside a single document value.&lt;/p&gt;</comment>
                    <comment id="13651990" author="shaie" created="Wed, 8 May 2013 16:29:22 +0100">&lt;p&gt;A user hit this limitation on the user list a couple weeks ago while try to index a very large number of facets: &lt;a href="http://markmail.org/message/dfn4mk3qe7advzcd" class="external-link"&gt;http://markmail.org/message/dfn4mk3qe7advzcd&lt;/a&gt;. So this is one usecase, and also it appears there is an exception thrown at indexing time?&lt;/p&gt;</comment>
                    <comment id="13651994" author="rcmuir" created="Wed, 8 May 2013 16:37:52 +0100">&lt;p&gt;Over 6,000 facets per doc?&lt;/p&gt;

&lt;p&gt;Yes there is an exception thrown at index time. I added this intentionally and added a test that ensures you get it.&lt;/p&gt;</comment>
                    <comment id="13652012" author="shaie" created="Wed, 8 May 2013 17:02:41 +0100">&lt;blockquote&gt;&lt;p&gt;Over 6,000 facets per doc?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right. Not sure how realistic is his case (something about proteins, he explains it here: &lt;a href="http://markmail.org/message/2wxavktzyxjtijqe" class="external-link"&gt;http://markmail.org/message/2wxavktzyxjtijqe&lt;/a&gt;), and whether he should enable facet partitions or not, but he agreed these are extreme cases and expects only few docs like that. At any rate, you asked for a usecase &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. Not saying we should support it, but it is one. If it can be supported without too much pain, then perhaps we should. Otherwise, I think we can live w/ the limitation and leave it for the app to worry about a workaround / write its own Codec.&lt;/p&gt;</comment>
                    <comment id="13652014" author="rcmuir" created="Wed, 8 May 2013 17:04:46 +0100">&lt;p&gt;For the faceting use case, we have SortedSetDocValuesField, which can hold 2B facets per-field and you can have.... maybe 2B fields per doc.&lt;/p&gt;

&lt;p&gt;So the limitation here is not docvalues.&lt;/p&gt;

&lt;p&gt;BINARY datatype isnt designed for faceting.&lt;/p&gt;</comment>
                    <comment id="13652045" author="mikemccand" created="Wed, 8 May 2013 17:43:35 +0100">&lt;p&gt;I think we should fix the limit in core, and then existing codecs should enforce their own limits, at indexing time?&lt;/p&gt;

&lt;p&gt;This way users that sometimes need to store &amp;gt; 32 KB binary doc value can do so, with the right DVFormat.&lt;/p&gt;</comment>
                    <comment id="13652097" author="shaie" created="Wed, 8 May 2013 18:33:41 +0100">&lt;blockquote&gt;&lt;p&gt;For the faceting use case, we have SortedSetDocValuesField&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, this is for one type of faceting, no hierarchies etc. It's also slower than the BINARY DV method ...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;BINARY datatype isnt designed for faceting.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe, but that's the best we have for now.&lt;/p&gt;</comment>
                    <comment id="13652105" author="rcmuir" created="Wed, 8 May 2013 18:40:10 +0100">&lt;blockquote&gt;
&lt;p&gt;No, this is for one type of faceting, no hierarchies etc. It's also slower than the BINARY DV method ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its not inherently slower. I just didnt spend a month inlining vint codes or writing custom codecs like you did for the other faceting method. &lt;br/&gt;
Instead its the simplest thing that can possibly work right now.&lt;/p&gt;

&lt;p&gt;I will call you guys out on this every single time you bring it up.&lt;/p&gt;

&lt;p&gt;I'm -1 to bumping the limit&lt;/p&gt;</comment>
                    <comment id="13652108" author="dsmiley" created="Wed, 8 May 2013 18:41:47 +0100">&lt;p&gt;I looked over the patch carefully and stepped through the relevant code in a debugging session and I think it's good.&lt;/p&gt;

&lt;p&gt;I don't see why this arbitrary limit was here. The use-case for why Barakat hit this is related to storing values per document so that a custom ValueSource I wrote can examine it.  It's for spatial multi-value fields and some businesses (docs) have a ton of locations out there (e.g. McDonalds).  FWIW very few docs have such large values, and if it were to become slow then I have ways to more cleverly examine a subset of the returned bytes.  I think its silly to force the app to write a Codec (even if its trivial) to get out of this arbitrary limit.&lt;/p&gt;
</comment>
                    <comment id="13652983" author="mikemccand" created="Thu, 9 May 2013 15:14:30 +0100">&lt;p&gt;I iterated from Barakat's patch: improved the test, and added&lt;br/&gt;
enforcing of the limit in the appropriate DocValuesFormats impls.&lt;/p&gt;

&lt;p&gt;Disk, SimpleText, CheapBastard and (I think ... still need a test&lt;br/&gt;
here) Facet42DVFormat don't have a limit, but Lucene40/42 still do.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I'm -1 to bumping the limit&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Are you also against just fixing the limit in the core code&lt;br/&gt;
(IndexWriter/BinaryDocValuesWriter) and leaving the limit enforced in&lt;br/&gt;
the existing DVFormats (my patch)?&lt;/p&gt;

&lt;p&gt;I thought that was a good compromise ...&lt;/p&gt;

&lt;p&gt;This way at least users can still build their own / use DVFormats that&lt;br/&gt;
don't have the limit.&lt;/p&gt;</comment>
                    <comment id="13653179" author="dsmiley" created="Thu, 9 May 2013 21:54:31 +0100">&lt;blockquote&gt;&lt;p&gt;the patch is not safe: Some codecs rely upon this limit (e.g. they use 64k-size pagedbytes and other things).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Rob, can you please elaborate?&lt;/p&gt;</comment>
                    <comment id="13655522" author="rcmuir" created="Sun, 12 May 2013 13:09:20 +0100">&lt;blockquote&gt;
&lt;p&gt;Are you also against just fixing the limit in the core code&lt;br/&gt;
(IndexWriter/BinaryDocValuesWriter) and leaving the limit enforced in&lt;br/&gt;
the existing DVFormats (my patch)?&lt;/p&gt;

&lt;p&gt;I thought that was a good compromise ...&lt;/p&gt;

&lt;p&gt;This way at least users can still build their own / use DVFormats that&lt;br/&gt;
don't have the limit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm worried about a few things:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I think the limit is ok, because in my eyes its the limit of a single term. I feel that anyone arguing for increasing the limit only has abuse cases (not use cases) in mind. I'm worried about making dv more complicated for no good reason.&lt;/li&gt;
	&lt;li&gt;I'm worried about opening up the possibility of bugs and index corruption (e.g. clearly MULTIPLE people on this issue dont understand why you cannot just remove IndexWriter's limit without causing corruption).&lt;/li&gt;
	&lt;li&gt;I'm really worried about the precedent: once these abuse-case-fans have their way and increase this limit, they will next argue that we should do the same for SORTED, maybe SORTED_SET, maybe even inverted terms. They will make arguments that its the same as binary, just with sorting, and why should sorting bring in additional limits. I can easily see this all spinning out of control.&lt;/li&gt;
	&lt;li&gt;I think that most people hitting the limit are abusing docvalues as stored fields, so the limit is providing a really useful thing today actually, and telling them they are doing something wrong.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The only argument i have &lt;b&gt;for&lt;/b&gt; removing the limit is that by expanding BINARY's possible abuse cases (in my opinion, thats pretty much all its useful for), we might prevent additional complexity from being added elsewhere to DV in the long-term.&lt;/p&gt;</comment>
                    <comment id="13655525" author="jkrupan" created="Sun, 12 May 2013 13:35:31 +0100">&lt;blockquote&gt;&lt;p&gt;abusing docvalues as stored fields&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Great point. I have to admit that I still don't have a 100% handle on the use case(s) for docvalues vs. stored fields, even though I've asked on the list. I mean, sometimes the chatter seems to suggest that dv is the successor to stored values. Hmmm... in that case, I should be able to store the full text of a 24 MB PDF file in a dv. Now, I know that isn't true.&lt;/p&gt;

&lt;p&gt;Maybe we just need to start with some common use cases, based on size: tiny (16 bytes or less), small (256 or 1024 bytes or less), medium (up to 32K), and large (upwards of 1MB, and larger.) It sounds like large implies stored field.&lt;/p&gt;

&lt;p&gt;A related "concern" is dv or stored fields that need a bias towards being in memory and in the heap, vs. a bias towards being "off heap". Maybe the size category is the hint: tiny and small bias towards on-heap, medium and certainly large bias towards off-heap. If people are only going towards DV because they think they get off-heap, then maybe we need to reconsider the model of what DV vs. stored is really all about. But then that leads back to DV somehow morphing out of column-stride fields.&lt;/p&gt;</comment>
                    <comment id="13655526" author="mikemccand" created="Sun, 12 May 2013 13:45:40 +0100">&lt;blockquote&gt;
&lt;p&gt;I'm worried about a few things:&lt;br/&gt;
I think the limit is ok, because in my eyes its the limit of a single term. I feel that anyone arguing for increasing the limit only has abuse cases (not use cases) in mind. I'm worried about making dv more complicated for no good reason.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess I see DV binary as more like a stored field, just stored&lt;br/&gt;
column stride for faster access.  Faceting (and I guess spatial)&lt;br/&gt;
encode many things inside one DV binary field.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I'm worried about opening up the possibility of bugs and index corruption (e.g. clearly MULTIPLE people on this issue dont understand why you cannot just remove IndexWriter's limit without causing corruption).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree this is a concern and we need to take it slow, add good&lt;br/&gt;
test coverage.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I'm really worried about the precedent: once these abuse-case-fans have their way and increase this limit, they will next argue that we should do the same for SORTED, maybe SORTED_SET, maybe even inverted terms. They will make arguments that its the same as binary, just with sorting, and why should sorting bring in additional limits. I can easily see this all spinning out of control.&lt;br/&gt;
I think that most people hitting the limit are abusing docvalues as stored fields, so the limit is providing a really useful thing today actually, and telling them they are doing something wrong.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think we should change the limit for sorted/set nor terms: I&lt;br/&gt;
think we should raise the limit ONLY for BINARY, and declare that DV&lt;br/&gt;
BINARY is for these "abuse" cases.  So if you really really want&lt;br/&gt;
sorted set with a higher limit then you will have to encode yourself&lt;br/&gt;
into DV BINARY.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The only argument i have for removing the limit is that by expanding BINARY's possible abuse cases (in my opinion, thats pretty much all its useful for), we might prevent additional complexity from being added elsewhere to DV in the long-term.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13655529" author="mikemccand" created="Sun, 12 May 2013 13:55:18 +0100">&lt;blockquote&gt;&lt;p&gt;I have to admit that I still don't have a 100% handle on the use case(s) for docvalues vs. stored fields, even though I've asked on the list. I mean, sometimes the chatter seems to suggest that dv is the successor to stored values. Hmmm... in that case, I should be able to store the full text of a 24 MB PDF file in a dv. Now, I know that isn't true.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The big difference is that DV fields are stored column stride, so you&lt;br/&gt;
can decide on a field by field basis whether it will be in RAM on disk&lt;br/&gt;
etc., and you get faster access if you know you just need to work with&lt;br/&gt;
just one or two fields.&lt;/p&gt;

&lt;p&gt;Vs stored fields where all fields for one document are stored&lt;br/&gt;
"together".&lt;/p&gt;

&lt;p&gt;Each has different tradeoffs so it's really up to the app to decide&lt;br/&gt;
which is best... if you know you need 12 fields loaded for each&lt;br/&gt;
document you are presenting on the current page, stored fields is&lt;br/&gt;
probably best.&lt;/p&gt;

&lt;p&gt;But if you need one field to use as a scoring factor (eg maybe you are&lt;br/&gt;
boosting by recency) then column-stride is better.&lt;/p&gt;</comment>
                    <comment id="13655998" author="dsmiley" created="Mon, 13 May 2013 15:28:16 +0100">&lt;p&gt;Quoting Mike:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I don't think we should change the limit for sorted/set nor terms: I&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;think we should raise the limit ONLY for BINARY, and declare that DV&lt;br/&gt;
BINARY is for these "abuse" cases. So if you really really want&lt;br/&gt;
sorted set with a higher limit then you will have to encode yourself&lt;br/&gt;
into DV BINARY.&lt;/p&gt;

&lt;p&gt;+1.  DV Binary is generic for applications to use as it might see fit.  &lt;em&gt;There is no use case to abuse.&lt;/em&gt;  If this issue passes, I'm not going to then ask for terms &amp;gt; 32k or something silly like that.&lt;/p&gt;</comment>
                    <comment id="13656831" author="rcmuir" created="Tue, 14 May 2013 06:31:31 +0100">&lt;p&gt;I can compromise with this. &lt;/p&gt;

&lt;p&gt;However, I don't like the current patch.&lt;/p&gt;

&lt;p&gt;I don't think we should modify ByteBlockPool. Instead, I think BinaryDocValuesWriter should do the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;use PagedBytes to append the bytes (which has append-only writing, via getDataOutput)&lt;/li&gt;
	&lt;li&gt;implement the iterator with PagedBytes.getDataInput (its just an iterator so this is simple)&lt;/li&gt;
	&lt;li&gt;store the lengths instead as absolute offsets with MonotonicAppendingLongBuffer (this should be more efficient)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The only thing ByteBlockPool gives is some "automagic" ram accounting, but this is not as good as it looks anyway.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;today, it seems to me ram accounting is broken for this dv type already (the lengths are not considered or am i missing something!?)&lt;/li&gt;
	&lt;li&gt;since we need to fix that bug anyway (by just adding updateBytesUsed like the other consumers), the magic accounting buys us nothing really anyway.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Anyway I can help with this, tomorrow.&lt;/p&gt;</comment>
                    <comment id="13656945" author="mikemccand" created="Tue, 14 May 2013 11:47:03 +0100">&lt;p&gt;+1 to cutover to PagedBytes!&lt;/p&gt;</comment>
                    <comment id="13656989" author="mikemccand" created="Tue, 14 May 2013 13:03:21 +0100">&lt;p&gt;New patch, cutting over to PagedBytes.  I also revived .getDataInput/Output (from 4.x).&lt;/p&gt;

&lt;p&gt;I also added javadocs about these limits in DocValuesType.&lt;/p&gt;

&lt;p&gt;There are still nocommits to resolve ... the biggest one is whether we should fix default DV impl to accept values &amp;gt; 32 KB ... right now it throws IAE.&lt;/p&gt;</comment>
                    <comment id="13656994" author="rcmuir" created="Tue, 14 May 2013 13:10:46 +0100">&lt;blockquote&gt;
&lt;p&gt;There are still nocommits to resolve ... the biggest one is whether we should fix default DV impl to accept values &amp;gt; 32 KB ... right now it throws IAE.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I do not think this should change. This disagrees from your earlier comments: its already spinning out of control just as I mentioned it might.&lt;/p&gt;</comment>
                    <comment id="13657156" author="jpountz" created="Tue, 14 May 2013 17:00:28 +0100">&lt;blockquote&gt;&lt;p&gt;store the lengths instead as absolute offsets with MonotonicAppendingLongBuffer (this should be more efficient)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I just had a quick discussion about this with Robert, and since AppendingLongBuffer stores deltas from the minimum value of the block (and not 0), AppendingLongBuffer is better (ie. faster and more compact) than MonotonicAppendingLongBuffer to store lengths. This means that if all lengths are 7, 8 or 9 in a block, it will only require 2 bits per value instead of 4.&lt;/p&gt;</comment>
                    <comment id="13657296" author="mikemccand" created="Tue, 14 May 2013 19:01:53 +0100">&lt;blockquote&gt;&lt;p&gt;I do not think this should change. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I just removed that nocommit.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I just had a quick discussion about this with Robert, and since AppendingLongBuffer stores deltas from the minimum value of the block (and not 0), AppendingLongBuffer is better (ie. faster and more compact) than MonotonicAppendingLongBuffer to store lengths. This means that if all lengths are 7, 8 or 9 in a block, it will only require 2 bits per value instead of 4.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh, OK: I switched back to AppendingLongBuffer.&lt;/p&gt;

&lt;p&gt;New patch.  I factored the testHugeBinaryValues up into the&lt;br/&gt;
BaseDocValuesFormatTestCase base class, and added protected method so&lt;br/&gt;
the codecs that don't accept huge binary values can say so.  I also&lt;br/&gt;
added a test case for Facet42DVFormat, and cut back to&lt;br/&gt;
AppendingLongBuffer.&lt;/p&gt;

&lt;p&gt;I downgraded the nocommit about the spooky unused PagedBytes.blockEnds&lt;br/&gt;
to a TODO ... this class is somewhat dangerous because e.g. you can&lt;br/&gt;
use copyUsingLengthPrefix method and then get a .getDataOutput and get&lt;br/&gt;
corrumpted bytes out.&lt;/p&gt;</comment>
                    <comment id="13657321" author="rcmuir" created="Tue, 14 May 2013 19:16:27 +0100">&lt;p&gt;this one is looking pretty good, but needs a few fixes. at least, we should fix the tests:&lt;/p&gt;

&lt;p&gt;NOTE: reproduce with: ant test  -Dtestcase=TestDocValuesFormat -Dtests.method=testHugeBinaryValues -Dtests.seed=5F68849875ABAC05 -Dtests.slow=true -Dtests.locale=es_PY -Dtests.timezone=Canada/Mountain -Dtests.file.encoding=ISO-8859-1&lt;/p&gt;
</comment>
                    <comment id="13657345" author="dsmiley" created="Tue, 14 May 2013 19:39:12 +0100">&lt;p&gt;I like the new test, Mike &amp;#8211; in particular it doesn't mandate a failure if the codec accepts &amp;gt; 32k.&lt;/p&gt;

&lt;p&gt;I want to make sure it's clear what the logic is behind the decisions being made by Mike &amp;amp; Rob on this thread regarding the limits for binary doc values (not other things).  Firstly there is no intrinsic technical limitation that the Lucene42Consumer has on these values to perhaps 2GB (not sure but "big").  Yet it is being decided to artificially neuter it to 32k.  I don't see anything in this thread establishing a particular use of binary DocValues that established it's &lt;em&gt;intended use&lt;/em&gt;; I see it as general purpose as stored values, with different performance characteristics (clearly it's column-stride, for example).  The particular use I established earlier would totally suck if it had to use stored values.  And the reason for this limit... I'm struggling to find the arguments in this thread but appears to be that hypothetically in the future, there might evolve newer clever encodings that simply can't handle more than 32k.  If that's it then wouldn't such a new implementation simply have this different limit, and leave both as reasonable choices by the application?  If that isn't it then what is the reason?&lt;/p&gt;</comment>
                    <comment id="13657357" author="rcmuir" created="Tue, 14 May 2013 19:44:14 +0100">&lt;blockquote&gt;
&lt;p&gt;Firstly there is no intrinsic technical limitation that the Lucene42Consumer has on these values to perhaps 2GB (not sure but "big"). Yet it is being decided to artificially neuter it to 32k.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is absolutely not correct. The fact that this limitation exists (based on pagedbytes blocksize) and that nobody sees it makes me really want to rethink what we are doing here: I dont want to allow index corruption, sorry.&lt;/p&gt;</comment>
                    <comment id="13657384" author="dsmiley" created="Tue, 14 May 2013 20:01:09 +0100">&lt;p&gt;I don't follow.  From the javadocs, I get the impression that PagedBytes can handle basically any data size.  blockBits appears to tune the block size, but I don't see how that limits the total capacity in any significant way. The only method I see that appears to have a limit is copy(BytesRef bytes, BytesRef out) which is only used in uninverting doc terms which doesn't apply to binary DocValues.&lt;/p&gt;</comment>
                    <comment id="13657388" author="rcmuir" created="Tue, 14 May 2013 20:04:10 +0100">&lt;p&gt;just look at the java docs for the only PagedBytes method that Lucene42's binary dv producer actually uses:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;     * ...
     * Slices spanning more than one block are not supported.
     * ...
     **/
    &lt;span class="code-keyword"&gt;public&lt;/span&gt; void fillSlice(BytesRef b, &lt;span class="code-object"&gt;long&lt;/span&gt; start, &lt;span class="code-object"&gt;int&lt;/span&gt; length) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13657491" author="dsmiley" created="Tue, 14 May 2013 21:41:05 +0100">&lt;p&gt;Aha; thanks for the clarification.  I see it now.  And I see that after I commented the limit check, the assertion was hit.  I didn't hit this assertion with Barakat's patch when I last ran it; weird but whatever.&lt;/p&gt;

&lt;p&gt;BTW ByteBlockPool doesn't really have this limit, notwithstanding the bug that Barakat fixed in his patch. It's not a hard limit as BBP.append() and readBytes() will conveniently loop for you whereas if code uses PagedBytes then you could loop on fillSlice() yourself to support big values.  That is a bona-fide bug on ByteBlockPool that it didn't implement that loop correctly and it should be fixed if not in this issue then another.&lt;/p&gt;

&lt;p&gt;So a DocValues codec that supports large binary values could be nearly identical to the current codec but call fillSlice() in a loop, and only for variable-sized binary values (just like BBP's algorithm), and that would basically be the only change. Do you support such a change? If not then why not (a technical reason please)?  If you can't support such a change, then would you also object to the addition of a new codec that simply lifted this limit as I proposed?  Note that would include potentially a bunch of duplicated code just to call fillSlice() in a loop; I propose it would be simpler and more maintainable to not limit binary docvalues to 32k.&lt;/p&gt;</comment>
                    <comment id="13657498" author="barakatx2" created="Tue, 14 May 2013 21:51:23 +0100">&lt;p&gt;Just to clarify, before 4.3 I was fixing the "bug" by changing PagedBytes#fillSlice to the first patch I posted in this issue.&lt;/p&gt;</comment>
                    <comment id="13657509" author="rcmuir" created="Tue, 14 May 2013 22:04:29 +0100">&lt;p&gt;No, I don't support changing this codec. Its an all-in-memory one (which is an unfortunate default, but must be until various algorithms in grouping/join/etc package are improved such that we can safely use something more like DiskDV as a default). Other all-memory implementations like DirectPostingsFormat/MemoryPostings have similar limitations, even the specialized faceting one (e.g. entire segment cannot have more than 2GB total bytes).&lt;/p&gt;

&lt;p&gt;I dont want to add a bunch of stuff in a loop here or any of that, because it only causes additional complexity for the normal case, and I think its unreasonable to use a RAM docvalues impl if you have more than &lt;b&gt;32KB&lt;/b&gt; per-document cost anyway. Sorry, thats just crazy: and I don't think we should add any additional trappy codec to support that.&lt;/p&gt;

&lt;p&gt;So if you want ridiculously huge per-document values, just use DiskDV which supports that. These abuse cases are extreme: if you really really want that all in RAM, then use it with FileSwitchDirectory.&lt;/p&gt;

&lt;p&gt;I mentioned before I was worried about this issue spinning out of control, and it appears this has taken place. Given these developments, i'd rather we not change the current limit at all.&lt;/p&gt;</comment>
                    <comment id="13657518" author="mikemccand" created="Tue, 14 May 2013 22:19:50 +0100">&lt;p&gt;David can you open a separate issue about changing the limit for existing codecs?  The default DVFormat today is all-in-RAM, and I think it's OK for it to have limits, and e.g. if/when we change the default to DiskDV, it has no limit.&lt;/p&gt;

&lt;p&gt;I think this issue should focus solely on fixing core/indexer to not enforce the limit (i.e., moving the limit enforcing to those DVFormats that have it).&lt;/p&gt;</comment>
                    <comment id="13658088" author="dsmiley" created="Wed, 15 May 2013 06:57:15 +0100">&lt;p&gt;I can understand that an all in-RAM codec has size sensitivities.  In that light, I can also understand that 32KB per document is a lot.  The &lt;em&gt;average&lt;/em&gt; per-document variable byte length size for Barakat's index is a measly 10 bytes.  The maximum is around 69k.  Likewise for the user Shai referenced on the list who was using it for faceting, it's only the worst-case document(s) that exceeded 32KB.&lt;/p&gt;

&lt;p&gt;Might the "new PagedBytes(16)" in Lucene42DocValuesProducer.loadBinary() be made configurable? i.e. Make 16 configurable?  And/or perhaps make loadBinary() protected so another codec extending this one can keep the change somewhat minimal.&lt;/p&gt;

&lt;p&gt;Mike, in your latest patch, one improvement that could be made is instead of Lucene42DocValuesConsumer assuming the limit is "ByteBlockPool.BYTE_BLOCK_SIZE - 2" (which it technically is &lt;em&gt;but only by coincidence&lt;/em&gt;), you could instead reference a calculated constant shared with the actual code that has this limit which is Lucene42DocValuesProducer.loadBinary().  For example, set the constant to 2^16-2 but then add an assert in loadBinary that the constant is consistent with the PagedBytes instance's config.  Or something like that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;David can you open a separate issue about changing the limit for existing codecs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Uh... all the discussion has been here so seems too late to me. And I'm probably done making my arguments.  I can't be more convincing than pointing out the 10-byte average figure for my use case.&lt;/p&gt;</comment>
                    <comment id="13658287" author="rcmuir" created="Wed, 15 May 2013 13:17:06 +0100">&lt;p&gt;You convinced me dont worry, you convinced me we shouldnt do anything on this whole issue at all. Because the stuff you outlined here is absolutely the wrong path for us to be going down.&lt;/p&gt;</comment>
                    <comment id="13659447" author="mikemccand" created="Thu, 16 May 2013 12:35:54 +0100">&lt;blockquote&gt;
&lt;p&gt;Mike, in your latest patch, one improvement that could be made is instead of Lucene42DocValuesConsumer assuming the limit is "ByteBlockPool.BYTE_BLOCK_SIZE - 2" (which it technically is but only by coincidence), you could instead reference a calculated constant shared with the actual code that has this limit which is Lucene42DocValuesProducer.loadBinary(). For example, set the constant to 2^16-2 but then add an assert in loadBinary that the constant is consistent with the PagedBytes instance's config. Or something like that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;But, again, let's keep this issue focused on not enforcing a limit in the core indexing code.&lt;/p&gt;

&lt;p&gt;Per-codec limits are separate issues.&lt;/p&gt;
</comment>
                    <comment id="13662991" author="dsmiley" created="Tue, 21 May 2013 15:02:28 +0100">&lt;p&gt;Ok.  I have nothing further to say in this issue; I welcome your improvements, Mike.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12583174" name="LUCENE-4583.patch" size="32110" author="mikemccand" created="Tue, 14 May 2013 19:01:53 +0100"/>
                    <attachment id="12583114" name="LUCENE-4583.patch" size="23751" author="mikemccand" created="Tue, 14 May 2013 13:03:21 +0100"/>
                    <attachment id="12582474" name="LUCENE-4583.patch" size="9633" author="mikemccand" created="Thu, 9 May 2013 15:14:30 +0100"/>
                    <attachment id="12582297" name="LUCENE-4583.patch" size="3283" author="barakatx2" created="Wed, 8 May 2013 15:20:21 +0100"/>
                    <attachment id="12555952" name="LUCENE-4583.patch" size="4677" author="barakatx2" created="Tue, 4 Dec 2012 16:59:17 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 1 Dec 2012 16:24:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>293233</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>166186</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4556] FuzzyTermsEnum creates tons of objects</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4556</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I ran into this problem in production using the DirectSpellchecker. The number of objects created by the spellchecker shoot through the roof very very quickly. We ran about 130 queries and ended up with &amp;gt; 2M transitions / states. We spend 50% of the time in GC just because of transitions. Other parts of the system behave just fine here.&lt;/p&gt;

&lt;p&gt;I talked quickly to robert and gave a POC a shot providing a LevenshteinAutomaton#toRunAutomaton(prefix, n) method to optimize this case and build a array based strucuture converted into UTF-8 directly instead of going through the object based APIs. This involved quite a bit of changes but they are all package private at this point. I have a patch that still has a fair set of nocommits but its shows that its possible and IMO worth the trouble to make this really useable in production. All tests pass with the patch - its a start....&lt;/p&gt;</description>
                <environment/>
            <key id="12615902">LUCENE-4556</key>
            <summary>FuzzyTermsEnum creates tons of objects</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 13 Nov 2012 14:38:35 +0000</created>
                <updated>Fri, 10 May 2013 00:05:07 +0100</updated>
                                    <version>4.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                <component>modules/spellchecker</component>
                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13496235" author="simonw" created="Tue, 13 Nov 2012 14:52:58 +0000">&lt;p&gt;here is a patch ...scary™&lt;/p&gt;</comment>
                    <comment id="13496315" author="mikemccand" created="Tue, 13 Nov 2012 16:43:23 +0000">&lt;p&gt;I hit this test failure:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;ant test  -Dtestcase=TestSlowFuzzyQuery2 -Dtests.method=testFromTestData -Dtests.seed=6019B3869272BDF0 -Dtests.locale=el_CY -Dtests.timezone=Asia/Anadyr -Dtests.file.encoding=ANSI_X3.4-1968
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13496439" author="mikemccand" created="Tue, 13 Nov 2012 19:13:48 +0000">&lt;p&gt;What spooks me about this patch is this code (LevenshteinAutomaton) is already REALLY hairy ... and this change would add yet more hair to it (when really we need to be doing the reverse, so the code becomes approachable to new eyeballs).&lt;/p&gt;

&lt;p&gt;Also: are we sure the objects created here are really such a heavy GC load...?&lt;/p&gt;

&lt;p&gt;I ran a quick test, respelling (using DirectSpellChecker() w/ its defaults) a set of 500 5-character terms against the full Wikipedia English (33.M docs) index, using concurrent mark/sweep collector w/ 2 GB heap and I couldn't see any difference in the net throughput on a 24 core box ... both got ~780 respells/sec.&lt;/p&gt;

&lt;p&gt;Simon can you describe what use case you're seeing where GC is cutting throughput by 50%?&lt;/p&gt;</comment>
                    <comment id="13499975" author="mikemccand" created="Mon, 19 Nov 2012 00:32:16 +0000">&lt;p&gt;I'm attaching a possible alternate way to reduce objects ... it's&lt;br/&gt;
only just a start ...&lt;/p&gt;

&lt;p&gt;I created a new LightAutomaton class (I'm not wed to that name!) which&lt;br/&gt;
places a severe "append only" restriction on how you are allowed to&lt;br/&gt;
build up the FSA: you must add all transitions for a given state&lt;br/&gt;
before adding another state's transitions.&lt;/p&gt;

&lt;p&gt;It operates with only "int state", and stores all transitions in a&lt;br/&gt;
private int[].&lt;/p&gt;

&lt;p&gt;This is a big restriction, but I think a number of our FSA ops would&lt;br/&gt;
work fine with this.  I'm pretty sure building the LevA, and doing the&lt;br/&gt;
UTF32-&amp;gt;UTF8 conversion would work fine append-only...&lt;/p&gt;

&lt;p&gt;In the patch, I added Automaton.toLightAutomaton to convert from&lt;br/&gt;
"heavy" to LightAutomaton, and then fixed CompiledAutomaton (and its&lt;br/&gt;
consumers) to use that.  Tests pass.&lt;/p&gt;

&lt;p&gt;I think it shouldn't be too hard to cut over the Lev building to this&lt;br/&gt;
too ... but wanted to get feedback first.&lt;/p&gt;

&lt;p&gt;Simon, it'd be great if you could try this patch on your benchmark&lt;br/&gt;
since I can't reproduce the too-heavy GC in my benchmark ... I'm&lt;br/&gt;
particularly curious whether the 50% time spent in GC you see is due&lt;br/&gt;
to 1) creating too many objects vs 2) holding onto those objects for&lt;br/&gt;
too long (in CompiledAutomaton, while the query runs...).  So this&lt;br/&gt;
patch would test whether it's case 2).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12554103" name="LUCENE-4556.patch" size="37475" author="mikemccand" created="Mon, 19 Nov 2012 00:32:16 +0000"/>
                    <attachment id="12553317" name="LUCENE-4556.patch" size="62718" author="simonw" created="Tue, 13 Nov 2012 14:52:58 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 13 Nov 2012 16:43:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>257484</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>114894</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4545] Better error reporting StemmerOverrideFilterFactory</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4545</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;If the dictionary contains an error such as a space instead of a tab somewhere in the dictionary it is hard to find the error in a long dictionary. This patch includes the file and line number in the exception, helping to debug it quickly.&lt;/p&gt;</description>
                <environment/>
            <key id="12615154">LUCENE-4545</key>
            <summary>Better error reporting StemmerOverrideFilterFactory</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="5" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.png">Trivial</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="markus17">Markus Jelsma</reporter>
                        <labels>
                    </labels>
                <created>Wed, 7 Nov 2012 13:27:01 +0000</created>
                <updated>Fri, 10 May 2013 00:05:08 +0100</updated>
                                    <version>4.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13492334" author="markus17" created="Wed, 7 Nov 2012 13:27:32 +0000">&lt;p&gt;Patch for trunk.&lt;/p&gt;</comment>
                    <comment id="13492366" author="rcmuir" created="Wed, 7 Nov 2012 14:08:28 +0000">&lt;p&gt;I'm for the idea, but not for the logic contained to this specific factory.&lt;/p&gt;

&lt;p&gt;Instead of tracking our own line numbers, we should use LineNumberReader and so on.&lt;/p&gt;

&lt;p&gt;WordListLoader.getStemDict should be changed to take a generic map (Not a chararraymap), so that it can be used by this method.&lt;br/&gt;
In fact, since nothing at all is using this method, we can do whatever we want with it.&lt;/p&gt;

&lt;p&gt;Also the logic should not use split(s, 2): I think instead it should just use split(s)? This way we detect the situation&lt;br/&gt;
where there are multiple tabs in a line unexpectedly, too.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12552463" name="LUCENE-4545-trunk-1.patch" size="1359" author="markus17" created="Wed, 7 Nov 2012 13:27:32 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 7 Nov 2012 14:08:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>255741</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>90161</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4526] Allow runtime settings on Codecs</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4526</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today we expose termIndexInterval and termIndexDivisor via several APIs and they are deprecated. Those settings are 1. codec / postingformat specific and 2. not extendable. We should provide a more flexible way to pass information down to our codecs.&lt;/p&gt;</description>
                <environment/>
            <key id="12614501">LUCENE-4526</key>
            <summary>Allow runtime settings on Codecs</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 2 Nov 2012 11:52:04 +0000</created>
                <updated>Fri, 10 May 2013 00:05:08 +0100</updated>
                                    <version>4.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                        <due/>
                    <votes>1</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13489373" author="simonw" created="Fri, 2 Nov 2012 11:54:04 +0000">&lt;p&gt;here is an starting point with lots of no-commits but something like this would be very powerful for users that have their own codecs.&lt;/p&gt;

&lt;p&gt;its really just a start for discussion. all test pass so far &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13489501" author="rcmuir" created="Fri, 2 Nov 2012 15:52:36 +0000">&lt;p&gt;I just glanced through in general: this is similar to the hack patch i used exploring &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4089" title="fix or document termsIndexInterval/Divisor for 4.0"&gt;LUCENE-4089&lt;/a&gt;, though I just used a Map&amp;lt;String,String&amp;gt;.&lt;/p&gt;

&lt;p&gt;The part i didnt like when exploring was more related to how term index/term dictionary are separated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;divisor: generalize this into something simple like a Map&amp;lt;String,String&amp;gt; of codec "parameters" that you set on IWC/IR. split divisor from "don't load terms index". define these as constants where they belong. I got unhappy here in the "splitting" part because I wanted the divisor part in TermsIndexReaderBase, but that doesnt extend FieldsProducer (where i wanted the "don't load" part) and wrap the terms dict, instead its backwards and terms dict wraps the TermsIndexReaderBase... maybe we should fix that too? I think this confusing the way it is but I didnt look at how difficult this would be.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But I think maybe I was trying to tackle too much at once... still as an "untyped" parameter I thought it would be useful to fix the semantics all in one break rather than causing confusion down the road.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12558530">LUCENE-4089</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12551849" name="LUCENE-4526.patch" size="113156" author="simonw" created="Fri, 2 Nov 2012 11:54:04 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 2 Nov 2012 15:52:36 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>253978</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>81669</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4524] Merge DocsEnum and DocsAndPositionsEnum into PostingsEnum</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4524</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;spinnoff from &lt;a href="http://www.gossamer-threads.com/lists/lucene/java-dev/172261" class="external-link"&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/172261&lt;/a&gt;&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;hey folks, 

I have spend a hell lot of time on the positions branch to make 
positions and offsets working on all queries if needed. The one thing 
that bugged me the most is the distinction between DocsEnum and 
DocsAndPositionsEnum. Really when you look at it closer DocsEnum is a 
DocsAndFreqsEnum and if we omit Freqs we should return a DocIdSetIter. 
Same is true for 
DocsAndPostionsAndPayloadsAndOffsets*YourFancyFeatureHere*Enum. I 
don't really see the benefits from this. We should rather make the 
interface simple and call it something like PostingsEnum where you 
have to specify flags on the TermsIterator and if we can't provide the 
sufficient enum we throw an exception? 
I just want to bring up the idea here since it might simplify a lot 
for users as well for us when improving our positions / offset etc. 
support. 

thoughts? Ideas? 

simon 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment/>
            <key id="12614442">LUCENE-4524</key>
            <summary>Merge DocsEnum and DocsAndPositionsEnum into PostingsEnum</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Nov 2012 22:49:39 +0000</created>
                <updated>Fri, 10 May 2013 00:05:08 +0100</updated>
                                    <version>4.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                <component>core/index</component>
                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13558905" author="simonw" created="Mon, 21 Jan 2013 16:53:07 +0000">&lt;p&gt;here is an initial patch that moves this over. I really just did some initial porting and this patch has still some problems.&lt;/p&gt;

&lt;p&gt;I removed DocsAndPosEnum entirely and changed how the DocsEnum Flags work such that we only have TermsEnum#docs and a simple sugar method for docsAndPos which should go away IMO. We need to figure out what kind of behavior those flags should trigger ie. if we have no freqs we still return and enum while no pos we return null.&lt;/p&gt;

&lt;p&gt;anyway, most of the patch is rename etc. all test pass, comments welcome&lt;/p&gt;</comment>
                    <comment id="13558962" author="simonw" created="Mon, 21 Jan 2013 18:26:26 +0000">&lt;p&gt;new patch bringing back TermsEnum#docsAndPositions(...) this make this entire thing way simpler and I think this is how it should be. All tests pass and I think this is pretty close already.&lt;/p&gt;</comment>
                    <comment id="13559039" author="romseygeek" created="Mon, 21 Jan 2013 20:00:54 +0000">&lt;p&gt;The javadocs on PostingsReaderBase are a bit funky, but otherwise this looks great.  Simplifies things a lot.&lt;/p&gt;</comment>
                    <comment id="13559048" author="simonw" created="Mon, 21 Jan 2013 20:10:54 +0000">&lt;p&gt;alan, I agree this might make a lot of things simple in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2878" title="Allow Scorer to expose positions and payloads aka. nuke spans "&gt;LUCENE-2878&lt;/a&gt; like passing instances down to the actual scorer. This might even help us to get the API straight. &lt;/p&gt;

&lt;p&gt;I will go over the patch tomorrow again and straight out javadocs etc. I plan to commit this to trunk and then backport to 4.2 I think this this case we should really break BW compat and just go ahead and remove the DocsAndPositionsEnum class entirely. Any objections?&lt;/p&gt;</comment>
                    <comment id="13559069" author="rcmuir" created="Mon, 21 Jan 2013 20:28:33 +0000">&lt;p&gt;I agree its a great! change, but perhaps this should be committed to the 2878 branch for now to see how it plays in practice?&lt;/p&gt;

&lt;p&gt;Otherwise I have a few concerns about committing to trunk (none of which have to do with the particular patch, just problems in general)&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Scorer.java will be strange, as it will have various methods like nextPosition(), getPayload(), that do not work. Of course this API would be fantastic and make total sense if this was the api for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2878" title="Allow Scorer to expose positions and payloads aka. nuke spans "&gt;LUCENE-2878&lt;/a&gt;, but in the meantime it would be odd and confusing.&lt;/li&gt;
	&lt;li&gt;I'm not sure how i feel about nextPosition(), getPayload(), start/endOffset() all going from abstract to 'default nonfunctioning impl'. It might be the right tradeoff, but i definitely liked them better as abstract. Its hard to tell without seeing how the Scorer integration would ultimately play out.&lt;/li&gt;
	&lt;li&gt;with DocsEnum and DocsAndPositionsEnum merged, the possibility of reuse bugs in various postings implementations is increased. We should review our own code here...&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13559167" author="simonw" created="Mon, 21 Jan 2013 22:24:10 +0000">&lt;p&gt;this change is really unrelated to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2878" title="Allow Scorer to expose positions and payloads aka. nuke spans "&gt;LUCENE-2878&lt;/a&gt; it removes a unnecessary duplication. The fact that Scorer extends DocEnum is not concerning me here since the main purpose of this class is not the Scorer API. This should really go on trunk since given the discussion on the list this is independent of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2878" title="Allow Scorer to expose positions and payloads aka. nuke spans "&gt;LUCENE-2878&lt;/a&gt;.&lt;br/&gt;
If there are bugs in reuse we should catch them in the tests no? I mean we can add even more tests for this particular problem so we catch them quicker. I would be ok with makeing them abstract but this really is not a big deal here. I would want to move forward here quickly on trunk at least we can merge later into 4.x if needed since this might go out of date quickly.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12565815" name="LUCENE-4524.patch" size="151674" author="simonw" created="Mon, 21 Jan 2013 18:26:26 +0000"/>
                    <attachment id="12565808" name="LUCENE-4524.patch" size="215265" author="simonw" created="Mon, 21 Jan 2013 16:53:07 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 21 Jan 2013 20:00:54 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>253872</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>81077</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4492] Immutable Suggester impls. should be immutable</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4492</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently we have a very clumsy interface to build and load the Suggesters that are immutable. All our FST impls must be somewhat pre-build and then loaded. But currently the code doesn't declare the fst member as final and lets you change it at any time. ie you can always call load / build. This makes safe publication tricky and required custom code to make this reasonable. Ie. you have as suggester that can reload it's dict every 20 min. Now if you want to swap this in once loaded you can create a new Lookup instance and assign it to a member in your app. Yet this member needs to be volatile otherwise threads won't fetch all the memory and you can run into NPE exceptions since the fst member is not final. I'd not want to pay the price for this volatile read in a suggest env since its really read-only. &lt;/p&gt;</description>
                <environment/>
            <key id="12612644">LUCENE-4492</key>
            <summary>Immutable Suggester impls. should be immutable</summary>
                <type id="5" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Wish</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Oct 2012 10:39:09 +0100</created>
                <updated>Fri, 10 May 2013 00:05:08 +0100</updated>
                                    <version>3.6.1</version>
                <version>4.0</version>
                <version>4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>249895</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>59613</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4491] Make analyzing suggester more flexible</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4491</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Today we have a analyzing suggester that is bound to a single key. Yet, if you want to have a totally different surface form compared to the key used to find the suggestion you either have to copy the code or play some super ugly analyzer tricks. For example I want to suggest "Barbar Streisand" if somebody types "strei" in that case the surface form is totally different from the analyzed form. &lt;/p&gt;

&lt;p&gt;Even one step further I want to embed some meta-data in the suggested key like a user id or some type my surface form could look like "Barbar Streisand|15". Ideally I want to encode this as binary and that might not be a valid UTF-8 byte sequence.&lt;/p&gt;

&lt;p&gt;I'm actually doing this in production and my only option was to copy the analyzing suggester and some of it's related classes.&lt;/p&gt;</description>
                <environment/>
            <key id="12612641">LUCENE-4491</key>
            <summary>Make analyzing suggester more flexible</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Oct 2012 10:25:16 +0100</created>
                <updated>Fri, 10 May 2013 00:05:08 +0100</updated>
                                    <version>4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13479906" author="simonw" created="Fri, 19 Oct 2012 10:29:33 +0100">&lt;p&gt;this patch moves all the explicit bytes to char conversions into protected methods, adds a TermFreqIterator#surfaceForm method and opens up LookupResult for subclassing. I also added a test that shows the possibilities for custom extension. All tests pass.&lt;/p&gt;</comment>
                    <comment id="13479911" author="rcmuir" created="Fri, 19 Oct 2012 10:33:56 +0100">&lt;p&gt;it doesnt make sense to me to have a surface form at all on TermFreqIterator.&lt;/p&gt;

&lt;p&gt;This is an iterator of terms + freqs... 'surface form' is a concept only within analyzing suggester.&lt;/p&gt;</comment>
                    <comment id="13479914" author="simonw" created="Fri, 19 Oct 2012 10:41:52 +0100">&lt;blockquote&gt;&lt;p&gt;This is an iterator of terms + freqs... 'surface form' is a concept only within analyzing suggester.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;fair enough. maybe we could use the Attribute API for this? I had this in mind too but this was the simplest thing I could do so I started with this.&lt;/p&gt;</comment>
                    <comment id="13479947" author="simonw" created="Fri, 19 Oct 2012 11:51:36 +0100">&lt;p&gt;next iteration. I remove the surfaceForm getter from TermFreqIter and added Attribute support to the it instead. Now we also have a SurfaceFromAttribute in the o.a.l.suggest.analyzing package that is respected by the AnalyzingSuggester. This looks much cleaner now.&lt;/p&gt;</comment>
                    <comment id="13480018" author="rcmuir" created="Fri, 19 Oct 2012 14:32:37 +0100">&lt;p&gt;What exactly is this 'surface form' here? Its confusing me because TermFreqIterator is terms and freqs.&lt;/p&gt;

&lt;p&gt;surface form is a concept inside analyzing suggester: so its confusing me to have a surface form on the termFreqIter,&lt;br/&gt;
doesnt matter really if its an attribute or not.&lt;/p&gt;
</comment>
                    <comment id="13480041" author="rcmuir" created="Fri, 19 Oct 2012 15:07:55 +0100">&lt;p&gt;I'm trying to boil down what you are actually doing with this surface form here?&lt;/p&gt;

&lt;p&gt;Is this issue really about wanting to add additional metadata attached to the suggestions?&lt;/p&gt;</comment>
                    <comment id="13480239" author="simonw" created="Fri, 19 Oct 2012 19:43:25 +0100">&lt;p&gt;ok so what this really boils down to is that I am working around the current api and its limitations. Maybe we tackles this from the other direction and make our API better to make distinctions between the different suggester impls. here are a couple of things I think we should tackle:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;To begin with I think it is really hard to have a common interface for all our different impls. We should flatten out the hierarchy and make dedicated / impl specific interfaces before we abstract (this might not be possible)&lt;/li&gt;
	&lt;li&gt;Lookup is a really bad name lets get rid of this&lt;/li&gt;
	&lt;li&gt;all the methods that apply to mutable impls should go away&lt;/li&gt;
	&lt;li&gt;we should separate building the suggester and the "suggest" impl. Most impls are immutable (FST ones) and they should not need to be pushed into a mutable interface.&lt;/li&gt;
	&lt;li&gt;Building should be impl specific ie. you should maybe even need to pass keys in order and we can provide utils?&lt;/li&gt;
	&lt;li&gt;Building should be much simpler. the TermFreqIterator is bogus here. FST suggestors should provide builders that have methods like FSTSuggestBuilder.put(BytesRef, long weight) that we can overload like AnalyzingSuggestBuilder.put(BytesRef input, BytesRef output, weight) &amp;lt;-- this would solve this issue btw.&lt;/li&gt;
	&lt;li&gt;all FST suggester impls should require the FST or an inputstream as ctor args to enforce immutability.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;let do this is in a sep issue and rip this all apart.&lt;/p&gt;</comment>
                    <comment id="13480322" author="rcmuir" created="Fri, 19 Oct 2012 20:53:37 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13589844" author="zolgatron" created="Thu, 28 Feb 2013 19:33:11 +0000">&lt;p&gt;Similar to the suggestion above for simplifying Suggester builder, spell check builder needs to be simplified - I am facing a issue similar to: &lt;a href="http://thread.gmane.org/gmane.comp.jakarta.lucene.solr.user/57004/focus=57030" class="external-link"&gt;http://thread.gmane.org/gmane.comp.jakarta.lucene.solr.user/57004/focus=57030&lt;/a&gt;&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12549969" name="LUCENE-4491.patch" size="29987" author="simonw" created="Fri, 19 Oct 2012 11:51:36 +0100"/>
                    <attachment id="12549966" name="LUCENE-4491.patch" size="22806" author="simonw" created="Fri, 19 Oct 2012 10:29:33 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 19 Oct 2012 09:33:56 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>249892</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>59609</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4438] Lucene40StoredFieldsReader's constructor calls close() instead of IOUtils.closeWhileHandlingException in its finally block</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4438</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It would be nice to have automated tests for this kind of things (similarly to the check-forbidden-api task).&lt;/p&gt;

&lt;p&gt;Here is the discussion I just had with Uwe on IRC:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;15:32 &amp;lt; jpountz&amp;gt; ThetaPh1: I just saw that Lucene40StoredFieldsReader's constructor calls close in its finally block. I think it is wrong since close might throw an IOE (should be catched), am I 
                 correct? If yes, then is it something we could test with ASM (similarly to the forbidden API checks)?
15:32 &amp;lt;@ThetaPh1&amp;gt; it does not use IOUtils?
15:33 &amp;lt;@ThetaPh1&amp;gt; we cannot check this with asm easily
15:33 &amp;lt;@ThetaPh1&amp;gt; we could only forbid calling Closeable.close() and exclude IOUtils from that
15:34 &amp;lt;@ThetaPh1&amp;gt; so the correct fix is to use IOUtils when clsoing
15:34 &amp;lt;@ThetaPh1&amp;gt; it also checks for null and suppresses exceptions
15:36 &amp;lt; jpountz&amp;gt; ThetaPh1: no, it does no
15:36 &amp;lt;@ThetaPh1&amp;gt; its a bug :-)
15:36 &amp;lt;@ThetaPh1&amp;gt; but not serious
15:36 &amp;lt;@ThetaPh1&amp;gt; for local files it never throws exceptions
15:36 &amp;lt; jpountz&amp;gt; ok
15:37 &amp;lt; jpountz&amp;gt; I think catching calls to Closeable.close would already be nice
15:37 &amp;lt;@ThetaPh1&amp;gt; but we might think about disallowing Closeable.close()
15:37 &amp;lt; jpountz&amp;gt; I'll open an issue
15:37 &amp;lt;@ThetaPh1&amp;gt; we can add that as a separate check-forbidden-apis file
15:37 &amp;lt;@ThetaPh1&amp;gt; but exclude IOUtils from the fileset
15:37 &amp;lt;@ThetaPh1&amp;gt; unfortunately all tests do this
15:38 &amp;lt;@ThetaPh1&amp;gt; so i would restriuct this to non-tests, too
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment/>
            <key id="12609240">LUCENE-4438</key>
            <summary>Lucene40StoredFieldsReader's constructor calls close() instead of IOUtils.closeWhileHandlingException in its finally block</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jpountz">Adrien Grand</reporter>
                        <labels>
                    </labels>
                <created>Wed, 26 Sep 2012 14:45:44 +0100</created>
                <updated>Fri, 10 May 2013 00:05:08 +0100</updated>
                                    <version>4.0</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13463814" author="thetaphi" created="Wed, 26 Sep 2012 14:47:25 +0100">&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;
[15:44]	ThetaPh1: jpountz: problem
[15:44]	ThetaPh1: super.close() in a subclass
[15:44]	ThetaPh1: this should be allowed
[15:46]	ThetaPh1: so the forbidden-apis checker cannot handle that without a special case
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13463817" author="rcmuir" created="Wed, 26 Sep 2012 14:51:43 +0100">&lt;p&gt;We could at least investigate the list?&lt;/p&gt;

&lt;p&gt;Also, unlike when writing (MockIndexOutputWrapper.java), we never throw random &lt;br/&gt;
exceptions from MockIndexInputWrapper.java when random exceptions are enabled.&lt;/p&gt;

&lt;p&gt;So any such bugs currently wont be found.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 26 Sep 2012 13:47:25 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>239747</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2420</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4417] Re-Add the backwards compatibility tests to 4.1 branch</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4417</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In 4.0 we have no backwards compatibility, but in 4.1 we must again ivy-retrieve the 4.0 JAR file and run the core tests again (like in 3.6). We may think about other modules, too, so all modules that must be backwards compatible should be added to this build.&lt;/p&gt;

&lt;p&gt;I will work on this once we have a release candidate in Maven Central.&lt;/p&gt;</description>
                <environment/>
            <key id="12608762">LUCENE-4417</key>
            <summary>Re-Add the backwards compatibility tests to 4.1 branch</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Sat, 22 Sep 2012 19:19:10 +0100</created>
                <updated>Fri, 10 May 2013 00:05:09 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13552079" author="steve_rowe" created="Sat, 12 Jan 2013 22:00:15 +0000">&lt;p&gt;We shouldn't release 4.1 until at least lucene-core backwards tests are re-enabled.&lt;/p&gt;</comment>
                    <comment id="13552118" author="rcmuir" created="Sun, 13 Jan 2013 01:44:54 +0000">&lt;p&gt;Seems pretty complicated, core tests depend upon things like test-framework and codecs, which have experimental APIs. (look at 4.0 codebase if you dont believe me, whole experimental codecs have been folded into core functionality and removed, and so on).&lt;/p&gt;

&lt;p&gt;Even if we were to do this, i don't think it would be maintainable. For example, take issues that will seriously change the codec API like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4547" title="DocValues field broken on large indexes"&gt;&lt;del&gt;LUCENE-4547&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;I'd be the first to simply disable the whole thing rather than waste a bunch of time fixing outdated tests and experimental codecs from a previous release.&lt;/p&gt;

&lt;p&gt;I think it would be more bang for the buck to integrate an API comparison tool (like jdiff or whatever) that shows the breaks so we know what they are.&lt;/p&gt;</comment>
                    <comment id="13554531" author="steve_rowe" created="Tue, 15 Jan 2013 23:49:19 +0000">&lt;p&gt;Blocker-&amp;gt;Major, Fix Version -&amp;gt; 4.2&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 12 Jan 2013 22:00:15 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>239768</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2441</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4386] Query parser should generate FieldValueFilter for pure wildcard terms to boost query performance</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4386</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In theory, a simple pure wildcard query (a single asterisk) is an inefficient way to select all documents that have any value in a field. Rather than users having to work around this issue by adding a separate boolean "has" field, it would be better to have the query parser directly generate the most efficient Lucene query for detecting all documents that have any value for a specified field. According to the discussion over on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4376" title="Add Query subclasses for selecting documents where a field is empty or not"&gt;&lt;del&gt;LUCENE-4376&lt;/del&gt;&lt;/a&gt;, the FieldValueFilter is the proper solution.&lt;/p&gt;

&lt;p&gt;Proposed solution:&lt;/p&gt;

&lt;p&gt;QueryParserBase.getPrefixQuery could detect when the query is a pure wildcard (a single asterisk) and then generate a FieldValueFilter instead of a PrefixQuery. My understanding from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4376" title="Add Query subclasses for selecting documents where a field is empty or not"&gt;&lt;del&gt;LUCENE-4376&lt;/del&gt;&lt;/a&gt; is that the following would work:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;new&lt;/span&gt; ConstantScoreQuery(&lt;span class="code-keyword"&gt;new&lt;/span&gt; FieldValueFilter(fieldname, &lt;span class="code-keyword"&gt;false&lt;/span&gt;))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Oh, and the check for whether "leading wildcard" is enabled would need to be bypassed for this case.&lt;/p&gt;

&lt;p&gt;I still think it would be better to have PrefixQuery perform this optimization internally so that all apps would benefit, but this should be sufficient to address the main concern.&lt;/p&gt;

&lt;p&gt;This improvement would improve the classic Lucene query parser and other query parsers based on it, including edismax. There might be other query parsers which won't see the impact of this change, but they can be updated separately.&lt;/p&gt;

&lt;p&gt;How much performance benefit? Unknown, but supposedly significant. The goal is simply to have a simple pure wildcard be the obvious tool to select fields that have a value in a field.&lt;/p&gt;</description>
                <environment/>
            <key id="12607519">LUCENE-4386</key>
            <summary>Query parser should generate FieldValueFilter for pure wildcard terms to boost query performance</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jkrupan">Jack Krupansky</reporter>
                        <labels>
                    </labels>
                <created>Thu, 13 Sep 2012 22:11:38 +0100</created>
                <updated>Fri, 10 May 2013 00:05:09 +0100</updated>
                                    <version>4.0-BETA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13455303" author="hossman" created="Thu, 13 Sep 2012 22:17:16 +0100">&lt;p&gt;I'm confused.  As Uwe allready noted in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4376" title="Add Query subclasses for selecting documents where a field is empty or not"&gt;&lt;del&gt;LUCENE-4376&lt;/del&gt;&lt;/a&gt;...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The problem is that it implicitely needs to build the FieldCache for that field, so automatism is no-go here. If you need that functionality, modify QueryParser.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;...that sounds to me like a pretty clear "we can not automate this" response, because using this class requires the FieldCache, and we can't know/assume if/when the FieldCache is safe for a field.&lt;/p&gt;

&lt;p&gt;am i missing something?&lt;/p&gt;</comment>
                    <comment id="13455343" author="jkrupan" created="Thu, 13 Sep 2012 22:45:11 +0100">&lt;p&gt;I took Uwe's "we can not automate this" comment as applying to my request to handle it automatically within PrefixQuery. But this Jira is to do it in the query parser as I thought Uwe was seeming to indicate would work. If there is more to using the filter than advertised in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3593" title="Add a filter returning all document without a value in a field"&gt;&lt;del&gt;LUCENE-3593&lt;/del&gt;&lt;/a&gt;, somebody will have to enlighten us.&lt;/p&gt;</comment>
                    <comment id="13455346" author="jkrupan" created="Thu, 13 Sep 2012 22:51:52 +0100">&lt;p&gt;I would also note that the Javadoc for FieldValueFilter says "This Filter request Bits from the FieldCache and build the bits if not present" which I take as indicating that the filter will assure that the bits get added to the field cache if they aren't already there.&lt;/p&gt;</comment>
                    <comment id="13455418" author="thetaphi" created="Fri, 14 Sep 2012 00:12:33 +0100">&lt;p&gt;The reason for my comment is not "unsafe" or whatever. It is just, that this filter needs FieldCache and that is a large performance impact on the first call when automatically build from QueryParser.&lt;/p&gt;

&lt;p&gt;I am strongly against adding this to Lucene's QueryParser by default. Solr already has support for &lt;b&gt;:&lt;/b&gt; and similar, so it could use this filter in its own QueryParser impl (as replacement for the current ConstantScore RangeQuery, which is slow.&lt;/p&gt;</comment>
                    <comment id="13455541" author="rcmuir" created="Fri, 14 Sep 2012 03:46:35 +0100">&lt;blockquote&gt;
&lt;p&gt;I am strongly against adding this to Lucene's QueryParser by default.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, I agree with Uwe. This is wrong to do by default.&lt;/p&gt;</comment>
                    <comment id="13456385" author="rcmuir" created="Sat, 15 Sep 2012 13:48:27 +0100">&lt;p&gt;Unassigned issues -&amp;gt; 4.1&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 13 Sep 2012 21:17:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>239799</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2472</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4382] Unicode escape no longer works for non-suffix-only wildcard terms</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4382</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-588" title="Escaped wildcard character in wildcard term not handled correctly"&gt;&lt;del&gt;LUCENE-588&lt;/del&gt;&lt;/a&gt; added support for escaping of wildcard characters, but when the de-escaping logic was pushed down from the query parser (QueryParserBase) into WildcardQuery, support for Unicode escaping (backslash, "u", and the four-digit hex Unicode code) was not included.&lt;/p&gt;

&lt;p&gt;Two solutions:&lt;/p&gt;

&lt;p&gt;1. Do the Unicode de-escaping in the query parser before calling getWildcardQuery.&lt;br/&gt;
2. Support Unicode de-escaping in WildcardQuery.&lt;/p&gt;

&lt;p&gt;A suffix-only wildcard does not exhibit this problem because full de-escaping is performed in the query parser before calling getPrefixQuery.&lt;/p&gt;

&lt;p&gt;My test case, added at the beginning of TestExtendedDismaxParser.testFocusQueryParser:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;

    assertQ(&lt;span class="code-quote"&gt;"expected doc is missing (using escaped edismax w/field)"&lt;/span&gt;,
        req(&lt;span class="code-quote"&gt;"q"&lt;/span&gt;, &lt;span class="code-quote"&gt;"t_special:literal\\:\\u0063olo*n"&lt;/span&gt;, 
            &lt;span class="code-quote"&gt;"defType"&lt;/span&gt;, &lt;span class="code-quote"&gt;"edismax"&lt;/span&gt;),
        &lt;span class="code-quote"&gt;"&lt;span class="code-comment"&gt;//doc[1]/str[@name='id'][.='46']"&lt;/span&gt;); 
&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: That test case was only used to debug into WildcardQuery to see that the Unicode escape was not processed correctly. It fails in all cases, but that's because of how the field type is analyzed.&lt;/p&gt;

&lt;p&gt;Here is a Lucene-level test case that can also be debugged to see that WildcardQuery is not processing the Unicode escape properly. I added it at the start of TestMultiAnalyzer.testMultiAnalyzer:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    assertEquals(&lt;span class="code-quote"&gt;"literal\\:\\u0063olo*n"&lt;/span&gt;, qp.parse(&lt;span class="code-quote"&gt;"literal\\:\\u0063olo*n"&lt;/span&gt;).toString());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: This case will always run correctly since it is only checking the input pattern string for WildcardQuery and not how the de-escaping was performed within WildcardQuery.&lt;/p&gt;</description>
                <environment/>
            <key id="12607367">LUCENE-4382</key>
            <summary>Unicode escape no longer works for non-suffix-only wildcard terms</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jkrupan">Jack Krupansky</reporter>
                        <labels>
                    </labels>
                <created>Thu, 13 Sep 2012 00:59:31 +0100</created>
                <updated>Fri, 10 May 2013 00:05:09 +0100</updated>
                                    <version>4.0-BETA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13456389" author="rcmuir" created="Sat, 15 Sep 2012 13:48:28 +0100">&lt;p&gt;Unassigned issues -&amp;gt; 4.1&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 15 Sep 2012 12:48:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>239803</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2476</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4381] support unicode 6.2</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4381</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;ICU will release a new version in about a month.&lt;/p&gt;

&lt;p&gt;They have a version for testing (&lt;a href="http://site.icu-project.org/download/milestone" class="external-link"&gt;http://site.icu-project.org/download/milestone&lt;/a&gt;) already out with some interesting features, e.g. dictionary-based CJK segmentation.&lt;/p&gt;

&lt;p&gt;This issue is just to test it out/integrate the new stuff/etc. We should try out the automation Steve did as well.&lt;/p&gt;</description>
                <environment/>
            <key id="12607365">LUCENE-4381</key>
            <summary>support unicode 6.2</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 13 Sep 2012 00:42:40 +0100</created>
                <updated>Fri, 10 May 2013 00:05:09 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13454494" author="rcmuir" created="Thu, 13 Sep 2012 00:45:37 +0100">&lt;p&gt;A hacked up patch for testing:&lt;/p&gt;

&lt;p&gt;I think its nice to offer the CJK dictionary-based stuff as an option? I'm not sure how good results will be on average yet (maybe I can enlist Christian to help investigate).&lt;/p&gt;

&lt;p&gt;So as a test I just added a boolean option, which if enabled, keeps all han/hiragana/katakana marked as "Chinese/Japanese" (uses the 15924 Japanese code, but I overrode the toString to try to prevent confusion).&lt;/p&gt;

&lt;p&gt;Seems to work ok: some trivial snippets from smartcn and kuromoji are analyzed fine, and testRandomStrings is happy &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310000">
                <name>Duplicate</name>
                                                <inwardlinks description="is duplicated by">
                            <issuelink>
            <issuekey id="12626639">LUCENE-4665</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12544907" name="LUCENE-4381.patch" size="21802" author="rcmuir" created="Thu, 13 Sep 2012 00:45:37 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>239804</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2477</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4281] Delegate to default thread factory in NamedThreadFactory</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4281</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;currently we state that we yield the same behavior as Executors#defaultThreadFactory() but this behavior could change over time even if it is compatible. We should just delegate to the default thread factory instead of creating the threads ourself.&lt;/p&gt;</description>
                <environment/>
            <key id="12601176">LUCENE-4281</key>
            <summary>Delegate to default thread factory in NamedThreadFactory</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Thu, 2 Aug 2012 09:07:11 +0100</created>
                <updated>Fri, 10 May 2013 00:05:09 +0100</updated>
                                    <version>3.6.1</version>
                <version>4.0-BETA</version>
                <version>5.0</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13427163" author="simonw" created="Thu, 2 Aug 2012 09:09:26 +0100">&lt;p&gt;here is a patch&lt;/p&gt;</comment>
                    <comment id="13427164" author="dweiss" created="Thu, 2 Aug 2012 09:15:38 +0100">&lt;p&gt;I actually banned all methods from Executors that take the default thread factory in a patch where thread leaks are detected. It was for a reason &amp;#8211; when trying to debug thread leaks it is virtually impossible to tell where a thread originated from because the default thread factory has a naming pattern that plain sucks. I'd insist that we don't use this:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
+  &lt;span class="code-keyword"&gt;private&lt;/span&gt; &lt;span class="code-keyword"&gt;static&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; ThreadFactory FACTORY = Executors.defaultThreadFactory();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and rely on Lucene's NamedThreadFactory instead (passing a base name that at least tries to indicate what a given threadpool does).&lt;/p&gt;</comment>
                    <comment id="13427165" author="dweiss" created="Thu, 2 Aug 2012 09:17:33 +0100">&lt;p&gt;Uh, sorry &amp;#8211; I see what you did now. Anything on your mind in particular when you talk about behavioral changes?&lt;/p&gt;</comment>
                    <comment id="13427167" author="dweiss" created="Thu, 2 Aug 2012 09:19:06 +0100">&lt;p&gt;I see the default one resets inherited priority and daemon status. Security manager I wouldn't worry about...&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
            &lt;span class="code-keyword"&gt;if&lt;/span&gt; (t.isDaemon())
                t.setDaemon(&lt;span class="code-keyword"&gt;false&lt;/span&gt;);
            &lt;span class="code-keyword"&gt;if&lt;/span&gt; (t.getPriority() != &lt;span class="code-object"&gt;Thread&lt;/span&gt;.NORM_PRIORITY)
                t.setPriority(&lt;span class="code-object"&gt;Thread&lt;/span&gt;.NORM_PRIORITY);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13427224" author="simonw" created="Thu, 2 Aug 2012 11:08:37 +0100">&lt;blockquote&gt;&lt;p&gt;Uh, sorry – I see what you did now. Anything on your mind in particular when you talk about behavioral changes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't have anything in mind I just wanna replace logic with already existing logic that is "guaranteed" consistent with the documentation. This won't change anything really.&lt;/p&gt;
</comment>
                    <comment id="13427228" author="dweiss" created="Thu, 2 Aug 2012 11:13:26 +0100">&lt;p&gt;This will require manual exclusion of that source file once the ban on Executors.defaultThreadFactory() is in. An alternate route is to change the documentation and not claim compatibility with defaultThreadFactory, instead just say that we create non-daemon threads with NORM_PRIORITY?&lt;/p&gt;</comment>
                    <comment id="13427230" author="thetaphi" created="Thu, 2 Aug 2012 11:15:58 +0100">&lt;blockquote&gt;&lt;p&gt;This will require manual exclusion of that source file once the ban on Executors.defaultThreadFactory() is in&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Then we need a separate forbiddenApis.txt file... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13427328" author="rcmuir" created="Thu, 2 Aug 2012 15:02:34 +0100">&lt;p&gt;+1 to the patch: the forbidden check is a 2nd priority. it can be a separate .txt file with its own ant fileset.&lt;/p&gt;</comment>
                    <comment id="13429694" author="rcmuir" created="Tue, 7 Aug 2012 04:41:19 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12538879" name="LUCENE-4281.patch" size="2050" author="simonw" created="Thu, 2 Aug 2012 09:09:26 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 Aug 2012 08:15:38 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243691</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23419</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4276] refuse to execute on broken corrupting jvms</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4276</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;There are some jvms where we know lucene does not work at all and will just produce things like corrupt indexes.&lt;/p&gt;

&lt;p&gt;We should detect this in a static block of Constants.java and refuse to run at all.&lt;/p&gt;</description>
                <environment/>
            <key id="12600824">LUCENE-4276</key>
            <summary>refuse to execute on broken corrupting jvms</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Tue, 31 Jul 2012 12:26:11 +0100</created>
                <updated>Fri, 10 May 2013 00:05:09 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>7</watches>
                                                    <comments>
                    <comment id="13425682" author="rcmuir" created="Tue, 31 Jul 2012 12:28:58 +0100">&lt;p&gt;first on the list is java 1.7.0 b147&lt;/p&gt;

&lt;p&gt;I think this is most important, ive already seen people on the list&lt;br/&gt;
mention they are using it.&lt;/p&gt;</comment>
                    <comment id="13425685" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 12:50:02 +0100">&lt;blockquote&gt;&lt;p&gt;We should detect this in a static block of Constants.java and refuse to run at all.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As I said on the mailing list, leaving it at that is too extreme.&lt;br/&gt;
At a minimum, there should be an override...  -Dlucene.forceRun=true or something.&lt;/p&gt;</comment>
                    <comment id="13425686" author="rcmuir" created="Tue, 31 Jul 2012 12:51:25 +0100">&lt;p&gt;what good can come from allowing someone to force lucene to run on 1.7.0?&lt;/p&gt;

&lt;p&gt;This makes no sense to me.&lt;/p&gt;</comment>
                    <comment id="13425689" author="thetaphi" created="Tue, 31 Jul 2012 12:58:36 +0100">&lt;p&gt;As said on the mailing list, disallowing specific JVM versions is fine, but only specific version ranges. Not open ranges, so something like disallowing JRockit completely with no version limit makes no sense.&lt;/p&gt;

&lt;p&gt;For 1.6.x versions of Java we must also look into other options (you can get them using the HotspotBean like RamUsageEstimator does). So fail if version is 1.6.0_19 to 1.6.0_28 (I think, have to look up) with -XX:+AggressiveOpts (lots of stupid Solr and ElasticSearch users did this on their installation) or -XX+FooBarStringConcatOptimization.&lt;/p&gt;

&lt;p&gt;If a bug is at release time not fixed in a JDK or there is a proposed fix in a hard stated future JDK version, we should set the upper limit of failing versions to the version currently on the market. So for the JRockit case, the version used here to test.&lt;/p&gt;</comment>
                    <comment id="13425690" author="thetaphi" created="Tue, 31 Jul 2012 13:01:59 +0100">&lt;p&gt;One addition from mailing list:&lt;br/&gt;
The pattern list of vendors and versions should be in a resource file, not in code. So it is easy to edit on release.&lt;/p&gt;</comment>
                    <comment id="13425691" author="rcmuir" created="Tue, 31 Jul 2012 13:02:26 +0100">&lt;p&gt;No it should not be Jrockit or any other JVM completely. But that exact version you tested has a problem!&lt;/p&gt;

&lt;p&gt;I just want the list of exact versions that cause corruption. We add a one-line if statement to prevent the corruption, just like we would do in any other case.&lt;/p&gt;</comment>
                    <comment id="13425693" author="rcmuir" created="Tue, 31 Jul 2012 13:03:43 +0100">&lt;blockquote&gt;
&lt;p&gt;but only specific version ranges&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Personally i'm not even arguing for ranges, i think thats dangerous.&lt;br/&gt;
Just a set of &lt;b&gt;precise&lt;/b&gt; versions that are known to do this.&lt;/p&gt;</comment>
                    <comment id="13425698" author="shaie" created="Tue, 31 Jul 2012 13:14:43 +0100">&lt;p&gt;I didn't follow the email thread closely, but I don't want to see my JVM version on the list, if there's some use case that may lead to index corruption. What if I never step on that use case?&lt;/p&gt;

&lt;p&gt;I'm just worried that such change would mean in the future preventing to run Lucene and a JVM that e.g. fails to properly detect the Codecs to use (i.e. ServiceLoader bug in IBM J9, which was fixed recently).&lt;/p&gt;

&lt;p&gt;So as long as this is a list of specific JVM versions, that are known to cause index corruption, and not other runtime bugs (locale issues, ServiceLoader and such), and as long as this list is controlled by a file that can be edited by the app, I'm ok.&lt;/p&gt;

&lt;p&gt;But if we're making all this trouble (file etc.), why fail to run? we can print a message to System.err &amp;#8211; let the app choose.&lt;/p&gt;

&lt;p&gt;Preventing Lucene from running on certain JVMs is too extreme IMO.&lt;/p&gt;

&lt;p&gt;In general, I wish we'd be less extreme in our changes in Lucene code, but that's a matter for a separate discussion.&lt;/p&gt;</comment>
                    <comment id="13425699" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 13:15:36 +0100">&lt;blockquote&gt;&lt;p&gt;what good can come from allowing someone to force lucene to run on 1.7.0? This makes no sense to me.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is indicative of the larger problem in Lucene development recently - the tendency to prohibit something just because you can't think of the use-case off the top of your head.&lt;/p&gt;

&lt;p&gt;So here's your use-case: one &lt;b&gt;can&lt;/b&gt; normally get most versions of most major JVMs to correctly run applications - it's often just a matter of disabling certain optimisations for certain methods, or in extreme cases prohibiting JIT of the method altogether.&lt;/p&gt;

&lt;p&gt;Hence, at a minimum, there should be a way for users to easily override this behavior w/o recompiling applications.  We should inform, not enforce.&lt;/p&gt;
</comment>
                    <comment id="13425700" author="rcmuir" created="Tue, 31 Jul 2012 13:23:27 +0100">&lt;blockquote&gt;
&lt;p&gt;This is indicative of the larger problem in Lucene development recently - the tendency to prohibit something just because you can't think of the use-case off the top of your head.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think this is the case at all. I just want to prevent the creation of corrupt indexes.&lt;/p&gt;

&lt;p&gt;Creating corrupt index is &lt;b&gt;NOT A USE CASE&lt;/b&gt;&lt;/p&gt;</comment>
                    <comment id="13425701" author="shaie" created="Tue, 31 Jul 2012 13:23:31 +0100">&lt;blockquote&gt;&lt;p&gt;We should inform, not enforce.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 for that !&lt;/p&gt;</comment>
                    <comment id="13425704" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 13:30:57 +0100">&lt;blockquote&gt;&lt;p&gt;Creating corrupt index is NOT A USE CASE&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sigh.&lt;/p&gt;

&lt;p&gt;-1 &lt;br/&gt;
(my formal VETO for this issue unless it contains an easy user override)&lt;/p&gt;</comment>
                    <comment id="13425708" author="shaie" created="Tue, 31 Jul 2012 13:33:24 +0100">&lt;blockquote&gt;&lt;p&gt;Creating corrupt index is NOT A USE CASE&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Depends &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. If the corrupt index is a result of using DirectPF Codec, or using payloads etc., then it is a use case. If the corrupt index is a result of adding one field with one word, untokenized and all, then that's a more serious corruption, and how we handle/inform it should be discussed.&lt;/p&gt;

&lt;p&gt;At any rate, I've seen apps preventing to run on old JVMs, but not future, potentially buggy, JVMs. What if we'll detect a corruption related to a certain Linux kernel version - would we prevent Lucene from running there too?&lt;/p&gt;</comment>
                    <comment id="13425710" author="rcmuir" created="Tue, 31 Jul 2012 13:36:31 +0100">&lt;blockquote&gt;
&lt;p&gt;-1&lt;br/&gt;
(my formal VETO for this issue unless it contains an easy user override)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats ok: I will make the issue a blocker and we won't release until we resolve this.&lt;/p&gt;

&lt;p&gt;I'm tired of seeing b147 in bug reports.&lt;/p&gt;</comment>
                    <comment id="13425713" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 13:39:33 +0100">&lt;blockquote&gt;&lt;p&gt;Thats ok: I will make the issue a blocker and we won't release until we resolve this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Luckily, a release can't be vetoed.&lt;/p&gt;</comment>
                    <comment id="13425725" author="rcmuir" created="Tue, 31 Jul 2012 13:43:59 +0100">&lt;p&gt;Sure it cant: but if there is any VOTE without doing something here for java 1.7.0, I will vote -1 against this release and point to this issue (that we just allow index corruption to slip through when a simple check can prevent it).&lt;/p&gt;

&lt;p&gt;I think people will listen to this.&lt;/p&gt;</comment>
                    <comment id="13425729" author="shaie" created="Tue, 31 Jul 2012 13:48:23 +0100">&lt;p&gt;Robert, why if someone disagrees with you, you react like that? Either VETO'ing the disagreement, or looking for a way to force your opinion, like marking this issue a Blocker?&lt;/p&gt;

&lt;p&gt;I don't think this is a blocker issue, as there's no bug in Lucene &amp;#8211; if there is, let's fix it (and preventing to run on a JVM is not a bug !). I won't remove the blocker myself, out of respect to you (and I also don't think that's how we should handle things in the community), but I ask that you remove it.&lt;/p&gt;

&lt;p&gt;It's a community, not always people will agree with you. I think several times before I pull out my PMC hat and VETO/force my opinion. In fact, I don't think I ever VETO'ed an issue, but recently I came very close to doing that (glad that so far I didn't need to).&lt;/p&gt;

&lt;p&gt;Getting to a consensus / majority is better IMO. Let's not make this community a "one man show".&lt;/p&gt;</comment>
                    <comment id="13425730" author="rcmuir" created="Tue, 31 Jul 2012 13:51:26 +0100">&lt;p&gt;I didnt VETO anything. That was Yonik.&lt;/p&gt;

&lt;p&gt;I made the issue a blocker because I want to prevent index corruption.&lt;/p&gt;</comment>
                    <comment id="13425732" author="rcmuir" created="Tue, 31 Jul 2012 13:55:38 +0100">&lt;blockquote&gt;
&lt;p&gt;At any rate, I've seen apps preventing to run on old JVMs, but not future, potentially buggy, JVMs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you read my comments on this issue, I already stated i want to only defend against exact, precise,&lt;br/&gt;
known buggy versions that create index corruption. No ranges, no future versions, none of that!&lt;/p&gt;

&lt;p&gt;Only the ones we know and have tested, that create corrupt indexes.&lt;/p&gt;</comment>
                    <comment id="13425733" author="shaie" created="Tue, 31 Jul 2012 13:57:19 +0100">&lt;blockquote&gt;&lt;p&gt;Thats ok: I will make the issue a blocker and we won't release until we resolve this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's what I'm talking about &amp;#8211; the attitude. Yonik didn't even completely VETO the issue, if the user can override this behavior he seems to agree with your solution (and I won't object to it either), and I read on the thread that Uwe prefers that as well.&lt;/p&gt;

&lt;p&gt;Yet instead of relating to Yonik's comment, you mark the issue blocker in a way to enforce your opinion.&lt;/p&gt;

&lt;p&gt;In the past, I've compromised on solutions raised by others, even when I didn't think they're perfect. That's reality &amp;#8211; Lucene is a tens of thousands apps project, not a one man's opinion. Sometimes, you (and I !) can't see the full breadth of use cases out there, that doesn't mean we need to discard them !&lt;/p&gt;

&lt;p&gt;Again, please remove the blocker mark. We should discuss it, in the worse case, we should vote (notice how VOTE and VETO are so close ?) on it. We should try to aleast not enforce our opinions on other developers.&lt;/p&gt;</comment>
                    <comment id="13425734" author="shaie" created="Tue, 31 Jul 2012 13:58:25 +0100">&lt;blockquote&gt;&lt;p&gt;Only the ones we know and have tested, that create corrupt indexes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The question that I asked is "when do those versions generate a corrupt index" &amp;#8211; is it always? a specific scenario? What I don't trip on that scenario? Why should I not run with that version?&lt;/p&gt;</comment>
                    <comment id="13425737" author="rcmuir" created="Tue, 31 Jul 2012 14:01:52 +0100">&lt;p&gt;I'm not gonna remove the blocker mark: i think we should defend against index corruption, its a big deal.&lt;/p&gt;

&lt;p&gt;You are totally welcome to ignore the fact i set this issue to blocker and VOTE +1 for all the releases you want.&lt;/p&gt;

&lt;p&gt;If someone creates a release candidate with three +1s its released! It doesn't matter if I vote against the release or have a blocker issue open.&lt;/p&gt;

&lt;p&gt;But in the case of such a VOTE, I will reply to the vote thread pointing to this issue, so that people are informed.&lt;/p&gt;
</comment>
                    <comment id="13425747" author="rcmuir" created="Tue, 31 Jul 2012 14:18:24 +0100">&lt;p&gt;Simple patch to detect the broken oracle 1.7.0 release.&lt;/p&gt;</comment>
                    <comment id="13425749" author="thetaphi" created="Tue, 31 Jul 2012 14:26:31 +0100">&lt;blockquote&gt;&lt;p&gt;Simple patch to detect the broken oracle 1.7.0 release.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It should be *Error not RuntimeException, as this prevents class and dependent classes to load.&lt;/p&gt;</comment>
                    <comment id="13425750" author="shaie" created="Tue, 31 Jul 2012 14:26:32 +0100">&lt;p&gt;Sigh. I was hoping to get to consensus here.&lt;/p&gt;

&lt;p&gt;As for the patch, if that comes from a file, I'd feel better to support it.&lt;/p&gt;

&lt;p&gt;Few separate comments:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Can we include some text in the message, describing why it's incompatible, such as a link to a JIRA issue / markmail thread?&lt;/li&gt;
	&lt;li&gt;How do you know to recommend a specific build version?
	&lt;ul&gt;
		&lt;li&gt;Perhaps instead of recommending a version, we should include in the exception all the versions that are not supported? That way, we only commit to what we tested and is not supported.&lt;/li&gt;
		&lt;li&gt;I'm worried that we'll recommend a version that will later turn out to be broken, and won't remove the 'recommendation' text.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;
</comment>
                    <comment id="13425752" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 14:30:08 +0100">&lt;blockquote&gt;&lt;p&gt;Yonik didn't even completely VETO the issue, if the user can override this behavior he seems to agree with your solution&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Precisely.  &lt;br/&gt;
Does anyone other than Robert object to having a user override?&lt;/p&gt;</comment>
                    <comment id="13425754" author="shaie" created="Tue, 31 Jul 2012 14:35:09 +0100">&lt;blockquote&gt;&lt;p&gt;Does anyone other than Robert object to having a user override?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not me. This file can come in the lucene.jar with the ability to either run with -Denforce.jvm.check or an override file (or the user can edit the file in the jar), I don't care.&lt;/p&gt;</comment>
                    <comment id="13425755" author="rcmuir" created="Tue, 31 Jul 2012 14:37:05 +0100">&lt;blockquote&gt;
&lt;p&gt;Can we include some text in the message, describing why it's incompatible, such as a link to a JIRA issue / markmail thread?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can, ideally we would link to the original bugs, but they are now hidden from view in Oracle's issue tracker as this was related to some security vulnerabilities.&lt;/p&gt;

&lt;p&gt;We could also link to &lt;a href="http://vimeo.com/33817739" class="external-link"&gt;http://vimeo.com/33817739&lt;/a&gt; &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How do you know to recommend a specific build version? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because 1.7.0_01 was a bugfix release addressing this issue (broken loop optimizations).&lt;/p&gt;

&lt;p&gt;I don't mind listing that "Oracle Corporation" 1.7.0 is broken and not suggesting a version to upgrade to though, I think thats fine.&lt;/p&gt;</comment>
                    <comment id="13425759" author="rcmuir" created="Tue, 31 Jul 2012 14:39:59 +0100">&lt;p&gt;I didnt say I objected to the override completely, I just said it makes no sense to me.&lt;/p&gt;

&lt;p&gt;For this particular one I absolutely don't object to something like:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;"If you really want to use this jvm, use -XX:-UseLoopPredicate -Drisky.override=true"
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13425761" author="erickerickson" created="Tue, 31 Jul 2012 14:42:21 +0100">&lt;p&gt;IMO, we &lt;em&gt;must&lt;/em&gt; provide the ability to override. Changing the JVM is not trivial in some organizations for a variety of reasons, from "policy" to worry about whether version X is compatible with all the &lt;em&gt;other&lt;/em&gt; apps (yeah, I know you can run multiple JVMs, but we're talking corporate policy here) to only using certified JVMs. Making this non-overridable would lock Solr/Lucene out of running in some organizations.&lt;/p&gt;

&lt;p&gt;The other issue is that now a new user may not be able to even take Solr/Lucene for a test drive very easily. Imagine you're coming to Solr just to give it a spin and you can't get it to run without changing your JVM. That would be one more barrier to people using it. One of the strengths of Solr/Lucene is all the hoops you &lt;em&gt;don't&lt;/em&gt; have to jump through to run the example code.&lt;/p&gt;

&lt;p&gt;That said, I &lt;em&gt;also&lt;/em&gt; agree it's just asking for trouble to run with known JVMs that can produce corrupt indexes. We should make it harder to do this naively. The current setup makes it easy to run with a JVM that can assassinate you down the road. Like after you've gone to production. And your commercial site is costing you $$$/second as long as it's down. That's, uhhhmmmm, bad.&lt;/p&gt;

&lt;p&gt;It's also a time sink, I'd like to head using known buggy JVMs off at the pass.&lt;/p&gt;

&lt;p&gt;I also like the fact that we'd have a concise, known place to go to see all the JVMs that have known problems. It gives us a nice place to point users at when they see problems ("Is your JVM listed in known_bad_jvms.txt?"), assuming we keep this in an external file. Hmmm would be a nice thing to keep on the Wiki too... In big bold red letters.&lt;/p&gt;

&lt;p&gt;So, +1 to making it harder to run with bad JVMs: If the user really wants to blow their leg off, let them run with known bad JVMs by providing a command-line override. I'll propose&lt;br/&gt;
-Dit.is.stupid.to.run.with.a.known.buggy.JVM.but.we.will.do.it.anyway=true. Or perhaps&lt;br/&gt;
-DI.really.hate.the.execs.so.we.will.devalue.stock.options=true.&lt;/p&gt;

&lt;p&gt;I'd really like to have a way to require the &lt;em&gt;name&lt;/em&gt; of the person making the decision to override in all the startup scripts they'd have that implemented the override, but that's just being mean....&lt;/p&gt;

&lt;p&gt;-1 to making it impossible to run with a bad JVM&lt;/p&gt;

&lt;p&gt;+1 to being really terse with users who override things and complain anyway.&lt;/p&gt;</comment>
                    <comment id="13425765" author="rcmuir" created="Tue, 31 Jul 2012 14:45:17 +0100">&lt;p&gt;It would be far far better to just detect if -XX:-UseLoopPredicate or -Xint are set instead and not have a -Drisky.override.&lt;/p&gt;

&lt;p&gt;Maybe someone knows how to do this. Then the workaround message is simpler also.&lt;/p&gt;</comment>
                    <comment id="13425771" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 14:51:58 +0100">&lt;blockquote&gt;&lt;p&gt;I didnt say I objected to the override completely, &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The historical revisions are tiresome.  How else is one supposed to interpret this: &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4276?focusedCommentId=13425710&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13425710" class="external-link"&gt;https://issues.apache.org/jira/browse/LUCENE-4276?focusedCommentId=13425710&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13425710&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It would be far far better to just detect if -XX:-UseLoopPredicate or -Xint are set instead and not have a -Drisky.override.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, we still need an override.  We do not always know best.&lt;/p&gt;</comment>
                    <comment id="13425772" author="rcmuir" created="Tue, 31 Jul 2012 14:52:01 +0100">&lt;blockquote&gt;
&lt;p&gt;IMO, we must provide the ability to override. Changing the JVM is not trivial in some organizations for a variety of reasons,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sure, I'm just saying I don't like this necessarily being some arbitrary option we introduce. This doesn't make sense to me if we can detect the actual workaround flags necessary to prevent the index corruption.&lt;/p&gt;

&lt;p&gt;For the two recent corruption bugs these flags already exist, why not just use those if we can?&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Oracle 1.7.0 b147: -XX:-UseLoopPredicate&lt;/li&gt;
	&lt;li&gt;JRockit 1.6.0_33-R28.2.4-4.1.0: -XnoOpt&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13425773" author="markrmiller@gmail.com" created="Tue, 31 Jul 2012 14:53:12 +0100">&lt;p&gt;+1 on user override from me as well. I agree with putting barriers in front of things we know are a problem - we should always make those barriers circumventable though.&lt;/p&gt;</comment>
                    <comment id="13425774" author="rcmuir" created="Tue, 31 Jul 2012 14:53:22 +0100">&lt;blockquote&gt;
&lt;p&gt;No, we still need an override. We do not always know best.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What does this mean? if you don't supply this option, loops are broken and it creates corrupt indexes.&lt;/p&gt;

&lt;p&gt;Why is it so important we have our own override that &lt;b&gt;allows&lt;/b&gt; someone to create a corrupt index?&lt;br/&gt;
Why can't the override be the workaround that also prevents corruption?&lt;/p&gt;</comment>
                    <comment id="13425775" author="thetaphi" created="Tue, 31 Jul 2012 14:53:42 +0100">&lt;blockquote&gt;&lt;p&gt;IMO, we must provide the ability to override. Changing the JVM is not trivial in some organizations for a variety of reasons, from "policy" to worry about whether version X is compatible with all the other apps (yeah, I know you can run multiple JVMs, but we're talking corporate policy here) to only using certified JVMs. Making this non-overridable would lock Solr/Lucene out of running in some organizations.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;On the other hand if you use this exactly JVM and startup your Solr instance it may crash when it uses PorterStemmer (OK, that's not bad at all because its another form of Robert's Runtime Exception - I just point that out: Solr in default configuration crashes after indexing the example docs, we tried that out), but if you are working around that by using not PorterStemmer, the worse thing is: After the first commit on your maybe holy index, it is correup, really. If you are in such a company, you have no chance to run Solr/Lucene - then its also fine to fail early, the override would be useless.&lt;/p&gt;

&lt;p&gt;If you are in such a company that forces you to use JDK 1.7.0b147 (also known as 1.7.0GA), then they also won't allow you to pass -XX:-UseLoopPredicate. So at the end you cannot run Solr at all, so why not fail early instaed of late when people complain at your Consulting Company.&lt;/p&gt;

&lt;p&gt;In general we should have a list of failing JDKs in the release notes. We started to do this in Lucene/Solr 3.5.0 where we "oficially declared Lucene as Java 7 compatible" with explicitely stating that "you have to use u1 or later".&lt;/p&gt;</comment>
                    <comment id="13425776" author="jkrupan" created="Tue, 31 Jul 2012 14:53:55 +0100">&lt;p&gt;What I would prefer to have is a tool/script that I can explicitly run that will alert me to potential issues before I even run Lucene/Solr on a box. Or maybe both a command line tool and a test app that can be deployed in the desired container to report potential issues.&lt;/p&gt;

&lt;p&gt;Then, as part of the support process, the first thing we would ask people is whether they ran the "environment check" tool and what it said.&lt;/p&gt;

&lt;p&gt;The JVM/compatibility check is just one example of checks that the tool could perform.&lt;/p&gt;</comment>
                    <comment id="13425777" author="rcmuir" created="Tue, 31 Jul 2012 14:56:36 +0100">&lt;blockquote&gt;
&lt;p&gt;What I would prefer to have is a tool/script that I can explicitly run that will alert me to potential issues before I even run Lucene/Solr on a box. Or maybe both a command line tool and a test app that can be deployed in the desired container to report potential issues.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its called 'ant test'&lt;/p&gt;</comment>
                    <comment id="13425779" author="rcmuir" created="Tue, 31 Jul 2012 15:00:25 +0100">&lt;blockquote&gt;
&lt;p&gt;If you are in such a company that forces you to use JDK 1.7.0b147 (also known as 1.7.0GA), then they also won't allow you to pass -XX:-UseLoopPredicate. So at the end you cannot run Solr at all, so why not fail early instaed of late when people complain at your Consulting Company.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't understand the situation where someone can pass -Dallow.corrupt.indexes=true but not -XX:-UseLoopPredicate&lt;/p&gt;

&lt;p&gt;And thats all i'm saying: there is already a flag for this bug to get past it, its -XX:-UseLoopPredicate. If you use it, you can use 1.7.0GA and it wont create a corrupt index.&lt;/p&gt;</comment>
                    <comment id="13425785" author="rcmuir" created="Tue, 31 Jul 2012 15:09:08 +0100">&lt;p&gt;patch changing the message to suggest -XX:-UseLoopPredicate, if this is set, we don't fail.&lt;/p&gt;
</comment>
                    <comment id="13425787" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 15:12:54 +0100">&lt;blockquote&gt;&lt;p&gt;&amp;gt; No, we still need an override. We do not always know best.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;What does this mean? if you don't supply this option, loops are broken and it creates corrupt indexes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You're still assuming that we know the best and only fixes for unknown and unseen applications using Lucene in different ways. We don't.&lt;/p&gt;

&lt;p&gt;First versions of workarounds can often be more crude (i.e. don't JIT this class or method at all) and later refinements can be more targeted and higher performing, and depending on the customer may even be in temporary binary patch form.  There are probably other cases I haven't even thought of yet - which was really the point in the first place.  An unconditional user override remains critical.&lt;/p&gt;</comment>
                    <comment id="13425789" author="thetaphi" created="Tue, 31 Jul 2012 15:14:05 +0100">&lt;blockquote&gt;&lt;p&gt;-XX:-UseLoopPredicate&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;With this flag it won't create corrumpt indexes, but will it crash PorterStemmer? I don't remember, we should try out again. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13425791" author="rcmuir" created="Tue, 31 Jul 2012 15:15:48 +0100">&lt;blockquote&gt;
&lt;p&gt;First versions of workarounds can often be more crude (i.e. don't JIT this class or method at all) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is not possible for this bug. If you prevent compilation of specific methods, it just miscompiles another loop.&lt;/p&gt;

&lt;p&gt;This is the oracle-suggested workaround in the release notes of java 1.7.0GA&lt;/p&gt;</comment>
                    <comment id="13425792" author="rcmuir" created="Tue, 31 Jul 2012 15:16:20 +0100">&lt;blockquote&gt;
&lt;p&gt;With this flag it won't create corrumpt indexes, but will it crash PorterStemmer? I don't remember, we should try out again.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm only worried about corruption here.&lt;/p&gt;</comment>
                    <comment id="13425796" author="shaie" created="Tue, 31 Jul 2012 15:23:03 +0100">&lt;p&gt;Can you add to the message "This JVM is incompatible with Lucene, and may/will lead to index corruption"? It's more informative.&lt;/p&gt;

&lt;p&gt;And, can you please describe here what's leading to the corruption? I.e. if I do:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;new IndexWriter().close()&lt;/li&gt;
	&lt;li&gt;new IndexWriter().forceMerge()/maybeMerge().close()&lt;/li&gt;
	&lt;li&gt;new IndexWriter().addDocument().commit().close()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Will all of these lead to corruption? Only some of them? What class causes the corruption, due to the bug?&lt;/p&gt;</comment>
                    <comment id="13425799" author="rcmuir" created="Tue, 31 Jul 2012 15:28:52 +0100">&lt;p&gt;Its not just one class: its a general problem of loops being miscompiled. &lt;/p&gt;

&lt;p&gt;So any merging at all is dangerous, checkindex might falsely say your existing index is corrupt (when its not), etc, etc.&lt;/p&gt;</comment>
                    <comment id="13425801" author="thetaphi" created="Tue, 31 Jul 2012 15:29:35 +0100">&lt;p&gt;Here the release notes: &lt;a href="http://www.oracle.com/technetwork/java/javase/jdk7-relnotes-418459.html#knownissues" class="external-link"&gt;http://www.oracle.com/technetwork/java/javase/jdk7-relnotes-418459.html#knownissues&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13425804" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 15:32:31 +0100">&lt;p&gt;Updated patch with generic user override&lt;br/&gt;
lucene.checkJVMVersion&lt;/p&gt;</comment>
                    <comment id="13425808" author="rcmuir" created="Tue, 31 Jul 2012 15:34:54 +0100">&lt;p&gt;Again I don't agree with the generic user override, but I'm not totally against it.&lt;/p&gt;

&lt;p&gt;However i don't like the name: i think it should sound significantly more dangerous.&lt;/p&gt;

&lt;p&gt;-Dlucene.allow.index.corruption seems fine to me.&lt;/p&gt;</comment>
                    <comment id="13425810" author="shaie" created="Tue, 31 Jul 2012 15:36:16 +0100">&lt;p&gt;An external file allows one to add additional versions later on. E.g. we don't know today that an already released JVM doesn't cause such bugs, because we didn't test it, or it will appear later on. It's another reason to put this check in a file.&lt;/p&gt;

&lt;p&gt;Then, for people that use Lucene to create software for other people, they can add such versions to the file, instead of recompiling Lucene.&lt;/p&gt;</comment>
                    <comment id="13425813" author="shaie" created="Tue, 31 Jul 2012 15:37:51 +0100">&lt;p&gt;Does it affect Lucene 3.6.1 too? 4.0-ALPHA? My guess is that "yes", and therefore we should issue a warning to our users.&lt;/p&gt;</comment>
                    <comment id="13425815" author="thetaphi" created="Tue, 31 Jul 2012 15:38:19 +0100">&lt;p&gt;It affects Lucene back to version 1.0&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Does it affect Lucene 3.6.1 too? 4.0-ALPHA? My guess is that "yes", and therefore we should issue a warning to our users.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That was done a year ago:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;&lt;a href="http://mail-archives.apache.org/mod_mbox/www-announce/201107.mbox/%3C001601cc4d6b$37618880$a6249980$@apache.org%3E" class="external-link"&gt;http://mail-archives.apache.org/mod_mbox/www-announce/201107.mbox/%3C001601cc4d6b$37618880$a6249980$@apache.org%3E&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href="http://blog.thetaphi.de/2011/07/real-story-behind-java-7-ga-bugs.html" class="external-link"&gt;http://blog.thetaphi.de/2011/07/real-story-behind-java-7-ga-bugs.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13425818" author="rcmuir" created="Tue, 31 Jul 2012 15:39:12 +0100">&lt;blockquote&gt;
&lt;p&gt;An external file allows one to add additional versions later on. E.g. we don't know today that an already released JVM doesn't cause such bugs, because we didn't test it, or it will appear later on. It's another reason to put this check in a file.&lt;/p&gt;

&lt;p&gt;Then, for people that use Lucene to create software for other people, they can add such versions to the file, instead of recompiling Lucene.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Those people can implement their own checks in their own apps for whatever criteria they want: I don't think we need to build infrastructure for them to do that.&lt;/p&gt;

&lt;p&gt;I just want to have a check against the most ridiculous index corruption bugs (thats why i suggested this 1.7.0 one only here, I know there are others but in my opinion its the most dangerous and widespread).&lt;/p&gt;</comment>
                    <comment id="13425935" author="mikemccand" created="Tue, 31 Jul 2012 18:24:58 +0100">&lt;p&gt;+1 to put a barrier up for running on JVMs that blatantly cause corruption.  This (early, hard failure) is a service to our users, else they will waste time trying to figure out what's going on.&lt;/p&gt;

&lt;p&gt;+1 to scarily-named user override.  -Dlucene.allow.index.corruption seems good.&lt;/p&gt;

&lt;p&gt;I don't think we need a separate file ... I think that's over designing it.&lt;/p&gt;

&lt;p&gt;Also, we actually do have a wiki page detailing java bugs we've hit: &lt;a href="http://wiki.apache.org/lucene-java/SunJavaBugs" class="external-link"&gt;http://wiki.apache.org/lucene-java/SunJavaBugs&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13425960" author="thetaphi" created="Tue, 31 Jul 2012 18:53:25 +0100">&lt;p&gt;As we already do the JVM option check for Java 7 (to detect is the "fixing" option is enabled), should we also check for corresponding Java options on 1.6.0_23 (or like that have to look up) to 1.6.0_28 (this is what I meant as "range") and detect if aggressive opts are enabled? We had users running on 1.6.0_26 with -XX:+AggressiveOpts and corrumpting their index (see mail from various users on Solr, Lucene and ElasticSearch). As we already do the HotspotMagangement reflection, we can also check this. The hotspot version numbers of the corresponding Java versions can be found out. Don't use Java version number like 1.6.0_26, as those are different in OpenJDK. Ubuntu ships with 1.6.0_24, but in fact the corrumption bugs are fixed (_24 in in that case has in fact nothing to do with Oracle bugfix version numbers). Ubuntu's version in 12.04LTS (called _24) is the same like _31 in Oracle numbers (too stupid, see &lt;a href="http://blog.thetaphi.de/2011/12/jdk-7u2-released-how-about-linux-and.html" class="external-link"&gt;http://blog.thetaphi.de/2011/12/jdk-7u2-released-how-about-linux-and.html&lt;/a&gt;). So we should only look at hotspot version numbers. The HotspotManagementBean can return that one, too. Its better than reading a sysprop...&lt;/p&gt;</comment>
                    <comment id="13425967" author="steve_rowe" created="Tue, 31 Jul 2012 18:59:18 +0100">&lt;p&gt;I agree with Mike M. on all three points:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;+1 to put a barrier up for running on JVMs that blatantly cause corruption. This (early, hard failure) is a service to our users, else they will waste time trying to figure out what's going on.&lt;/p&gt;

&lt;p&gt;+1 to scarily-named user override. -Dlucene.allow.index.corruption seems good.&lt;/p&gt;

&lt;p&gt;I don't think we need a separate file ... I think that's over designing it.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                    <comment id="13425973" author="thetaphi" created="Tue, 31 Jul 2012 19:07:54 +0100">&lt;p&gt;To come back to my last post:&lt;br/&gt;
The crazy thing is that OpenJDK versions shipped with Linux oerating systems often have stupid version numbers (see above). In fact the Java 7 version shipped with Ubuntu is called "7~b147-2.0-0ubuntu0.11.10.1", see &lt;a href="https://launchpad.net/ubuntu/+source/openjdk-7/7~b147-2.0-0ubuntu0.11.10.1" class="external-link"&gt;https://launchpad.net/ubuntu/+source/openjdk-7/7~b147-2.0-0ubuntu0.11.10.1&lt;/a&gt;, but it is &lt;b&gt;not&lt;/b&gt; broken. This may also be the confusion we have with some users when they report bugs (especially when they come from Linux). I am not happy about that, but this is how it is, I dont think detecting broken versions is easy and Robert's code can easily cause the problem. Vendor for OpenJDK is also "Oracle" (they return it for conformance with original). And the O/S verndor patches don't always raise version numbers of Hotspot.&lt;/p&gt;

&lt;p&gt;So I think its too risky to fail hard! I think we should put it in the release notes (like any other vendor of software does).&lt;/p&gt;</comment>
                    <comment id="13425994" author="rcmuir" created="Tue, 31 Jul 2012 19:27:59 +0100">&lt;p&gt;This patch doesnt look for b147, it looks at hotspot version (21.0-b17)&lt;/p&gt;

&lt;p&gt;If Ubuntu fixed hotspot without changing even its build number, then they fucked up. Not my problem.&lt;/p&gt;</comment>
                    <comment id="13426004" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 19:39:24 +0100">&lt;blockquote&gt;&lt;p&gt;So I think its too risky to fail hard! I think we should put it in the release notes (like any other vendor of software does).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 - we should be &lt;b&gt;very&lt;/b&gt; sure about our code before we purposely break something.  Thanks for looking deeper into it Uwe.&lt;/p&gt;</comment>
                    <comment id="13426011" author="rcmuir" created="Tue, 31 Jul 2012 19:43:29 +0100">&lt;p&gt;We arent purposely breaking anything. This JVM is broken.&lt;/p&gt;

&lt;p&gt;If some packager, somewhere, created a fixed version mislabeled the same as the broken version,&lt;br/&gt;
then thats &lt;b&gt;their bug&lt;/b&gt;.&lt;/p&gt;</comment>
                    <comment id="13426026" author="shaie" created="Tue, 31 Jul 2012 20:01:52 +0100">&lt;blockquote&gt;&lt;p&gt;...  then they fucked up. Not my problem.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But you make it the poor user's problem. What will he do - not use this JVM? Wait for the next one? When all the while that JVM is actually good.&lt;/p&gt;</comment>
                    <comment id="13426030" author="rcmuir" created="Tue, 31 Jul 2012 20:04:41 +0100">&lt;p&gt;He can pass the flag (-XX:-UseLoopPredicate) to ensure he doesnt create a corrupt index.&lt;/p&gt;

&lt;p&gt;Because how does this poor user actually know his jvm is not a buggy one since the version&lt;br/&gt;
is the same as a buggy one?&lt;/p&gt;</comment>
                    <comment id="13426032" author="rcmuir" created="Tue, 31 Jul 2012 20:06:08 +0100">&lt;p&gt;in fact, if you are so worried about it (I am not!), then actually look at how this "ubuntu fixed version" uwe is complaining about is actually implemented.&lt;/p&gt;

&lt;p&gt;Chances are high they just turned off the broken optimization so its not going to be a problem since we look for UseLoopPredicate in the patch.&lt;/p&gt;</comment>
                    <comment id="13426048" author="thetaphi" created="Tue, 31 Jul 2012 20:34:06 +0100">&lt;blockquote&gt;&lt;p&gt;Chances are high they just turned off the broken optimization so its not going to be a problem since we look for UseLoopPredicate in the patch.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Read my blog post from last November (lik see above). They patched all the fixes into Oracle build 147.&lt;/p&gt;</comment>
                    <comment id="13429698" author="rcmuir" created="Tue, 7 Aug 2012 04:41:22 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12538554" name="LUCENE-4276.patch" size="1855" author="yseeley@gmail.com" created="Tue, 31 Jul 2012 15:32:31 +0100"/>
                    <attachment id="12538551" name="LUCENE-4276.patch" size="1729" author="rcmuir" created="Tue, 31 Jul 2012 15:09:08 +0100"/>
                    <attachment id="12538548" name="LUCENE-4276.patch" size="667" author="rcmuir" created="Tue, 31 Jul 2012 14:18:24 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 31 Jul 2012 11:50:02 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243696</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23424</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4258] Incremental Field Updates through Stacked Segments</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4258</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Shai and I would like to start working on the proposal to Incremental Field Updates outlined here (&lt;a href="http://markmail.org/message/zhrdxxpfk6qvdaex" class="external-link"&gt;http://markmail.org/message/zhrdxxpfk6qvdaex&lt;/a&gt;).&lt;/p&gt;</description>
                <environment/>
            <key id="12600298">LUCENE-4258</key>
            <summary>Incremental Field Updates through Stacked Segments</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="sivany">Sivan Yogev</reporter>
                        <labels>
                    </labels>
                <created>Thu, 26 Jul 2012 12:48:44 +0100</created>
                <updated>Sun, 26 May 2013 11:48:40 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>5</votes>
                        <watches>15</watches>
                          <timeoriginalestimate seconds="9072000">2,520h</timeoriginalestimate>
                    <timeestimate seconds="9072000">2,520h</timeestimate>
                                  <comments>
                    <comment id="13423055" author="rcmuir" created="Thu, 26 Jul 2012 14:33:15 +0100">&lt;p&gt;A few things I dont understand:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;when updating a document, how do you know which terms to apply negative postings to?&lt;/li&gt;
	&lt;li&gt;how can the idea of updating "individual terms" work as far as length normalization information? How will that be reconciled?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In truth I think term is too fine-grained of a level for updates in lucene because of norms, &lt;br/&gt;
only updating whole fields of a document will really work (as then the norm can simply be recomputed and replaced). &lt;/p&gt;
</comment>
                    <comment id="13423139" author="shaie" created="Thu, 26 Jul 2012 16:16:40 +0100">&lt;p&gt;There is more to it than just the referenced email. I've had a couple of discussions in the past about this with various people (and it is my fault that I didn't wrote them down and shared them with the rest of you) &amp;#8211; I'll try to summarize below a more detailed proposal:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;API&lt;/b&gt;&lt;br/&gt;
Add an updateFields method which takes a Constraint and OP (eventually, it might replace today's updateDocument):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Constraint defines 'which documents' should be updated, and follows today's deleteDocument API (takes Term, Query and arrays of each)&lt;/li&gt;
	&lt;li&gt;OP defines the actual update to do on those documents:
	&lt;ul&gt;
		&lt;li&gt;It has a TYPE, with 3 values (At least for now):
		&lt;ol&gt;
			&lt;li&gt;REPLACE_DOC &amp;#8211; replaces an entire document (essentially what updateDocument is today)&lt;/li&gt;
			&lt;li&gt;UPDATE_FIELD &amp;#8211; incrementally update a field&lt;/li&gt;
			&lt;li&gt;REPLACE_FIELD &amp;#8211; replaces a field entirely&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
		&lt;li&gt;In addition, it takes a Field[] (or Iterable) to remove/add.&lt;/li&gt;
		&lt;li&gt;In light of the recent changes to IndexableField and friends, perhaps what it should take is a concrete UpdateField with a boolean specifying whether to add/remove its content. Suggestions are welcome !&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;b&gt;Implementation&lt;/b&gt;&lt;br/&gt;
The idea is to create StackedSegments, which, well, stack on top of current segments. The inspiration came from deletes, which can be viewed as a segment stacked on an existing segment, that marks which documents are deleted.&lt;/p&gt;

&lt;p&gt;Following that semantics, a segment could be comprised of these files:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Layer 1: _0.prx, _0.fnm, _0.fdt ...&lt;/li&gt;
	&lt;li&gt;Layer 2: _0_1.prx, _0_1.fdt (no updates to .fnm) &amp;#8211; override/merge info from layer 1&lt;/li&gt;
	&lt;li&gt;Layer 3: _0_2.prx  &amp;#8211; override/merge info from layer 2&lt;/li&gt;
	&lt;li&gt;Layer 4: _0_1.del &amp;#8211; deletes are &lt;b&gt;always&lt;/b&gt; the last layer, irregardless of their 'layer id' &amp;#8211; _0_1.del overrides everything, even _0_100.prx.
	&lt;ul&gt;
		&lt;li&gt;And they can be stacked on themselves as today, e.g. _0_2.del etc.&lt;br/&gt;
I believe that we'll need an UpdateCodec or something ... this is the part of the internal API that we still need to understand better. Help from folks like you Robert will be greatly appreciated !&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Two options to encode the posting lists:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;field:value --&amp;gt; +1, -5, +8, +12, -17 ... (simple, but cannot be encoded efficiently
	&lt;ol&gt;
		&lt;li&gt;+field:value --&amp;gt; 1, 8, 12&lt;/li&gt;
		&lt;li&gt;-field:value --&amp;gt; 5, 17&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Ideally, the way incremental updates will be applied will follow how deletes are applied today:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;An update always applies to &lt;b&gt;all&lt;/b&gt; documents that are flushed&lt;/li&gt;
	&lt;li&gt;And to all documents currently in the RAM buffer&lt;/li&gt;
	&lt;li&gt;But never to documents that are indexed later&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Again, this is an internal detail that I'd appreciate if someone can give us a pointer to where that happens in the code today (now with concurrent flushing). I remember PackedDeletes existed at some point, has that changed?&lt;/p&gt;

&lt;p&gt;If it's a new Codec, then SegmentReader may not even need to change ...&lt;/p&gt;

&lt;p&gt;The REPLACE_FIELD OP is tricky ... perhaps it's like how deletes are materialized on disk &amp;#8211; as a sparse bit vector that marks the documents that are no longer associated with it ...&lt;/p&gt;

&lt;p&gt;I also think that we should introduce this feature in steps:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Support only fields that omit TFAP (i.e. DOCS_ONLY). This is very valuable for fields like ACL, TAGS, CATEGORIES etc.
	&lt;ul&gt;
		&lt;li&gt;Ideally, the app would just need to say "add/remove ACL:SHAI to/from document X", rather than passing the entire list of ACLs every on every update operation.&lt;/li&gt;
		&lt;li&gt;This I believe is also the most common use case for incremental field updates&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Support stored fields, whether as part of (1) or a follow-on, but adding TAG:LUCENE to the postings, but not the stored fields, is limiting ...&lt;/li&gt;
	&lt;li&gt;Support terms with positions, but no norms. What I'm thinking about are terms that store stuff in the payload, but don't care about the positions themselves. An example are the category dimensions of the facet module, which stores category ordinals in the payload
	&lt;ul&gt;
		&lt;li&gt;Positions are tricky, and we'll need to do this carefully, I know. But I don't rule it out at this point.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Then, support fields with norms. I get your concern Robert, and I agree it's a challenge, hence why I leave it to last. The scenario I have in mind is: a search engine that lets you comment on a result or tag it, and the comment/tag should be added to the document's 'catchall' field for later searches. I think it's a valuable scenario, and this is something I'd like to support. If we cannot find a way to deal with it and the norms, then I see two options:
	&lt;ol&gt;
		&lt;li&gt;Document a limitation to updating a field with norms, at your own risk.&lt;/li&gt;
		&lt;li&gt;Enforce REPLACE_FIELD OP on fields with norms.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul&gt;
	&lt;li&gt;Since norms are under DocValues now, maybe that's solvable, I don't know. At the moment I think that we have a lot to do before we worry about norms ...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I also think that we should start with the simpler ADD_FIELD operation, and not support REMOVE_FIELD ... really to keep things simple at start.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I suggest we do this work in a dedicated branch of course. Ideally, we can port everything to 4.x at some point, as I think most of the changes are internal details ...&lt;/p&gt;</comment>
                    <comment id="13423159" author="rcmuir" created="Thu, 26 Jul 2012 16:40:50 +0100">&lt;p&gt;I don't think I'm sold on introducing the feature in steps.&lt;/p&gt;

&lt;p&gt;I think its critical for something of this magnitude that we figure out the design totally, up-front,&lt;br/&gt;
so it will work for the major use-cases. I think its fine to implement in steps if we need though.&lt;/p&gt;

&lt;p&gt;Honestly I think we should throw it all out on the table and get to the real problems I think&lt;br/&gt;
that most people face today:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;For many document sizes, use-cases (especially rapidly changing stuff): The real problem is not the&lt;br/&gt;
  speed of lucene reindexing the document, its that the user must rebuild the entire document. Solr solved&lt;br/&gt;
  this by providing an option where you just say "update field X" and internally it reindexes the&lt;br/&gt;
  document from stored fields (for that feature to work, the whole thing must be stored). We shouldn't&lt;br/&gt;
  discard the possibility of implementing cleaner support for a solution like this, which wouldnt &lt;br/&gt;
  complicate indexwriter at all.&lt;/li&gt;
	&lt;li&gt;A second problem (not solved by the above) is that many people are using scoring factors with a variety&lt;br/&gt;
  of signals and these are changing often. I think unfortunately, people are often putting these in&lt;br/&gt;
  a normal indexed field and uninverting these on the fieldcache, requiring the whole document to&lt;br/&gt;
  be reindexed just because of how they implemented the scoring factor. People could instead solve this&lt;br/&gt;
  by putting their apps primary key into a docvalues field, allowing them to keep these scoring factors&lt;br/&gt;
  completely external to lucene (e.g. their own array or whatever), indexed by their own primary key. But&lt;br/&gt;
  the problem is I think people want lucene to manage this, they don't want to implement themselves whats&lt;br/&gt;
  necessary to make it consistent with commits etc.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So we can look at several approaches to solving this stuff. I feel like both these problems could be&lt;br/&gt;
solved via a contrib module without modifying indexwriter at all for many use cases: maybe better if&lt;br/&gt;
we go for more tight integration. And with those simple approaches I describe above, searching doesn't&lt;br/&gt;
get any slower.&lt;/p&gt;

&lt;p&gt;But if we really feel like we need a "full incremental update API" (i know there are a few use cases&lt;br/&gt;
where it can help, I'm not discarding that), then I feel like there are a few things I want:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I want scoring to be correct: this is a must. If we provide a incremental update API on IW and it doesnt&lt;br/&gt;
  achieve the same thing as updateDocument today, then its broken. But I think its ok for things to &lt;br/&gt;
  be temporarily off (as long as this is in a consistent way) until merging takes place, just like &lt;br/&gt;
  deletes today.&lt;/li&gt;
	&lt;li&gt;I want to know for any incremental update API, the cost to search performance.&lt;br/&gt;
  I want to know, at what document size is any incremental update API actually faster than us just &lt;br/&gt;
  reindexing the document internally, and how much faster is it? I also want us to consider that&lt;br/&gt;
  compared to the slowdown in search performance. We should know what the tradeoffs are before committing &lt;br/&gt;
  such APIs.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I strongly feel like if we just add these incremental APIs to indexwriter without being careful about these&lt;br/&gt;
things, the end result could be that people use them without thinking and end out with slower search&lt;br/&gt;
and worse relevance, thats why I am asking so many questions.&lt;/p&gt;</comment>
                    <comment id="13423240" author="shaie" created="Thu, 26 Jul 2012 18:05:03 +0100">&lt;p&gt;I think it's ok if we introduced IFU for DOCS_ONLY at first, throwing exceptions otherwise. E.g., UpdateField override setOmitNorms and such and throws UOE... at first.&lt;/p&gt;

&lt;p&gt;Everything else will still work as it is today...&lt;/p&gt;

&lt;p&gt;Codecs didn't handle all segment files first... stored fields and such were added later. I do agree though that we should keep in mind the full range of scenarios.&lt;/p&gt;

&lt;p&gt;Sorry for the short response, JIRA isn't great with smart phones: -).&lt;/p&gt;</comment>
                    <comment id="13423258" author="rcmuir" created="Thu, 26 Jul 2012 18:26:15 +0100">&lt;blockquote&gt;
&lt;p&gt;Codecs didn't handle all segment files first... stored fields and such were added later. I do agree though that we should keep in mind the full range of scenarios.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think thats really comparable at all, for two reasons:&lt;br/&gt;
1. Codecs can be considered a "rote" refactoring of the XXXWriter in 3.x. I'm not trying to diminish the value but its just an introduced abstraction layer. Something like this is different in that its algorithmic.&lt;br/&gt;
2. The fact that Codecs only handled postings at first wasn't easy to fix after they were introduced as postings-only. Once they handled postings initially, this was a significant refactoring.&lt;/p&gt;

&lt;p&gt;I'm not trying to pick on your proposal, I'm just saying there are things I don't like about the design.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I think that updating individual terms is a fringe use-case, and not the major use case for incremental updates, which is to update the contents of one field, without reindexing the entire document. This was also noted by someone else on the discussion thread. This issue seems to be solely about supporting the 'tagging' use case, which is just one of many.&lt;/li&gt;
	&lt;li&gt;I think requiring no positions, no frequencies, and no norms makes it even more fringe. This means its not really useful for any search purposes. And we are a search engine library.&lt;/li&gt;
	&lt;li&gt;I think that negatives won't compress well, as in general compression algorithms for IR in the last years focus on positive integers.&lt;/li&gt;
	&lt;li&gt;I think merging the postings will be slow: I don't like the tradeoff of slowing down searching so much for what I'm not even sure will be a significant speedup to indexing.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13423397" author="shaie" created="Thu, 26 Jul 2012 20:52:00 +0100">&lt;blockquote&gt;&lt;p&gt;...which is to update the contents of one field, without reindexing the entire document&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, but I distinguish between two operations:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;replacing the content of a field entirely with a new content (or remove the field)&lt;/li&gt;
	&lt;li&gt;update the field's content by adding/removing individual terms&lt;/li&gt;
&lt;/ol&gt;


&lt;blockquote&gt;&lt;p&gt;I think requiring no positions, no frequencies, and no norms makes it even more fringe. This means its not really useful for any search purposes. And we are a search engine library.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I disagree. Where I come from, the most common use case where such operation will be useful is when a single change affects hundreds and sometimes thousands of documents. An example is a document library like application which manages folders with ACLs. You can add an ACL to a top-level folder and it affects the entire documents and folder beneath it. That results in reindexing, sometimes, a huge amount of documents.&lt;/p&gt;

&lt;p&gt;I don't diminish the use case of updating a field for scoring purposes, not at all. Just saying that starting by supporting one use case is more than supporting no use case.&lt;/p&gt;

&lt;p&gt;Now, and this probably stems from my lack of understanding of the Lucene internals &amp;#8211; I see "supporting terms that omit TFAP" as a starting point because that is the easiest case, and even that requires a lot of understanding of the internals. After we do that, I'll feel more comfortable discussing other types of updates for other field types ... at least, I'll feel that I have more intelligent things to say &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;

&lt;p&gt;Regarding your other concerns, I share them with you, and we of course need to benchmark everything. I don't know how this affect search or not. But those updates will get merged away when segments are merged, so while I'm sure search will be affected, it's not for eternity - only until that segment is merged. And, I think we need to add capability to MergePolicy to findSegmentsForMergeUpdates, just like we expungeDeletes.&lt;/p&gt;

&lt;p&gt;If the first step means that in order to update a field used for scoring (i.e. w/ norms) means that you need to replace the content of the field entirely by a new content, I'm ok with it. As one esteem member of this community always says "progress, not perfection" - I'm totally soled for that !&lt;/p&gt;</comment>
                    <comment id="13423798" author="rcmuir" created="Fri, 27 Jul 2012 12:10:37 +0100">&lt;p&gt;I don't think its progress if we add a design that &lt;b&gt;can only work with omitTFAP and no norms&lt;/b&gt;,&lt;br/&gt;
and can only update individual terms, but not fields.&lt;/p&gt;

&lt;p&gt;it means to support these things we have to also totally clear out whats there, and then introduce a new design&lt;/p&gt;

&lt;p&gt;In fact this issue shouldnt be called incremental field updates: its not. its "term updates" or something else entirely different.&lt;/p&gt;</comment>
                    <comment id="13423813" author="shaie" created="Fri, 27 Jul 2012 12:38:13 +0100">&lt;blockquote&gt;&lt;p&gt;can only update individual terms, but not fields&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Who said that? One of the update operations I listed is REPLACE_FIELD, which means replace the field's content entirely with the new content.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don't think its progress if we add a design that can only work with omitTFAP and no norms&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I never said that will be the design. What I said is that in order to update a field at the term level, we'll start with such fields only. The rest of the fields (i.e. w/ norms, payloads and what not) will be updated through REPLACE_FIELD. The way I see it, we still address all the issues, only for some fields we require a whole field replace, and not an optimized term-based update. That can be improved further along, or not.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In fact this issue shouldnt be called incremental field updates: its not. its "term updates" or something else entirely different.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is my idea of incremental field updates and I'm not sure that it's not your idea as well &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. You seem to only want to support REPLACE_FIELD, while I say that for some field types we can support UPDATE_FIELD (i.e. at the term level), that's it !&lt;/p&gt;</comment>
                    <comment id="13424683" author="shaie" created="Mon, 30 Jul 2012 05:12:04 +0100">&lt;p&gt;I had a chat about this with Robert a couple of days ago, figured it'll be easier to discuss the differences in approaches/opinions, rather than back and forth JIRA comments. Our idea of incremental field updates is not much different. Robert stressed that in his opinion we should tackle first the REPLACE_FIELD operation, which replaces the content of a field entirely by a new content, because he believes that's the most common scenario (i.e., update the title field). I believe that term-based updates are very important too, at least in the scenarios that I face (i.e. adding/removing one ACL, one social tag, one category etc.).&lt;/p&gt;

&lt;p&gt;We concluded that the design should take REPLACE_FIELD into consideration from the get go. Whether we'll also implement UPDATE_FIELD (or UPDATE_TERMS as a better name?) depends on the complexity of it. Because initially UPDATE_TERMS can be implemented through REPLACE_FIELD, so we don't lose functionality. UPDATE_TERMS can come later as an optimization.&lt;/p&gt;

&lt;p&gt;Robert, if I misrepresented our conclusions, please correct me.&lt;/p&gt;</comment>
                    <comment id="13424736" author="sivany" created="Mon, 30 Jul 2012 08:54:43 +0100">&lt;p&gt;Seems like in any case we need to have a separation between fields given with UPDATE_FIELD and REPLACE_FIELD. There are two ways I could think of for implementing this separation. &lt;/p&gt;

&lt;p&gt;The first is at the segment level, where we can have separate "update" and "replace" segments, where the semantic is that a field in an "update" segment is merged with fields in previous segments, while a field in a "replace" segment ignores previous segments.&lt;/p&gt;

&lt;p&gt;The second option is to separate at the field level, choosing one type as the default behavior (maybe this can be configurable) and marking the fields of the non-default type by altering the field name or some other solution.&lt;/p&gt;

&lt;p&gt;I lean towards the segment level separation, since it requires less conventions and will probably require less work for Codec implementations to handle.&lt;/p&gt;</comment>
                    <comment id="13424793" author="sivany" created="Mon, 30 Jul 2012 12:09:48 +0100">&lt;p&gt;BTW, since the new method is to handle multiple fields (as the name suggests), the operation descriptions should also be in plural: UPDATE_FIELDS and REPLACE_FIELDS.&lt;/p&gt;</comment>
                    <comment id="13424803" author="mikemccand" created="Mon, 30 Jul 2012 12:53:34 +0100">&lt;blockquote&gt;&lt;p&gt;BTW, since the new method is to handle multiple fields (as the name suggests), the operation descriptions should also be in plural: UPDATE_FIELDS and REPLACE_FIELDS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I think this design sounds good!  REPLACE_FIELDS should easily be able&lt;br/&gt;
to update norms correctly, right?  Because the full per-field stats&lt;br/&gt;
are recomputed from scratch.  So then scores should be identical:&lt;br/&gt;
should be a nice simple testcase to create &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I don't see how UPDATE_FIELDS can do so unless we somehow save the raw&lt;br/&gt;
stats (FieldInvertState) in the index.  It seems like UPDATE_FIELDS&lt;br/&gt;
should forever be limited to DOCS_ONLY, no norms updating?  Positions&lt;br/&gt;
also seems hard to update, and if the only reason to do so is for&lt;br/&gt;
payloads... seems like the app should be using doc values instead, and&lt;br/&gt;
we should (eventually) make doc values updatable?.&lt;/p&gt;

&lt;p&gt;I do think this is a common use case (ACLs, filters, social&lt;br/&gt;
tags)... though I'm not sure how bad it'd really be in practice for&lt;br/&gt;
the app to simply REPLACE_FIELDS with the full set of tags.  I guess&lt;br/&gt;
if we build REPLACE_FIELDS first we can test that.&lt;/p&gt;

&lt;p&gt;The implementation should be able to piggy-back on all the&lt;br/&gt;
buffering/tracking we currently do for buffered deletes.&lt;/p&gt;

&lt;p&gt;I think this change should live entirely above Codec?  Ie Codec just&lt;br/&gt;
thinks it's writing a segment, not knowing if that segment is the base&lt;br/&gt;
segment, or one of the stacked ones.  If the +postings and -postings&lt;br/&gt;
are simply 2 terms then the Codec need not know...&lt;/p&gt;

&lt;p&gt;Seems like only SegmentInfos needs to track how segments stack up, and&lt;br/&gt;
then I guess we'd need a new StackedSegmentReader that is atomic,&lt;br/&gt;
holds N SegmentReaders, and presents the merged codec APIs by merging&lt;br/&gt;
down the stack on the fly?  I suspect this (having to use a PQ to&lt;br/&gt;
merge the docIDs in the postings) will be a huge search performance&lt;br/&gt;
hit....&lt;/p&gt;

&lt;p&gt;I think UnionDocs/AndPositionsEnum (in MultiPhraseQuery.java) is&lt;br/&gt;
already doing what we want?  (Except it doesn't handle negative&lt;br/&gt;
postings).&lt;/p&gt;

&lt;p&gt;What about merging?  Seems like the merge policy should know about&lt;br/&gt;
stacking and should sometimes (aggressively?) merge a stack down?&lt;/p&gt;</comment>
                    <comment id="13424806" author="erickerickson" created="Mon, 30 Jul 2012 13:03:05 +0100">&lt;p&gt;How does this relate (if at all, I confess I just looked at the title) to Andrzej's proposal here? &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3837" class="external-link"&gt;https://issues.apache.org/jira/browse/LUCENE-3837&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13424870" author="rcmuir" created="Mon, 30 Jul 2012 15:02:09 +0100">&lt;blockquote&gt;
&lt;p&gt;I don't see how UPDATE_FIELDS can do so unless we somehow save the raw&lt;br/&gt;
stats (FieldInvertState) in the index. It seems like UPDATE_FIELDS&lt;br/&gt;
should forever be limited to DOCS_ONLY, no norms updating?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually its DOCS_ONLY plus OMIT_NORMS.&lt;/p&gt;

&lt;p&gt;Anyway why not start with updating the entire contents of a field as I suggested?&lt;br/&gt;
It seems to be the most general solution, and there is some discussion about how scoring can&lt;br/&gt;
work correctly on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3837" title="A modest proposal for updateable fields"&gt;LUCENE-3837&lt;/a&gt; (the stats, not just norms).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I do think this is a common use case (ACLs, filters, social&lt;br/&gt;
tags)... though I'm not sure how bad it'd really be in practice for&lt;br/&gt;
the app to simply REPLACE_FIELDS with the full set of tags. I guess&lt;br/&gt;
if we build REPLACE_FIELDS first we can test that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is why we should do 'replace contents of a field' first. Its the most well-defined and general.&lt;/p&gt;

&lt;p&gt;Its also still controversial, myself I'm not convinced it will actually help most people that think&lt;br/&gt;
they want it, I think it will just slow down searches.&lt;/p&gt;</comment>
                    <comment id="13425079" author="shaie" created="Mon, 30 Jul 2012 19:15:31 +0100">&lt;blockquote&gt;&lt;p&gt;BTW, since the new method is to handle multiple fields (as the name suggests), the operation descriptions should also be in plural: UPDATE_FIELDS and REPLACE_FIELDS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok. I think to not confuse though, we should call it UPDATE_TERMS (not FIELDS). Then someone can updateFields() twice, once for all the fields which he wants to REPLACE and second for the fields he just wants to update their terms.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What about merging?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wrote about it above &amp;#8211; MergePolicy will need to take care of these stacked segments, and we'll add something like ,merge/expungeFieldUpdates so the app can call it deliberately.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;seems like the app should be using doc values instead, and we should (eventually) make doc values updatable?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree we should not UPDATE_TERMS fields that record norms. I'm not sure that every use case of storing info in the payload today can be translated to using DocValues, so I don't want to limit things. So, let's start with UPDATE_TERMS taking care of fields that omit norms. Then, if we handle payload or not for few use cases, can become as an optimization later on. In the meanwhile, apps will just need to replace the entire field.&lt;/p&gt;

&lt;p&gt;Progress, not perfection ! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13426821" author="sivany" created="Wed, 1 Aug 2012 19:57:16 +0100">&lt;blockquote&gt;&lt;p&gt;How does this relate (if at all, I confess I just looked at the title) to Andrzej's proposal here?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The basic idea is the same. One major difference is that in Andrzej's proposal the stacked updates are added to a new index with different doc IDs, and then the SegmentReader needs to map to the original doc IDs. The plan in this proposal (Shai correct me if I'm wrong) is for the stacked updates not to be stand alone segments. Although they will have the structure of regular segments they will be tightly coupled with the original segment, with doc IDs matching those of the original segment.&lt;/p&gt;</comment>
                    <comment id="13430322" author="sivany" created="Tue, 7 Aug 2012 13:49:20 +0100">&lt;p&gt;Working on the details, it seems that we need to add a new layer of information for stacked segments. For each field that was added with REPLACE_FIELDS, we need to hold the documents in which a replace took place, with the number of the latest generation that had the replacement. Name this list the "generation vector". That way, TermDocs provided by StackedSegmentReader for a certain term is a special merge of that term's TermDocs for all stacked segments. The "special" part about it is that we ignore occurrences from documents in which the term's field was replaced in a later generation.&lt;/p&gt;

&lt;p&gt;An example. Assume we have doc 1 with title "I love bananas" and doc 2 with title "I love oranges", and the segment is flushed. We will have the following base segment (ignoring positions):&lt;/p&gt;

&lt;p&gt;bananas: doc 1&lt;br/&gt;
I: doc1, doc 2&lt;br/&gt;
love: doc 1, doc 2&lt;br/&gt;
oranges: doc2&lt;/p&gt;

&lt;p&gt;Now we add to doc 1 additional title field "I hate apples", and replace the title of doc 2 with "I love lemons", and flush. We will have the following segment for generation 1:&lt;/p&gt;

&lt;p&gt;apples: doc 1&lt;br/&gt;
hate: doc 1&lt;br/&gt;
I: doc 1, doc 2&lt;br/&gt;
lemons: doc 2&lt;br/&gt;
love: doc 2&lt;br/&gt;
generation vector for field "title": (doc 2, generation 1)&lt;/p&gt;

&lt;p&gt;TermDocs for a few terms: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;title:bananas : 
{1}
&lt;p&gt;, uses the TermDocs of the base segment and not affected by the field title generation vector.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;title:oranges : {}, uses the TermDocs of the base segment, doc 2 title affected for generations &amp;lt; 1, and the generation is 0.&lt;/li&gt;
	&lt;li&gt;title:lemons : 
{2}
&lt;p&gt;, uses the TermDocs of generation 1. Doc 2 title affected for generations &amp;lt; 1, but the term appears in generation 1.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;title:love : 
{1,2}
&lt;p&gt;, uses the TermDocs of both segments. Doc 2 title affected for generations &amp;lt; 1, but the term appears in generation 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I propose to initially use PackedInts for the generation vector, since we know how many generations the curent segment has upon flushing. Later we might consider special treatment for sparse vectors.&lt;/p&gt;</comment>
                    <comment id="13452918" author="sivany" created="Tue, 11 Sep 2012 12:08:23 +0100">&lt;p&gt;Adding a design proposal presentation, and two patches following the proposal concepts. The first patch includes proposed API changes (does not compile) for, and the other one inner changes for those interested in the implementation details. The second patch contains a new test named TestFieldsUpdates which currently fails.&lt;/p&gt;</comment>
                    <comment id="13452928" author="sivany" created="Tue, 11 Sep 2012 12:21:36 +0100">&lt;p&gt;Forgot to mention that the implementation patch still missing many components...&lt;/p&gt;</comment>
                    <comment id="13453050" author="jpountz" created="Tue, 11 Sep 2012 15:23:11 +0100">&lt;p&gt;On slide 4 one of the enumerated operations is field deletion but I am not sure how to do it with the proposed API on slide 5?&lt;/p&gt;

&lt;p&gt;It is just a tought, but your work plan only mentions Lucene fields. Wouldn't it be easier to start working with DocValues? I guess it would help us get started with document updates and would already solve most use-cases (I'm especially thinking of scoring factors).&lt;/p&gt;</comment>
                    <comment id="13453739" author="shaie" created="Wed, 12 Sep 2012 06:50:10 +0100">&lt;p&gt;We will take care of DocValues too, eventually. I think this can be handled in a separate issue though.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;and would already solve most use-cases &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have a problem with that statement. Robert thinks that the most common use cases are to replace a field's content entirely. In our world (Sivan and mine's), updating a field's terms (removing / adding single terms) is the most common use case. And perhaps in your world updating DocValues for scoring purposes is the most common use case.&lt;/p&gt;

&lt;p&gt;Therefore I don't think that there is one common use case, and therefore IMO we shouldn't aim at solving one first. Personally, DocValues are relatively new (compared to posting lists and payloads) and therefore I believe that being able to update them should come second (just because I estimate they are not as widely used as the others). But that's just my opinion &amp;#8211; obviously one that relies solely on DocValues would state otherwise &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;

&lt;p&gt;The design currently doesn't cover DocValues at all. I think, in order to keep this issue focused, we should handle that in a separate issue after we land updateable fields.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On slide 4 one of the enumerated operations is field deletion but I am not sure how to do it with the proposed API on slide 5?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point. Well, you could say that replaceField("f", "value") called as replaceField("f", null) would mean "delete 'f'". That should work, but perhaps we can come up with something more explicit.&lt;/p&gt;</comment>
                    <comment id="13485229" author="sivany" created="Fri, 26 Oct 2012 22:52:47 +0100">&lt;p&gt;New patch, naive test of adding updates to a single-document segment before or after update working. Working on more complex tests with multiple segments, documents and updates.&lt;/p&gt;</comment>
                    <comment id="13499681" author="sivany" created="Sun, 18 Nov 2012 10:39:57 +0000">&lt;p&gt;New patch implementing some previously missing parts, with preliminary code to enable field replacements.&lt;/p&gt;</comment>
                    <comment id="13502756" author="sivany" created="Thu, 22 Nov 2012 13:02:57 +0000">&lt;p&gt;New patch with additional testing and bug fixes. &lt;/p&gt;

&lt;p&gt;Currently the term statistics does not take into account field replacements, and therefore term counts are wrong and CheckIndex fails. &lt;/p&gt;

&lt;p&gt;I can think of two possible solutions for this. The first is for CheckIndex to identify updated segments and ignore term statistics - is there similar mechanism for deletions?&lt;/p&gt;

&lt;p&gt;The other solution is to pre-compute term statistics for updated segments. However, this will be costly - requires going through the entire posting list for every term, and count non-replaced occurrences.&lt;/p&gt;

&lt;p&gt;Any suggestions?&lt;/p&gt;</comment>
                    <comment id="13508643" author="sivany" created="Mon, 3 Dec 2012 10:53:29 +0000">&lt;p&gt;New patch. This patch contains some failing tests, some probably due to problems in my implementation of SegmentReader.getTermVectors(int), and others in handling of stored fields. &lt;/p&gt;

&lt;p&gt;Can anyone with knowledge of these two areas check what is it that I do wrong? Thanks.&lt;/p&gt;</comment>
                    <comment id="13508709" author="sivany" created="Mon, 3 Dec 2012 13:10:09 +0000">&lt;p&gt;Solved the problem with stored fields, I understood that in order to have the right number of documents for the stored fields reader the last document must have a stored field. Term Vectors still failing...&lt;/p&gt;</comment>
                    <comment id="13508831" author="erickerickson" created="Mon, 3 Dec 2012 16:24:14 +0000">&lt;p&gt;Guys:&lt;/p&gt;

&lt;p&gt;It's great that you're tackling this. I wanted to encourage you to be patient about responses, there are just a few people in the universe who understand the details well enough to comment on the code (I'm sure not one of them!) , so it might feel like you're shouting down a well....&lt;/p&gt;</comment>
                    <comment id="13509020" author="sivany" created="Mon, 3 Dec 2012 20:34:03 +0000">&lt;p&gt;Patch with stored fields bug fixed.&lt;/p&gt;</comment>
                    <comment id="13509314" author="mikemccand" created="Mon, 3 Dec 2012 23:15:32 +0000">&lt;p&gt;Trying to catch up here ... I just have a bunch of random questions&lt;br/&gt;
(don't fully understand the patch yet):&lt;/p&gt;

&lt;p&gt;Not sure why some files show as all deleted / all added lines, eg at&lt;br/&gt;
least FrozenBufferedDeletes.java.&lt;/p&gt;

&lt;p&gt;Patch also has tabs, which should be spaces... (eg IndexWriter.java).&lt;/p&gt;

&lt;p&gt;Why do we have FieldsUpdate.Operation.ADD_DOCUMENT?  It seems weird to&lt;br/&gt;
pass that to IW.updateFields?  Shouldn't apps just use&lt;br/&gt;
IW.addDocument?&lt;/p&gt;

&lt;p&gt;Why do we need SegmentInfoReader.readFilesList?  It seems like it's&lt;br/&gt;
only privately used inside the codec?  I'm confused why the "normal"&lt;br/&gt;
file tracking we have on write is insufficient... oh I see, a single&lt;br/&gt;
SegmentInfo references all stacked segments too?  But since they are&lt;br/&gt;
written "later" their files won't be automatically tracked ... ok.  I&lt;br/&gt;
wonder if each stacked segment should get its own SegmentInfo, linked&lt;br/&gt;
to the base segment...&lt;/p&gt;

&lt;p&gt;It looks like merge policies don't yet know about / target stacked&lt;br/&gt;
segments ...&lt;/p&gt;

&lt;p&gt;It seems like we don't invert the document updates until the updates&lt;br/&gt;
are applied?  Ie, we just buffer the IndexableField provided by the&lt;br/&gt;
user and when it's time to apply updates, we then analyzing/invert?&lt;br/&gt;
How do we track RAM in this case?  (Eg the field could be something&lt;br/&gt;
arbitrary, eg pre-tokenized).  Another option is to do the invert and&lt;br/&gt;
buffer the resulting postings, and then later "replay" them (remapping&lt;br/&gt;
docIDs) when it's time to apply.&lt;/p&gt;

&lt;p&gt;Why does StoredFieldsReader.visitDocument need a Set for ignored&lt;br/&gt;
fields?&lt;/p&gt;</comment>
                    <comment id="13510441" author="sivany" created="Wed, 5 Dec 2012 11:57:31 +0000">&lt;p&gt;Erick - thanks for the support!&lt;/p&gt;

&lt;p&gt;Mike - thanks for the comments, here's an attempt to supply answers:&lt;/p&gt;

&lt;p&gt;Regarding formatting and all deletes - I will check and try to fix those.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why do we have FieldsUpdate.Operation.ADD_DOCUMENT? It seems weird to&lt;br/&gt;
pass that to IW.updateFields? Shouldn't apps just use&lt;br/&gt;
IW.addDocument?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We have ADD_ and REPLACE_ for FIELDS, and also REPLACE_DOCUMENTS, so having ADD_DOCUMENT would allow applications to work only with updateFields. There certainly are actions that can be performed in more than one way in this API, do you find this too confusing?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why do we need SegmentInfoReader.readFilesList? ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I considered the alternative you propose of having a segmentInfo for each stacked segment, and it seemed too complex to manage than what is done with .del files, so I chose the .del files approach. You are right about it's privacy, I removed it from SegmentInfoReader and the actual readers have it privately.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It looks like merge policies don't yet know about / target stacked&lt;br/&gt;
segments ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was planning to have it in another issue. should I create it already?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It seems like we don't invert the document updates until the updates&lt;br/&gt;
are applied? ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I went for the simple solution trying to introduce as less new concepts as possible (and still the patch size is &amp;gt;7000 lines). Your proposal should certainly be considered and maybe tested. I need to make sure I do the RAM calculations right, the added documents must be reflected in the RAM consumption of the deletions queue.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why does StoredFieldsReader.visitDocument need a Set for ignored&lt;br/&gt;
fields?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When fetching stored fields from a segment with replacements, it is possible that all contents of a certain field for the base and first n stacked segments should be ignored. Therefore, the implementation starts the visiting from the most recent updates. If we encounter at some stage a field replacement, that field name is added to the Set of ignored fields, and later the content of that field in the stacked segments we encounter (which are older updates) is ignored.&lt;/p&gt;</comment>
                    <comment id="13526609" author="mikemccand" created="Fri, 7 Dec 2012 18:29:35 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Why do we have FieldsUpdate.Operation.ADD_DOCUMENT? It seems weird to pass that to IW.updateFields? Shouldn't apps just use IW.addDocument?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We have ADD_ and REPLACE_ for FIELDS, and also REPLACE_DOCUMENTS, so having ADD_DOCUMENT would allow applications to work only with updateFields. There certainly are actions that can be performed in more than one way in this API, do you find this too confusing?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well I just generally prefer that there is one &lt;span class="error"&gt;&amp;#91;obvious&amp;#93;&lt;/span&gt; way to do&lt;br/&gt;
something ... it can cause confusion otherwise, ie users will wonder&lt;br/&gt;
what's the difference between addDocument and&lt;br/&gt;
updateFields(Operation.ADD_DOCUMENT, ...)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Why do we need SegmentInfoReader.readFilesList? ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I considered the alternative you propose of having a segmentInfo for each stacked segment, and it seemed too complex to manage than what is done with .del files, so I chose the .del files approach. You are right about it's privacy, I removed it from SegmentInfoReader and the actual readers have it privately.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;It looks like merge policies don't yet know about / target stacked segments ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was planning to have it in another issue. should I create it already?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Another issue is a good idea!  No need to create it yet ... but it&lt;br/&gt;
seems like it will be important for real usage.&lt;/p&gt;

&lt;p&gt;Do we have any sense of how performance degrades as the stack gets&lt;br/&gt;
bigger?  It's more on-the-fly merging at search-time...&lt;/p&gt;

&lt;p&gt;I'm worried about that search-time merge cost ... I think it's usually&lt;br/&gt;
better to pay a higher indexing cost in exchange for faster search&lt;br/&gt;
time, which makes &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4272" title="another idea for updatable fields"&gt;LUCENE-4272&lt;/a&gt; a compelling alternate approach...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;It seems like we don't invert the document updates until the updates are applied? ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I went for the simple solution trying to introduce as less new concepts as possible (and still the patch size is &amp;gt;7000 lines). Your proposal should certainly be considered and maybe tested. I need to make sure I do the RAM calculations right, the added documents must be reflected in the RAM consumption of the deletions queue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK that makes sense; we should definitely do whatever's&lt;br/&gt;
easiest/fastest to get to a dirt path.&lt;/p&gt;

&lt;p&gt;We should think through the tradeoffs.  I think it may confuse apps&lt;br/&gt;
that the Field is not "consumed" after IW.updateFields returns, but&lt;br/&gt;
rather cached and processed later.  This means you cannot reuse&lt;br/&gt;
fields, you have to be careful with pre-tokenized fields (can't reuse&lt;br/&gt;
the TokenStream), etc.&lt;/p&gt;

&lt;p&gt;It also means NRT reopen is unexpectedly costly, because only on flush&lt;br/&gt;
will we invert &amp;amp; index the documents, and it's a single-threaded&lt;br/&gt;
operation during reopen (vs per-thread if we invert up front).&lt;/p&gt;

&lt;p&gt;Still it makes sense to do this for starters ... it's simpler.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Why does StoredFieldsReader.visitDocument need a Set for ignored fields?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;When fetching stored fields from a segment with replacements, it is possible that all contents of a certain field for the base and first n stacked segments should be ignored. Therefore, the implementation starts the visiting from the most recent updates. If we encounter at some stage a field replacement, that field name is added to the Set of ignored fields, and later the content of that field in the stacked segments we encounter (which are older updates) is ignored.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahhh right.&lt;/p&gt;

&lt;p&gt;Are stored fields now sparse?  Meaning if I have a segment w/ many&lt;br/&gt;
docs, and I update stored fields on one doc, in that tiny stacked&lt;br/&gt;
segments will the stored fields files also be tiny?&lt;/p&gt;</comment>
                    <comment id="13533359" author="sivany" created="Sun, 16 Dec 2012 11:16:28 +0000">&lt;p&gt;Let me start with the last question.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Are stored fields now sparse? Meaning if I have a segment w/ many docs, and I update stored fields on one doc, in that tiny stacked segments will the stored fields files also be tiny?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In such case you will get the equivalent of a segment with multiple docs with only one of them containing stored fields. I assume the impls of stored fields handle these cases well and you will indeed get tiny stored fields.&lt;/p&gt;

&lt;p&gt;Regarding the API - I made some cleanup, and removed also Operation.ADD_DOCUMENT. Now there is only one way to perform each operation, and updateFields only allows you to add or replace fields given a term.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This means you cannot reuse fields, you have to be careful with pre-tokenized fields (can't reuse the TokenStream), etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is referred in the Javadoc of updateFields, let me know if there's a better way to address it.&lt;/p&gt;

&lt;p&gt;As for the heavier questions. NRT support should be considered separately, but the guideline I followed was to keep things as closely as possible to the way deletions are handled. Therefore, we need to add to SegmentReader a field named liveUpdates - an equivalent to liveDocs. I already put a TODO for this (SegmentReader line 131), implementing it won't be simple...&lt;/p&gt;

&lt;p&gt;The performance tradeoff you are rightfully concerned about should be handled through merging. Once you merge an updated segment all updates are "cleaned", and the new segment has no performance issues. Apps that perform updates should make sure (through MergePolicy) to avoid reader-side updates as much as possible.&lt;/p&gt;
</comment>
                    <comment id="13533361" author="sivany" created="Sun, 16 Dec 2012 11:37:15 +0000">&lt;p&gt;Patch with some additional bug fixes and more elaborate tests, all working. Ready to commit?&lt;/p&gt;</comment>
                    <comment id="13533410" author="sivany" created="Sun, 16 Dec 2012 15:38:46 +0000">&lt;p&gt;Some existing tests fail with the latest patch, working on fixes.&lt;/p&gt;</comment>
                    <comment id="13534054" author="sivany" created="Mon, 17 Dec 2012 16:38:18 +0000">&lt;p&gt;New patch, concurrency bugs fixed. All tests pass.&lt;/p&gt;</comment>
                    <comment id="13534352" author="dsmiley" created="Mon, 17 Dec 2012 22:04:20 +0000">&lt;p&gt;Its exciting to see progress here!  I too live in the "world" that Shai speaks of &amp;#8211; DOCS_ONLY (w/ no norms).  I don't need to update a title field, I need to update ACLs and various categorical "tag" fields that will subsequently influence faceting or filtering.&lt;/p&gt;

&lt;p&gt;Hey Rob, early on you made this excellent point:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A second problem (not solved by the above) is that many people are using scoring factors with a variety&lt;br/&gt;
of signals and these are changing often. I think unfortunately, people are often putting these in&lt;br/&gt;
a normal indexed field and uninverting these on the fieldcache, requiring the whole document to&lt;br/&gt;
be reindexed just because of how they implemented the scoring factor. People could instead solve this&lt;br/&gt;
by putting their apps primary key into a docvalues field, allowing them to keep these scoring factors&lt;br/&gt;
completely external to lucene (e.g. their own array or whatever), indexed by their own primary key. But&lt;br/&gt;
the problem is I think people want lucene to manage this, they don't want to implement themselves whats&lt;br/&gt;
necessary to make it consistent with commits etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So true.  What if Lucene had more hooks to make it easier to manage commit-consistency with side-car data?  I have no clue what's needed exactly, only that I don't dare do this without such hooks because I fear the complexity.  With hooks and documentation, it can become clear how to maintain data alongside Lucene's index, and this opens doors.  Like making it easier to store data in something custom (e.g. a DB) instead of stored-fields (won't have to pay needless merge cost), or putting metrics that influence scoring somewhere as you hinted at above. &lt;/p&gt;</comment>
                    <comment id="13534859" author="mikemccand" created="Tue, 18 Dec 2012 12:43:24 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Are stored fields now sparse? Meaning if I have a segment w/ many docs, and I update stored fields on one doc, in that tiny stacked segments will the stored fields files also be tiny?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In such case you will get the equivalent of a segment with multiple docs with only one of them containing stored fields. I assume the impls of stored fields handle these cases well and you will indeed get tiny stored fields.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You're right, this is up to the codec ... hmm but the API isn't sparse (you have&lt;br/&gt;
to .addDocument 1M times to "skip over" 1M docs right?), and I'm not sure how well our&lt;br/&gt;
current default (Lucene41StoredFieldsFormat) handles it.  Have you tested it?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Regarding the API - I made some cleanup, and removed also Operation.ADD_DOCUMENT. Now there is only one way to perform each operation, and updateFields only allows you to add or replace fields given a term.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK thanks!&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;This means you cannot reuse fields, you have to be careful with pre-tokenized fields (can't reuse the TokenStream), etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is referred in the Javadoc of updateFields, let me know if there's a better way to address it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe also state that one cannot reuse Field instances, since the&lt;br/&gt;
Field may not be actually "consumed" until some later time (we should&lt;br/&gt;
be vague since this really is an implementation detail).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As for the heavier questions. NRT support should be considered separately, but the guideline I followed was to keep things as closely as possible to the way deletions are handled. Therefore, we need to add to SegmentReader a field named liveUpdates - an equivalent to liveDocs. I already put a TODO for this (SegmentReader line 131), implementing it won't be simple...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK ... yeah it's not simple!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The performance tradeoff you are rightfully concerned about should be handled through merging. Once you merge an updated segment all updates are "cleaned", and the new segment has no performance issues. Apps that perform updates should make sure (through MergePolicy) to avoid reader-side updates as much as possible.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Merging is very important.  Hmm, are we able to just merge all updates&lt;br/&gt;
down to a single update?  Ie, without merging the base segment?  We&lt;br/&gt;
can't express that today from MergePolicy right?  In an NRT setting&lt;br/&gt;
this seems very important (ie it'd be best bang (= improved search&lt;br/&gt;
performance) for the buck (= merge cost)).&lt;/p&gt;

&lt;p&gt;I suspect we need to do something with merging before committing&lt;br/&gt;
here.&lt;/p&gt;

&lt;p&gt;Hmm I see that&lt;br/&gt;
StackedTerms.size()/getSumTotalTermFreq()/getSumDocFreq() pulls a&lt;br/&gt;
TermsEnum and goes and counts/aggregates all terms ... which in&lt;br/&gt;
general is horribly costly?  EG these methods are called per-query to&lt;br/&gt;
setup the Sim for scoring ... I think we need another solution here&lt;br/&gt;
(not sure what).  Also getDocCount() just returns -1 now ... maybe we&lt;br/&gt;
should only allow updates against DOCS_ONLY/omitsNorms fields for now?&lt;/p&gt;

&lt;p&gt;Have you done any performance tests on biggish indices?&lt;/p&gt;

&lt;p&gt;I think we need a test that indexes a known (randomly generated) set&lt;br/&gt;
of documents, randomly sometimes using add and sometimes using&lt;br/&gt;
update/replace field, mixing in deletes (just like TestField.addDocuments()),&lt;br/&gt;
for the first index, and for the second index only using addDocument&lt;br/&gt;
on the "surviving" documents, and then we assertIndexEquals(...) in the&lt;br/&gt;
end?  Maybe we can factor out code from TestDuelingCodecs or&lt;br/&gt;
TestStressIndexing2.&lt;/p&gt;

&lt;p&gt;Where do we account for the RAM used by these buffered updates?  I see&lt;br/&gt;
BufferedUpdates.addTerm has some accounting the first time it sees a&lt;br/&gt;
given term, but where do we actually add in the RAM used by the&lt;br/&gt;
FieldsUpdate itself?&lt;/p&gt;</comment>
                    <comment id="13536992" author="sivany" created="Thu, 20 Dec 2012 12:25:32 +0000">&lt;p&gt;After rethinking the point-of-inversion issue, seems like the right time to do it is ASAP - not to hold the added fields and invert them later, but rather invert them immediately and save their inverted version. 3 reasons for that:&lt;br/&gt;
1. Take out the constraint I inserted to the API, so update fields can be reused and contain Reader/TokenStrem,&lt;br/&gt;
2. NRT support: we cannot search until we invert, and if we invert earlier NRT support will be less complicated, probably some variation on multi-reader to view uncommitted updates,&lt;br/&gt;
3. You are correct that we currently do not account for the RAM usage of the FieldsUpdate, since I thought using RAMUsageEstimator will be too costly. It will probably be more efficient to calculate RAM usage of the inverted fields, maybe even during inversion?&lt;/p&gt;

&lt;p&gt;So my question in that regard is how can I invert a document and hold its inverted form to be used by NRT and later inserted into stacked segment? Should I create a temporary Directory and invert into it? Is there another way to do this?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Merging is very important. Hmm, are we able to just merge all updates down to a single update? Ie, without merging the base segment? We can't express that today from MergePolicy right? In an NRT setting this seems very important (ie it'd be best bang (= improved search performance) for the buck (= merge cost)).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Shai is helping in creation of a benchmark to test performance in various scenarios. I will start adding updates aspects to the merge policy. I am not sure if merging just updates of a segment is feasible. In what cases would it be better than collapsing all updates into the base segment?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think we need a test that indexes a known (randomly generated) set of documents, randomly sometimes using add and sometimes using update/replace field, mixing in deletes (just like TestField.addDocuments()), for the first index, and for the second index only using addDocument on the "surviving" documents, and then we assertIndexEquals(...) in the end? Maybe we can factor out code from TestDuelingCodecs or TestStressIndexing2.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;TestFieldReplacements already had a test which randomly adds documents, replaces documents, adds fields and replaces fields. I refactored it to enable using a seed, and created a "clean" version with only addDocument(...) calls. However, the FieldInfos of the "clean" version do not include things that the "full" version includes because in the full version fields possessing certain field traits where added and then deleted. I will look at the other suggestions.&lt;/p&gt;</comment>
                    <comment id="13536999" author="shaie" created="Thu, 20 Dec 2012 12:44:07 +0000">&lt;blockquote&gt;&lt;p&gt;I am not sure if merging just updates of a segment is feasible. In what cases would it be better than collapsing all updates into the base segment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Just like expungeDeletes, I think that we should have collapseFieldUpdates() which can be called explicitly by the app, but also IW should call MP.findSegmentsForFieldUpdates() (or some such name). And it should collapse all updates into the segment, implies rewriting that segment. If we collapse all updates but keep the base segment + a single stacked segment, I don't think that we're doing much. The purpose is to get rid of updates entirely.&lt;/p&gt;

&lt;p&gt;Also, regarding statistics. I think that as a first step, we should not go out of our way to return the correct statistics. Just like the stats today do not account for deleted documents, so should the updates. I realize that it's not the same as deleted documents, but it certainly simplifies matters. Stats will be correct following collapseFieldUpdates or regular segment merges.&lt;/p&gt;

&lt;p&gt;As a second step, we can try to return statistics including stacked segments more efficiently. I.e., if a term appears in both the base and stacked segment, we return the stats from base. But if it exists only in the stacked segment, we can return the stats from there? I'm not too worried about the stats though, because that's a temporary thing, which gets fixed once updates are collapsed.&lt;/p&gt;

&lt;p&gt;And if the MergePolicy will have separate settings for collapsing field updates (I think it should!), then the collapsing could occur more frequently than regular merges (and expunging deleted documents). Also, it will give apps a way to control how often do they want to get accurate statistics.&lt;/p&gt;

&lt;p&gt;Can we leave statistics outside the scope of this issue? And for now change CheckIndex to detect that it's a segment with field updates, and therefore check stats from the base segment only? I think it does something like that with deleted documents already, no?&lt;/p&gt;</comment>
                    <comment id="13537000" author="mikemccand" created="Thu, 20 Dec 2012 12:45:16 +0000">
&lt;blockquote&gt;
&lt;p&gt;After rethinking the point-of-inversion issue, seems like the right time to do it is ASAP - not to hold the added fields and invert them later, but rather invert them immediately and save their inverted version. 3 reasons for that:&lt;br/&gt;
1. Take out the constraint I inserted to the API, so update fields can be reused and contain Reader/TokenStrem,&lt;br/&gt;
2. NRT support: we cannot search until we invert, and if we invert earlier NRT support will be less complicated, probably some variation on multi-reader to view uncommitted updates,&lt;br/&gt;
3. You are correct that we currently do not account for the RAM usage of the FieldsUpdate, since I thought using RAMUsageEstimator will be too costly. It will probably be more efficient to calculate RAM usage of the inverted fields, maybe even during inversion?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I would also add "4. Inversion of updates is single-threaded", ie once&lt;br/&gt;
we move inversion into .updateFields it will be multi-threaded again.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So my question in that regard is how can I invert a document and hold its inverted form to be used by NRT and later inserted into stacked segment? Should I create a temporary Directory and invert into it? Is there another way to do this?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should somehow re-use the existing code that inverts (eg&lt;br/&gt;
FreqProxTermsWriter)?  Ie, invert into an in-RAM segment, with&lt;br/&gt;
"temporary" docIDs, and then when it's time to apply the updates, you&lt;br/&gt;
need to rewrite the postings to disk with the re-mapped docIDs.&lt;/p&gt;

&lt;p&gt;I wouldn't do anything special for NRT for starters, meaning, from&lt;br/&gt;
NRT's standpoint, it opens these stacked segments from disk as it&lt;br/&gt;
would if a new non-NRT reader was being opened.  So I would leave that&lt;br/&gt;
TODO in SegmentReader as a TODO for now &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  Later, we can optimize&lt;br/&gt;
this and have updates carry in RAM like we do for deletes, but I&lt;br/&gt;
wouldn't start with that ...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Merging is very important. Hmm, are we able to just merge all updates down to a single update? Ie, without merging the base segment? We can't express that today from MergePolicy right? In an NRT setting this seems very important (ie it'd be best bang (= improved search performance) for the buck (= merge cost)).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Shai is helping in creation of a benchmark to test performance in various scenarios. I will start adding updates aspects to the merge policy. I am not sure if merging just updates of a segment is feasible. In what cases would it be better than collapsing all updates into the base segment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Imagine a huge segment that's accumulating updates ... say it has 20&lt;br/&gt;
stacked segments.  First off, those stacked segments are each tying up&lt;br/&gt;
N file descriptors on open, right?  (Well, only one if it's CFS).  But&lt;br/&gt;
second off, I would expect search perf with 1 base + 20 stacked is&lt;br/&gt;
worse than 1 base + 1 stacked?  We need to test if that's true&lt;br/&gt;
... it's likely that the most perf loss is going from no stacked&lt;br/&gt;
segments to 1 stacked segment ... and then going from 1 to 20 stacked&lt;br/&gt;
segments doesn't hurt "that much".  We have to test and see.&lt;/p&gt;

&lt;p&gt;Simply merging that big base segment with its 20 stacked segments is&lt;br/&gt;
going to be too costly to do very often.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think we need a test that indexes a known (randomly generated) set of documents, randomly sometimes using add and sometimes using update/replace field, mixing in deletes (just like TestField.addDocuments()), for the first index, and for the second index only using addDocument on the "surviving" documents, and then we assertIndexEquals(...) in the end? Maybe we can factor out code from TestDuelingCodecs or TestStressIndexing2.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;TestFieldReplacements already had a test which randomly adds documents, replaces documents, adds fields and replaces fields. I refactored it to enable using a seed, and created a "clean" version with only addDocument(...) calls. However, the FieldInfos of the "clean" version do not include things that the "full" version includes because in the full version fields possessing certain field traits where added and then deleted. I will look at the other suggestions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It should be fine if the FieldInfos don't match?  Ie, when comparing&lt;br/&gt;
the two indices we should not compare field numbers?  We should be&lt;br/&gt;
comparing by only external things like fieldName, which id we had&lt;br/&gt;
indexed, etc.&lt;/p&gt;</comment>
                    <comment id="13538995" author="shaie" created="Sun, 23 Dec 2012 08:43:47 +0000">&lt;p&gt;Woops, resolved wrong issue &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13538999" author="shaie" created="Sun, 23 Dec 2012 08:53:38 +0000">&lt;p&gt;I branched &lt;a href="https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4258" class="external-link"&gt;https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4258&lt;/a&gt;. The patch is really immense and I think it'll be easier to work on this feature in a separate branch. Committed rev 1425441.&lt;/p&gt;</comment>
                    <comment id="13541053" author="sivany" created="Sun, 30 Dec 2012 08:39:08 +0000">&lt;p&gt;Started switching to the invert-first approach following Mike's advices. My thought was to have a single directory for each fields update, and when flushing do something similar to IndexWriter.addIndexes(IndexReader...) and build the stacked segment. However, I encountered two problems with this approach:&lt;br/&gt;
1. If a certain document is updated more than once in a certain generation, two inverted documents should be merged into one,&lt;br/&gt;
2. extension to 1, where a field added in the first update is to be replaced in the second one.&lt;br/&gt;
So, what I will try to do in such cases is to move the later updates to a new update generation. This will increase the number of generations, but I think it's a fair price to pay in light of the benefits offered by the invert-first approach.&lt;/p&gt;</comment>
                    <comment id="13561571" author="sivany" created="Thu, 24 Jan 2013 11:27:53 +0000">&lt;p&gt;New patch over the issue branch. Inversion of updated fields done directly when added into a RAMDirectory, and updated segments are created by merging such directories. If more than one update to be applied on same document, the later update is moved to another updated segment.&lt;/p&gt;

&lt;p&gt;Still missing:&lt;br/&gt;
1. Implement RAM usage computation for updates,&lt;br/&gt;
2. fix TestFieldReplacements.testIndexEquality().&lt;/p&gt;</comment>
                    <comment id="13598283" author="shaie" created="Sun, 10 Mar 2013 15:14:57 +0000">&lt;p&gt;I upgraded branch to trunk (w/o latest patch). TestFieldUpdate fails on norms assertion. I'm not sure exactly what happened but SegmentReader had many strange conflicts, so I hope I didn't screw something up when resolving them. Will try to get to the bottom of it later.&lt;/p&gt;</comment>
                    <comment id="13598347" author="shaie" created="Sun, 10 Mar 2013 18:35:26 +0000">&lt;p&gt;Sivan, I tried to apply the patch to the branch (after the upgrade to trunk) but it does not apply. Some files are missing, some have issues. Can you perhaps bring this patch up to the current branch's version and post another one?&lt;/p&gt;</comment>
                    <comment id="13598380" author="sivany" created="Sun, 10 Mar 2013 20:53:46 +0000">&lt;p&gt;Thanks Shai for applying the patch, it will take me some time to do adjustments to the changes in trunk, including some fixes to the branch.&lt;/p&gt;</comment>
                    <comment id="13605025" author="sivany" created="Mon, 18 Mar 2013 11:21:16 +0000">&lt;p&gt;Updated patch over branch after merging with trunk. New tests, still with bugs in handling replacements.&lt;/p&gt;</comment>
                    <comment id="13607794" author="shaie" created="Wed, 20 Mar 2013 16:17:21 +0000">&lt;p&gt;Committed the latest patch to the branch and upgraded branch to latest trunk.&lt;/p&gt;</comment>
                    <comment id="13646429" author="sivany" created="Wed, 1 May 2013 07:58:17 +0100">&lt;p&gt;Added tests and fixed bugs. The most thorough test is testIndexEquality() which runs a complicated scenario of adding and replacing documents and fields and compares the resulting index to an index created only using addDocument. The IndexWriterConfig used can be either created using new IndexWriterConfig() or newIndexWriterConfig() which introduces randomness. When there is no randomness the test passes, with randomness and -Dtests.seed=FFC28997A6951FFB it fails.&lt;/p&gt;</comment>
                    <comment id="13646875" author="sivany" created="Wed, 1 May 2013 20:52:51 +0100">&lt;p&gt;Shai, can you please commit the patch to the branch? Thanks.&lt;/p&gt;</comment>
                    <comment id="13650567" author="shaie" created="Tue, 7 May 2013 07:47:18 +0100">&lt;p&gt;Committed the patch to the branch. I'll also upgrade branch to trunk.&lt;/p&gt;</comment>
                    <comment id="13650734" author="shaie" created="Tue, 7 May 2013 12:31:07 +0100">&lt;p&gt;I upgraded branch to trunk. Sivan, I get many NPEs from tests like TestFieldReplacements. The NPEs come from FrozenBufferedDeletes which I had some difficulty merging. The NPEs are caused because 'deletes' are null. I checked trunk code, and FrozenBufferedDeletes assumes they are not null (as in the branch), so either I broke something while merging, or something has changed in trunk that the updates code doesn't respect. Can you take a look?&lt;/p&gt;</comment>
                    <comment id="13650737" author="sivany" created="Tue, 7 May 2013 12:40:26 +0100">&lt;p&gt;Thanks Shai, will look into it, modifying FrozenBufferedDeletes is the main challenge so far.&lt;/p&gt;</comment>
                    <comment id="13655640" author="sivany" created="Sun, 12 May 2013 22:22:17 +0100">&lt;p&gt;1. Removed some assertions which were based on assumptions that stacked segments break&lt;br/&gt;
2. Added a mechanism to apply updates on already applied ones, now all tests pass&lt;br/&gt;
3. Did some house cleaning&lt;br/&gt;
What's left? Improve calculation of bytes used, make merge policy updates-aware, add the option to collapse stacked segments for segments that cannot be merged, check performance...&lt;/p&gt;</comment>
                    <comment id="13655904" author="sivany" created="Mon, 13 May 2013 12:27:24 +0100">&lt;p&gt;New patch handling another scenario of an update relating to previous update in the same segment.&lt;/p&gt;</comment>
                    <comment id="13655957" author="shaie" created="Mon, 13 May 2013 14:25:18 +0100">&lt;p&gt;Committed this to the branch + upgraded to trunk. Sivan, there are tests still failing &amp;#8211; is this expected?&lt;/p&gt;</comment>
                    <comment id="13667276" author="sivany" created="Sun, 26 May 2013 11:48:40 +0100">&lt;p&gt;New patch, all tests pass. After adding a test which mixed updates and deletes I realized that mixing should be constrained, since it opens the door to complex scenarios and will cause changes to the way deletes are saved and that's not a good idea. So I put in IndexWriter a flag which marks whether there are deletes pending, so when a fields update is added and the flag is set there is an automatic commit. So in FrozenBufferedDeletes we can be sure that deletes are to be applied after updates.&lt;br/&gt;
I guess my implementation is not safe, Shai &amp;amp; Mike can you please take a look?&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12544806">LUCENE-3837</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12600706">LUCENE-4272</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12544626" name="IncrementalFieldUpdates.odp" size="26694" author="sivany" created="Tue, 11 Sep 2012 12:08:23 +0100"/>
                    <attachment id="12544627" name="LUCENE-4258-API-changes.patch" size="13745" author="sivany" created="Tue, 11 Sep 2012 12:08:23 +0100"/>
                    <attachment id="12566293" name="LUCENE-4258.branch.1.patch" size="115997" author="sivany" created="Thu, 24 Jan 2013 11:27:53 +0000"/>
                    <attachment id="12574133" name="LUCENE-4258.branch.2.patch" size="136882" author="sivany" created="Mon, 18 Mar 2013 11:21:16 +0000"/>
                    <attachment id="12581342" name="LUCENE-4258.branch3.patch" size="62574" author="sivany" created="Wed, 1 May 2013 07:58:17 +0100"/>
                    <attachment id="12582868" name="LUCENE-4258.branch.4.patch" size="29207" author="sivany" created="Sun, 12 May 2013 22:22:17 +0100"/>
                    <attachment id="12582916" name="LUCENE-4258.branch.5.patch" size="33980" author="sivany" created="Mon, 13 May 2013 12:27:24 +0100"/>
                    <attachment id="12584865" name="LUCENE-4258.branch.6.patch" size="55101" author="sivany" created="Sun, 26 May 2013 11:48:40 +0100"/>
                    <attachment id="12553960" name="LUCENE-4258.r1410593.patch" size="249775" author="sivany" created="Sun, 18 Nov 2012 10:39:57 +0000"/>
                    <attachment id="12554675" name="LUCENE-4258.r1412262.patch" size="262384" author="sivany" created="Thu, 22 Nov 2012 13:02:57 +0000"/>
                    <attachment id="12555736" name="LUCENE-4258.r1416438.patch" size="269745" author="sivany" created="Mon, 3 Dec 2012 10:53:29 +0000"/>
                    <attachment id="12555823" name="LUCENE-4258.r1416617.patch" size="270748" author="sivany" created="Mon, 3 Dec 2012 20:34:03 +0000"/>
                    <attachment id="12561181" name="LUCENE-4258.r1422495.patch" size="455744" author="sivany" created="Sun, 16 Dec 2012 11:37:15 +0000"/>
                    <attachment id="12561307" name="LUCENE-4258.r1423010.patch" size="457136" author="sivany" created="Mon, 17 Dec 2012 16:38:18 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>14.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 26 Jul 2012 13:33:15 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243712</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23440</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4251] Port Solr's ValueSourceRangeFilter to Lucene query module</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4251</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Lucene spatial has ValueSourceFilter and Solr has ValueSourceRangeFilter (that has some more features).  And someone asked for this in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3875" title="ValueSourceFilter"&gt;&lt;del&gt;LUCENE-3875&lt;/del&gt;&lt;/a&gt; too.  It should be ported over to the Lucene query module.&lt;/p&gt;</description>
                <environment/>
            <key id="12599940">LUCENE-4251</key>
            <summary>Port Solr's ValueSourceRangeFilter to Lucene query module</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="dsmiley">David Smiley</reporter>
                        <labels>
                    </labels>
                <created>Tue, 24 Jul 2012 05:12:07 +0100</created>
                <updated>Fri, 10 May 2013 00:05:10 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13421160" author="cmale" created="Tue, 24 Jul 2012 05:15:21 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13421170" author="dsmiley" created="Tue, 24 Jul 2012 05:39:30 +0100">&lt;p&gt;For reference:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/spatial/src/java/org/apache/lucene/spatial/util/ValueSourceFilter.java" class="external-link"&gt;https://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/spatial/src/java/org/apache/lucene/spatial/util/ValueSourceFilter.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java" class="external-link"&gt;https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Lucene spatial util one wraps a Filter whereas Solr's wraps a ValueSource.  Furthermore, even after this refactoring, it's not likely Solr's will go away because it extends SolrFilter.  I like that Solr's has configurable inclusive &amp;amp; exclusivity.&lt;/p&gt;

&lt;p&gt;My initial goal was to move out the one in spatial because it seemed like something useful in the query module, and then I found the Solr one which seemed the same but not quite.&lt;/p&gt;</comment>
                    <comment id="13429719" author="rcmuir" created="Tue, 7 Aug 2012 04:41:32 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12546657">LUCENE-3875</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 24 Jul 2012 04:15:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243719</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23447</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4246] Fix IndexWriter.close() to not commit or wait for pending merges</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4246</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description/>
                <environment/>
            <key id="12599843">LUCENE-4246</key>
            <summary>Fix IndexWriter.close() to not commit or wait for pending merges</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Mon, 23 Jul 2012 13:36:42 +0100</created>
                <updated>Fri, 10 May 2013 00:05:10 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13420612" author="shaie" created="Mon, 23 Jul 2012 13:49:02 +0100">&lt;p&gt;Moving my comment from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4245" title="IndexWriter#close(true) should either not be interruptible or should abort background merge threads before returning"&gt;&lt;del&gt;LUCENE-4245&lt;/del&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;Why is that different from OutputStream.close() calling flush()? Would you like to call flush() yourself everytime before you close()?&lt;/p&gt;

&lt;p&gt;I don't think there's anything we should 'fix' ...&lt;/p&gt;

&lt;p&gt;Perhaps, if you want to simplify, close() should just commit and NEVER wait for merges. If someone wants that, he can call commit() + waitForMerges().&lt;/p&gt;</comment>
                    <comment id="13420616" author="thetaphi" created="Mon, 23 Jul 2012 13:52:42 +0100">&lt;blockquote&gt;&lt;p&gt;Perhaps, if you want to simplify, close() should just commit and NEVER wait for merges. If someone wants that, he can call commit() + waitForMerges().&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes!&lt;/p&gt;

&lt;p&gt;I just repeat from the other issue: As CMS is involved, e.g. CMS.close() is interruptible, as it calls join() on all spawned merge threads after informing them to stop working. But as they are separate threads, there is some wait operation involved. We can simplify here, but we should still make IndexWriter.close() not interruptible.&lt;/p&gt;</comment>
                    <comment id="13420619" author="rcmuir" created="Mon, 23 Jul 2012 13:56:38 +0100">&lt;p&gt;Because is transactional. Outputstream is not&lt;/p&gt;</comment>
                    <comment id="13420622" author="shaie" created="Mon, 23 Jul 2012 14:03:04 +0100">&lt;p&gt;Still, I don't want to call commit(), then close() every time I want to close. And I think it will confuse users, i.e. we have close(), commit() and rollback(). We know what commit() and rollback() do, but what does close() without commit() does? Is it like rollback()? If so, should we remove rollback()?&lt;/p&gt;

&lt;p&gt;I think that close() doing commit() is fine. Even if it is transactional. Someone can always call rollback() if he wants to close w/o commit. That's the purpose of rollback().&lt;/p&gt;
</comment>
                    <comment id="13420627" author="mikemccand" created="Mon, 23 Jul 2012 14:07:18 +0100">&lt;blockquote&gt;
&lt;p&gt;Perhaps, if you want to simplify, close() should just commit and NEVER wait for merges. If someone wants that, he can call commit() + waitForMerges().&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think you mean waitForMerges(), then commit()?  Else the merges aren't committed...&lt;/p&gt;

&lt;p&gt;I think this idea (close simply closes) makes sense!  But:&lt;/p&gt;

&lt;p&gt;Would close() throw an exception if there were pending (uncommitted, un-rolled-back) changes?  Ie to avoid the obvious trap of users on upgrading assuming close still implicitly calls commit else they silently lose their changes.&lt;/p&gt;

&lt;p&gt;Also if merges are still running I think close() should throw an exception, else CMS will be silently starved (never able to complete a large merge) for apps that open IW, add/update some docs, commit, close.  We'd need to open up a "stopAllMerges" call.&lt;/p&gt;</comment>
                    <comment id="13420628" author="rcmuir" created="Mon, 23 Jul 2012 14:08:00 +0100">&lt;p&gt;Close should alias to rollback. Otherwise its broken. We&lt;br/&gt;
 should fix this before we have to support backwards... Committing shit on close is a massive bug, no other transactional apis act this way.&lt;/p&gt;

&lt;p&gt;If you are against this, fine, another option is to remove the transactional API.&lt;/p&gt;

&lt;p&gt;But the current situation is broken beyond words.&lt;/p&gt;</comment>
                    <comment id="13420634" author="rcmuir" created="Mon, 23 Jul 2012 14:14:15 +0100">&lt;p&gt;I don't think we need to worry too much about apps that close indexwriter each time they add a doc being starved. Such apps are likely optimizing each time too&lt;/p&gt;</comment>
                    <comment id="13420638" author="thetaphi" created="Mon, 23 Jul 2012 14:17:51 +0100">&lt;p&gt;I am fine with either proposal, but this has nothing to do with the interruptible bug, because close() has to still "wait" for CMS to shutdown (the mergeScheduler.close() call would keep alive, and that one is interruptible at the moment). My patch in the other issue ensures that the close of the scheduler and shutting down all its threads works, although some threads were interrupted by Jetty or any other automatic shutdown. Thats all this issue was opened, this here is not related at all.&lt;/p&gt;

&lt;p&gt;By this issue we just have less interruptible calls on close, but that can be done after discussion, so I want to fix issue &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4245" title="IndexWriter#close(true) should either not be interruptible or should abort background merge threads before returning"&gt;&lt;del&gt;LUCENE-4245&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="13420693" author="rcmuir" created="Mon, 23 Jul 2012 15:58:01 +0100">&lt;p&gt;I think its related: I think close() is too complicated today.&lt;/p&gt;

&lt;p&gt;In my opinion close() should just close resources: we should try to minimize what it does so it can easily be&lt;br/&gt;
used in e.g. finally block.&lt;/p&gt;

&lt;p&gt;As far as commit() goes, I think you should always be explicit about that and not mix it up with close().&lt;/p&gt;

&lt;p&gt;As far as pending merges, if we want to have an option to do that, I would prefer it not be the default&lt;br/&gt;
Closeable behavior of close(), instead something separate like shutdown().&lt;/p&gt;</comment>
                    <comment id="13420696" author="rcmuir" created="Mon, 23 Jul 2012 16:02:30 +0100">&lt;blockquote&gt;
&lt;p&gt;By this issue we just have less interruptible calls on close, but that can be done after discussion, so I want to fix issue &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4245" title="IndexWriter#close(true) should either not be interruptible or should abort background merge threads before returning"&gt;&lt;del&gt;LUCENE-4245&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I opened a separate issue because I think its related but separate. I think it would be good to fix &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4245" title="IndexWriter#close(true) should either not be interruptible or should abort background merge threads before returning"&gt;&lt;del&gt;LUCENE-4245&lt;/del&gt;&lt;/a&gt; first as its easier, I just want an issue open for the bigger issue of what close() does.&lt;/p&gt;</comment>
                    <comment id="13420700" author="thetaphi" created="Mon, 23 Jul 2012 16:06:49 +0100">&lt;p&gt;OK. That's all I wanted to make the test failures go away!&lt;/p&gt;</comment>
                    <comment id="13420710" author="rcmuir" created="Mon, 23 Jul 2012 16:20:22 +0100">&lt;blockquote&gt;
&lt;p&gt;Would close() throw an exception if there were pending (uncommitted, un-rolled-back) changes? Ie to avoid the obvious trap of users on upgrading assuming close still implicitly calls commit else they silently lose their changes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm torn here mostly because rollback() closes the IndexWriter. if it didn't do this then the path to me seems clear.&lt;br/&gt;
But since it does, its confusing.&lt;/p&gt;

&lt;p&gt;E.g., just looking at the JDBC Connection api as a parallel, its a little vague but I think some impls actually throw Exception here? officially its of course...&lt;/p&gt;

&lt;p&gt;"It is &lt;b&gt;strongly&lt;/b&gt; recommended that an application explicitly commits or rolls back an active transaction prior to calling the close method. If the close method is called and there is an active transaction, the results are implementation-defined."&lt;/p&gt;

&lt;p&gt;Seeing that we used to commit() on close(), I think its probably best that we would throw an Exception (if possible, i have no idea at a glance how). Otherwise the behavior change would be a trap... this would make it a lot easier.&lt;/p&gt;

&lt;p&gt;We already do this if you entered the first phase of a two-phase commit.&lt;/p&gt;

&lt;p&gt;But I really don't think we should do anything with pending merges: like i said we could have a separate 'graceful slow shutdown' for that that waits for merges: its unrelated to transactional semantics.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Is it like rollback()? If so, should we remove rollback()?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should leave ourselves open to the possibility that one day, maybe rollback() can be done without closing.&lt;/p&gt;</comment>
                    <comment id="13420884" author="shaie" created="Mon, 23 Jul 2012 20:15:10 +0100">&lt;p&gt;I still don't understand why change behavior that is out there for ages. Everyone is used to calling close() and that commits things. I agree with you that if from the beginning people would need to call commit() before close(), then that's better. But I don't think there is a good justification to change current behavior.&lt;/p&gt;

&lt;p&gt;Now you're talking about detecting whether there were any changes or not &amp;#8211; that doesn't simplify close() logic IMO.&lt;/p&gt;

&lt;p&gt;In short, I don't feel that changing current behavior is going to do any good, on the contrary, it would confuse people.&lt;/p&gt;</comment>
                    <comment id="13420890" author="rcmuir" created="Mon, 23 Jul 2012 20:18:59 +0100">&lt;blockquote&gt;
&lt;p&gt;I still don't understand why change behavior that is out there for ages. Everyone is used to calling close() and that commits things. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm not used to it: its awkward for a transactional API.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I agree with you that if from the beginning people would need to call commit() before close(), then that's better.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Then we should fix it! Just because "well its already this broken way", is no reason at all to not change things.&lt;br/&gt;
We should fix things so they work in a sane way.&lt;/p&gt;

&lt;p&gt;Previous behavior doesn't matter at all here.&lt;/p&gt;</comment>
                    <comment id="13420894" author="yseeley@gmail.com" created="Mon, 23 Jul 2012 20:28:19 +0100">&lt;blockquote&gt;&lt;p&gt;But I don't think there is a good justification to change current behavior.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;We have a rollback call already.  Changing the semantics of close() does not simplify anything (i.e. it doesn't make the issue that spawned this one any easier).&lt;/p&gt;</comment>
                    <comment id="13420898" author="rcmuir" created="Mon, 23 Jul 2012 20:31:34 +0100">&lt;p&gt;Thats ok: resistance to change is just encouragement to me.&lt;/p&gt;

&lt;p&gt;"it used to work this way" is no good reason not to fix broken stuff.&lt;/p&gt;</comment>
                    <comment id="13421255" author="shaie" created="Tue, 24 Jul 2012 09:05:42 +0100">&lt;p&gt;I disagree with you about OutStream.close not being transactional ... you know what, that's not even the point. You got used to the data being flushed when you call OS.close(). Same as you (or anyone else if you would like to pretend you're not used to it) got used to documents being committed when you close IndexWriter.&lt;/p&gt;

&lt;p&gt;There is no good justification changing that behavior for IndexWriter. It's not broken ... at least, I didn't hear anyone besides you claiming it is - on contrary, I hear more voices against this change.&lt;/p&gt;

&lt;p&gt;I think that that we've made changes to the API and Lucene code behavior too lightly in the last couple of years. When it's justified because e.g. a user might run himself into troubles, then you're right &amp;#8211; that that the program worked like that doesn't mean it shouldn't change. But the change you're proposing here is likely to cause more damage (even if the damage is mostly less convenient coding) than be useful to anyone.&lt;/p&gt;

&lt;p&gt;IndexWriter.close() won't be simplified a lot, yet users will need to change their code and think what to do with exceptions like YouHaveUncommittedChangesException ...&lt;/p&gt;

&lt;p&gt;I repeat my proposal again &amp;#8211; let's change close() to never wait for merges (i.e. remove the close(boolean) variant) &amp;#8211; that IMO is behavior I doubt if people rely on (merges are a form of optimization, not program correctness and potential data loss).&lt;/p&gt;</comment>
                    <comment id="13421258" author="thetaphi" created="Tue, 24 Jul 2012 09:15:09 +0100">&lt;blockquote&gt;&lt;p&gt;I repeat my proposal again – let's change close() to never wait for merges (i.e. remove the close(boolean) variant) – that IMO is behavior I doubt if people rely on (merges are a form of optimization, not program correctness and potential data loss).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 - If you look at my changes in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4245" title="IndexWriter#close(true) should either not be interruptible or should abort background merge threads before returning"&gt;&lt;del&gt;LUCENE-4245&lt;/del&gt;&lt;/a&gt; you will see that waitForMerges is a pain. I finally got it working that IW.close() now for sure cleans up (means IOUtils.close MergePolicy and MergeScheduler, whatever happened). The implicit flush() and commit() are hairy, but doable.&lt;/p&gt;</comment>
                    <comment id="13421319" author="mikemccand" created="Tue, 24 Jul 2012 11:45:07 +0100">&lt;blockquote&gt;&lt;p&gt;I'm torn here mostly because rollback() closes the IndexWriter. if it didn't do this then the path to me seems clear.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we could change rollback() to not close ... I can't remember offhand why it closes as a side effect.&lt;/p&gt;</comment>
                    <comment id="13421320" author="mikemccand" created="Tue, 24 Jul 2012 11:46:40 +0100">&lt;p&gt;Pain or not, waiting for merges is important, to avoid the starvation&lt;br/&gt;
case where the user never waits for merges and so the biggish ones&lt;br/&gt;
never get a chance to complete.&lt;/p&gt;

&lt;p&gt;We can't make that the default: it's too trappy.  Apps that open an&lt;br/&gt;
IW, index the current batch of new or changed docs, close IW (a&lt;br/&gt;
reasonable way to use IW), would at some point get waaay too many&lt;br/&gt;
segments in their index.&lt;/p&gt;

&lt;p&gt;Or: maybe, close() could throw an exception if merges are still in&lt;br/&gt;
progress (just like it should (I think) if pending changes weren't&lt;br/&gt;
committed nor rolled back)?  And, add a public killMerges method (just&lt;br/&gt;
calls waitForMerges(false)).  This way if you try to close without&lt;br/&gt;
having first killed or waited for merges, the close fails and you see&lt;br/&gt;
why.&lt;/p&gt;</comment>
                    <comment id="13421354" author="rcmuir" created="Tue, 24 Jul 2012 13:30:45 +0100">&lt;p&gt;Shai: look at the JDBC connection API.&lt;/p&gt;

&lt;p&gt;I dont know what you are talking about saying OutputStream is transactional: Are you seriously joking?!&lt;/p&gt;</comment>
                    <comment id="13421363" author="rcmuir" created="Tue, 24 Jul 2012 13:49:35 +0100">&lt;blockquote&gt;
&lt;p&gt;Pain or not, waiting for merges is important, to avoid the starvation&lt;br/&gt;
case where the user never waits for merges and so the biggish ones&lt;br/&gt;
never get a chance to complete.&lt;/p&gt;

&lt;p&gt;We can't make that the default: it's too trappy. Apps that open an&lt;br/&gt;
IW, index the current batch of new or changed docs, close IW (a&lt;br/&gt;
reasonable way to use IW), would at some point get waaay too many&lt;br/&gt;
segments in their index.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think its also trappy to wait for merges by default: many&lt;br/&gt;
people have rapidly changing content and time to visibility &lt;br/&gt;
is important.&lt;/p&gt;

&lt;p&gt;Unfortunately we know some apps close() the IndexWriter whenever&lt;br/&gt;
they make changes visible: you can tell me these apps are broken,&lt;br/&gt;
etc, etc, but our PMC just released one of those this weekend.&lt;/p&gt;</comment>
                    <comment id="13421384" author="shaie" created="Tue, 24 Jul 2012 14:11:45 +0100">&lt;blockquote&gt;&lt;p&gt;Shai: look at the JDBC connection API.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm arguing about changing current behavior with no apparent gains. And I'm not sure that I want to compare IW API to JDBC ... to me they are not the same things, but this may be just semantics.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I dont know what you are talking about saying OutputStream is transactional&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I didn't say OS is transactional, but rather that it's not the point. The point is changing API that works one way for years, which users rely on its behavior for years, their apps will be broken in who knows how many ways etc etc ... again, for no apparent gains.&lt;/p&gt;

&lt;p&gt;This is what I object to &amp;#8211; changing semantics of API which people are happy with and used to it. If it's so important to you, you can make a new API closeNoCommit(). Today it's rollback(), but if that ever changes, we have an explicit API.&lt;/p&gt;

&lt;p&gt;And I don't understand if you argue for the sake arguing, or you really believe there is anyone out there intending to close IW without committing the changes? Do &lt;b&gt;you&lt;/b&gt; intend to do it?&lt;/p&gt;</comment>
                    <comment id="13429695" author="rcmuir" created="Tue, 7 Aug 2012 04:41:20 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                    <comment id="13536232" author="markrmiller@gmail.com" created="Wed, 19 Dec 2012 18:18:34 +0000">&lt;blockquote&gt;&lt;p&gt;you really believe there is anyone out there intending to close IW without committing the changes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I do it, but I use rollback for this...&lt;/p&gt;</comment>
                    <comment id="13536292" author="mikemccand" created="Wed, 19 Dec 2012 18:52:17 +0000">&lt;p&gt;I think a good compromise here is for close to throw an exception if merges are still running or if there are uncommitted changes.  We'd also add an "abortRunningMerges()".&lt;/p&gt;

&lt;p&gt;This way the app can be explicit about discarding changes, aborting merges, and separate that from close(), which becomes a fast operation.&lt;/p&gt;</comment>
                    <comment id="13536341" author="shaie" created="Wed, 19 Dec 2012 19:29:37 +0000">&lt;p&gt;Close() throws an exception &amp;#8211; that's just for back-compat right? I.e., the app can't do anything with that exception except either calling commit() (that's the back-compat part) or rollback() (if it needs to close fast). And abortRunningMerges is just like close(false) today (without the commit())?&lt;/p&gt;

&lt;p&gt;I mean, if an app wants to commit + wait + close, the 1 line close() becomes wait(), commit() and close(). And just like I made a mistake saying you should call commit() then wait(), I'm sure others will make that mistake too.&lt;/p&gt;

&lt;p&gt;So again, why go to great lengths to complicate the lives of the innocent developer? Can't we just decide (I think that's the 3rd time I'm proposing it) to never wait for merges on close(), and keep close() committing changes? Would close() be really simplified if it will need to detect running merges and uncommitted changes?&lt;/p&gt;

&lt;p&gt;Maybe someone puts up a patch with the changes to IW only, so we can review? I personally don't see the gain in changing the behavior of close(), but maybe a patch will clarify the gains ...&lt;/p&gt;</comment>
                    <comment id="13536385" author="mikemccand" created="Wed, 19 Dec 2012 20:18:13 +0000">&lt;blockquote&gt;&lt;p&gt;Can't we just decide (I think that's the 3rd time I'm proposing it) to never wait for merges on close(), and keep close() committing changes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think we can never wait for merges on close.&lt;/p&gt;

&lt;p&gt;That can easily lead to an accidental "denial of service attack on big merges", which would be an awful trap.  Ie, a big merge kicks off, but never has a chance to finish because the app closes/opens new IWs frequently.  Then every IW that's opened will restart the merge, spend CPU/IO resources, only to abort when the IW is closed.&lt;/p&gt;

&lt;p&gt;I've never liked that close "hides" this wait-for-massive-merge to finish, but I also don't like this "just abort the massive merge" solution: it would create a nasty trap.  I would prefer  that it's explicit (abortMerges or waitForMerges).&lt;/p&gt;</comment>
                    <comment id="13536424" author="shaie" created="Wed, 19 Dec 2012 21:00:49 +0000">&lt;p&gt;Today a simple app just calls close(). Now you propose that it will call close(), but try-catch two exceptions RunningMerges and UncommittedChanges. Then the poor developer will need to decide what to do with them .. should he call rollback()? should he call commit()? then close(), only to try-catch those two ex, now documenting "it's fine now"?&lt;/p&gt;

&lt;p&gt;While the app today can choose to call waitForMerges, or close(false), or commit, then close.&lt;/p&gt;

&lt;p&gt;The changes will make the code more verbose and I'm afraid will raise the bar for simple apps. Aren't you the one that always pushes for simple, more approachable API?&lt;/p&gt;

&lt;p&gt;And with that solution, what would rollback() do? Unless we change rollback to not close, it's just an alias, as Robert put it, to close, only it doesn't throw the two new exceptions?&lt;/p&gt;</comment>
                    <comment id="13536987" author="mikemccand" created="Thu, 20 Dec 2012 12:15:59 +0000">&lt;p&gt;I agree it'd make IndexWriter more verbose for simple apps, which is not great, but it'd also make it more transparent, which may be the right tradeoff here.  It's important for apps to know that IW.waitForMerges can sometimes take a very long time.  Also, in issues like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4638" title="If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks."&gt;LUCENE-4638&lt;/a&gt;, it'd be clearer what went wrong, ie Mark would have hit the exception during commit().&lt;/p&gt;

&lt;p&gt;Those two exceptions from IW.close() are there to catch coding errors, ie the app was not explicit about whether it wanted to abort merges and discard changes.  What to do about those exceptions would be clear: you either rollback or commit, and you either waitForMerges or abortMerges, before calling close.&lt;/p&gt;

&lt;p&gt;close() does too many things now, and they are surprising.  Maybe we should rename it to a new method to make it clear it "waits for merges, commits, closes", and then shift to the new semantics for close.&lt;/p&gt;

&lt;p&gt;If we did this change, rollback() would discard changes since the last commit but NOT close.&lt;/p&gt;</comment>
                    <comment id="13538011" author="markrmiller@gmail.com" created="Fri, 21 Dec 2012 14:30:57 +0000">&lt;p&gt;What I'm really dying for is just a close that is sure to close. Whatever problems it encounters, whatever goes wrong - sure, I'd like to know about it - but please still close and release the writer lock. I know my index will still be good up till the last commit and I can open a new IndexWriter. I really need this for some tests - mainly because jetty and/or the test framework can interrupt threads - this can cause interruption exceptions and channel interruption exceptions that kill the close. For a std interruption exception I seem to be able to retry close and succeed - but not a channel interruption exception.&lt;/p&gt;</comment>
                    <comment id="13538031" author="rcmuir" created="Fri, 21 Dec 2012 14:33:04 +0000">&lt;blockquote&gt;
&lt;p&gt;What I'm really dying for is just a close that is sure to close. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And thats &lt;b&gt;all&lt;/b&gt; close() should do!&lt;/p&gt;

&lt;p&gt;close() should throw IOException (not other things)&lt;br/&gt;
close() shouldnt wait on background merges&lt;br/&gt;
close() definitely shouldnt commit anything.&lt;/p&gt;

&lt;p&gt;I don't understand why this is so controversial. Just look at the java.io.Closeable semantics if you want to understand how users expect close() to work.&lt;/p&gt;</comment>
                    <comment id="13538037" author="markrmiller@gmail.com" created="Fri, 21 Dec 2012 14:33:30 +0000">&lt;p&gt;See &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4638" title="If IndexWriter is interrupted on close and is using a channel (mmap/nio), it can throw a ClosedByInterruptException and prevent you from opening a new IndexWriter in the same proceses if you are using Native locks."&gt;LUCENE-4638&lt;/a&gt; - I really want a close that means close. If this issue is too tied up, I'd like to start another issue that at least accomplish that goal.&lt;/p&gt;</comment>
                    <comment id="13538098" author="rcmuir" created="Fri, 21 Dec 2012 14:39:11 +0000">&lt;p&gt;Well i think this issue compounds the situation. close() is doing so many things that its actually buggy:&lt;br/&gt;
its not releasing resources (the lock). Releasing resources is the purpose of close() &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13538722" author="mikemccand" created="Sat, 22 Dec 2012 10:49:09 +0000">&lt;p&gt;Maybe we could rename the current "close" to "commitAndClose", which would wait on merges (maybe take a boolean to instead abort merges), commit changes, close (for the simplicty/convenience for basic apps).&lt;/p&gt;

&lt;p&gt;And then "close" simply closes.&lt;/p&gt;

&lt;p&gt;The only issue is this is a trappy migration path for apps ... if they miss the back-compat-break advertisement that IW.close no longer commits then they silently lose changes.&lt;/p&gt;</comment>
                    <comment id="13538816" author="rcmuir" created="Sat, 22 Dec 2012 13:52:37 +0000">&lt;p&gt;No, the issue is that close() doesn't do its #1 job.&lt;/p&gt;

&lt;p&gt;The rest of what its doing, it does not need to do.&lt;/p&gt;</comment>
                    <comment id="13538850" author="markrmiller@gmail.com" created="Sat, 22 Dec 2012 16:28:28 +0000">&lt;p&gt;+1 on addressing this issue!&lt;/p&gt;

&lt;p&gt;Can I just point out that the 'proper' way to close an IndexWriter is perhaps:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt; &lt;span class="code-keyword"&gt;while&lt;/span&gt; (&lt;span class="code-keyword"&gt;true&lt;/span&gt;) {
      &lt;span class="code-keyword"&gt;try&lt;/span&gt; {
        &lt;span class="code-keyword"&gt;super&lt;/span&gt;.close();
      } &lt;span class="code-keyword"&gt;catch&lt;/span&gt; (ThreadInterruptedException e) {
        &lt;span class="code-comment"&gt;// don't allow interruption
&lt;/span&gt;        &lt;span class="code-keyword"&gt;continue&lt;/span&gt;;
      } &lt;span class="code-keyword"&gt;catch&lt;/span&gt; (Exception e) {
        iw.rollback(); &lt;span class="code-comment"&gt;// mabye
&lt;/span&gt;      }
      &lt;span class="code-keyword"&gt;break&lt;/span&gt;;
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Except we don't know if rollback is good enough there yet, and it's a little scary to have the while loop on the interrupted exception without a timeout. Writing a line of code to close the IW is an expert task if you want to get it right. If its currently possible to get it right.&lt;/p&gt;

&lt;p&gt;So perhaps thats what you need for a safe close. How many people do you think are doing that? Can we agree that it's insane that you do have to do that? Have you ever seen anything like it?&lt;/p&gt;

&lt;p&gt;Let's come to some compromise on exactly what close does so that we can un crazify it.&lt;/p&gt;</comment>
                    <comment id="13538851" author="markrmiller@gmail.com" created="Sat, 22 Dec 2012 16:32:01 +0000">&lt;p&gt;Sorry - that's not even right for close - I forgot to put in an IW#unlock call a finally around this. I think thats all you need...perhaps also a hammer...and a chainsaw...&lt;/p&gt;</comment>
                    <comment id="13538852" author="markrmiller@gmail.com" created="Sat, 22 Dec 2012 16:33:09 +0000">&lt;p&gt;Wait...you also need a try catch around the IW#unlock in case its a native lock and you cant actually release it...&lt;/p&gt;</comment>
                    <comment id="13539036" author="mikemccand" created="Sun, 23 Dec 2012 14:53:21 +0000">&lt;p&gt;The proper way to close IW today is:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;try {
  writer.close();
} catch (Throwable t) {
  writer.rollback();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13539038" author="markrmiller@gmail.com" created="Sun, 23 Dec 2012 15:08:00 +0000">&lt;p&gt;Why don't the docs say that then? I'm not even confident that will work.&lt;/p&gt;

&lt;p&gt;But look at the docs - it's telling you to keep calling close, its trying to get you to force unlock in a finally - it doesnt even mention rollback.&lt;/p&gt;

&lt;p&gt;And are you sure rollback will not be interrupted either? Rollback throws IOException and you are not dealing with it. If it's really that simple, why has the javadocs evolved into a mess of bad recommendations and information?&lt;/p&gt;

&lt;p&gt;And if that is the proper way, why doesnt writer.close do that for the user and then throw an exception?&lt;/p&gt;

&lt;p&gt;These are crazy semantics.&lt;/p&gt;</comment>
                    <comment id="13539040" author="rcmuir" created="Sun, 23 Dec 2012 15:26:03 +0000">&lt;blockquote&gt;
&lt;p&gt;These are crazy semantics.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The proper way to close IW today is:&lt;/p&gt;

&lt;p&gt;try {&lt;br/&gt;
  writer.close();&lt;br/&gt;
} catch (Throwable t) {&lt;br/&gt;
  writer.rollback();&lt;br/&gt;
}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then we should remove java.io.Closeable interface from IndexWriter. This is especially bad for java7 users who have syntactical support from the language for doing things "the wrong way".&lt;/p&gt;

&lt;p&gt;The proper way to close IndexWriter should be: writer.close()&lt;/p&gt;</comment>
                    <comment id="13539042" author="thetaphi" created="Sun, 23 Dec 2012 15:48:24 +0000">&lt;blockquote&gt;&lt;p&gt;Then we should remove java.io.Closeable interface from IndexWriter. This is especially bad for java7 users who have syntactical support from the language for doing things "the wrong way".&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let's make IndexWrite do nothing additional like merging or similar stuff. I already fixed some leaks in the past, but the ones discussed here are more crazy (like InterruptedException handling).&lt;/p&gt;

&lt;p&gt;-1 to remove Closeable, if the semantics of closeable are correct. This is currently not the case, but one we fix to close without committing and so on, we are fine. The syntactic sugar here is good, because it warns you (when writing tests) that you miss to close. Somebody using a try-with-resources block is a different story, but if somebody wants to do this he can really do that (and its useful for lots of tasks). It is just the "one long running thread keeps IndexWriter open and indexes documents that you may think of. But when its run() method ends, the IndexWriter should be closed, too. So Closeable is perfectly fine. And we cannot prevent people from writing ineffective code, but those people would not use try-with-resources at all.&lt;/p&gt;</comment>
                    <comment id="13539044" author="rcmuir" created="Sun, 23 Dec 2012 15:53:19 +0000">&lt;p&gt;I agree Uwe. I'm just saying that we should either:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;fix IndexWriter semantics to be sane so close() actually and only close()s&lt;/li&gt;
	&lt;li&gt;don't fix crazy semantics, and remove Closeable.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The first solution is greatly preferred.&lt;br/&gt;
But we shouldn't leave it like today: with the current code I don't think having Closeable is appropriate.&lt;/p&gt;</comment>
                    <comment id="13539045" author="thetaphi" created="Sun, 23 Dec 2012 16:02:17 +0000">&lt;p&gt;Robert: Yes it was not 100% clear to me what your comment wanted to say. close() should do at least as possible. And if you use try-with-resources and forget to commit, the cleanup (similar to our IOUtils) would close IW correctly, but don't commit. The use then knows that something is wrong. By the way, this is similar to SQL and transactions with try-with-resources. There were lengthly discussion about that on the JDK mailing list before Java 7. Although close() &lt;b&gt;may&lt;/b&gt; do more than just closing (if autocommit is enabled, it may also commit), they decided to add Closeable (e.g., see &lt;a href="http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html" class="external-link"&gt;http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html&lt;/a&gt;) - but the discussion here was similar.&lt;/p&gt;</comment>
                    <comment id="13539245" author="mikemccand" created="Mon, 24 Dec 2012 11:05:02 +0000">&lt;blockquote&gt;&lt;p&gt;The proper way to close IndexWriter should be: writer.close()&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But I don't like the trap this sets up on apps that upgrade ... so if&lt;br/&gt;
there are pending uncommitted changes, IW.close should throw an exc&lt;br/&gt;
(IllegalStateExc?).  Ie, the app must be explicit about discarding or&lt;br/&gt;
committing those changes, before calling close.&lt;/p&gt;

&lt;p&gt;Separately, if there are running merges then we should throw a similar&lt;br/&gt;
exception, otherwise we set up a denial-of-service on biggish merges&lt;br/&gt;
trap: for apps that open a writer, index a batch of docs, and close&lt;br/&gt;
the writer, a given biggish merge would forever kick off when the&lt;br/&gt;
writer is opened and then be aborted when it's closed, wasting CPU/IO&lt;br/&gt;
and never finishing the merge.&lt;/p&gt;

&lt;p&gt;If we handle those two traps then I'm OK with making close "just&lt;br/&gt;
close".  The "proper" shutdown sequence for IW would then be:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;w.waitForMerges() OR w.abortMerges()
w.commit() OR w.rollback()
w.close()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(And we'd fix rollback to not close).&lt;/p&gt;

&lt;p&gt;Shai objected to how verbose this is for "normal" usage, and I agree,&lt;br/&gt;
so I proposed adding a sugar mehod "commitAndClose" that would just&lt;br/&gt;
call waitForMerges(), commit(), close().  I was hoping to hear from&lt;br/&gt;
Shai whether that's an OK compromise...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Although close() may do more than just closing (if autocommit is enabled, it may also commit), they decided to add Closeable (e.g., see &lt;a href="http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html" class="external-link"&gt;http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html&lt;/a&gt;) - but the discussion here was similar.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's an interesting precedent.  Uwe do you a pointer to the&lt;br/&gt;
discussion?&lt;/p&gt;

&lt;p&gt;So I think another option here is to leave close as is (it waits for&lt;br/&gt;
merges, commits) except, if an exception is thrown, it suppresses that&lt;br/&gt;
exception, finishes closing, and then throws it.  Like IOUtils.close&lt;br/&gt;
...&lt;/p&gt;

&lt;p&gt;This way on calling IW.close(), if an exception is thrown, the IW will&lt;br/&gt;
in fact have been closed / relased its lock / etc., and then the app&lt;br/&gt;
sees the first exception that was hit while closing.&lt;/p&gt;

&lt;p&gt;Hmm... does java.io.Closeable document semantics on exception?  Ie if&lt;br/&gt;
I call RAF.close and hit an exception, is it "really closed"?  It does&lt;br/&gt;
document that calling it more than once is fine ...&lt;/p&gt;</comment>
                    <comment id="13539253" author="thetaphi" created="Mon, 24 Dec 2012 11:46:09 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Although close() may do more than just closing (if autocommit is enabled, it may also commit), they decided to add Closeable (e.g., see &lt;a href="http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html" class="external-link"&gt;http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html&lt;/a&gt;) - but the discussion here was similar.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's an interesting precedent. Uwe do you a pointer to the discussion?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am digging... I have no pointer at the moment, I just read blog posts and followed some mailing lists at that time, but that's now almost 2 years ago. The SQL stuff had more problems, which lead finally to the new interface AutoCloseable in Java 7 (java.io.Closeable extends java.lang.AutoCloseable). The reason is: SQLException does not extend IOException so the already existent java.io.Closeable was not applicable, so the new java.lang.Autocloseable was defined to throw any Exception subclass on its close() method, just java.io.Closeable limits this to IOException by subclassing. The try-with-resources magic in Java 7 only affects AUtocloseable, but as Closeable extends Autocloseable, it also applies to Closeable.&lt;/p&gt;

&lt;p&gt;Unless we move to Java 7 we are also limited to only throw IOException on close(), no other checked Exception. In Java 7 we could make IndexWriter implement AutoCloseable and let it throw any other Exception, too.&lt;/p&gt;

&lt;p&gt;One important information about AutoCloseable from JavaDocs: "Implementers of this interface are also strongly advised to not have the close method throw InterruptedException. This exception interacts with a thread's interrupted status, and runtime misbehavior is likely to occur if an InterruptedException is suppressed. More generally, if it would cause problems for an exception to be suppressed, the AutoCloseable.close method should not throw it. Note that unlike the close method of Closeable, this close method is not required to be idempotent. In other words, calling this close method more than once may have some visible side effect, unlike Closeable.close which is required to have no effect if called more than once. However, implementers of this interface are strongly encouraged to make their close methods idempotent."&lt;/p&gt;

&lt;p&gt;This last sentence explains why java.sql.Connection is not required to be idempotent (see below). &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So I think another option here is to leave close as is (it waits for merges, commits) except, if an exception is thrown, it suppresses that exception, finishes closing, and then throws it. Like IOUtils.close.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am fine with that, only that it &lt;b&gt;must&lt;/b&gt; record suppressed Exceptions (unfortunately the IOUtils in Lucene is in most cases used not in the optimal way, so the supressed exceptions get lost).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Hmm... does java.io.Closeable document semantics on exception? Ie if I call RAF.close and hit an exception, is it "really closed"? It does document that calling it more than once is fine ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It says that you may call close() multiple times, but later calls mst be side-effect free, so after the first call to close it must be closed (otherwise later calls would have side-effects), although an IOException is thrown.&lt;/p&gt;</comment>
                    <comment id="13539255" author="shaie" created="Mon, 24 Dec 2012 11:58:41 +0000">&lt;blockquote&gt;
&lt;p&gt;Shai objected to how verbose this is for "normal" usage, and I agree,&lt;br/&gt;
so I proposed adding a sugar mehod "commitAndClose" that would just&lt;br/&gt;
call waitForMerges(), commit(), close(). I was hoping to hear from&lt;br/&gt;
Shai whether that's an OK compromise...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am ok with commitAndClose. I think it will be valuable for simple apps, not to mention our tests. Complex apps can probably add 2-3 more lines of code to their "close" logic.&lt;/p&gt;

&lt;p&gt;Also, I think that the separation to the different methods give apps more control over what happens on close(). I.e. today, app can choose between close(true) and close(false), both always commit and optionally wait for merges. With the separation, apps could distinguish between "I need to close in a hurry" to "I need to close quickly, but commit changes" to "I have time to close, so finish merges, commit and close". Maybe we should even give these recipes in IW#close or IW class javadocs.&lt;/p&gt;</comment>
                    <comment id="13539279" author="yseeley@gmail.com" created="Mon, 24 Dec 2012 14:59:27 +0000">&lt;p&gt;I'd suggest that close() should remain "commit and close" since it's been that way forever and certainly doesn't make sense to change in 4x, esp since if you go back far enough the only way to commit was to call close().  Many older apps that have been upgraded will start breaking.  IMO, the mistake was introduced when close() became partial (i.e. if it throws an exception it doesn't fully clean up).  That's the only thing that really needs fixing here.&lt;/p&gt;

&lt;p&gt;We already have rollback for "close immediately even if it means dropping uncommitted changes".&lt;/p&gt;</comment>
                    <comment id="13552092" author="steve_rowe" created="Sat, 12 Jan 2013 23:02:02 +0000">&lt;p&gt;I'd like to push this to 4.2.  Any objections?&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12624660">LUCENE-4638</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 23 Jul 2012 12:49:02 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243724</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23452</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4236] clean up booleanquery conjunction optimizations a bit</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4236</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;After &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3505" title="BooleanScorer2.freq() doesnt work unless you call score() first."&gt;&lt;del&gt;LUCENE-3505&lt;/del&gt;&lt;/a&gt;, I want to do a slight cleanup:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;compute the term conjunctions optimization in scorer(), so its applied even if we have optional and prohibited clauses that dont exist in the segment (e.g. return null)&lt;/li&gt;
	&lt;li&gt;use the term conjunctions optimization when optional.size() == minShouldMatch, as that means they are all mandatory, too.&lt;/li&gt;
	&lt;li&gt;don't return booleanscorer1 when optional.size() == minShouldMatch, because it means we have required clauses and in general BS2 should do a much better job (e.g. use advance).&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12599393">LUCENE-4236</key>
            <summary>clean up booleanquery conjunction optimizations a bit</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 Jul 2012 02:53:13 +0100</created>
                <updated>Fri, 10 May 2013 00:05:10 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13417978" author="rcmuir" created="Thu, 19 Jul 2012 03:01:43 +0100">&lt;p&gt;updated patch with a code comment&lt;/p&gt;</comment>
                    <comment id="13417980" author="simonw" created="Thu, 19 Jul 2012 03:01:53 +0100">&lt;p&gt;cool nice little optimization! looks good to me while you could add that minShouldMatch only applies to optional scorers so nobody needs to hunt that down again&lt;/p&gt;</comment>
                    <comment id="13417982" author="rcmuir" created="Thu, 19 Jul 2012 03:02:57 +0100">&lt;p&gt;I removed reference to the SOLR issue, i thought for some reason it was a performance problem, but that was just faulty brain cells (its just some unrelated parsing bug).&lt;/p&gt;

&lt;p&gt;Still it seems like apps like Solr might be generating these disjunctions with minShouldmatch = optional.size() and we should handle them as conjunctions always.&lt;/p&gt;</comment>
                    <comment id="13418102" author="rcmuir" created="Thu, 19 Jul 2012 07:18:39 +0100">&lt;p&gt;The previous patch had a bug: coord was computed for the conjunction as scorers.length/scorers.length but it should be scorers.length/maxCoord (in the case of optional clauses that happened to not exist in the segment). &lt;/p&gt;

&lt;p&gt;Random testing is great.&lt;/p&gt;

&lt;p&gt;This patch removes the specialized TermConjunctionScorer, making the TermConjunctionScorer algorithm ConjunctionScorer. I added a method to Scorer so that any scorer can return an estimated cost of how many postings it will read:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;term: docFreq()&lt;/li&gt;
	&lt;li&gt;disjunction: sum(cost)&lt;/li&gt;
	&lt;li&gt;conjunction/phrase: min(cost) * numSubs&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;and so on, which ConjunctionScorer uses.&lt;/p&gt;

&lt;p&gt;No perf degradation... refactoring continues&lt;/p&gt;</comment>
                    <comment id="13418107" author="rcmuir" created="Thu, 19 Jul 2012 07:29:36 +0100">&lt;p&gt;Current benchmarks with the generalized conjunction scorer:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;                Task   QPS trunkStdDev trunk   QPS patchStdDev patch      Pct diff
              Phrase       15.69        0.51       15.29        0.30   -7% -    2%
          AndHighMed       44.35        0.55       43.34        0.63   -4% -    0%
              Fuzzy1       83.95        2.84       83.05        3.35   -8% -    6%
             Respell       75.00        3.43       74.30        3.79  -10% -    9%
        SloppyPhrase        7.05        0.44        7.00        0.17   -8% -    8%
      TermBGroup1M1P       58.41        1.15       58.17        0.96   -3% -    3%
         TermGroup1M       30.72        0.14       30.73        0.32   -1% -    1%
              Fuzzy2       33.81        1.67       33.87        1.26   -8% -    9%
        TermBGroup1M       48.73        0.34       48.89        0.27    0% -    1%
         AndHighHigh        8.04        0.16        8.09        0.10   -2% -    3%
            PKLookup      296.11        2.98      298.19        3.29   -1% -    2%
                Term      109.66        3.70      110.69        2.76   -4% -    7%
            Wildcard       61.99        0.80       63.00        2.53   -3% -    7%
             Prefix3       70.64        1.63       72.07        3.21   -4% -    9%
           OrHighMed       21.48        1.29       21.94        1.05   -8% -   13%
          OrHighHigh        8.48        0.47        8.68        0.40   -7% -   13%
            SpanNear        7.70        0.37        7.96        0.41   -6% -   14%
              IntNRQ        8.89        0.52        9.40        0.96  -10% -   23%
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Luceneutil doesnt yet benchmark more complicated BQs (e.g. nested ones, or minShouldMatch, or whatever).&lt;br/&gt;
So we don't see any benefit in these benchmarks.&lt;/p&gt;</comment>
                    <comment id="13418202" author="rcmuir" created="Thu, 19 Jul 2012 11:20:44 +0100">&lt;p&gt;Updated patch... I tried to speed up BS2 some more here in this patch:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;               Task   QPS trunkStdDev trunk   QPS patchStdDev patch      Pct diff
             Respell       76.69        1.75       74.06        1.36   -7% -    0%
          AndHighMed       89.69        1.85       86.70        2.83   -8% -    1%
            SpanNear        2.75        0.09        2.70        0.07   -7% -    4%
              Fuzzy2       34.94        0.66       34.46        0.54   -4% -    2%
              Fuzzy1      115.28        2.24      113.81        1.59   -4% -    2%
         AndHighHigh       12.97        0.27       12.84        0.34   -5% -    3%
        TermBGroup1M       47.87        0.27       47.42        0.51   -2% -    0%
      TermBGroup1M1P       53.84        1.08       53.35        0.81   -4% -    2%
         TermGroup1M       42.55        0.51       42.36        0.56   -2% -    2%
            PKLookup      298.51        0.80      297.81        2.68   -1% -    0%
            Wildcard       20.66        1.13       20.75        1.09   -9% -   11%
              Phrase        6.64        0.26        6.67        0.31   -7% -    9%
             Prefix3       32.82        1.69       33.07        1.72   -9% -   11%
        SloppyPhrase       26.12        0.38       26.46        0.42   -1% -    4%
              IntNRQ       11.24        1.55       11.39        1.38  -21% -   31%
                Term      138.34        0.93      141.27        8.71   -4% -    9%
          OrHighHigh        7.28        0.38        7.71        0.50   -5% -   19%
           OrHighMed       20.29        1.21       21.81        1.61   -6% -   22%
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It could use some code comments and cleanup but its time for a break &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13429720" author="rcmuir" created="Tue, 7 Aug 2012 04:41:33 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                    <comment id="13504825" author="mikemccand" created="Tue, 27 Nov 2012 18:36:38 +0000">&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;This patch looks great!&lt;/p&gt;

&lt;p&gt;It cleans up BS2 and specialized term conjunction scorer, and makes more accurate decisions about which sub-scorer to enumerate first (no more first docID heuristic).&lt;/p&gt;

&lt;p&gt;We could also use the cost estimate to sometimes let BooleanScorer take MUST clauses.&lt;/p&gt;</comment>
                    <comment id="13504827" author="mikemccand" created="Tue, 27 Nov 2012 18:39:02 +0000">&lt;p&gt;Maybe we should make it .estimateHitCount instead of estimateCost, so it's more explicit?&lt;/p&gt;</comment>
                    <comment id="13504854" author="rcmuir" created="Tue, 27 Nov 2012 19:17:00 +0000">&lt;p&gt;There's a lot of things i'm not happy with in the patch, i think it was more of an exploration of ideas.&lt;/p&gt;

&lt;p&gt;I think we could split out the cost/hitcount/conjunctionscorer idea into a separate issue as a start?&lt;/p&gt;

&lt;p&gt;This would keep things contained.&lt;/p&gt;</comment>
                    <comment id="13527989" author="simonw" created="Mon, 10 Dec 2012 15:08:22 +0000">&lt;p&gt;Parts of this issue are contained in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4607" title="Add estimateDocCount to DocIdSetIterator"&gt;&lt;del&gt;LUCENE-4607&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe we should make it .estimateHitCount instead of estimateCost, so it's more explicit?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I changed this to estimateDocCount since its now on DISI&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12623154">LUCENE-4607</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12537160" name="LUCENE-4236.patch" size="74699" author="rcmuir" created="Thu, 19 Jul 2012 11:20:44 +0100"/>
                    <attachment id="12537137" name="LUCENE-4236.patch" size="47482" author="rcmuir" created="Thu, 19 Jul 2012 07:18:39 +0100"/>
                    <attachment id="12537121" name="LUCENE-4236.patch" size="6127" author="rcmuir" created="Thu, 19 Jul 2012 03:01:43 +0100"/>
                    <attachment id="12537115" name="LUCENE-4236.patch" size="5969" author="rcmuir" created="Thu, 19 Jul 2012 02:53:56 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 19 Jul 2012 02:01:53 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243733</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23461</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4159] Code review before 4.0 release</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4159</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Before the 4.0 release I think it makes sense to plan for a (Lucene and Solr) comprehensive code review in order to improve APIs, performance and code style.&lt;/p&gt;</description>
                <environment/>
            <key id="12595397">LUCENE-4159</key>
            <summary>Code review before 4.0 release</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="teofili">Tommaso Teofili</reporter>
                        <labels>
                    </labels>
                <created>Thu, 21 Jun 2012 09:10:06 +0100</created>
                <updated>Fri, 10 May 2013 00:05:10 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13412305" author="hossman" created="Thu, 12 Jul 2012 00:03:46 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429713" author="rcmuir" created="Tue, 7 Aug 2012 04:41:29 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 11 Jul 2012 23:03:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243808</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23536</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4126] Remove FieldType copy constructor</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4126</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently FieldTypes can be created using new FieldType(someOtherFieldType) which copies the properties and allows them to then changed.  This reduces readability since it hides what properties someOtherFieldType has enabled.  We should encourage users (and ourselves) to explicitly state what properties are enabled so to prevent any surprises. &lt;/p&gt;</description>
                <environment/>
            <key id="12560042">LUCENE-4126</key>
            <summary>Remove FieldType copy constructor</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="cmale">Chris Male</reporter>
                        <labels>
                    </labels>
                <created>Sat, 9 Jun 2012 13:50:27 +0100</created>
                <updated>Fri, 10 May 2013 00:05:11 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13292335" author="rcmuir" created="Sat, 9 Jun 2012 14:14:52 +0100">&lt;p&gt;Lets play out a few typical realistic scenarios that are not too expert:&lt;br/&gt;
1. user has a text field and wants to enable term vectors (so they can use highlighting/MLT)&lt;br/&gt;
2. user has a string field and wants to enable norms (so they can use index-time boosting)&lt;/p&gt;

&lt;p&gt;what does the before/after picture look like here? Is it easier? Is it trappy?&lt;/p&gt;</comment>
                    <comment id="13292344" author="cmale" created="Sat, 9 Jun 2012 15:09:16 +0100">&lt;p&gt;Good question.  &lt;/p&gt;

&lt;p&gt;Currently specifying your own FieldType means you have to use &lt;tt&gt;Field&lt;/tt&gt; rather than &lt;tt&gt;StringField&lt;/tt&gt; or &lt;tt&gt;TextField&lt;/tt&gt; as neither of them accept a FieldType.  This is messy and basically the same problem that &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4101" title="Remove XXXField.TYPE_STORED"&gt;&lt;del&gt;LUCENE-4101&lt;/del&gt;&lt;/a&gt; is fixing for storing.  Hmm..&lt;/p&gt;

&lt;p&gt;In relation to the the copy constructor issue, for scenario #1 currently users could do:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
FieldType myNewFieldType = &lt;span class="code-keyword"&gt;new&lt;/span&gt; FieldType(TextField.TYPE_STORED);
myNewFieldType.setStoreTermVectors(&lt;span class="code-keyword"&gt;true&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the copy constructor removed, they would need to do:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
FieldType myNewFieldType = &lt;span class="code-keyword"&gt;new&lt;/span&gt; FieldType();
myNewFieldType.setIndexed(...);
myNewFieldType.setStored(...);
... &lt;span class="code-comment"&gt;// set other properties
&lt;/span&gt;myNewFieldType.setStoreTermVectors(&lt;span class="code-keyword"&gt;true&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the current case the user can easily rely on the pre-existing type and just change the property they're interested in.  In their code it would be clear what was changed since no other properties need to be set.  At the same time any changes to the pre-existing type would flow into their type without them being notified and they cannot scan over their code and see exactly what properties are set for a field, they'd have to look up the definition.&lt;/p&gt;

&lt;p&gt;With the copy constructor removed, we make changing a property more of a task for the user since they would need to define all the properties themselves.  Yet at the same time they would be protected from any changes to pre-existing types and they could see in their code exactly what properties were set.  But it also wouldn't be so easily to see which property was specifying changed.&lt;/p&gt;

&lt;p&gt;I'm not really sure what's best, what do you think?&lt;/p&gt;</comment>
                    <comment id="13292346" author="rcmuir" created="Sat, 9 Jun 2012 15:15:08 +0100">&lt;blockquote&gt;
&lt;p&gt;Currently specifying your own FieldType means you have to use Field rather than StringField or TextField as neither of them accept a FieldType. This is messy and basically the same problem that &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4101" title="Remove XXXField.TYPE_STORED"&gt;&lt;del&gt;LUCENE-4101&lt;/del&gt;&lt;/a&gt; is fixing for storing. Hmm..&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually i think this is ok: these are still expertish things but just not totally crazy.&lt;/p&gt;

&lt;p&gt;I dont understand the benefit removing this: having someone create a FieldType from scratch is crazy. Its way too ridiculous: too easy to forget to set tokenized to true or whatever.&lt;br/&gt;
Creating a FieldType from scratch is pretty much only useful for committers or people extending things in super-expert ways.&lt;/p&gt;

&lt;p&gt;So I think its clear whats best: we have to keep lucene useable.&lt;/p&gt;</comment>
                    <comment id="13292350" author="cmale" created="Sat, 9 Jun 2012 15:25:53 +0100">&lt;p&gt;I can agree with that&lt;/p&gt;</comment>
                    <comment id="13292354" author="rcmuir" created="Sat, 9 Jun 2012 15:39:25 +0100">&lt;p&gt;Also i think today, anyone that wants to do things the way you describe can just create the FieldType from scratch already?&lt;/p&gt;

&lt;p&gt;they can do this and set everything from scratch, add the field twice, whatever they want &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;But if we remove the ability to do simpler things like 'i want a TextField with term vectors enabled' or 'I want a StringField with index-time boosts', then I think thats a big loss to less advanced users, with no gain to the experts who can already do things from scratch anyway if they prefer to do that.&lt;/p&gt;</comment>
                    <comment id="13412308" author="hossman" created="Thu, 12 Jul 2012 00:03:46 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429715" author="rcmuir" created="Tue, 7 Aug 2012 04:41:30 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 9 Jun 2012 13:14:52 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243841</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23569</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4121] Standardize ramBytesUsed/sizeInBytes/memSize</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4121</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We should standardize the names of the methods we use to estimate the sizes of objects in memory and on disk. (cf. discussion on dev@lucene &lt;a href="http://search-lucene.com/m/VbXSx1BP60G" class="external-link"&gt;http://search-lucene.com/m/VbXSx1BP60G&lt;/a&gt;).&lt;/p&gt;</description>
                <environment/>
            <key id="12559856">LUCENE-4121</key>
            <summary>Standardize ramBytesUsed/sizeInBytes/memSize</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="jpountz">Adrien Grand</reporter>
                        <labels>
                    </labels>
                <created>Fri, 8 Jun 2012 16:16:54 +0100</created>
                <updated>Fri, 10 May 2013 00:05:11 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13291819" author="jpountz" created="Fri, 8 Jun 2012 16:18:49 +0100">&lt;p&gt;I am currently thinking of &lt;tt&gt;memSize&lt;/tt&gt; for the in-memory size and &lt;tt&gt;diskSize&lt;/tt&gt; for the on-disk size.&lt;/p&gt;</comment>
                    <comment id="13291831" author="simonw" created="Fri, 8 Jun 2012 16:51:33 +0100">&lt;p&gt;Adrien I think we can do that for 4.0 too though&lt;/p&gt;</comment>
                    <comment id="13291834" author="jpountz" created="Fri, 8 Jun 2012 16:54:20 +0100">&lt;p&gt;Updated fix version.&lt;/p&gt;</comment>
                    <comment id="13294556" author="jpountz" created="Wed, 13 Jun 2012 17:52:39 +0100">&lt;p&gt;First version of the patch.&lt;/p&gt;

&lt;p&gt;I first tried to replace the size methods by either &lt;tt&gt;memSize&lt;/tt&gt; or &lt;tt&gt;diskSize&lt;/tt&gt; depending on what they measured. But I was not happy with the fact that some &lt;tt&gt;diskSize&lt;/tt&gt; methods might actually measure in-memory sizes when the underlying directory implementation is a &lt;tt&gt;RAMDirectory&lt;/tt&gt;. So I ended up with 3 method names:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;memSize, for in-memory usage,&lt;/li&gt;
	&lt;li&gt;diskSize, for on-disk size (hybrid data structures may implement both methods),&lt;/li&gt;
	&lt;li&gt;byteSize for abstract types (&lt;tt&gt;SegmentInfo&lt;/tt&gt;, &lt;tt&gt;MergePolicy&lt;/tt&gt;, ...). In that case, the &lt;tt&gt;byteSize&lt;/tt&gt; may actually be a &lt;tt&gt;memSize&lt;/tt&gt; or a &lt;tt&gt;diskSize&lt;/tt&gt; depending on the directory implementation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Or maybe we could still use &lt;tt&gt;diskSize&lt;/tt&gt;, RAMDirectory being a special case. &lt;tt&gt;diskSize&lt;/tt&gt; would even make sense for &lt;tt&gt;CachingRAMDirectory&lt;/tt&gt; actually.&lt;/p&gt;</comment>
                    <comment id="13412291" author="hossman" created="Thu, 12 Jul 2012 00:03:43 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429723" author="rcmuir" created="Tue, 7 Aug 2012 04:41:35 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12532001" name="LUCENE-4121.patch" size="55036" author="jpountz" created="Wed, 13 Jun 2012 17:52:39 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 8 Jun 2012 15:51:33 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243846</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23574</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4100] Maxscore - Efficient Scoring</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4100</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;At Berlin Buzzwords 2012, I will be presenting 'maxscore', an efficient algorithm first published in the IR domain in 1995 by H. Turtle &amp;amp; J. Flood, that I find deserves more attention among Lucene users (and developers).&lt;br/&gt;
I implemented a proof of concept and did some performance measurements with example queries and lucenebench, the package of Mike McCandless, resulting in very significant speedups.&lt;/p&gt;

&lt;p&gt;This ticket is to get started the discussion on including the implementation into Lucene's codebase. Because the technique requires awareness about it from the Lucene user/developer, it seems best to become a contrib/module package so that it consciously can be chosen to be used.&lt;/p&gt;</description>
                <environment/>
            <key id="12559049">LUCENE-4100</key>
            <summary>Maxscore - Efficient Scoring</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="spo">Stefan Pohl</reporter>
                        <labels>
                        <label>api-change</label>
                        <label>patch</label>
                        <label>performance</label>
                    </labels>
                <created>Sat, 2 Jun 2012 16:38:54 +0100</created>
                <updated>Fri, 10 May 2013 00:05:11 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                <component>core/query/scoring</component>
                <component>core/search</component>
                        <due/>
                    <votes>6</votes>
                        <watches>9</watches>
                                                                                  <comments>
                    <comment id="13287963" author="spo" created="Sat, 2 Jun 2012 16:47:52 +0100">&lt;p&gt;Attached is a tarball that includes maxscore code (to be unpacked in /lucene/contrib/), and a patch that integrates it into core Lucene (for now, basis for both is Lucene40 trunk r1300967).&lt;/p&gt;

&lt;p&gt;From the README, included in the tarball:&lt;br/&gt;
This contrib package implements the 'maxscore' optimization, orginally presented by in the IR domain in 1995 by H. Turtle &amp;amp; J. Flood.&lt;/p&gt;

&lt;p&gt;If you'd like to play with this implementation, for instance, to estimate its usefulness for your kind of queries and index data, follow these steps:&lt;br/&gt;
1) Build a normal Lucene40 index with your data&lt;br/&gt;
2) Rewrite this index using the main method of the class&lt;br/&gt;
   org.apache.lucene.index.IndexRewriter&lt;br/&gt;
   with source and destination directories as arguments. This class will iterate over your index segments, parse them, compute a maxscore for each term using collection statistics of the source index and write them to the destination directory using the Lucene40Maxscore codec. The resulting index should be slightly bigger. Currently, Lucene's DefaultSimilarity will be used to estimate maxscores, meaning that this has to be the Similarity used at querying time for maxscore to be effective.&lt;br/&gt;
3) Apply the patch to a checkout of Lucene4 trunk revision 1300967 and place the maxscore code directory below /lucene/contrib/.&lt;br/&gt;
4) After the patch, there should be the required logic in org.apache.lucene.search.BooleanQuery to use the MaxscoreScorer on the index in 2) when the index is searched as usual:&lt;/p&gt;

&lt;p&gt;   int topk = 10;&lt;br/&gt;
   searcher.setSimilarity(new DefaultSimilarity());&lt;br/&gt;
   Query q = queryparser.parse("t1 t2 t3 t4");&lt;br/&gt;
   MaxscoreDocCollector ms_coll = new MaxscoreDocCollector(topk);&lt;br/&gt;
   searcher.search(q, ms_coll);&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Your index at 1) does not have to be 'optimized' (it does not have to consist of one index segment only). In fact, maxscore can be more efficient with multiple segments because multiple maxscores are computed for many frequent terms for subsets of documents, resulting in tighter bounds and more effective pruning.&lt;/li&gt;
	&lt;li&gt;Don't expect totalHits to return the same counts as before. MaxscoreDocCollector sole purpose is to notify you about this by throwing an exception when you try to use the getter.&lt;/li&gt;
	&lt;li&gt;Currently, purely disjunctive, flat queries are supported&lt;/li&gt;
	&lt;li&gt;DefaultSimilarity tested only&lt;/li&gt;
	&lt;li&gt;@experimental !&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13406992" author="rcmuir" created="Thu, 5 Jul 2012 12:11:22 +0100">&lt;p&gt;Hello, thank you for working on this! &lt;/p&gt;

&lt;p&gt;I have just taken a rough glance at the code, and think we should probably look at what API changes would make &lt;br/&gt;
this sort of thing fit better into Lucene and it easier to implement.&lt;/p&gt;

&lt;p&gt;Random thoughts:&lt;/p&gt;

&lt;p&gt;Specifically: what you are doing in the PostingsWriter is similar to computing impacts (I don't have a copy of&lt;br/&gt;
the paper so admittedly don't know the exact algorithm you are using). But it seems to me that you are putting &lt;br/&gt;
a maxScore in the term dictionary metadata for all of the terms postings (as a float).&lt;/p&gt;

&lt;p&gt;With the tool you provide, this works because you have access to e.g. the segment's length normalization information&lt;br/&gt;
etc (your postingswriter takes a reader). But we would have to think about how to give postingswriters access to this &lt;br/&gt;
on flush... it seems possible to me though.&lt;/p&gt;

&lt;p&gt;Giving the postingswriter full statistics (e.g. docfreq) for Similarity computation seems difficult: while I think&lt;br/&gt;
we could accum this stuff in FreqProxTermsWriter before we flush to the codec, it wouldn't solve the problem at merge time,&lt;br/&gt;
so you would have to do a 2-pass merge in the codec somehow...&lt;/p&gt;

&lt;p&gt;But the alternative of splitting the "impact" (tf/norm) from the document-independent weight (e.g. IDF) isn't that pretty&lt;br/&gt;
either, because it limits the scoring systems (Similarity implementations) that could use the optimization.&lt;/p&gt;

&lt;p&gt;as many terms will be low frequency (e.g. docfreq=1), i think its not&lt;br/&gt;
worth it to encode the maxscore for these low freq terms: we could save space by omitting maxscore for low freq terms &lt;br/&gt;
and just treat it as infinitely large?&lt;/p&gt;

&lt;p&gt;the opposite problem: is it really optimal to encode maxscore for the entire term? or would it be better for high-freq&lt;br/&gt;
terms to encode maxScore for a range of postings (e.g. block). This way, you could skip over ranges of postings that cannot&lt;br/&gt;
compete (rather than limiting the optimization to an entire term). A codec could put this information into a block header,&lt;br/&gt;
or at certain intervals, into the skip data, etc.&lt;/p&gt;

&lt;p&gt;do we really need a full 4-byte float? How well would the algorithm work with degraded precision: e.g. something like&lt;br/&gt;
SmallFloat. (I think this SmallFloat currently computes a lower bound, we would have to bump to the next byte to make an upper bound).&lt;/p&gt;

&lt;p&gt;another idea: it might be nice if this optimization could sit underneath the codec, such that you dont need a special&lt;br/&gt;
Scorer. One idea here would be for your collector to set an attribute on the DocsEnum (maxScore): of course a normal&lt;br/&gt;
codec would totally ignore this and proceed as today. But codecs like this one could return NO_MORE_DOCS when postings&lt;br/&gt;
for that term can no longer compete. I'm just not positive if this algorithm can be refactored in this way, and this&lt;br/&gt;
would also require some clean way of getting these attributes from Collector -&amp;gt; Scorer -&amp;gt; DocsEnum. Currently Scorer&lt;br/&gt;
is in the way here &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Just some random thoughts, I'll try to get a copy of this paper so I have a better idea whats going on with this particular&lt;br/&gt;
optimization...&lt;/p&gt;</comment>
                    <comment id="13407472" author="rcmuir" created="Thu, 5 Jul 2012 21:46:19 +0100">&lt;p&gt;I spun off a sub-issue (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4198" title="Allow codecs to index term impacts"&gt;LUCENE-4198&lt;/a&gt;) to see how we can first fix this Codec API so that&lt;br/&gt;
you don't need an IndexRewriter and this patch could work "live".&lt;/p&gt;</comment>
                    <comment id="13412304" author="hossman" created="Thu, 12 Jul 2012 00:03:45 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13413055" author="rcmuir" created="Thu, 12 Jul 2012 19:34:04 +0100">&lt;blockquote&gt;
&lt;p&gt;Your index at 1) does not have to be 'optimized' (it does not have to consist of one index segment only). In fact, maxscore can be more efficient with multiple segments because multiple maxscores are computed for many frequent terms for subsets of documents, resulting in tighter bounds and more effective pruning.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I've been thinking about this a lot lately: while what you say is true, thats because you reprocess all segments with IndexRewriter (which is fine for a static collection).&lt;/p&gt;

&lt;p&gt;But this algorithm in general is not rank safe with incremental indexing: the problem is that when doing actual scoring,&lt;br/&gt;
scores consist of per-segment/within document stats (term frequency, document length), but also are affected by collection-wide&lt;br/&gt;
statistics from many other segments (IDF, average document length, ...) or even machines in a distributed collection.&lt;/p&gt;

&lt;p&gt;So I think for this to work and remain rank-safe, we cannot write the entire score into the segment, because the score&lt;br/&gt;
at actual search time is dependent on all the other segments being searched. Instead I think this can only work when&lt;br/&gt;
we can easily factor out an impact (e.g. in the case of DefaultSimilarity the indexed maxscore excludes the IDF component,&lt;br/&gt;
this is instead multiplied in at search time).&lt;/p&gt;

&lt;p&gt;I don't see how it can be rank-safe with algorithms like BM25 and incremental indexing, where parameters like average document&lt;br/&gt;
length are not simple multiplicative factors into the formula: and determine exactly how important tf versus document length play&lt;br/&gt;
a role in the score, but I'll think about it some more.&lt;/p&gt;</comment>
                    <comment id="13413246" author="spo" created="Thu, 12 Jul 2012 22:33:42 +0100">&lt;p&gt;Thanks for the feedback! You're on spot with everything you're saying.&lt;/p&gt;

&lt;p&gt;Yes, the methods as suggested in the different papers have (semi-)static indexes in mind, that is, such that batch-index many new documents, then recompute maxscores (hence, IndexRewriter) and roll out the new version of the indexes. This is a Lucene use-case common to many large installations (or part thereof) and as such important. Moreover, this approach can easily be generalized to the other Similarities, without that they necessarily have to know about maxscore, and can be simplified by some minor API changes within Lucene. The PoC code as-is might be of help to showcase dependencies in general, and such that currently are not well supported within Lucene (because there was no need for it yet).&lt;/p&gt;

&lt;p&gt;If you really want to go the full distance: I already thought about doing maxscore live and got some ideas in this regard, see below.&lt;/p&gt;

&lt;p&gt;Comments to your thoughts:&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;PostingsWriter&amp;#93;&lt;/span&gt;&lt;br/&gt;
You're right. For simplicity, I was computing each term's overall contribution (as explained in the talk), including all but query-dependent factors. You can consider this as un-quantized impacts (in the sense of Anh et al.) which necessitates a second pass over a static index, hence IndexRewriter.&lt;/p&gt;

&lt;p&gt;As a side note: I noticed a drop in the PKLookup benchmark, suggesting that it might be better not to extend the size of dictionary items, but to store maxscores in the beginning of inverted lists, or next to skip data. This effect should be smaller or disappear though when maxscores are not stored for many terms.&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;Length normalization&amp;#93;&lt;/span&gt;&lt;br/&gt;
Yes, this might be a necessary dependency. It should be a general design-principle though to have as many as possible statistics at hand everywhere, as long as it doesn't hurt performance in terms of efficiency.&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;splitting impacts / incremental indexing&amp;#93;&lt;/span&gt;&lt;br/&gt;
Yes, this would be more intrusive, requiring Similarity-dependent maxscore computations. Here is how it could work:&lt;br/&gt;
Very exotic scoring functions simply don't have to support maxscore and will thus fall back to the current score-all behaviour.&lt;br/&gt;
DefaultSimilarity is simple, but BM25 and LMDirichlet can't as easily be factored out, as you correctly point out, but we could come up with bounds for collection statistics (those that go into the score) within which it is safe to use maxscore, otherwise we fallback to score-all until a merge occurs, or we notify the user to better do a merge/optimize, or Lucene does a segment-rewrite with new maxscore and bound computations on basis of more current collection stats. I got first ideas for an algorithm to compute these bounds.&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;docfreq=1 treatment&amp;#93;&lt;/span&gt;&lt;br/&gt;
Definitely agree. Possibly, terms with docfreq &amp;lt; x=10? could not store a maxscore. x configurable and default to be evaluated; x should be stored in index so that it can be determined which terms don't contain maxscores.&lt;br/&gt;
Having a special treatment for these terms (not considering them for exclusion within the algorithm) allows for easier exchange of the core of the algorithm to get the WAND algorithm, or also to ignore a maxscore for a term for which collection stats went out of bounds.&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;maxscores per posting ranges&amp;#93;&lt;/span&gt;&lt;br/&gt;
+1. As indicated in the description, having multiple maxscores per term can be more efficient, possibly leading to tighter bounds and more skipping. Chakrabarti'11 opted for one extreme, computing a maxscore for each compressed posting block, whereas the optimal choice might have been a multiple of blocks, or a postings range not well aligned with block size.&lt;br/&gt;
Optimal choice will be very dependent on skip list implementation and its parameters, but also posting de-compression overhead.&lt;br/&gt;
The question is how to get access to this codec-dependent information inside of the scoring algorithm, tunneled through the TermQuery?&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;store &amp;lt;4 bytes per maxscore&amp;#93;&lt;/span&gt;&lt;br/&gt;
Possible. As long as the next higher representable real number is stored (ceil, not floor), no docs will be missed and the algorithm remains correct. But because of more loose bounds the efficiency gain will be affected at some point with too few bits.&lt;/p&gt;

&lt;p&gt;If the score is anyway factored out, it might be better to simply store all document-dependent stats (TF, doclen) of the document with the maximum score contribution (as ints) instead of one aggregate intermediate float score contribution.&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;implementation inside codec&amp;#93;&lt;/span&gt;&lt;br/&gt;
Please be aware that while terms are at some point excluded from merging, they still are advanced to the docs in other lists to gain complete document knowledge and compute exact scores. Maxscores can also be used to minimize how often this happens, but the gains are often compensated by the more complex scoring. Still having to skip inside of excluded terms complicates your suggested implementation. But we definitely should consider architecture alternatives. The MaxscoreCollector, for instance, does currently only have a user interface function, keeping track of the top-k and their entry threshold could well be done inside the Maxscorer.&lt;br/&gt;
I was thinking though to extend the MaxscoreCollector to provide different scoring information, e.g. an approximation of the number of hits next to the actual number of scored documents (currently totalHits).&lt;/p&gt;</comment>
                    <comment id="13413335" author="rcmuir" created="Fri, 13 Jul 2012 00:48:12 +0100">&lt;blockquote&gt;
&lt;p&gt;As a side note: I noticed a drop in the PKLookup benchmark, suggesting that it might be better not to extend the size of dictionary items, but to store maxscores in the beginning of inverted lists, or next to skip data. This effect should be smaller or disappear though when maxscores are not stored for many terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wouldn't worry about this, I noticed a few things that might speed that up:&lt;/p&gt;

&lt;p&gt;1. Currently it does writeVInt(Float.floatToIntBits(term.maxscoreScore)) . But I think this should be writeInt, not writeVInt?&lt;br/&gt;
So I think currently we often write 5 bytes here, with all the vint checks for each byte, and as an Int it would always be 4 and faster.&lt;br/&gt;
2. Yes, with low freq terms (e.g. docFreq &amp;lt; skipMinimum), its probably best to just omit this at both read and write time. Then PK lookup would be fine.&lt;br/&gt;
3. As far as &amp;lt; 4 bytes ceiling, my motivation there was not to save in the term dictionary, but instead to make these smaller and allow us to add these at regular intervals. We can take advantage of a few things, e.g. it should never be a negative number for a &lt;br/&gt;
well-formed Similarity (i think that would screw up the algorithm looking at your tests anyway).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;DefaultSimilarity is simple, but BM25 and LMDirichlet can't as easily be factored out, as you correctly point out, but we could come up with bounds for collection statistics (those that go into the score) within which it is safe to use maxscore, otherwise we fallback to score-all until a merge occurs, or we notify the user to better do a merge/optimize, or Lucene does a segment-rewrite with new maxscore and bound computations on basis of more current collection stats. I got first ideas for an algorithm to compute these bounds.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, I'm not sure I totally see how the bounds computation can work, but if it can we might be ok in general. If the different segments are somewhat homogeneous then these stats should pretty much be very close anyway.&lt;/p&gt;

&lt;p&gt;The other idea i had was more intrusive, adding a computeImpact() etc to Similarity or whatever.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the score is anyway factored out, it might be better to simply store all document-dependent stats (TF, doclen) of the document with the maximum score contribution (as ints) instead of one aggregate intermediate float score contribution.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That might be a good idea. with TF as a vint and doclen as a byte, we would typically only have two bytes but not actually lose any information (by default, all these sims encode doclen as a byte anyway).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;span class="error"&gt;&amp;#91;implementation inside codec&amp;#93;&lt;/span&gt;&lt;br/&gt;
Please be aware that while terms are at some point excluded from merging, they still are advanced to the docs in other lists to gain complete document knowledge and compute exact scores. Maxscores can also be used to minimize how often this happens, but the gains are often compensated by the more complex scoring. Still having to skip inside of excluded terms complicates your suggested implementation. But we definitely should consider architecture alternatives. The MaxscoreCollector, for instance, does currently only have a user interface function, keeping track of the top-k and their entry threshold could well be done inside the Maxscorer.&lt;br/&gt;
I was thinking though to extend the MaxscoreCollector to provide different scoring information, e.g. an approximation of the number of hits next to the actual number of scored documents (currently totalHits).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My current line of thinking is even crazier, but I don't yet have anything close to a plan.&lt;/p&gt;

&lt;p&gt;As a start, I do think that IndexSearcher.search() methods should take a "Score Mode" of sorts from the user (some enum), which would allow Lucene to do less work if its not necessary. We would pass this down via Weight.scorer() as a parameter... solely&lt;br/&gt;
looking at the search side I think this would open up opportunities in general for us to optimize things: e.g. instantiate the appropriate Collector impl, and for Weights to create the most optimal Scorers. Not yet sure how it would tie into the code API.&lt;/p&gt;

&lt;p&gt;I started hacking up on a prototype that looks like this (I might have tried to refactor too hard also shoving the Sort options in here...)&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;/**
 * Different modes of search.
 */
public enum ScoreMode {
  /** 
   * No guarantees that the ranking is correct,
   * the results may come back in a different order than if all
   * documents were actually scored. Total hit count may be 
   * unavailable or approximate.
   */
  APPROXIMATE,
  /** 
   * Ranking is the same as {@link COMPLETE}, but total hit 
   * count may be unavailable or approximate.
   */
  SAFE,
  /**
   * Guarantees complete iteration over all documents, but scores
   * may be unavailable.
   */
  COMPLETE_NO_SCORES,
  /**
   * Guarantees complete iteration over all documents, and scores
   * will be computed, but the maximum score may be unavailable 
   */
  COMPLETE_NO_MAX_SCORE,
  /**
   * Guarantees complete iteration and scoring of all documents.
   */
  COMPLETE,
};
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13429696" author="rcmuir" created="Tue, 7 Aug 2012 04:41:21 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                    <comment id="13540741" author="otis" created="Sat, 29 Dec 2012 03:18:30 +0000">&lt;p&gt;Stefan, this sounds exciting.  What sort of speedups did you get?  Are you still looking to include this into Lucene (4.1)?&lt;/p&gt;</comment>
                    <comment id="13542058" author="spo" created="Wed, 2 Jan 2013 08:38:53 +0000">&lt;p&gt;Otis, thank you for your interest and I wish everyone a happy New Year!&lt;/p&gt;

&lt;p&gt;Regarding speedups, have a look at &lt;a href="http://vimeo.com/44300228" class="external-link"&gt;http://vimeo.com/44300228&lt;/a&gt; from 12 minutes onwards. It obviously depends on your mix of queries and collection size, but on average more than 100% should be achievable for large collections and typical query sets, and much higher speedups for problem queries (many frequent terms).&lt;/p&gt;

&lt;p&gt;The contribution as-is (after adaptation to latest Lucene API/code-base) could already be included as a separate Lucene module for people to use who can live with its current limitations (for static indexes only, smaller totalHitCount).&lt;br/&gt;
In my spare time (unfortunately not much of that recently), I continue working on different approaches to make this more general, but this is pure experimentation and any production-ready offspring of that will take longer than 4.x and might require API changes (such as the ones suggested above by Robert), so perhaps 5.0 is a good aim.&lt;/p&gt;

&lt;p&gt;I suggest to continue rolling this ticket forward, or only attach version 5.0 to it.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12530651" name="contrib_maxscore.tgz" size="26609" author="spo" created="Sat, 2 Jun 2012 16:47:52 +0100"/>
                    <attachment id="12530652" name="maxscore.patch" size="10713" author="spo" created="Sat, 2 Jun 2012 16:47:52 +0100"/>
                </attachments>
            <subtasks>
            <subtask id="12597647">LUCENE-4198</subtask>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 5 Jul 2012 11:11:22 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243867</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23595</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4089] fix or document termsIndexInterval/Divisor for 4.0</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4089</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;There are a few parameters on IndexWriterConfig/DirectoryReader that are going to be confusing unless we do something about it: at least documentation at the minimum:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;IWC.termsIndexInterval: really a codec parameter, actually ignored by 4.0's default impl (BlockTree)&lt;/li&gt;
	&lt;li&gt;IWC.readerDivisor/DirectoryReader.divisor: really two things, if its -1 it means "don't load terms index", and this is respected by the current impls. Otherwise, it means "sample the terms index", and this is also actually ignored by 4.0's default impl (BlockTree)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think people will be confused if they set these things and they do nothing. As far as fixing, I took a stab at this and its an annoyingly big change. But this is the rough sketch of one idea i had so far:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;remove interval: its only applicable if you customize codec and select a different terms index/dict impl anyway, so you can just pass this to FixedGap or whatever yourself.&lt;/li&gt;
	&lt;li&gt;divisor: generalize this into something simple like a Map&amp;lt;String,String&amp;gt; of codec "parameters" that you set on IWC/IR. split divisor from "don't load terms index". define these as constants where they belong. I got unhappy here in the "splitting" part because I wanted the divisor part in TermsIndexReaderBase, but that doesnt extend FieldsProducer (where i wanted the "don't load" part) and wrap the terms dict, instead its backwards and terms dict wraps the TermsIndexReaderBase... maybe we should fix that too? I think this confusing the way it is but I didnt look at how difficult this would be.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12558530">LUCENE-4089</key>
            <summary>fix or document termsIndexInterval/Divisor for 4.0</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Wed, 30 May 2012 04:26:30 +0100</created>
                <updated>Fri, 10 May 2013 00:05:11 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13412319" author="hossman" created="Thu, 12 Jul 2012 00:03:47 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13428180" author="rcmuir" created="Fri, 3 Aug 2012 16:21:44 +0100">&lt;p&gt;I committed some javadocs in r1369022 as a temporary solution.&lt;/p&gt;</comment>
                    <comment id="13429702" author="rcmuir" created="Tue, 7 Aug 2012 04:41:23 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12614501">LUCENE-4526</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 11 Jul 2012 23:03:47 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>243878</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23606</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4057] Add Codec.merge()</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4057</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently individual codec components can override/optimize merging, e.g. the stored fields impl&lt;br/&gt;
uses bulk copying when possible, and so on.&lt;/p&gt;

&lt;p&gt;SegmentMerger contains the logic for merging the different codec components, for example it does&lt;br/&gt;
mergeFieldInfos(), then mergeFields(), mergeTerms(), and so on. Each of these methods interacts&lt;br/&gt;
with the codec apis to finish the merge.&lt;/p&gt;

&lt;p&gt;I think it would be cleaner if SegmentMerger called a new method, Codec.merge(), which contained&lt;br/&gt;
this logic instead. This way someone could customize this process. I think we could probably even &lt;br/&gt;
push some of the impl-dependent stuff (like matchingSegmentReaders) into the impl and out of SegmentMerger.&lt;/p&gt;

&lt;p&gt;Setting this for 4.1, I think it would be a nice cleanup but I don't plan on working on this immediately, &lt;br/&gt;
and I think we can do this in a backwards compatible way in a minor release.&lt;/p&gt;</description>
                <environment/>
            <key id="12555147">LUCENE-4057</key>
            <summary>Add Codec.merge()</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Mon, 14 May 2012 16:52:14 +0100</created>
                <updated>Fri, 10 May 2013 00:05:11 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13274695" author="mikemccand" created="Mon, 14 May 2012 16:57:21 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 14 May 2012 15:57:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>239401</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23638</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-4042] New snowball stemmers (Irish gaelic and Czech)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-4042</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;New stemmers have been added to snowball (Irish gaelic and Czech).&lt;/p&gt;</description>
                <environment/>
            <key id="12554324">LUCENE-4042</key>
            <summary>New snowball stemmers (Irish gaelic and Czech)</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="5" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.png">Trivial</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="dweiss">Dawid Weiss</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 May 2012 14:54:04 +0100</created>
                <updated>Fri, 10 May 2013 00:05:12 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13270511" author="rcmuir" created="Tue, 8 May 2012 16:05:19 +0100">&lt;p&gt;We have the irish one already, Jim contributed that on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3883" title="Analysis for Irish"&gt;&lt;del&gt;LUCENE-3883&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I verified the .sbl is the same and already removed our local copy.&lt;/p&gt;

&lt;p&gt;We should add the Czech one imo. It differs from cz.CzechStemmer.java in&lt;br/&gt;
that it implements the more aggressive variant, stemming derivational&lt;br/&gt;
endings etc, so it gives users a choice.&lt;/p&gt;</comment>
                    <comment id="13412312" author="hossman" created="Thu, 12 Jul 2012 00:03:47 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429709" author="rcmuir" created="Tue, 7 Aug 2012 04:41:27 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 8 May 2012 15:05:19 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>238556</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23655</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3997] join module should not depend on grouping module</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3997</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I think TopGroups/GroupDocs should simply be in core? &lt;/p&gt;

&lt;p&gt;Both grouping and join modules use these trivial classes, but join depends on grouping just for them.&lt;/p&gt;

&lt;p&gt;I think its better that we try to minimize these inter-module dependencies.&lt;br/&gt;
Of course, another option is to combine grouping and join into one module, but&lt;br/&gt;
last time i brought that up nobody could agree on a name. &lt;/p&gt;

&lt;p&gt;Anyway I think the change is pretty clean: its similar to having basic stuff like Analyzer.java in core,&lt;br/&gt;
so other things can work with Analyzer without depending on any specific implementing modules.&lt;/p&gt;</description>
                <environment/>
            <key id="12551472">LUCENE-3997</key>
            <summary>join module should not depend on grouping module</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Wed, 18 Apr 2012 14:56:16 +0100</created>
                <updated>Fri, 10 May 2013 00:05:12 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13256573" author="rcmuir" created="Wed, 18 Apr 2012 15:01:31 +0100">&lt;p&gt;Patch, after:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;svn mv lucene/grouping/src/java/org/apache/lucene/search/grouping/TopGroups.java lucene/core/src/java/org/apache/lucene/search

svn mv lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupDocs.java lucene/core/src/java/org/apache/lucene/search
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13256575" author="rcmuir" created="Wed, 18 Apr 2012 15:02:33 +0100">&lt;p&gt;Patch, after:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;svn mv lucene/grouping/src/java/org/apache/lucene/search/grouping/TopGroups.java lucene/core/src/java/org/apache/lucene/search

svn mv lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupDocs.java lucene/core/src/java/org/apache/lucene/search
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13256577" author="rcmuir" created="Wed, 18 Apr 2012 15:03:05 +0100">&lt;p&gt;Sorry for the duplicate upload... jira was going nutso on me&lt;/p&gt;</comment>
                    <comment id="13256740" author="martijn.v.groningen" created="Wed, 18 Apr 2012 18:40:13 +0100">&lt;p&gt;+1! Good idea. Maybe we can move FunctionValues and ValueSource from the queries modules to core? Then grouping module doesn't have to depend on the queries module.&lt;/p&gt;</comment>
                    <comment id="13256744" author="rcmuir" created="Wed, 18 Apr 2012 18:45:52 +0100">&lt;blockquote&gt;
&lt;p&gt;Maybe we can move FunctionValues and ValueSource from the queries modules to core? Then grouping module doesn't have to depend on the queries module.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 (I didn't even think of that or investigate it yet though, but at a glance it looks like the right thing to do).&lt;/p&gt;</comment>
                    <comment id="13256765" author="yseeley@gmail.com" created="Wed, 18 Apr 2012 19:07:49 +0100">&lt;p&gt;I think that if you try to make no modules depend on other modules, you'll end up just pulling pretty much everything into core.&lt;/p&gt;

&lt;p&gt;Also, the function query stuff is supposed to be marked as experimental - the notice only got added to FunctionQuery (I think?), so it should be applied to FunctionValues and ValueSource if they are moved to core.&lt;/p&gt;</comment>
                    <comment id="13256772" author="rcmuir" created="Wed, 18 Apr 2012 19:13:40 +0100">&lt;blockquote&gt;
&lt;p&gt;I think that if you try to make no modules depend on other modules, you'll end up just pulling pretty much everything into core.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think we should pull everything into core, but if we pull in the simple abstract APIs we can &lt;br/&gt;
have a more pluggable API: just like the abstract Analyzer api is in core, which Highlighter uses,&lt;br/&gt;
but you can highlight UIMA or Japanese or ICU or whatever analyzers this way...&lt;/p&gt;
</comment>
                    <comment id="13256931" author="martijn.v.groningen" created="Wed, 18 Apr 2012 21:39:30 +0100">&lt;p&gt;I also think we can move these classes to core. These are small classes and we can mark these classes as experimental.   &lt;/p&gt;

&lt;p&gt;Maybe we can even make this classes 'lighter' by only moving the public methods to core (maybe as interface?). E.g. ValueSource would have all the public methods in core and a BaseValueSource (Or AbstractValueSource) in the queries module that contains ValueSourceComparatorSource and ValueSourceComparator. Just an idea.&lt;/p&gt;

&lt;p&gt;I'll create a new issue to not make grouping module depend on the queries module.&lt;/p&gt;</comment>
                    <comment id="13257191" author="mikemccand" created="Thu, 19 Apr 2012 04:23:48 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13257942" author="cmale" created="Fri, 20 Apr 2012 01:34:51 +0100">&lt;p&gt;I do echo Yonik's concern here, I don't think we should prevent inter-module dependencies.  I think if we move something to lucene/core, it should be because we think its a core API/concept, not just because its used by multiple modules.  Analyzer fits into that category, it belongs in core because it is a core concept.  &lt;/p&gt;

&lt;p&gt;Do we feel the same about TopGroups and GroupDocs? I kind of think we do.  But we should move them for that reason, not just to remove the dependency.&lt;/p&gt;</comment>
                    <comment id="13257948" author="cmale" created="Fri, 20 Apr 2012 01:40:38 +0100">&lt;blockquote&gt;
&lt;p&gt;Of course, another option is to combine grouping and join into one module, but&lt;br/&gt;
last time i brought that up nobody could agree on a name.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If that is the better option, lets do that.  The name seems less important at this stage, we can call it grouping-join if needs be.&lt;/p&gt;</comment>
                    <comment id="13258822" author="martijn.v.groningen" created="Sat, 21 Apr 2012 10:49:23 +0100">&lt;p&gt;The reason that joining and grouping are different modules is that these are different functionalities. However these functionalities do overlap a bit with each other. Both joining and grouping can be used for a parent child like search. I'm not sure what would be a good option. Joining does use grouping's TopGroups and GroupDocs... If we are going to have a combined module maybe we should name it relational module or parent child module?&lt;/p&gt;</comment>
                    <comment id="13259074" author="mikemccand" created="Sun, 22 Apr 2012 13:20:12 +0100">&lt;p&gt;I don't think we should combine the two modules.&lt;/p&gt;

&lt;p&gt;While they do share a couple classes (to represent a 'grouped' result), the two functions (joining and grouping) are really orthogonal: you can join w/o doing grouping, and you can group w/o doing joining.&lt;/p&gt;

&lt;p&gt;I think we should move TopGroups/GroupDocs to core.&lt;/p&gt;</comment>
                    <comment id="13259075" author="cmale" created="Sun, 22 Apr 2012 13:33:34 +0100">&lt;p&gt;To me they seem to share a lot of similarities and the fact they both use the 'grouped' result notion is an illustration of that.  &lt;/p&gt;

&lt;p&gt;While a group could consist of Documents with any kind of relationship, that kind of a relationship could be parent-child.  The nature of the relationship and what the result should consist of (if its a parent-child relationship, should the 'grouped' result be parent and children, just children or just the parent) seem to be what dictates the implementations used.  &lt;/p&gt;

&lt;p&gt;I feel that having them as a single module would allow us to build some APIs which focus on user land concepts and perhaps hide some of the implementation details and differences in the joining and grouping algorithms.&lt;/p&gt;</comment>
                    <comment id="13455896" author="steve_rowe" created="Fri, 14 Sep 2012 17:16:18 +0100">&lt;p&gt;I propose, instead of using lucene-core as the location for code used by multiple modules, that we create a (single) new module that serves this purpose, something like lucene-shared or lucene-common (though common analyzers already use this name...).&lt;/p&gt;

&lt;p&gt;That way the number of inter-module dependencies is limited, and lucene-core doesn't get roped into the act.&lt;/p&gt;</comment>
                    <comment id="13456882" author="martijn.v.groningen" created="Mon, 17 Sep 2012 09:38:57 +0100">&lt;p&gt;Steven, I think this is a good idea for the reasons you mentioned. I think the new shared module should be named 'parent-child'. This name describes the overlapping functionality both existing modules have.&lt;/p&gt;

&lt;p&gt;Directory layout:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;-- lucene
|
|--- parent-child
|         |
|         |--- grouping
|         |
|         |--- join
|
|--- ...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;          </comment>
                    <comment id="13456892" author="cmale" created="Mon, 17 Sep 2012 09:54:58 +0100">&lt;blockquote&gt;&lt;p&gt;I propose, instead of using lucene-core as the location for code used by multiple modules, that we create a (single) new module that serves this purpose, something like lucene-shared or lucene-common (though common analyzers already use this name...)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I actually created lucene-common that when I first refactored out the FunctionQuery codebase.  After some time it was decided (in an issue I can't remember) that the code would go into lucene-core.  I agree with your assessment that we shouldn't use lucene-core as a dumping ground, but we should get a discussion about this going.&lt;/p&gt;</comment>
                    <comment id="13456954" author="rcmuir" created="Mon, 17 Sep 2012 13:01:36 +0100">&lt;p&gt;Can we please not add more modules here. I'm against that, this is crazy: its only a few classes in question already. it doesnt need 3 modules...&lt;/p&gt;

&lt;p&gt;the purpose of this issue was to help simplify modules and dependencies, not make it worse.&lt;/p&gt;</comment>
                    <comment id="13456966" author="mikemccand" created="Mon, 17 Sep 2012 13:33:53 +0100">&lt;p&gt;I think moving TopGroups/GroupDocs to core is fine.  Pragmatism over purity.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12523197" name="LUCENE-3997.patch" size="13335" author="rcmuir" created="Wed, 18 Apr 2012 15:02:33 +0100"/>
                    <attachment id="12523194" name="LUCENE-3997.patch" size="13335" author="rcmuir" created="Wed, 18 Apr 2012 14:58:54 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 18 Apr 2012 17:40:13 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>236281</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23700</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3994] some nightly tests take hours</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3994</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The nightly builds are taking 4-7 hours.&lt;/p&gt;

&lt;p&gt;This is caused by a few bad apples (can be seen &lt;a href="https://builds.apache.org/job/Lucene-trunk/1896/testReport/" class="external-link"&gt;https://builds.apache.org/job/Lucene-trunk/1896/testReport/&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The top 5 are (all in analysis):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;TestSynonymMapFilter: 1 hr 54 min&lt;/li&gt;
	&lt;li&gt;TestRandomChains: 1 hr 22 min&lt;/li&gt;
	&lt;li&gt;TestRemoveDuplicatesTokenFilter: 32 min&lt;/li&gt;
	&lt;li&gt;TestMappingCharFilter: 28 min&lt;/li&gt;
	&lt;li&gt;TestWordDelimiterFilter: 22 min&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;so thats 4.5 hours right there for that run....&lt;/p&gt;</description>
                <environment/>
            <key id="12551241">LUCENE-3994</key>
            <summary>some nightly tests take hours</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 Apr 2012 16:12:33 +0100</created>
                <updated>Fri, 10 May 2013 00:05:12 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13255692" author="rcmuir" created="Tue, 17 Apr 2012 17:20:37 +0100">&lt;p&gt;Patch, removing n^2 growth in these tests, and some other tuning of atLeast.&lt;/p&gt;

&lt;p&gt;In general, when tests like this hog the cpu for so long, we lose coverage overall.&lt;/p&gt;

&lt;p&gt;I'll keep an eye on the nightlies for other cpu-hogs.&lt;/p&gt;

&lt;p&gt;Here are the new timings for analyzers/ tests after the patch.&lt;/p&gt;

&lt;p&gt;'ant test' with no multiplier:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;BUILD SUCCESSFUL
Total time: 1 minute 28 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;'ant test -Dtests.multiplier=3 -Dtests.nightly=true'&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;BUILD SUCCESSFUL
Total time: 3 minutes 15 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13255790" author="dweiss" created="Tue, 17 Apr 2012 19:17:19 +0100">&lt;p&gt;You could also update statistics &amp;#8211; remove the previous ones and run two three times, then update.&lt;/p&gt;

&lt;p&gt;Alternatively, we could have jenkins update stats and fetch these from time to time.&lt;/p&gt;</comment>
                    <comment id="13255794" author="rcmuir" created="Tue, 17 Apr 2012 19:19:46 +0100">&lt;p&gt;I think statistics are mostly useless for nightly builds: since we pass huge multipliers and such?&lt;/p&gt;

&lt;p&gt;If anything, this issue did more for the stats than any stats update could do, as these tests&lt;br/&gt;
now grow linearly instead of quadratically with the multiplier...&lt;/p&gt;</comment>
                    <comment id="13255803" author="dweiss" created="Tue, 17 Apr 2012 19:24:07 +0100">&lt;p&gt;Ok. I'll recalculate them from time to time. There is a large variance in tests anyway (this can also be computed from log stats because we can keep a history of N runs... it'd be interesting to see which tests have the largest variance).&lt;/p&gt;</comment>
                    <comment id="13255805" author="rcmuir" created="Tue, 17 Apr 2012 19:24:48 +0100">&lt;p&gt;Another thing i toned down here was the multithreaded testing in basetokenstreamtestcase,&lt;br/&gt;
there is something os-specific about freebsd's java that causes this to take a lot more time&lt;br/&gt;
than locally... thats why analysis tests take so long in nightly builds (especially with the n^2!)&lt;/p&gt;</comment>
                    <comment id="13255814" author="rcmuir" created="Tue, 17 Apr 2012 19:28:24 +0100">&lt;blockquote&gt;
&lt;p&gt;There is a large variance in tests anyway&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Like this? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://builds.apache.org/job/Lucene-trunk/1896/testReport/org.apache.lucene.index/TestIndexWriterReader/history/" class="external-link"&gt;https://builds.apache.org/job/Lucene-trunk/1896/testReport/org.apache.lucene.index/TestIndexWriterReader/history/&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13256535" author="rcmuir" created="Wed, 18 Apr 2012 14:15:47 +0100">&lt;p&gt;This still occurs.&lt;/p&gt;

&lt;p&gt;I profiled the slow tests and found its because of the huge 1GB linedocs file. The problem is opening this 1GB zipped file and seeking to a random place (which is what LineDocs does), is really costly.&lt;/p&gt;

&lt;p&gt;so all the time is spent in GZIPInputStream.inflateBytes!&lt;/p&gt;

&lt;p&gt;I will temporary disable the huge file for nightly builds.&lt;/p&gt;
</comment>
                    <comment id="13256548" author="rcmuir" created="Wed, 18 Apr 2012 14:29:19 +0100">&lt;p&gt;I'll leave the issue open, until we get the next nightly done, but this was pretty difficult to debug:&lt;/p&gt;

&lt;p&gt;Jenkins test time is now a total lie! I think its the clover time? &lt;/p&gt;

&lt;p&gt;Have a look at last nights build: &lt;a href="https://builds.apache.org/job/Lucene-Trunk/1898/" class="external-link"&gt;https://builds.apache.org/job/Lucene-Trunk/1898/&lt;/a&gt;&lt;br/&gt;
The entire build took 5 hours, yet it says tests took only 47 minutes: &lt;a href="https://builds.apache.org/job/Lucene-Trunk/1898/testReport/" class="external-link"&gt;https://builds.apache.org/job/Lucene-Trunk/1898/testReport/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Looking at the console you can see this is not the case:&lt;/p&gt;

&lt;p&gt;Actual tests:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;BUILD SUCCESSFUL
Total time: 225 minutes 56 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Clovered tests:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;BUILD SUCCESSFUL
Total time: 54 minutes 31 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Its possible i screwed this up with the nightly build changes from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3965" title="consolidate all api modules in one place and un!@$# packaging for 4.0"&gt;&lt;del&gt;LUCENE-3965&lt;/del&gt;&lt;/a&gt;. I'll investigate.&lt;/p&gt;</comment>
                    <comment id="13256550" author="dweiss" created="Wed, 18 Apr 2012 14:33:26 +0100">&lt;p&gt;I've fixed that per-suite constant suite randomization already in github but I'll need some time to push to maven central, etc. &lt;/p&gt;</comment>
                    <comment id="13256556" author="rcmuir" created="Wed, 18 Apr 2012 14:39:11 +0100">&lt;p&gt;Thanks Dawid, I am looking forward to that!&lt;/p&gt;</comment>
                    <comment id="13259762" author="mikemccand" created="Mon, 23 Apr 2012 18:41:01 +0100">&lt;blockquote&gt;&lt;p&gt;so all the time is spent in GZIPInputStream.inflateBytes!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ugh, nice find Robert!&lt;/p&gt;

&lt;p&gt;I think for nightly hudson we should just pre-gunzip the file?&lt;/p&gt;

&lt;p&gt;I was also curious if this is substantially slowing down tests for the checked-in lines file ... it's much smaller so much less seeking.  I ran a few tests (ran all lucene tests, using the python runner, with compressed vs uncompressed) and it seems to be in the noise...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12522972" name="LUCENE-3994.patch" size="6158" author="rcmuir" created="Tue, 17 Apr 2012 17:20:37 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 17 Apr 2012 18:17:19 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>236106</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23703</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3990] TestRandomChains failure caused by incorrect delegation in CharReader/CharFilter/CharStream API</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3990</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;100% reproduces for me:&lt;/p&gt;

&lt;p&gt;2&amp;gt; NOTE: reproduce with: ant test -Dtests.class=*.TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=88CA02C2BB7B1DA -Dargs="-Dfile.encoding=UTF-8"&lt;/p&gt;

&lt;p&gt;Running org.apache.lucene.analysis.core.TestRandomChains&lt;br/&gt;
FAILURE 7.22s | TestRandomChains.testRandomChains&lt;br/&gt;
   &amp;gt; Throwable #1: java.lang.AssertionError: endOffset 1 expected:&amp;lt;7&amp;gt; but was:&amp;lt;8&amp;gt;&lt;br/&gt;
   &amp;gt;    at __randomizedtesting.SeedInfo.seed(&lt;span class="error"&gt;&amp;#91;88CA02C2BB7B1DA:356D894D6CA5AC1A&amp;#93;&lt;/span&gt;:0)&lt;br/&gt;
   &amp;gt;    at org.junit.Assert.fail(Assert.java:93)&lt;br/&gt;
   &amp;gt;    at org.junit.Assert.failNotEquals(Assert.java:647)&lt;br/&gt;
   &amp;gt;    at org.junit.Assert.assertEquals(Assert.java:128)&lt;br/&gt;
   &amp;gt;    at org.junit.Assert.assertEquals(Assert.java:472)&lt;br/&gt;
   &amp;gt;    at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:165)&lt;br/&gt;
   &amp;gt;    at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:662)&lt;br/&gt;
   &amp;gt;    at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:486)&lt;br/&gt;
   &amp;gt;    at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:429)&lt;br/&gt;
   &amp;gt;    at org.apache.lucene.analysis.core.TestRandomChains.testRandomChains(TestRandomChains.java:820)&lt;/p&gt;


&lt;p&gt;The root cause of this is inconsequent override of several Reader methods in subclasses of CharFilter. We should fix this urgently, thanks to the random chains we found this bug.&lt;/p&gt;</description>
                <environment/>
            <key id="12550997">LUCENE-3990</key>
            <summary>TestRandomChains failure caused by incorrect delegation in CharReader/CharFilter/CharStream API</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="steve_rowe">Steve Rowe</reporter>
                        <labels>
                    </labels>
                <created>Sun, 15 Apr 2012 18:24:03 +0100</created>
                <updated>Fri, 10 May 2013 00:05:12 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13254479" author="rcmuir" created="Mon, 16 Apr 2012 02:31:29 +0100">&lt;p&gt;Definitely something sneaky going on...&lt;/p&gt;

&lt;p&gt;first i found a minor bug in the test. the logic for determining if we should apply&lt;br/&gt;
additional offsets checks doesn't look at the return value, so if we ever try&lt;br/&gt;
a tokenizer in brokenOffsetsComponents (even if it throws IAE and we decide not to use it)&lt;br/&gt;
then we lose offsets checks for the eventually-working-chain.&lt;/p&gt;

&lt;p&gt;seed still works after fixing this... I'll commit a fix for that (still doesnt help this problem)&lt;/p&gt;</comment>
                    <comment id="13254491" author="rcmuir" created="Mon, 16 Apr 2012 03:56:04 +0100">&lt;p&gt;The problem is that we are getting different results the first time we create the tokenstream components,&lt;br/&gt;
versus after we reset(Reader) with the same text again.&lt;/p&gt;

&lt;p&gt;The bug was introduced by Uwe Schindler in r1311358: when the reader-wrapper was changed to use CharFilter&lt;br/&gt;
instead. because of crazy CharFilter-Reader delegation.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1311358" class="external-link"&gt;http://svn.apache.org/viewvc?view=revision&amp;amp;revision=1311358&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Attached is a patch demonstrating the bug: with a standalone testcase, and backing out that change.&lt;br/&gt;
Seed now passes (in addition to the test.&lt;/p&gt;</comment>
                    <comment id="13254523" author="thetaphi" created="Mon, 16 Apr 2012 06:54:58 +0100">&lt;p&gt;Hi Robert,&lt;/p&gt;

&lt;p&gt;if this patch fixes the problem we should fix the CharFilter interface. Simply reverting my change hides the bug, sorry.&lt;/p&gt;</comment>
                    <comment id="13254527" author="thetaphi" created="Mon, 16 Apr 2012 06:57:27 +0100">&lt;p&gt;We have to fix the underlying bug. Subclassing CharFilter here is correct.&lt;/p&gt;</comment>
                    <comment id="13254539" author="thetaphi" created="Mon, 16 Apr 2012 07:45:52 +0100">&lt;p&gt;Hi Robert,&lt;/p&gt;

&lt;p&gt;I investigated the problem, its indeed crazy wrapping: The problem was that CharFilter did not override read() (without char[]). The same applied to CharReader!!! By not overriding that method it was also slowing down &lt;b&gt;all&lt;/b&gt; charfilters, because the base class Reader automatically delegates to read(char[]), creating a new char&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; every time.&lt;/p&gt;

&lt;p&gt;The attached patch fixes this to delegate correctly.&lt;/p&gt;</comment>
                    <comment id="13254544" author="dweiss" created="Mon, 16 Apr 2012 08:02:02 +0100">&lt;p&gt;I remember looking at the fact that read() is not overridden when I was working on that pattern char filter and was surprised a bit about it, but then thought maybe it's wrapped in a buffered stream someplace else (then it doesn't matter).&lt;/p&gt;

&lt;p&gt;A good way to make sure it does get proper implementation is to make read() abstract again in CharStream...&lt;/p&gt;</comment>
                    <comment id="13254545" author="thetaphi" created="Mon, 16 Apr 2012 08:03:01 +0100">&lt;p&gt;Committed trunk revision: 1326512&lt;/p&gt;</comment>
                    <comment id="13254547" author="thetaphi" created="Mon, 16 Apr 2012 08:08:45 +0100">&lt;p&gt;Dawid: I did not notice, indeed PatternCharReplaceFilter now gets a failure. I'll revert. The problem is exactly as you said. Thats completely broken, we should make that abstract!&lt;/p&gt;</comment>
                    <comment id="13254552" author="thetaphi" created="Mon, 16 Apr 2012 08:20:45 +0100">&lt;p&gt;I reverted the CharFilter changes in revision: 1326515&lt;/p&gt;

&lt;p&gt;We should really fix the broken delegation and make CharFilters require to implement both read() and read(char[], int, int)&lt;/p&gt;</comment>
                    <comment id="13254553" author="thetaphi" created="Mon, 16 Apr 2012 08:21:16 +0100">&lt;p&gt;We should fix CharFilters in trunk to not have the horrible delegating.&lt;/p&gt;</comment>
                    <comment id="13254554" author="thetaphi" created="Mon, 16 Apr 2012 08:24:16 +0100">&lt;p&gt;Robert: Sorry for the noise with heavy committing, and thanks for pointing to that bug!&lt;/p&gt;</comment>
                    <comment id="13254569" author="thetaphi" created="Mon, 16 Apr 2012 09:07:55 +0100">&lt;p&gt;I investigated why the original commit lead to the bug:&lt;br/&gt;
The current committed stuff (Robert's revert and also my latest patch), just hide a bug in MappingCharFilter that causes this. Let me explain:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;With Robert's patch (and also my current fix): read(char[],int,int) and read() both delegate to the underlying charstream.&lt;/li&gt;
	&lt;li&gt;With the original CharFilter approach, int read() was not overridden, so the default method in java.io.Reader did the following: allocate char&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, call read(buffer,0,1) -&amp;gt; MappingCharFilters's buffered read method was called. Unfortunately this method is behaving different than MappingCharFilter's one-character read(). And thats the bug here. So neither my code nor Roberts code is to blame, its MappingCharFilter or MockCharFilter that behave different in the two read methods.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13254583" author="thetaphi" created="Mon, 16 Apr 2012 09:35:13 +0100">&lt;p&gt;We should have a asserting method for CharFilter consistency. Indeed the read(char[],int,int) method in MappingCharFilter is failing horribly (which is caused by the underlying MockCharFilter somehow).&lt;/p&gt;

&lt;p&gt;I propose to adda CharFilter consistency method that reads two instances of the same CharFilter, one using read() and in parallel using read(char[]) with varying buffer sizes. It should check offsets (and that is what is heavy buggy in MappingCharOffsetCorrumpter / MockCharCorrumpter).&lt;/p&gt;

&lt;p&gt;I'll prepare a patch with the test method in BaseTokenStreamTestCase.&lt;/p&gt;</comment>
                    <comment id="13254638" author="rcmuir" created="Mon, 16 Apr 2012 11:52:54 +0100">&lt;p&gt;Hi Uwe, i tried these same things, I know the interface is horrible but that wasnt really the bug here.&lt;/p&gt;

&lt;p&gt;If we want to fix CharFilter/CharStream i think we should use &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2788" title="Make CharFilter reusable"&gt;LUCENE-2788&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;That removes CharFilter/CharStream/CharReader and replaces with just one CharFilter class that extends FilterReader and is reusable. I think its a simpler way to go at least to only have one class...&lt;/p&gt;</comment>
                    <comment id="13254641" author="rcmuir" created="Mon, 16 Apr 2012 11:54:51 +0100">&lt;blockquote&gt;
&lt;p&gt;We should have a asserting method for CharFilter consistency. Indeed the read(char[],int,int) method in MappingCharFilter is failing horribly (which is caused by the underlying MockCharFilter somehow).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We don't need to do all that actually. I think its enough if we have a CharFilter or Tokenizer that&lt;br/&gt;
randomly uses read(CharBuffer)/read(char[])/read()/read(char[], int,int)...&lt;/p&gt;

&lt;p&gt;the other methods should be tested too.&lt;/p&gt;</comment>
                    <comment id="13412285" author="hossman" created="Thu, 12 Jul 2012 00:03:42 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429704" author="rcmuir" created="Tue, 7 Aug 2012 04:41:24 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12548042">LUCENE-3919</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12522712" name="analysis-common.tests-report.txt" size="7452" author="steve_rowe" created="Sun, 15 Apr 2012 18:24:25 +0100"/>
                    <attachment id="12522739" name="LUCENE-3990-CharFilterFix.patch" size="3524" author="thetaphi" created="Mon, 16 Apr 2012 07:45:50 +0100"/>
                    <attachment id="12522734" name="LUCENE-3990.patch" size="5784" author="rcmuir" created="Mon, 16 Apr 2012 03:56:04 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 16 Apr 2012 01:31:29 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>235862</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23707</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3986] Support running tests with a simple, non-asserting (and possibly shared) Random</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3986</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Robert asked for it. Calling random() (and its methods) can obscure memory visibility issues (because random() is thread local, context-sensitive, etc.). An option (or randomly selected mode) of running with a simple Random (static/ test method level only) would simulate the framework as it was before (reading &amp;#8211; possibly non-reproducible thread races but at the same time testing memory visibility issues in the core code).&lt;/p&gt;</description>
                <environment/>
            <key id="12550945">LUCENE-3986</key>
            <summary>Support running tests with a simple, non-asserting (and possibly shared) Random</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="dweiss">Dawid Weiss</assignee>
                                <reporter username="dweiss">Dawid Weiss</reporter>
                        <labels>
                    </labels>
                <created>Sat, 14 Apr 2012 13:34:20 +0100</created>
                <updated>Fri, 10 May 2013 00:05:12 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13412307" author="hossman" created="Thu, 12 Jul 2012 00:03:46 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429705" author="rcmuir" created="Tue, 7 Aug 2012 04:41:25 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 11 Jul 2012 23:03:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>235810</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23711</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3978] redo how our download redirect pages work</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3978</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;the download "latest" redirect pages are kind of a pain to change when we release a new version...&lt;/p&gt;

&lt;p&gt;&lt;a href="http://lucene.apache.org/core/mirrors-core-latest-redir.html" class="external-link"&gt;http://lucene.apache.org/core/mirrors-core-latest-redir.html&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://lucene.apache.org/solr/mirrors-solr-latest-redir.html" class="external-link"&gt;http://lucene.apache.org/solr/mirrors-solr-latest-redir.html&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12550756">LUCENE-3978</key>
            <summary>redo how our download redirect pages work</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="hossman">Hoss Man</reporter>
                        <labels>
                    </labels>
                <created>Thu, 12 Apr 2012 23:34:13 +0100</created>
                <updated>Fri, 10 May 2013 00:05:12 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13252940" author="hossman" created="Thu, 12 Apr 2012 23:40:42 +0100">&lt;p&gt;when we released 3.6, we ran into a few annoyances...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;these pages require that you edit the template (not availbale in the bookmarklet) to change the 3.5.0 to 3.6.0 in the final URL&lt;/li&gt;
	&lt;li&gt;these pages were in browser caches, so they weren't seeing the cahnges in the javascript redirect (rmuir added some no-cache metadata headers, so hopefully this won't be a problem again)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;My suggestion for the future...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;eliminate these templates and their mdtext pages entirely&lt;/li&gt;
	&lt;li&gt;replace them with a .htaccess redirect rule that looks like:
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
/([^/*)/mirrors-(.*)-latest-redir.html /$1/mirrors-$2-redir.html?3.6.0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;update the templates for mirrors-solr-redir.mdtext and mirrors-core-redir.mdtext so that the javascript will use the query string when building the final URL&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...that way whenever we release a new version, we can just tweak the .htaccess rule, and the only "html pages" that might ever show up in an http or browser caches will have unique URLs per version.&lt;/p&gt;

&lt;p&gt;(EDIT: 1. fix fucking code markup, 2. didn't mean for redir rule to include "latest" ... sigh: 3. only ment to remove latest from the redir dest)&lt;/p&gt;</comment>
                    <comment id="13253121" author="thetaphi" created="Fri, 13 Apr 2012 06:20:46 +0100">&lt;p&gt;Hi Hoss,&lt;br/&gt;
I have seen your commit and I now understand the reason for the redirect pages (to also count downloads by Google Analytics). As I also did GA tracking for webpages not long ago, there is a better/more correct solution to track downloads. The trick is to add some javascript to the source link that tells Google Analytics to create a "virtual pageview" when clicking on the link. The virtual pageview is counted on a "virtual" URI (e.g., the current URL with the redirect page, without http and hostname): &lt;a href="http://support.google.com/googleanalytics/bin/answer.py?hl=en&amp;amp;answer=55529" class="external-link"&gt;http://support.google.com/googleanalytics/bin/answer.py?hl=en&amp;amp;answer=55529&lt;/a&gt; In fact, the trick is to execute the analytics javascript on the link click and pass a "custom" url instead the default one from the current page.&lt;/p&gt;</comment>
                    <comment id="13253126" author="hossman" created="Fri, 13 Apr 2012 06:29:40 +0100">&lt;p&gt;Uwe: if i'm understanding that page correctly, this would only be possible for links where:&lt;br/&gt;
 a) link html is on our site&lt;br/&gt;
 b) we can control the html used to generate them&lt;br/&gt;
...which isfine for the bug buttons on lucene.apache.org, and any other download links we might want to include on those CMS pages, but not for things like links from wiki.apache.org, or the URLs we include in our plain text release announcement emails (that users just cut/paste) or that we submit to any other site to promote the release.&lt;/p&gt;</comment>
                    <comment id="13253157" author="thetaphi" created="Fri, 13 Apr 2012 07:11:52 +0100">&lt;p&gt;Hoss: That's the problem. For links on external pages to our downloads we cannot track, only with a redirect page. But people adding links to their pages will always link-through, so we cannot enforce that they go through analytics code.&lt;/p&gt;

&lt;p&gt;But analytics should track page usage (means the action "clicking link on our homepage") and not download usage in general (which is diametral somehow). Tracking of downloads is in Apache's responsibility.&lt;/p&gt;</comment>
                    <comment id="13412293" author="hossman" created="Thu, 12 Jul 2012 00:03:44 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429710" author="rcmuir" created="Tue, 7 Aug 2012 04:41:27 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 13 Apr 2012 05:20:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>235621</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23719</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3968] Factor MockGraphTokenFilter into LookaheadTokenFilter + random tokens</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3968</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;MockGraphTokenFilter is rather hairy... I've managed to simplify it (I think!) by breaking apart its two functions...&lt;/p&gt;

&lt;p&gt;I think LookaheadTokenFilter can be used in the future for other graph aware filters.&lt;/p&gt;</description>
                <environment/>
            <key id="12550169">LUCENE-3968</key>
            <summary>Factor MockGraphTokenFilter into LookaheadTokenFilter + random tokens</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 8 Apr 2012 17:28:40 +0100</created>
                <updated>Fri, 10 May 2013 00:05:13 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13249568" author="mikemccand" created="Sun, 8 Apr 2012 17:33:21 +0100">&lt;p&gt;Patch... all changes are in test-framework.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12521891" name="LUCENE-3968.patch" size="26252" author="mikemccand" created="Sun, 8 Apr 2012 17:33:21 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>235035</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23729</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3961] don't build and rebuild jar files for dependencies in tests</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3961</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Hossman's comments about when jars are built had me thinking,&lt;br/&gt;
its not really great how dependencies are managed currently.&lt;/p&gt;

&lt;p&gt;say i have contrib/hamburger that depends on contrib/cheese&lt;/p&gt;

&lt;p&gt;if I do 'ant test' in contrib/hamburger, you end out with a situation&lt;br/&gt;
where you have no hamburger.jar but you have a cheese.jar.&lt;/p&gt;

&lt;p&gt;The reason for this: i think is how we implement the contrib-uptodate,&lt;br/&gt;
via .jar files. I think instead contrib-uptodate shouldnt use actual&lt;br/&gt;
jar files (cheese.jar) but a simple file we 'touch' like cheese.compiled.&lt;/p&gt;

&lt;p&gt;This will make the build faster, especially I think the solr tests&lt;br/&gt;
which uses these dependencies across a lot of lucene modules. we won't&lt;br/&gt;
constantly jar their stuff.&lt;/p&gt;</description>
                <environment/>
            <key id="12549807">LUCENE-3961</key>
            <summary>don't build and rebuild jar files for dependencies in tests</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 5 Apr 2012 23:22:50 +0100</created>
                <updated>Fri, 10 May 2013 00:05:13 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13247778" author="hossman" created="Thu, 5 Apr 2012 23:33:44 +0100">&lt;p&gt;I don't have a strong opinion about this, but there are two counter arguments i've heard from over the years made in favor of &lt;b&gt;always&lt;/b&gt; building the jar(s) even though it's a bit slower for the tests...&lt;/p&gt;

&lt;p&gt;1) it mean you always test against hte same jars that you ship &amp;#8211; so there is no risk that the classpath you build for testing is subtly different then the files that make it into the jar (ie: maybe contrib/cheeseburger/build.xml copies a cheese_types.xml file into it's classes dir, but it accidentally gets excluded from contrib-cheeses.jar&lt;/p&gt;

&lt;p&gt;2) it means less risk that someone accidentally uses an older jar then they thing...&lt;/p&gt;

&lt;p&gt;a) "ant something" ... builds contrib-hamburger.jar and contrib-cheese.jar&lt;br/&gt;
b) you realize it doesn't work the way you want, so you apply a patch (with tests!)&lt;br/&gt;
c) "ant test" rebuilds contrib/*/classes and you see your new hamburger test passes&lt;br/&gt;
d) you copy contrib-hamburger.jar and contrib-cheese.jar not realizing they are still left over from #a above, and don't have your patch.&lt;/p&gt;</comment>
                    <comment id="13247785" author="rcmuir" created="Thu, 5 Apr 2012 23:39:13 +0100">&lt;p&gt;Hossman: I don't understand your #2: We currently don't generally use jars &lt;br/&gt;
as the actual classpath for testing though, and i strongly disagree that&lt;br/&gt;
we should. This really REALLY REALLY slow things down.&lt;/p&gt;

&lt;p&gt;we might build them in module/contrib-uptodate, but its just because they &lt;br/&gt;
are a file with a timestamp. Touch is a much faster way to make a file with &lt;br/&gt;
a timestamp.&lt;/p&gt;

&lt;p&gt;by never creating a jar in the first place your #2 doesn't happen at all really.&lt;br/&gt;
in fact i think whatever we put in build/ is "private" implementation details&lt;br/&gt;
of our build. If someone wants to rebuild jar files they should be running&lt;br/&gt;
the packaging tasks and getting them from dist/&lt;/p&gt;</comment>
                    <comment id="13247789" author="rcmuir" created="Thu, 5 Apr 2012 23:48:07 +0100">&lt;p&gt;Also, if we were to go with your logic, really we should be rebuilding the solr.war everytime&lt;br/&gt;
we run 'ant test' too.&lt;/p&gt;

&lt;p&gt;(I'm just pointing out why i think its infeasible).&lt;/p&gt;

&lt;p&gt;I think we need to keep this stuff fast so that compile-test-debug lifecycle is as fast as possible,&lt;br/&gt;
especially stuff like ant test -Dtestcase&lt;/p&gt;</comment>
                    <comment id="13247795" author="rcmuir" created="Thu, 5 Apr 2012 23:53:43 +0100">&lt;blockquote&gt;
&lt;p&gt;a) "ant something" ... builds contrib-hamburger.jar and contrib-cheese.jar&lt;br/&gt;
b) you realize it doesn't work the way you want, so you apply a patch (with tests!)&lt;br/&gt;
c) "ant test" rebuilds contrib/*/classes and you see your new hamburger test passes&lt;br/&gt;
d) you copy contrib-hamburger.jar and contrib-cheese.jar not realizing they are still left over from #a above, and don't have your patch.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This also fails if your patch actually fixes contrib-cheese. &lt;/p&gt;

&lt;p&gt;Going with this logic, any time i run ant test -Dtestcase in lucene, i should rebuild the entirety&lt;br/&gt;
of lucene/solr, because this poor user might have out of date jars &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13247801" author="hossman" created="Thu, 5 Apr 2012 23:58:25 +0100">&lt;blockquote&gt;&lt;p&gt;We currently don't generally use jars as the actual classpath for testing though&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;understood, #1 is just an argument i've seen as to why it would be better to do so &amp;#8211; otherwise we never actually know when testing that our jars are useful &amp;#8211; someone could accidentally put "excludes="*.class" on a jar task and you'd never notice because all the tests would still pass.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;by never creating a jar in the first place your #2 doesn't happen at all really.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;note step #a ... the point is if someone does whatever officially blessed step there is to build the jars ("ant", "ant jar", "ant whatever") and then decides they want to change the behavior of those jars &amp;#8211; they may never run "ant clean" and it may not occur to then to  re-run whatever that official way to build jars is and they may not notice that the jar's aren't rebuilt when they do "ant test" &amp;#8211; because they can already see the new code was "compiled" and running based on the test output.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, if we were to go with your logic, really we should be rebuilding the solr.war everytime&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;correct, a war is just a jar with a special structure&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(I'm just pointing out why i think its infeasible). ... I think we need to keep this stuff fast so that compile-test-debug lifecycle is as fast as possible&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;agreed ... like i said, i don't have a strong opinion about it, but since we're discussing it i just wanted to point out the arguments i've heard over and over when having this discussion in the past on other projects.&lt;/p&gt;

&lt;p&gt;I think in an ideal world, devs could run fast tests against ../*/classes/ directories, but jenkins would run all those same tests against fully build jars to ensure they aren't missing anything ... but that would probably be an anoying build.xml to maintain&lt;/p&gt;</comment>
                    <comment id="13247825" author="rcmuir" created="Fri, 6 Apr 2012 00:14:09 +0100">&lt;blockquote&gt;
&lt;p&gt;I think in an ideal world, devs could run fast tests against ../*/classes/ directories, but jenkins would run all those same tests against fully build jars to ensure they aren't missing anything ... but that would probably be an anoying build.xml to maintain&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree with that, but maybe in the future we could do it without it being so troublesome...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 5 Apr 2012 22:33:44 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>234799</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23736</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3960] improve build -projecthelp as far as building jars and such</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3960</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;HossmanSays: "ant compile" in lucene src artifacts builds jars for some contribs not&lt;br/&gt;
  all &amp;#8211; kind of confusing.  I did a double take before i realized i&lt;br/&gt;
  needed "build-contrib" to get them all and the ones i was seeing were just&lt;br/&gt;
  because of cross-contrb dependencies.&lt;/p&gt;

&lt;p&gt;Basically the ant -p is confusing in general. The 'default' task is actually 'jar-core',&lt;br/&gt;
which makes jars, but it has no description (doesnt show up in project help), and neither&lt;br/&gt;
does 'jar' (which maps to jar-core).&lt;/p&gt;

&lt;p&gt;solr/build.xml does a much better job about this. The ant -p seems more userfriendly&lt;br/&gt;
and its obvious what targets do what. For lucene its not.&lt;/p&gt;

&lt;p&gt;Also solr/build.xml defines 'ant compile' (lucene's doesnt, it maps to compile-core, which&lt;br/&gt;
then behaves differently than solr for people used to typing it).&lt;/p&gt;

&lt;p&gt;Basically I think the best solution is to try to make these two build.xml's targets &lt;br/&gt;
as consistent as possible.&lt;/p&gt;</description>
                <environment/>
            <key id="12549806">LUCENE-3960</key>
            <summary>improve build -projecthelp as far as building jars and such</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 5 Apr 2012 23:18:21 +0100</created>
                <updated>Fri, 10 May 2013 00:05:13 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>234798</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23737</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3954] prepare-release should put changesToHtml and KEYS output in dist/</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3954</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently changes2html output is in build/docs/changes&lt;br/&gt;
But really, this just means RM must manually deal with it separately from dist/&lt;/p&gt;

&lt;p&gt;I think 'prepare-release' should ensure dist/ houses everything that goes into the RC,&lt;/p&gt;

&lt;p&gt;Additionally, KEYS files for both lucene/solr which should be fetched from&lt;br/&gt;
&lt;a href="http://people.apache.org/keys/group/lucene.asc" class="external-link"&gt;http://people.apache.org/keys/group/lucene.asc&lt;/a&gt; automatically.&lt;/p&gt;

&lt;p&gt;Sorry, i know there are python scripts etc in dev-tools as well as ant logic&lt;br/&gt;
to do some of this, but it would be much easier for 'prepare-release' to&lt;br/&gt;
actually make the full release, and some people don't want to give their&lt;br/&gt;
ssh password to any script like that.&lt;/p&gt;</description>
                <environment/>
            <key id="12549614">LUCENE-3954</key>
            <summary>prepare-release should put changesToHtml and KEYS output in dist/</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 5 Apr 2012 06:08:05 +0100</created>
                <updated>Fri, 10 May 2013 00:05:13 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>234606</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23743</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3924] Optimize buffer size handling in RAMDirectory to make it more GC friendly</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3924</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;RAMDirectory currently uses a fixed buffer size of 1024 bytes to allocate memory. This is very wasteful for large indexes. Improvements may be:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;per file buffer sizes based on IOContext and maximum segment size&lt;/li&gt;
	&lt;li&gt;allocate only one buffer for files that are copied from another directory&lt;/li&gt;
	&lt;li&gt;dynamically increae buffer size when files grow (makes seek() complicated)&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12548131">LUCENE-3924</key>
            <summary>Optimize buffer size handling in RAMDirectory to make it more GC friendly</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Mar 2012 16:38:06 +0100</created>
                <updated>Fri, 10 May 2013 00:05:13 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                                                  <comments>
                    <comment id="13412283" author="hossman" created="Thu, 12 Jul 2012 00:03:42 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429691" author="rcmuir" created="Tue, 7 Aug 2012 04:41:17 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
            <subtask id="12535851">LUCENE-3659</subtask>
            <subtask id="12548133">LUCENE-3926</subtask>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 11 Jul 2012 23:03:42 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>233229</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23773</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3917] Port pruning module to trunk apis</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3917</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Pruning module was added in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1812" title="Static index pruning by in-document term frequency (Carmel pruning)"&gt;&lt;del&gt;LUCENE-1812&lt;/del&gt;&lt;/a&gt;, but we need to port&lt;br/&gt;
this to trunk (4.0)&lt;/p&gt;</description>
                <environment/>
            <key id="12548013">LUCENE-3917</key>
            <summary>Port pruning module to trunk apis</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Sun, 25 Mar 2012 17:50:58 +0100</created>
                <updated>Fri, 10 May 2013 00:05:14 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>3</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13282077" author="risbood" created="Thu, 24 May 2012 01:42:21 +0100">&lt;p&gt;Any ETA for this port ?&lt;/p&gt;</comment>
                    <comment id="13649977" author="gbowyer@fastmail.co.uk" created="Mon, 6 May 2013 19:49:48 +0100">&lt;p&gt;Recently at $DAYJOB the horror that is high frequency terms in OR search came to bite us, as a result I have an interest in pruning again.&lt;/p&gt;

&lt;p&gt;As such I made an attempt to forward port the existing pruning package directly to Lucene 4.0.&lt;/p&gt;

&lt;p&gt;This is largely a mechanical port, I have not put any real thought into it so its probably terrible.&lt;/p&gt;

&lt;p&gt;This does not pass its unit test, and is a mess internally in the code, I am going to try to get the unit test working and then loop back on making the code more lucene 4.x friendly.&lt;/p&gt;

&lt;p&gt;One question that occurs from this is how AtomicReaders are handled, do we want to pruning per segment with global stats, prune based on segment stats or just do the terrible thing and work with a SlowCompositeReader.&lt;/p&gt;

&lt;p&gt;I also think, given the work that went on with &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4752" title="Merge segments to sort them"&gt;&lt;del&gt;LUCENE-4752&lt;/del&gt;&lt;/a&gt; it might be possible to do the pruning in a similar fashion to the sorting merge such that we do a pruning merge.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581930" name="LUCENE-3917-Initial-port-of-index-pruning.patch" size="92630" author="gbowyer@fastmail.co.uk" created="Mon, 6 May 2013 19:49:48 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 24 May 2012 00:42:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>233111</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23778</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3912] Improved the checked-in tiny line file docs</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3912</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I think it may not have any surrogate pairs.... (it was derived from Europarl).&lt;/p&gt;</description>
                <environment/>
            <key id="12547952">LUCENE-3912</key>
            <summary>Improved the checked-in tiny line file docs</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sat, 24 Mar 2012 14:12:12 +0000</created>
                <updated>Fri, 10 May 2013 00:05:14 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>233050</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23783</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3907] Improve the Edge/NGramTokenizer/Filters</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3907</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Our ngram tokenizers/filters could use some love.  EG, they output ngrams in multiple passes, instead of "stacked", which messes up offsets/positions and requires too much buffering (can hit OOME for long tokens).  They clip at 1024 chars (tokenizers) but don't (token filters).  The split up surrogate pairs incorrectly.&lt;/p&gt;</description>
                <environment/>
            <key id="12547840">LUCENE-3907</key>
            <summary>Improve the Edge/NGramTokenizer/Filters</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="jpountz">Adrien Grand</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Fri, 23 Mar 2012 17:33:01 +0000</created>
                <updated>Mon, 13 May 2013 18:56:05 +0100</updated>
                    <resolved>Mon, 13 May 2013 18:56:05 +0100</resolved>
                                            <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>2</votes>
                        <watches>12</watches>
                                                    <comments>
                    <comment id="13241303" author="adhit" created="Thu, 29 Mar 2012 16:23:27 +0100">&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I'm interested in this project. I have done a Natural Language Processing project in language classification in which I did tokenization using Stanford's NLP tool. I'm also currently doing an Information Retrieval project in documents indexing and searching using Lucene and Weka. I might not be too familiar with Lucene's ngram tokenizer, but I have been working with NGram and Lucene before, so I believe that I would be able to learn quickly. Thanks &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Best regards,&lt;br/&gt;
Reinardus Surya Pradhitya&lt;/p&gt;</comment>
                    <comment id="13241357" author="mikemccand" created="Thu, 29 Mar 2012 17:19:27 +0100">&lt;p&gt;Awesome!  We just need a possible mentor here... volunteers...?&lt;/p&gt;</comment>
                    <comment id="13245492" author="thetaphi" created="Tue, 3 Apr 2012 17:56:32 +0100">&lt;p&gt;I would like to be the mentor for this. I wanted to fix those long time ago and I am happy somebody helps.&lt;/p&gt;

&lt;p&gt;P.S.: Maybe we also get a new ShingleMatrix &lt;b&gt;LOL&lt;/b&gt;&lt;/p&gt;</comment>
                    <comment id="13596002" author="mikemccand" created="Thu, 7 Mar 2013 16:06:51 +0000">&lt;p&gt;I think we should remove the Side (BACK/FRONT) enum: an app can always use ReverseStringFilter if it really wants BACK grams (what are BACK grams used for?).&lt;/p&gt;</comment>
                    <comment id="13596346" author="wunder" created="Thu, 7 Mar 2013 20:48:08 +0000">&lt;p&gt;Back grams would work for leading wildcards. They might be useful for things where the head is at the end (tail-first?), like domain names.&lt;/p&gt;

&lt;p&gt;Not super-useful, but it is a small part of the code in the tokenizer.&lt;/p&gt;</comment>
                    <comment id="13596413" author="thetaphi" created="Thu, 7 Mar 2013 21:39:00 +0000">&lt;blockquote&gt;&lt;p&gt;Back grams would work for leading wildcards. They might be useful for things where the head is at the end (tail-first?), like domain names.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you need reverse n-grams, you could always add a filter to do that afterwards. There is no need to have this as separate logic in &lt;b&gt;this&lt;/b&gt; filter. We should split logic and keep filters as simple as possible.&lt;/p&gt;</comment>
                    <comment id="13596419" author="steve_rowe" created="Thu, 7 Mar 2013 21:42:26 +0000">&lt;p&gt;Edge is the wrong name for something that only works on one edge.  Maybe rename to LeadingNgram? &lt;/p&gt;</comment>
                    <comment id="13650209" author="jpountz" created="Tue, 7 May 2013 00:04:26 +0100">&lt;p&gt;Here is a patch that fixes EdgeNGramTokenizer and EdgeNGramTokenFilter so that they pass TestRandomChains.&lt;/p&gt;

&lt;p&gt;It also deprecates Side.BACK so when committing, I propose to follow Steve's advice to rename them to LeadingNGramTokenizer and LeadingNGramTokenFilter in trunk.&lt;/p&gt;</comment>
                    <comment id="13650530" author="thetaphi" created="Tue, 7 May 2013 06:37:01 +0100">&lt;p&gt;Hi Adrien, thanks for the fixes. You can take the issue and assign it to you!&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="13650856" author="jpountz" created="Tue, 7 May 2013 15:16:07 +0100">&lt;p&gt;As Steve suggested, I think these tokenizers/filters need to be renamed (trunk only) since they don't support backward graming anymore. Please don't hesitate to let me know if you have a good idea for a name, otherwise I plan to rename them to "Leading..." in the next few days.&lt;/p&gt;</comment>
                    <comment id="13652770" author="jkrupan" created="Thu, 9 May 2013 07:20:13 +0100">&lt;p&gt;Why not make the big changes for trunk/5.0, but leave the existing filters/tokenziers in 4.x as deprecated. Add the "leading" replacements as well in 4x, but be sure to preserve the existing stuff with support for "back" in 4x - as deprecated.&lt;/p&gt;
</comment>
                    <comment id="13652878" author="jpountz" created="Thu, 9 May 2013 12:24:41 +0100">&lt;p&gt;The previous behaviour could trigger highlighting bugs so I think it is important that we fix it in 4.x. In case the broken behaviour is still needed, it can be emulated by providing Version.LUCENE_43 as the Lucene match version.&lt;/p&gt;</comment>
                    <comment id="13652885" author="jkrupan" created="Thu, 9 May 2013 12:50:33 +0100">&lt;p&gt;Look, the "fix" of position bugs here is to keep the position the same for all tokens, right? And that logic can simply be applied to "back" as well, for the same reasons and with the same effect. So, how could "back" - which should apply that same position logic be a separate cause of "highlighting bugs"?&lt;/p&gt;

&lt;p&gt;"previous behavior" (incremented position) is simply NOT linked to front vs. back. I'm not sure why you are claiming that it is!&lt;/p&gt;

&lt;p&gt;The Jira record simply shows that some people "want" to eliminate a feature... not that the feature (if fixed in the same manner as the rest of the fix) "could trigger highlighting bugs" - unless I'm missing something, and if I'm missing something it is because you are not stating it clearly! So, please do so.&lt;/p&gt;</comment>
                    <comment id="13653011" author="jpountz" created="Thu, 9 May 2013 16:25:21 +0100">&lt;blockquote&gt;&lt;p&gt;"previous behavior" (incremented position) is simply NOT linked to front vs. back. I'm not sure why you are claiming that it is!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Indeed these issues are unrelated, and backward n-graming doesn't cause highlighting issues. Sorry if I seemed to mean the opposite, it was not intentional.&lt;/p&gt;

&lt;p&gt;My main motivation was to fix the positions/offsets bugs. I also deprecated support for backward n-graming since there seemed to be lazy consensus: as Uwe noted, backward n-graming can be obtained by applying ReverseStringFilter, then EdgeNGramTokenFilter and then ReverseStringFilter again. This helps make filters simpler, hence easier to understand and to test.&lt;/p&gt;

&lt;p&gt;So now, here is how you would use filters depending on whether you want front or back n-graming and with or without the new positions/offsets.&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;&amp;nbsp;&lt;/td&gt;
&lt;td class='confluenceTd'&gt; previous positions/offsets (broken) &lt;/td&gt;
&lt;td class='confluenceTd'&gt; new positions/offsets &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt; front n-graming &lt;/td&gt;
&lt;td class='confluenceTd'&gt; EdgeNGramTokenFilter(version=LUCENE_43,side=FRONT) &lt;/td&gt;
&lt;td class='confluenceTd'&gt; EdgeNGramTokenFilter(version=LUCENE_44,side=FRONT) &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt; back n-graming &lt;/td&gt;
&lt;td class='confluenceTd'&gt; EdgeNGramTokenFilter(version=LUCENE_43,side=BACK) &lt;/td&gt;
&lt;td class='confluenceTd'&gt; ReverseStringFilter, EdgeNGramTokenFilter(version=LUCENE_44,side=FRONT), ReverseStringFilter &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;It is true that the patch prevents users from constructing EdgeNGramTokenFilter with version&amp;gt;=LUCENE_44 and side=BACK to encourage users to upgrade their analysis chain. But if you think we should allow for it, I'm open for discussion.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12624823">LUCENE-4641</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12548050">LUCENE-3920</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12634938">LUCENE-4810</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12581988" name="LUCENE-3907.patch" size="43976" author="jpountz" created="Tue, 7 May 2013 00:04:26 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 29 Mar 2012 15:23:27 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>232938</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23788</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-3888] split off the spell check word and surface form in spell check dictionary</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3888</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The "did you mean?" feature by using Lucene's spell checker cannot work well for Japanese environment unfortunately and is the longstanding problem, because the logic needs comparatively long text to check spells, but for some languages (e.g. Japanese), most words are too short to use the spell checker.&lt;/p&gt;

&lt;p&gt;I think, for at least Japanese, the things can be improved if we split off the spell check word and surface form in the spell check dictionary. Then we can use ReadingAttribute for spell checking but CharTermAttribute for suggesting, for example.&lt;/p&gt;</description>
                <environment/>
            <key id="12547151">LUCENE-3888</key>
            <summary>split off the spell check word and surface form in spell check dictionary</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="koji">Koji Sekiguchi</assignee>
                                <reporter username="koji">Koji Sekiguchi</reporter>
                        <labels>
                    </labels>
                <created>Tue, 20 Mar 2012 07:08:19 +0000</created>
                <updated>Fri, 10 May 2013 00:05:14 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/spellchecker</component>
                        <due/>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13233286" author="koji" created="Tue, 20 Mar 2012 07:54:28 +0000">&lt;p&gt;The patch cannot be compiled now because I changed the return type of the method in Dictionary interface but all implemented classes have not been changed.&lt;/p&gt;

&lt;p&gt;Please give some comment because I'm new to spell checker. If no problem to go, I'll continue to work.&lt;/p&gt;</comment>
                    <comment id="13233291" author="rcmuir" created="Tue, 20 Mar 2012 08:07:11 +0000">&lt;p&gt;Koji: hmm I think the problem is not in the Dictionary interface (which is actually ok),&lt;br/&gt;
but instead in the spellcheckers and suggesters themselves?&lt;/p&gt;

&lt;p&gt;For spellchecking, I think we need to expose more Analysis options in Spellchecker:&lt;br/&gt;
currently this is actually hardcoded at KeywordAnalyzer (it uses NOT_ANALYZED). &lt;br/&gt;
Instead I think you should be able to pass Analyzer: we would also&lt;br/&gt;
have a TokenFilter for Japanese that replaces term text with Reading from ReadingAttribute.&lt;/p&gt;

&lt;p&gt;In the same way, suggest can analyze too. (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3842" title="Analyzing Suggester"&gt;&lt;del&gt;LUCENE-3842&lt;/del&gt;&lt;/a&gt; is already some work for that, especially&lt;br/&gt;
with the idea to support Japanese this exact same way).&lt;/p&gt;

&lt;p&gt;So in short I think we should:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;create a TokenFilter (similar to BaseFormFilter) which copies ReadingAttribute into termAtt.&lt;/li&gt;
	&lt;li&gt;refactor the 'n-gram analysis' in spellchecker to work on actual tokenstreams (this can&lt;br/&gt;
  also likely be implemented as tokenstreams), allowing user to set an Analyzer on Spellchecker&lt;br/&gt;
  to control how it analyzes text.&lt;/li&gt;
	&lt;li&gt;continue to work on 'analysis for suggest' like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3842" title="Analyzing Suggester"&gt;&lt;del&gt;LUCENE-3842&lt;/del&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Note this use of analyzers in spellcheck/suggest is unrelated to Solr's current use of 'analyzers' &lt;br/&gt;
which is only for some query manipulation and not very useful.&lt;/p&gt;</comment>
                    <comment id="13233305" author="rcmuir" created="Tue, 20 Mar 2012 08:37:10 +0000">&lt;p&gt;Here is a simple prototype of what I was suggesting, allows you to specify Analyzer to SpellChecker.&lt;/p&gt;

&lt;p&gt;This Analyzer converts the 'surface form' into 'analyzed form' at index and query time: at index-time it forms n-grams based on the analyzed form, but stores the surface form for retrieval.&lt;/p&gt;

&lt;p&gt;At query-time we have a similar process: the docFreq() etc checks are done on the surface form, but the actual spellchecking on the analyzed form.&lt;/p&gt;

&lt;p&gt;The default Analyzer is null which means do nothing, and the patch has no tests, refactoring, or any of that.&lt;/p&gt;</comment>
                    <comment id="13233306" author="rcmuir" created="Tue, 20 Mar 2012 08:38:45 +0000">&lt;p&gt;fix the obvious reset() problem... the real problem is I need to reset() my coffee mug.&lt;/p&gt;</comment>
                    <comment id="13237452" author="koji" created="Sat, 24 Mar 2012 06:18:47 +0000">&lt;p&gt;I added a test for the surface analyzer. I also added code for the analyzer in Solr.&lt;/p&gt;

&lt;p&gt;Currently, due to classpath problem, the test cannot be compiled. I should dig in, but if someone could, it would be appreciated.&lt;/p&gt;</comment>
                    <comment id="13237457" author="cm" created="Sat, 24 Mar 2012 06:30:14 +0000">&lt;p&gt;This is excellent, Koji and Robert.  We should be able to do basic spellchecking for Japanese with this.&lt;/p&gt;</comment>
                    <comment id="13237458" author="koji" created="Sat, 24 Mar 2012 06:30:42 +0000">&lt;p&gt;The test itself is not good.&lt;/p&gt;</comment>
                    <comment id="13237614" author="rcmuir" created="Sat, 24 Mar 2012 17:44:05 +0000">&lt;p&gt;lemme see if I can help with the test. I feel bad I didn't supply one with the prototype patch.&lt;/p&gt;

&lt;p&gt;About the Solr integration: this looks good! We can use a similar approach for autosuggest, too,&lt;br/&gt;
so this could configure the analyzer for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3842" title="Analyzing Suggester"&gt;&lt;del&gt;LUCENE-3842&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I wonder if we should allow separate configuration of "index" and "query" analyzers? I know&lt;br/&gt;
I came up with some use-cases for that for autosuggest, but I'm not sure about spellchecking.&lt;br/&gt;
I guess it wouldn't be overkill to allow it though.&lt;/p&gt;</comment>
                    <comment id="13237895" author="rcmuir" created="Sun, 25 Mar 2012 16:46:33 +0100">&lt;p&gt;I updated the patch and fixed Koji's test, its passing BUT there is a nocommit:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-comment"&gt;// nocommit: we need to fix SuggestWord to separate surface and analyzed forms.
&lt;/span&gt;&lt;span class="code-comment"&gt;// currently the 're-rank' is based on the surface forms!
&lt;/span&gt;spellChecker.setAccuracy(0F);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To explain with the Japanese case how the patch currently works, the spellchecker has two phases:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Phase 1: n-gram approximation phase. Here we generate a n-gram boolean query on the Readings. This is working fine.&lt;/li&gt;
	&lt;li&gt;Phase 2: re-rank phase. Here we take the candidates from Phase 1 and do a real comparison (e.g. Levenshtein) to give them the final score. The problem is this currently uses surface form!&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think phase 2 should re-rank based on the 'analyzed form' too? Inside spellchecker itself, I don't think this is very difficult, when analyzed != surface, we just store it for later retrieval.&lt;/p&gt;

&lt;p&gt;The problem is the spellcheck comparison APIs such as SuggestWord don't even have any getters or setters and present no way for me to migrate to surface+analyzed in any backwards compatible way...&lt;/p&gt;

&lt;p&gt;I'll think about this in the meantime. Maybe we should just break and cleanup these APIs since its a contrib module and they are funky? &lt;/p&gt;</comment>
                    <comment id="13237907" author="rcmuir" created="Sun, 25 Mar 2012 17:22:05 +0100">&lt;p&gt;updated patch (note with this one: Solr does not yet compile).&lt;/p&gt;

&lt;p&gt;I went the route of trying to clean up these apis correctly: I think there are serious problems here.&lt;/p&gt;

&lt;p&gt;The biggest violation is stuff like:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-comment"&gt;// convert to array string: 
&lt;/span&gt;&lt;span class="code-comment"&gt;// nocommit: why don't we just &lt;span class="code-keyword"&gt;return&lt;/span&gt; SuggestWord[] with all the information?
&lt;/span&gt;&lt;span class="code-comment"&gt;// consumers such as Solr must be recomputing &lt;span class="code-keyword"&gt;this&lt;/span&gt; stuff again?!
&lt;/span&gt;&lt;span class="code-object"&gt;String&lt;/span&gt;[] list = &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;String&lt;/span&gt;[sugQueue.size()];
&lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; i = sugQueue.size() - 1; i &amp;gt;= 0; i--) {
 list[i] = sugQueue.pop().getSurface();
}

&lt;span class="code-keyword"&gt;return&lt;/span&gt; list;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;DirectSpellChecker already returns all this data, I think its doing the right thing, but I think SpellChecker should be fixed. Even for the normal case surely we are recomputing docFreq etc on all the candidates which is wasteful.&lt;/p&gt;

&lt;p&gt;I'll keep plugging away but it seems like this will be a pretty serious refactoring (including e.g. distributed spellcheck refactoring) and difficult for 3.6.&lt;/p&gt;</comment>
                    <comment id="13237934" author="rcmuir" created="Sun, 25 Mar 2012 18:06:25 +0100">&lt;p&gt;In my opinion we should set this as fix for 4.0&lt;/p&gt;

&lt;p&gt;The only option for 3.6 would be something like my previous patch &lt;br/&gt;
(&lt;a href="https://issues.apache.org/jira/secure/attachment/12519860/LUCENE-3888.patch" class="external-link"&gt;https://issues.apache.org/jira/secure/attachment/12519860/LUCENE-3888.patch&lt;/a&gt;) which &lt;br/&gt;
has the disadvantages of doing the second-phase re-ranking on surface forms.&lt;/p&gt;

&lt;p&gt;Any other opinions?&lt;/p&gt;</comment>
                    <comment id="13238178" author="koji" created="Mon, 26 Mar 2012 08:42:50 +0100">&lt;p&gt;Thanks Robert for giving some patches and comment.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The only option for 3.6 would be something like my previous patch&lt;br/&gt;
(&lt;a href="https://issues.apache.org/jira/secure/attachment/12519860/LUCENE-3888.patch" class="external-link"&gt;https://issues.apache.org/jira/secure/attachment/12519860/LUCENE-3888.patch&lt;/a&gt;) which&lt;br/&gt;
has the disadvantages of doing the second-phase re-ranking on surface forms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;With the disadvantages, the spell checker won't work well for Japanese anyway. I give up this for 3.6.&lt;/p&gt;</comment>
                    <comment id="13238397" author="rcmuir" created="Mon, 26 Mar 2012 14:55:51 +0100">&lt;p&gt;Thanks for the feedback Koji.&lt;/p&gt;

&lt;p&gt;I'm not happy with the situation: I thought it would be easy to support&lt;br/&gt;
some rough Japanese spellcheck in 3.6 &lt;/p&gt;

&lt;p&gt;But it just seems like we need to do a lot of cleanup to make it work,&lt;br/&gt;
I would rather fix all of these APIs and do it right the first time so&lt;br/&gt;
that things like distributed support work too.&lt;/p&gt;</comment>
                    <comment id="13412298" author="hossman" created="Thu, 12 Jul 2012 00:03:44 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429699" author="rcmuir" created="Tue, 7 Aug 2012 04:41:22 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12519863" name="LUCENE-3888.patch" size="26768" author="rcmuir" created="Sun, 25 Mar 2012 17:22:05 +0100"/>
                    <attachment id="12519860" name="LUCENE-3888.patch" size="15920" author="rcmuir" created="Sun, 25 Mar 2012 16:46:33 +0100"/>
                    <attachment id="12519787" name="LUCENE-3888.patch" size="14497" author="koji" created="Sat, 24 Mar 2012 06:18:46 +0000"/>
                    <attachment id="12519047" name="LUCENE-3888.patch" size="5927" author="rcmuir" created="Tue, 20 Mar 2012 08:38:45 +0000"/>
                    <attachment id="12519046" name="LUCENE-3888.patch" size="5927" author="rcmuir" created="Tue, 20 Mar 2012 08:37:10 +0000"/>
                    <attachment id="12519038" name="LUCENE-3888.patch" size="3807" author="koji" created="Tue, 20 Mar 2012 07:54:25 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>6.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 20 Mar 2012 08:07:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>232310</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23807</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3844] Deprecate Token class and remove in trunk</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3844</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;After issues like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3843" title="implement PositionLengthAttribute for all tokenstreams where its appropriate"&gt;LUCENE-3843&lt;/a&gt;, introducing new attributes, we should remove Token class in trunk, as it leads to code that ignores those new attributes (like PositionLengthAttribute, ScriptAttribute, KeywordAttribute,...). If you want a holder for all Attributes a TokenStream, use TS.cloneAttributes().&lt;/p&gt;</description>
                <environment/>
            <key id="12545064">LUCENE-3844</key>
            <summary>Deprecate Token class and remove in trunk</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Sat, 3 Mar 2012 23:10:33 +0000</created>
                <updated>Fri, 10 May 2013 00:05:14 +0100</updated>
                                    <version>3.5</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13221728" author="thetaphi" created="Sat, 3 Mar 2012 23:13:59 +0000">&lt;p&gt;And finally: die, Token, die &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;, see @UweSays: &lt;a href="https://twitter.com/#!/UweSays/status/176082735792787456" class="external-link"&gt;https://twitter.com/#!/UweSays/status/176082735792787456&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13222938" author="rcmuir" created="Tue, 6 Mar 2012 02:31:28 +0000">&lt;p&gt;How much work is this? I think even removing would be difficult.&lt;/p&gt;

&lt;p&gt;Maybe we should push deprecating this out to 4.0, giving us a lot of time to clean things up&lt;br/&gt;
before its totally removed.&lt;/p&gt;</comment>
                    <comment id="13237026" author="hossman" created="Fri, 23 Mar 2012 20:28:19 +0000">&lt;p&gt;Bulk changing fixVersion 3.6 to 4.0 for any open issues that are unassigned and have not been updated since March 19.&lt;/p&gt;

&lt;p&gt;Email spam suppressed for this bulk edit; search for hoss20120323nofix36 to identify all issues edited&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12545062">LUCENE-3843</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 6 Mar 2012 02:31:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>230251</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23850</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3843] implement PositionLengthAttribute for all tokenstreams where its appropriate</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3843</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3767" title="Explore streaming Viterbi search in Kuromoji"&gt;&lt;del&gt;LUCENE-3767&lt;/del&gt;&lt;/a&gt; introduces PositionLengthAttribute, which extends the tokenstream API&lt;br/&gt;
from a sausage to a real graph. &lt;/p&gt;

&lt;p&gt;Currently tokenstreams such as WordDelimiterFilter and SynonymsFilter theoretically&lt;br/&gt;
work at a graph level, but then serialize themselves to a sausage, for example:&lt;/p&gt;

&lt;p&gt;wi-fi with WDF creates:&lt;br/&gt;
wi(posinc=1), fi(posinc=1), wifi(posinc=0)&lt;/p&gt;

&lt;p&gt;So the lossiness is that the 'wifi' is simply stacked ontop of 'fi'&lt;/p&gt;

&lt;p&gt;PositionLengthAttribute fixes this by allowing a token to declare how far it "spans",&lt;br/&gt;
so we don't lose any information.&lt;/p&gt;

&lt;p&gt;While the indexer currently can only support sausages anyway (and for performance reasons,&lt;br/&gt;
this is probably just fine!), other tokenstream consumers such as queryparsers and suggesters&lt;br/&gt;
such as &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3842" title="Analyzing Suggester"&gt;&lt;del&gt;LUCENE-3842&lt;/del&gt;&lt;/a&gt; can actually make use of this information for better behavior.&lt;/p&gt;

&lt;p&gt;So I think its ideal if the TokenStream API doesn't reflect the lossiness of the index format,&lt;br/&gt;
but instead keeps all information, and after &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3767" title="Explore streaming Viterbi search in Kuromoji"&gt;&lt;del&gt;LUCENE-3767&lt;/del&gt;&lt;/a&gt; is committed we should fix tokenstreams&lt;br/&gt;
to preserve this information for consumers that can use it.&lt;/p&gt;</description>
                <environment/>
            <key id="12545062">LUCENE-3843</key>
            <summary>implement PositionLengthAttribute for all tokenstreams where its appropriate</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Sat, 3 Mar 2012 22:13:03 +0000</created>
                <updated>Fri, 10 May 2013 00:05:14 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>1</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13221719" author="rcmuir" created="Sat, 3 Mar 2012 22:22:10 +0000">&lt;p&gt;We should probably also discuss if a separate attribute is the best here.&lt;/p&gt;

&lt;p&gt;It seems to me, position increment + length might make more sense in a single&lt;br/&gt;
attribute (similar to startOffset + endOffset).&lt;/p&gt;

&lt;p&gt;we can probably do a similar backwards compat thing to TermAttribute/CharTermAttribute?&lt;br/&gt;
If you used the old PositionIncrementAttribute maybe even the posLenght is hardcoded at 1,&lt;br/&gt;
but its deprecated for a PositionAttribute or something instead?&lt;/p&gt;

&lt;p&gt;Its more work, but for a cleaner API it might be worth it.&lt;/p&gt;</comment>
                    <comment id="13221721" author="rcmuir" created="Sat, 3 Mar 2012 22:30:05 +0000">&lt;p&gt;we also shouldnt forget to add basic bounds checks to basetokenstreamtestcase,&lt;br/&gt;
and probably to the attribute itself if it doesnt have them already&lt;br/&gt;
(positionincrementatt at least checks its &amp;gt;= 0, i think this one must be &amp;gt;= 1 always)&lt;/p&gt;</comment>
                    <comment id="13221740" author="mikemccand" created="Sat, 3 Mar 2012 23:53:59 +0000">&lt;p&gt;+1 for a new PositionAttribute holding both posInc &amp;amp; posLength.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12545064">LUCENE-3844</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                <outwardlinks description="depends upon">
                            <issuelink>
            <issuekey id="12542053">LUCENE-3767</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 3 Mar 2012 23:53:59 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>230249</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23851</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3840] Threads leak out from Executors created in LuceneTestCase#newSearcher</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3840</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Not every IndexReader/close listener is called and this results in executors not being properly shut down/ disposed of.&lt;/p&gt;</description>
                <environment/>
            <key id="12544939">LUCENE-3840</key>
            <summary>Threads leak out from Executors created in LuceneTestCase#newSearcher</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="dweiss">Dawid Weiss</reporter>
                        <labels>
                    </labels>
                <created>Fri, 2 Mar 2012 13:28:59 +0000</created>
                <updated>Fri, 10 May 2013 00:05:15 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>230126</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23853</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3813] add tests for JaSpell/TST/HighFrequencyDictionary to the suggest module</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3813</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently we are trying to refactor the suggest APIs in another issue...&lt;br/&gt;
I think if we had lucene-level tests for these implementations in the module it&lt;br/&gt;
would make things a lot easier.&lt;/p&gt;</description>
                <environment/>
            <key id="12543503">LUCENE-3813</key>
            <summary>add tests for JaSpell/TST/HighFrequencyDictionary to the suggest module</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 Feb 2012 15:36:18 +0000</created>
                <updated>Fri, 10 May 2013 00:05:15 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>228750</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23880</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3799] improve IndexSearcher javadocs: TotalHitCountCollector and paging</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3799</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The javadocs is so skimpy here that its somewhat of a trap.&lt;/p&gt;

&lt;p&gt;I think for the class javadocs, we can have a few 'recommendations' like:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Use is.search() for the normal search case &lt;span class="error"&gt;&amp;#91;users viewing top few pages&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;Use is.search() + TotalHitCountCollector if you only want the total number of hits&lt;/li&gt;
	&lt;li&gt;Use is.searchAfter() if you allow users to page deeply.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12543254">LUCENE-3799</key>
            <summary>improve IndexSearcher javadocs: TotalHitCountCollector and paging</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Sun, 19 Feb 2012 15:06:25 +0000</created>
                <updated>Fri, 10 May 2013 00:05:15 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>228501</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23894</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3797] 3xCodec should throw UOE if a DocValuesConsumer is pulled </title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3797</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;currently we just return null if a DVConsumer is pulled from 3.x which is trappy since it causes an NPE in DocFieldProcessor. We should rather throw a UOE.&lt;/p&gt;</description>
                <environment/>
            <key id="12543110">LUCENE-3797</key>
            <summary>3xCodec should throw UOE if a DocValuesConsumer is pulled </summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 17 Feb 2012 22:21:58 +0000</created>
                <updated>Fri, 10 May 2013 00:05:15 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13210617" author="simonw" created="Fri, 17 Feb 2012 22:23:01 +0000">&lt;p&gt;here is a simple patch&lt;/p&gt;</comment>
                    <comment id="13210622" author="simonw" created="Fri, 17 Feb 2012 22:30:09 +0000">&lt;p&gt;scratch that I the tests actually fail with this patch.. I looked at the wrong test run... stay tuned &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13210634" author="simonw" created="Fri, 17 Feb 2012 22:47:01 +0000">&lt;p&gt;yet another patch. not sure if this is really the way to go but I rather don't wanna have a NPE in IW&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12515042" name="LUCENE-3797.patch" size="2528" author="simonw" created="Fri, 17 Feb 2012 22:47:01 +0000"/>
                    <attachment id="12515038" name="LUCENE-3797.patch" size="935" author="simonw" created="Fri, 17 Feb 2012 22:23:01 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>228397</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23896</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3792] Remove StringField</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3792</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Often on the mailing list there is confusion about NOT_ANALYZED.&lt;/p&gt;

&lt;p&gt;Besides being useless (Just use KeywordAnalyzer instead), people trip up on this&lt;br/&gt;
not being consistent at query time (you really need to configure KeywordAnalyzer for the field &lt;br/&gt;
on your PerFieldAnalyzerWrapper so it will do the same thing at query time... oh wait&lt;br/&gt;
once you've done that, you dont need NOT_ANALYZED).&lt;/p&gt;

&lt;p&gt;So I think StringField is a trap too for the same reasons, just under a &lt;br/&gt;
different name, lets remove it.&lt;/p&gt;</description>
                <environment/>
            <key id="12542845">LUCENE-3792</key>
            <summary>Remove StringField</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 16 Feb 2012 10:20:02 +0000</created>
                <updated>Fri, 10 May 2013 00:05:15 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13209250" author="rcmuir" created="Thu, 16 Feb 2012 10:25:57 +0000">&lt;p&gt;Setting this as blocker (sorry).&lt;/p&gt;

&lt;p&gt;Its a huge trap when someone sets the same Analyzer on IndexWriter and QueryParser&lt;br/&gt;
but the analysis isn't "actually the same".&lt;/p&gt;</comment>
                    <comment id="13209254" author="rcmuir" created="Thu, 16 Feb 2012 10:33:19 +0000">&lt;p&gt;On 3.x, I'd like to deprecate NOT_ANALYZED for the same reasons. This at &lt;br/&gt;
least discourages people from running into that trap there and using&lt;br/&gt;
KeywordAnalyzer instead.&lt;/p&gt;</comment>
                    <comment id="13209256" author="thetaphi" created="Thu, 16 Feb 2012 10:44:43 +0000">&lt;p&gt;The backside of this is now, that you need to explicitely use a KeywordAnalyzer now for Primary Key fields. If you don't run those through a query analyzer (e.g. generally produce TermQuery directly) then you have lots of additional work. For simple "lookup" queries and indexing a PK key, this is a no go.&lt;/p&gt;

&lt;p&gt;-1 on removing that completely, it should simply called different. We should maybe add PKQuery (a ConstantScore TermQuery) and PKField to have a symmetry.&lt;/p&gt;</comment>
                    <comment id="13209258" author="rcmuir" created="Thu, 16 Feb 2012 10:48:45 +0000">&lt;p&gt;Well we are at a standstill. We constantly get these problems on the users list from NOT_ANALYZED&lt;br/&gt;
and I don't like reintroducing the trap again.&lt;/p&gt;

&lt;p&gt;So I'm -1 to StringField in 4.0&lt;/p&gt;</comment>
                    <comment id="13209260" author="thetaphi" created="Thu, 16 Feb 2012 10:51:56 +0000">&lt;p&gt;I said call it different.&lt;/p&gt;</comment>
                    <comment id="13209267" author="rcmuir" created="Thu, 16 Feb 2012 10:59:21 +0000">&lt;blockquote&gt;
&lt;p&gt;If you don't run those through a query analyzer (e.g. generally produce TermQuery directly) then you have lots of additional work. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats not true, because keywordanalyzer does nothing to the terms, you can continue to produce termquery directly and it will work.&lt;br/&gt;
So expert users are fine.&lt;/p&gt;

&lt;p&gt;This issue isnt about expert users, its about how our API traps people that are not expert users.&lt;/p&gt;</comment>
                    <comment id="13209269" author="thetaphi" created="Thu, 16 Feb 2012 10:59:34 +0000">&lt;blockquote&gt;&lt;p&gt;Well we are at a standstill. We constantly get these problems on the users list from NOT_ANALYZED&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You cannot prevent users from doing the wrong thing. If you remove StringField completely, pleaese also remove NumericField and force users to use PerFieldAnalyzerWrapper with a NumericTokenStream. If you add a numeric field you cannot ask for it with query parser. If you add a StringField, you cann ask with QueryParser. Simple rule. It must just be clearly documented. And possible StringField renamed.&lt;/p&gt;

&lt;p&gt;People using primary keys or other untokenized values should simply not use QueryParser. Use a ComstantScoreTermyQuery and you are fine.&lt;/p&gt;

&lt;p&gt;This is all just a documentation problem, so I am completely against removing that. Not everybody is using Lucene purely as a full-text engine.&lt;/p&gt;</comment>
                    <comment id="13209272" author="rcmuir" created="Thu, 16 Feb 2012 11:03:51 +0000">&lt;blockquote&gt;
&lt;p&gt;If you remove StringField completely, pleaese also remove NumericField and force users to use PerFieldAnalyzerWrapper with a NumericTokenStream.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I actually am not sure this is such a bad idea?&lt;/p&gt;

&lt;p&gt;If we were to enforce such a thing, it would also be possible to add a modification to the queryparser (instanceof NumericTokenStream)&lt;br/&gt;
so that numeric fields then work out of the box with the query parser nicely?&lt;/p&gt;</comment>
                    <comment id="13209273" author="rcmuir" created="Thu, 16 Feb 2012 11:06:10 +0000">&lt;blockquote&gt;
&lt;p&gt;Not everybody is using Lucene purely as a full-text engine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But we cannot let "non-fulltext" uses break the design for the intended use case (full-text).&lt;/p&gt;</comment>
                    <comment id="13209287" author="rcmuir" created="Thu, 16 Feb 2012 11:45:08 +0000">&lt;p&gt;Its obvious Uwe and I aren't going to agree here immediately, so here is a patch adding a big warning to 3.x javadocs.&lt;/p&gt;

&lt;p&gt;For now I'd like to apply the same warning to StringField in trunk (I just made the patch against 3.x)&lt;/p&gt;</comment>
                    <comment id="13209288" author="rcmuir" created="Thu, 16 Feb 2012 11:46:10 +0000">&lt;p&gt;Sorry, incomplete wording (I forgot to save before svn diff).&lt;/p&gt;</comment>
                    <comment id="13209290" author="thetaphi" created="Thu, 16 Feb 2012 11:53:53 +0000">&lt;p&gt;I am fine with that. As I said: We need correct documentation to prevent traps. But removing traps by removing possibilities is the wrong way to go. It will force users to have crazy code for simple things, thats my problem. I don't use QueryParser, so I don't want solutions prefering QueryParser over anything else. QueryParser is just a convenience API around new BQ()...new TQ... E.g., I have my own query parser (and I suggest everybody else to have the same for user-facing queries) that simply analyzes the text and creates a BQ out of all anaylzed terms. No syntax, nothing. And I only execute that one on the field in question. Thats very simplistic but another way to look on things.&lt;/p&gt;</comment>
                    <comment id="13209430" author="rcmuir" created="Thu, 16 Feb 2012 15:25:55 +0000">&lt;p&gt;OK, i think seriously it would take major work to do something here that would make everyone happy.&lt;/p&gt;

&lt;p&gt;I still don't like the situation, but unless there are serious objections, I'd like to commit the javadocs,&lt;br/&gt;
just to hopefully reduce the amount of time this trap gets answered on the user list.&lt;/p&gt;</comment>
                    <comment id="13209630" author="hossman" created="Thu, 16 Feb 2012 19:17:44 +0000">&lt;p&gt;StrawMan suggestion off the top of my head:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;rename NOT_ANALYZED to something like KEYWORD_ANALYZED&lt;/li&gt;
	&lt;li&gt;document KEYWORD_ANALYZED as being a convenience flag (and/or optimization?) for achieving equivalent behavior as using PerFieldAnalyzer with KeywordAnalyzer for this field, and keep using / re-word rmuir's patch warning to make it clear that if you use this at index time, any attempts to construct queries against it using the QueryParser will need KeywordAnalyzer.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...would that flag name == analyzer name equivalence help people remember not to get trapped by this?&lt;/p&gt;</comment>
                    <comment id="13209697" author="rcmuir" created="Thu, 16 Feb 2012 20:20:40 +0000">&lt;p&gt;Hossman I think KEYWORD_ANALYZED is the ideal name for 3.x actually. I think in combination with the javadocs it would be more clear.&lt;/p&gt;

&lt;p&gt;This still leaves the question for trunk (currently StringField):&lt;br/&gt;
positives are that its actually a "nice" name, concise and to the point.&lt;br/&gt;
another positive is that StringField omits things like positions, and in trunk we don't silently fail if you form a phrase from this.&lt;/p&gt;

&lt;p&gt;one negative is that both StringField and TextField confusingly take String in their ctors, (I've chosen the wrong one myself before on accident).&lt;/p&gt;

&lt;p&gt;Basically to me, this is a combination of traps. Trunk is somewhat better because it throws exceptions for positional queries if&lt;br/&gt;
you actually excluded positions...&lt;/p&gt;

&lt;p&gt;in all cases in 3.x, the wrong 'configuration' here creates a situation where the user just 'does not get results' and they have&lt;br/&gt;
no idea why... despite the fact they used the same Analyzer at query-time and index-time like a good user. thats what I find so frustrating.&lt;/p&gt;</comment>
                    <comment id="13209973" author="cmale" created="Fri, 17 Feb 2012 01:43:17 +0000">&lt;blockquote&gt;
&lt;p&gt;one negative is that both StringField and TextField confusingly take String in their ctors, (I've chosen the wrong one myself before on accident).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Would renaming StringField, as suggested earlier, alleviate this? KeywordAnalyzedField?&lt;/p&gt;</comment>
                    <comment id="13210062" author="shaie" created="Fri, 17 Feb 2012 05:19:12 +0000">&lt;p&gt;A couple of comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;NOT_ANALYZED has two variants - with and without norms. I always uses NOT_ANALYZED_NO_NORMS. Is the KWD_ANALYZED suggestion meant to create two variants too? If not, we should somehow let the user be able to express a KWD analyzed field with and without norms.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;How about renaming StringField to KeywordField? Same as Chris's idea, but removing Analyzed ... shorter name and still captures the essence of it.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;And maybe TextField -&amp;gt; TokenizedField?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13210105" author="rcmuir" created="Fri, 17 Feb 2012 06:40:24 +0000">&lt;blockquote&gt;
&lt;p&gt;NOT_ANALYZED has two variants - with and without norms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You are right, I forgot about this. For NOT_ANALYZED with norms, we should probably just throw CoderMalfunctionError()&lt;/p&gt;</comment>
                    <comment id="13210135" author="thetaphi" created="Fri, 17 Feb 2012 08:29:53 +0000">&lt;p&gt;+1 for KeywordField&lt;/p&gt;

&lt;p&gt;For symmetry i would propose to add a KeywordQuery, too (that rewrites to new ConstantScoreQuery(new TermQuery())).&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310051">
                <name>Supercedes</name>
                                                <inwardlinks description="is superceded by">
                            <issuelink>
            <issuekey id="12606794">LUCENE-4369</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12514790" name="LUCENE-3792_javadocs_3x.patch" size="1022" author="rcmuir" created="Thu, 16 Feb 2012 11:46:10 +0000"/>
                    <attachment id="12514789" name="LUCENE-3792_javadocs_3x.patch" size="869" author="rcmuir" created="Thu, 16 Feb 2012 11:45:08 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 16 Feb 2012 10:44:43 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>228132</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23901</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3758] Allow the ComplexPhraseQueryParser to search order or un-order proximity queries.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3758</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The ComplexPhraseQueryParser use SpanNearQuery, but always set the "inOrder" value hardcoded to "true". This could be configurable.&lt;/p&gt;</description>
                <environment/>
            <key id="12541684">LUCENE-3758</key>
            <summary>Allow the ComplexPhraseQueryParser to search order or un-order proximity queries.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="tomasflobbe">Tomás Fernández Löbbe</reporter>
                        <labels>
                    </labels>
                <created>Tue, 7 Feb 2012 19:52:55 +0000</created>
                <updated>Fri, 10 May 2013 00:05:15 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>2</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="13204507" author="iorixxx" created="Thu, 9 Feb 2012 13:28:29 +0000">&lt;p&gt;patch for trunk&lt;/p&gt;</comment>
                    <comment id="13206996" author="rcmuir" created="Mon, 13 Feb 2012 17:27:02 +0000">&lt;p&gt;ok guys try to bare with me, since I don't know this thing at all:&lt;/p&gt;

&lt;p&gt;Would this apply to both exact and sloppy phrase queries?&lt;/p&gt;

&lt;p&gt;It seems to me, that instead of hardcoding inOrder to true, we should only set inOrder=true if its an exact phrase query&lt;br/&gt;
But if the user (not this qp itself, but the user actually used ~) supplied slop, then inOrder should be false.&lt;/p&gt;

&lt;p&gt;This would better emulate the behavior of the regular lucene queryparser... I'm wondering if we even need an option&lt;br/&gt;
since it just seems like the way it should work.&lt;/p&gt;</comment>
                    <comment id="13207614" author="dmitry_key" created="Tue, 14 Feb 2012 10:02:14 +0000">&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;If I were to implement two different versions of span near queries: with order and without order, would this class be the right point to start?&lt;br/&gt;
I'm thinking to add support for new query operator that would support order of terms in the near query, as (if I correctly understand), "~" operator doesn't preserve the order.&lt;/p&gt;</comment>
                    <comment id="13207710" author="dmitry_key" created="Tue, 14 Feb 2012 13:50:44 +0000">&lt;p&gt;Implemented new query operator "#", that allows to do what's described in my previous message. Let me know, if someone needs a "patch" for this.&lt;/p&gt;</comment>
                    <comment id="13207729" author="tomasflobbe" created="Tue, 14 Feb 2012 14:16:50 +0000">&lt;p&gt;Basically, a search like '"foo bar"#2' would match documents with the terms "foo" and "bar" with up to 2 positions of distance from each other and only if "foo" is before "bar"?&lt;/p&gt;</comment>
                    <comment id="13226021" author="dmitry_key" created="Fri, 9 Mar 2012 11:24:35 +0000">&lt;p&gt;That's correct Tomás. We have already internally tested this operator and it works just fine.&lt;/p&gt;</comment>
                    <comment id="13227514" author="tomasflobbe" created="Mon, 12 Mar 2012 13:28:45 +0000">&lt;p&gt;I think that makes sense. The query is different so it should have a different syntax.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310040">
                <name>Required</name>
                                                <inwardlinks description="is required by">
                            <issuelink>
            <issuekey id="12441718">SOLR-1604</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12513955" name="LUCENE-3758.patch" size="4696" author="iorixxx" created="Thu, 9 Feb 2012 13:28:29 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 9 Feb 2012 13:28:29 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>226972</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23935</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3689] DocValuesConsumer should implement closeable and release resource in close instead of during finish / flush</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3689</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;DocValuesConsumer currently doesn't have a close method and releases its resources during finish / flush. This is confusing, makes debugging more complex and mixes concerns. DocValuesConsumer should impl. Closeable and users should release resources safely.&lt;/p&gt;</description>
                <environment/>
            <key id="12538056">LUCENE-3689</key>
            <summary>DocValuesConsumer should implement closeable and release resource in close instead of during finish / flush</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Jan 2012 14:28:12 +0000</created>
                <updated>Fri, 10 May 2013 00:05:16 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13184099" author="simonw" created="Wed, 11 Jan 2012 14:30:54 +0000">&lt;p&gt;here is a patch that adds closeable to DocValuesConsumer and cleans up resource / stream management. I also renamed finish to flush since thats what it really is.&lt;/p&gt;
</comment>
                    <comment id="13184387" author="rcmuir" created="Wed, 11 Jan 2012 20:55:14 +0000">&lt;p&gt;I'm not sure we should rename finish to flush. I havent looked at how docvalues is using this, but I think it should be consistent with the &lt;br/&gt;
other per-document codec APIs (stored fields and term vectors)&lt;/p&gt;

&lt;p&gt;really they should flush on close(). finish is defined as just a hook for verification that you actually wrote the # of docs that the caller&lt;br/&gt;
thought it wrote:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;  /** Called before {@link #close()}, passing in the number
   *  of documents that were written. Note that this is 
   *  intentionally redundant (equivalent to the number of
   *  calls to {@link #startDocument(int)}, but a Codec should
   *  check that this is the case to detect the JRE bug described 
   *  in LUCENE-1282. */
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12510191" name="LUCENE-3689.patch" size="40403" author="simonw" created="Wed, 11 Jan 2012 14:30:54 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 11 Jan 2012 20:55:14 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>223563</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24003</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3664] Speed up SegementDocsAndPositionsEnum by making it more friendly for JIT optimizations</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3664</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3648" title="Speed up SegementDocsEnum by making it more friendly for JIT optimizations"&gt;&lt;del&gt;LUCENE-3648&lt;/del&gt;&lt;/a&gt; applied some refactoring to make SegmentDocsEnum reuse some code and divorce the liveDocs and no-liveDocs case into sep classes to make more friendly for jit optimizations. I did the same thing for SegmentDocsAndPositions&lt;span class="error"&gt;&amp;#91;AndPayloads&amp;#93;&lt;/span&gt;Enum removing a couple of hundred lines of code abstracting it into a base class. patch follows in a sec...&lt;/p&gt;</description>
                <environment/>
            <key id="12536133">LUCENE-3664</key>
            <summary>Speed up SegementDocsAndPositionsEnum by making it more friendly for JIT optimizations</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Thu, 22 Dec 2011 08:58:51 +0000</created>
                <updated>Fri, 10 May 2013 00:05:16 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13174702" author="simonw" created="Thu, 22 Dec 2011 09:00:33 +0000">&lt;p&gt;initial patch&lt;/p&gt;</comment>
                    <comment id="13174703" author="simonw" created="Thu, 22 Dec 2011 09:01:03 +0000">&lt;p&gt;here are some benchmark results using Java 1.6.0_30&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;

                Task   QPS trunkStdDev trunkQPS jit_patchStdDev jit_patch      Pct diff
        SloppyPhrase        2.73        0.16        2.72        0.15  -10% -   11%
      TermBGroup1M1P       43.89        0.53       44.53        0.50    0% -    3%
        TermBGroup1M       30.12        0.59       30.66        0.28   -1% -    4%
         TermGroup1M       25.77        0.63       26.26        0.46   -2% -    6%
                Term       49.82        2.63       51.23        2.15   -6% -   13%
              Fuzzy1       73.03        3.70       75.19        1.73   -4% -   10%
              Fuzzy2       48.70        2.29       50.23        0.97   -3% -   10%
             Respell       51.05        3.14       52.75        1.51   -5% -   13%
         AndHighHigh       11.18        0.36       11.56        0.21   -1% -    8%
           OrHighMed       10.70        0.32       11.12        0.26   -1% -    9%
          OrHighHigh        6.19        0.21        6.44        0.17   -2% -   10%
             Prefix3       53.55        1.84       55.73        1.03   -1% -    9%
            Wildcard       22.75        0.49       23.81        0.41    0% -    8%
            SpanNear        5.48        0.37        5.79        0.13   -3% -   15%
            PKLookup       96.23        2.02      101.66        4.39    0% -   12%
              Phrase        6.55        0.61        6.93        0.41   -9% -   23%
          AndHighMed       22.81        1.05       24.14        0.38    0% -   12%
              IntNRQ        5.75        0.52        6.27        0.38   -6% -   27%
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13555623" author="john.wang@gmail.com" created="Wed, 16 Jan 2013 23:48:10 +0000">&lt;p&gt;What do you think about moving the delete check into IndexSearcher? For most cases, the number of deletes are small and furthermore, but moving the delete check to the searcher, we handle this case implicitly while making sure the number of checks into liveDocs is &amp;lt;= the sum of the number of checks needed in each leaf node.&lt;/p&gt;</comment>
                    <comment id="13556328" author="simonw" created="Thu, 17 Jan 2013 15:50:45 +0000">&lt;p&gt;hey john,&lt;/p&gt;

&lt;p&gt;I agree this could safe a bunch of checks along the lines. For heavy queries this could be problematic ie phrases would check way more docs than needed if the live docs is very sparse. I think we should have it in both places but making it optional where to do it is an interesting idea. Depending on the cardinality we could do the checks in the IndexSearcher or rather in the top level collect method or in the Collector. &lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12508359" name="LUCENE-3664.patch" size="24710" author="simonw" created="Thu, 22 Dec 2011 09:00:32 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 16 Jan 2013 23:48:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>221817</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24028</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3659] Allow per-RAMFile buffer sizes based on IOContext and source of data (e.g. copy from another directory)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3659</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff from several dev@lao issues:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;&lt;a href="http://mail-archives.apache.org/mod_mbox/lucene-dev/201112.mbox/%3C001001ccbf1c%2471845830%24548d0890%24%40thetaphi.de%3E" class="external-link"&gt;http://mail-archives.apache.org/mod_mbox/lucene-dev/201112.mbox/%3C001001ccbf1c%2471845830%24548d0890%24%40thetaphi.de%3E&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;issue &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3653" title="Lucene Search not scalling"&gt;&lt;del&gt;LUCENE-3653&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The use cases for RAMDirectory are very limited and to prevent users from using it for e.g. loading a 50 Gigabyte index from a file on disk, we should improve the javadocs.&lt;/p&gt;</description>
                <environment/>
            <key id="12535851">LUCENE-3659</key>
            <summary>Allow per-RAMFile buffer sizes based on IOContext and source of data (e.g. copy from another directory)</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                    <parent id="12548131">LUCENE-3924</parent>
                        <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Tue, 20 Dec 2011 15:14:15 +0000</created>
                <updated>Fri, 10 May 2013 00:05:16 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13173247" author="erickerickson" created="Tue, 20 Dec 2011 15:27:14 +0000">&lt;p&gt;From the dev list, didn't want to lose this background (or make Uwe type it again &amp;lt;G&amp;gt;)&lt;/p&gt;

&lt;p&gt;The idea was to maybe replace RAMDirectory by a “clone” of MMapDirectory that uses large DirectByteBuffers outside the JVM heap. The current RAMDirectory is very limited (buffersize hardcoded to 8 KB, if you have a 50 Gigabyte Index in this RAMDirectory, your GC simply drives crazy – we investigated this several times for customers. RAMDirectory was in fact several times slower than a simple disk-based MMapDir). Also the locking on the RAMFile class is horrible, as for large indexes you have to change buffer several times when seeking/reading/…, which does heavily locking. In contrast, MMapDir is completely lock-free!&lt;/p&gt;

&lt;p&gt;Until there is no replacement we will not remove it, but the current RAMDirectory is not useable for large indexes. That’s a limitation and the design of this class does not support anything else. It’s currently unfixable and instead of putting work into fixing it, the time should be spent in working on a new ByteBuffer-based RAMDir with larger blocs/blocs that merge or IOContext helping to calculate the file size before writing it (e.g. when triggering a merge you know the approximate size of the file before, so you can allocate a buffer that’s better than 8 Kilobytes). Also directByteBuffer helps to make GC happy, as the RAMdir is outside JVM heap.....&lt;/p&gt;

&lt;p&gt;RAMdir uses more time for switching buffers than reading the data. The problem is that MMapDir does not support &lt;b&gt;writing&lt;/b&gt; and that why we plan to improve this. Have you tried MMapDir for read access in comparison to RAMDirectory for a larger index, it outperforms several times (depending on OS and if file data is in FS cache already). The new directory will simply mimic the MMapIndexInput, add MMapIndexOutput, but not based on a mmapped buffer, instead a in-memory (Direct)ByteBuffer (outside or inside JVM heap – both will be supported). This simplifies code a lot.&lt;/p&gt;

&lt;p&gt;The discussions about the limitations of crappy RAMDirectory were discussed on conferences, sorry. We did *not*decide to remove it (without a patch/replacement). The whole “message” on the issue was that RAMDirectory is a bad idea. The recommended approach at the moment to handle large in-ram directories would be to use a tmpfs on Linux/Solaris and use MMapDir on top (for larger indexes). The MMap would then directly map the RAM of the underlying tmpfs.....&lt;/p&gt;</comment>
                    <comment id="13173252" author="thetaphi" created="Tue, 20 Dec 2011 15:31:44 +0000">&lt;p&gt;It's even worse, it uses a buffer size of 1 Kilobyte:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;public&lt;/span&gt; class RAMOutputStream &lt;span class="code-keyword"&gt;extends&lt;/span&gt; IndexOutput {
  &lt;span class="code-keyword"&gt;static&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt; BUFFER_SIZE = 1024;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A 50 Gigabyte file means 52428800 byte[] arrays &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13173255" author="rcmuir" created="Tue, 20 Dec 2011 15:39:58 +0000">&lt;p&gt;I actually think heap versus direct is just an optimization (and ideally would just be an option, in my opinion not the default).&lt;br/&gt;
The problem is mostly tiny buffers.&lt;/p&gt;

&lt;p&gt;I think a good idea is to rename MMapIndexInput to ByteBufferIndexInput, it does not really care if something is mapped or not, it has all the logic for dealing with multiple fixed-size buffers.&lt;/p&gt;

&lt;p&gt;And RAMDirectory could then just use 1MB shift by default (normal heap array-backed buffers). Sure it wastes at most 1MB for tiny files but the RAMFile is wasteful today too.&lt;/p&gt;

&lt;p&gt;Down the road we could optimize this: e.g. add IOContext.METADATA for tiny files (segments.gen, .fnm, segments_N, .per, etc), and RAMDir could use say a 256KB shift there and 4MB otherwise. This iocontext flag could also be used if someone didnt want to MMap tiny-files too.&lt;/p&gt;</comment>
                    <comment id="13174946" author="kimchy" created="Thu, 22 Dec 2011 18:15:39 +0000">&lt;p&gt;Uwe: Hey, have you looked at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2292" title="ByteBuffer Directory - allowing to store the index outside the heap"&gt;LUCENE-2292&lt;/a&gt;, this can be a good candidate to use to replace RAM directory. I just "refreshed" it and it seems to work find (all tests pass).&lt;/p&gt;</comment>
                    <comment id="13188008" author="simonw" created="Tue, 17 Jan 2012 21:20:10 +0000">&lt;p&gt;should we make this a blocker for 4.0?&lt;/p&gt;</comment>
                    <comment id="13188111" author="thetaphi" created="Tue, 17 Jan 2012 23:08:21 +0000">&lt;p&gt;If I remember the misinformation about using RAMDir going on, yes, it's a blocker - just look at java-user@lao last week, those mails about people saying "I want to copy my  20 Gigabyte FSDir to a RAMDir because it's faster as it has the word 'RAM' in it" looks more like a XY problem than a good reason for using it). MMapDir and even NIOFSDir work mostly from RAM, as the OS will cache for you.&lt;/p&gt;

&lt;p&gt;On the other hand, it's just documentation we can do it always.&lt;/p&gt;

&lt;p&gt;Fixing the default buffer size to something more suitable for real-world use cases is something that can be done with a one-line-patch. I would prefer 64 Kilobytes buffer size.&lt;/p&gt;</comment>
                    <comment id="13235083" author="thetaphi" created="Wed, 21 Mar 2012 21:59:00 +0000">&lt;p&gt;I think we should fix at least JavaDocs for 3.6 and maybe raise buffersize. I will provide a patch tomorrow.&lt;/p&gt;</comment>
                    <comment id="13237605" author="thetaphi" created="Sat, 24 Mar 2012 17:16:46 +0000">&lt;p&gt;I started to work on this, here is just a first step (trunk). This patch removes the BUFFER_SIZE constant and moves it up to RAMDirectory (but for now only as default, see below!). RAMDirectory inherits the default buffersize for now to its RAMFile childs (newRAMFile() method), but this can likely change (see below).&lt;/p&gt;

&lt;p&gt;As every RAMFile has its own buffer size, optimizations are possible:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;when you open an IndexOutput, in trunk we get the IOContext, which may contain a Merge/Flush desc containing the complete segment size (unfortunately the &lt;b&gt;complete&lt;/b&gt; segment size). But this number can be used as a order of magnitude for specifiing the buffer size.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The patch does not yet implement that, but an idea would be to maybe allocate 1/32 of the segment size as buffer size. By that the buffer size does not get too big, but on the other hand the number of slices has an upper limit (approx 32 slices per merged segment). Currently a merged segment with a size of say 32 Gigabytes would have 32 million byte[] arrays, after the change only 32 byte[] arrays with a size of 1 Gigabyte each. This should make GC happy.&lt;/p&gt;

&lt;p&gt;When backporting to 3.x, the IOContext is not yet available and RAMDirectory always uses the default buffer size (maybe randomize in tests). Rainsing the buffer size should bring improvements here.&lt;/p&gt;

&lt;p&gt;We should still add some warnings into the Javadocs, that for &lt;b&gt;large&lt;/b&gt; indexes it is often preferable to use MMapDir, especially when you store it on disk. We should also peple tell that new RAMDirectoty(OtherDirectory) maybe a bad idea...&lt;/p&gt;

&lt;p&gt;The new default buffer size was raised from 1024 to 8192.&lt;/p&gt;</comment>
                    <comment id="13237927" author="rcmuir" created="Sun, 25 Mar 2012 17:57:44 +0100">&lt;p&gt;Honestly I don't even have the time to review the patch: I'm sure Uwe's changes (as always)&lt;br/&gt;
are very nice and thorough.&lt;/p&gt;

&lt;p&gt;I just want to propose the idea of a javadocs-only fix for 3.6: I am afraid of any .store changes,&lt;br/&gt;
except serious bugfixes (with serious tests to go with them) this close to release.&lt;/p&gt;</comment>
                    <comment id="13238274" author="thetaphi" created="Mon, 26 Mar 2012 12:12:16 +0100">&lt;p&gt;A played a little bit around and implemented the IOContext / filename dependent buffer sizes for RAMFiles.&lt;/p&gt;

&lt;p&gt;The code currently prints out lot's of size infornation (like buffer sizes) on RAMDirectory.close(). This is just for debugging and to show what happens.&lt;/p&gt;

&lt;p&gt;To catually see real-world use cases, execute tests with ant test -Dtests.directory=RAMDirectory -Dtests.nightly=true&lt;/p&gt;</comment>
                    <comment id="13238286" author="thetaphi" created="Mon, 26 Mar 2012 12:33:46 +0100">&lt;p&gt;More improvements:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;If you use new RAMDirectory(existingDir), the RAMFiles in the created RAMDirectory will have the original fileSize (if less then 1L &amp;lt;&amp;lt; 30 bytes) as bufferSize, as we know the file size upfront.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13238402" author="mikemccand" created="Mon, 26 Mar 2012 15:00:20 +0100">&lt;p&gt;This looks great Uwe!&lt;/p&gt;

&lt;p&gt;I'm a little worried about the tiny file case; you're checking for&lt;br/&gt;
SEGMENTS_* now, but many other files can be much smaller than 1/64th&lt;br/&gt;
of the estimated segment size.&lt;/p&gt;

&lt;p&gt;I wonder if we should "improve" IOContext to hold the &lt;span class="error"&gt;&amp;#91;rough&amp;#93;&lt;/span&gt;&lt;br/&gt;
estimated file size (not just overall segment size)... the thing is&lt;br/&gt;
that's sort of a hassle on codec impls.&lt;/p&gt;

&lt;p&gt;Or: maybe, on closing the ROS/RAMFile, we can downsize the final&lt;br/&gt;
buffer (yes, this means copying the bytes, but that cost is vanishingly&lt;br/&gt;
small as the RAMDir grows).  Then tiny files stay tiny, though they&lt;br/&gt;
are still &lt;span class="error"&gt;&amp;#91;relatively&amp;#93;&lt;/span&gt; costly to create...&lt;/p&gt;

&lt;p&gt;I don't this RAMDir.createOutput should publish the RAMFile until the&lt;br/&gt;
ROS is closed?  Ie, you are not allowed to openInput on something&lt;br/&gt;
still opened with createOutput in any Lucene Dir impl..?  This would&lt;br/&gt;
allow us to make RAMFile frozen (eg if ROS holds its own buffers and&lt;br/&gt;
then creates RAMFile on close), that requires no sync when reading?&lt;/p&gt;

&lt;p&gt;I also don't think RAMFile should be public, ie, the only way to make&lt;br/&gt;
changes to a file stored in a RAMDir is via RAMOutputStream.  We can&lt;br/&gt;
do this separately...&lt;/p&gt;

&lt;p&gt;Maybe we should pursue a growing buffer size...?  Ie, where each newly&lt;br/&gt;
added buffer is bigger than the one before (like ArrayUtil.oversize's&lt;br/&gt;
growth function)... I realize that adds complexity&lt;br/&gt;
(RAMInputStream.seek is more fun), but this would let tiny files use&lt;br/&gt;
tiny RAM and huge files use few buffers.  Ie, RAMDir would scale up&lt;br/&gt;
and scale down well.&lt;/p&gt;

&lt;p&gt;Separately: I noticed we still have IndexOutput.setLength, but, nobody&lt;br/&gt;
calls it anymore I think?  (In 3.x we call this when creating a CFS).&lt;br/&gt;
Maybe we should remove it...&lt;/p&gt;</comment>
                    <comment id="13238407" author="rcmuir" created="Mon, 26 Mar 2012 15:04:15 +0100">&lt;blockquote&gt;
&lt;p&gt;I'm a little worried about the tiny file case; you're checking for&lt;br/&gt;
SEGMENTS_* now, but many other files can be much smaller than 1/64th&lt;br/&gt;
of the estimated segment size.&lt;/p&gt;

&lt;p&gt;I wonder if we should "improve" IOContext to hold the &lt;span class="error"&gt;&amp;#91;rough&amp;#93;&lt;/span&gt;&lt;br/&gt;
estimated file size (not just overall segment size)... the thing is&lt;br/&gt;
that's sort of a hassle on codec impls.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe its enough for IOContext to specify that its writing a 'metadata'&lt;br/&gt;
file? These are all the tiny ones (fieldinfos, segmentinfos, .cfe, etc),&lt;br/&gt;
as opposed to 'real files' like frq or prx that are expected to be possibly huge.&lt;/p&gt;
</comment>
                    <comment id="13238441" author="thetaphi" created="Mon, 26 Mar 2012 15:45:16 +0100">&lt;p&gt;Robert: That was the first idea that came to my mind, too. I think thats a good idea. It especially strange that the segments_xx/segments.gen file (which is not part of the current segment) is written with MERGE/FLUSH context. It should be written with a standard context? Or do I miss something? (This was the reason why I added the file name check). Initially I was expecting that writing the commit is done with a separate IOContext, but it isn't - the noisy debugging helps.&lt;/p&gt;</comment>
                    <comment id="13238448" author="rcmuir" created="Mon, 26 Mar 2012 15:55:08 +0100">&lt;p&gt;I think if we were to implement it this way, its not a burden on codecs.&lt;br/&gt;
By default, somewhere in lucene core inits the codec APIs with a context always.&lt;br/&gt;
For example SegmentInfos.write():&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
infosWriter.writeInfos(directory, segmentFileName, codec.getName(), &lt;span class="code-keyword"&gt;this&lt;/span&gt;, IOContext.DEFAULT);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and DocFieldProcessor/SegmentMerger for fieldinfos:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These guys would just set this in the IOContext. Most/All codecs just pass this along.&lt;br/&gt;
If a codec wants to ignore the IOContext and lie about it, thats its own choice.&lt;br/&gt;
So I think its an easy change.&lt;/p&gt;</comment>
                    <comment id="13238451" author="mikemccand" created="Mon, 26 Mar 2012 15:57:01 +0100">&lt;p&gt;I'm torn on the binary "metadata" idea... not all files cleanly fall into one category?&lt;/p&gt;

&lt;p&gt;Eg what about live doc bits?  It can easily be tiny (we write a sparse set sparsely).&lt;/p&gt;

&lt;p&gt;Indices w/ immense docs will also start to look like they have tiny files that are not metadata (eg, fdx file, if they don't store fields).&lt;/p&gt;</comment>
                    <comment id="13238453" author="rcmuir" created="Mon, 26 Mar 2012 15:57:45 +0100">&lt;p&gt;But also codecs that write their own private tiny metadata files (like .per from PerFieldPostingsFormat)&lt;br/&gt;
should set this in the context.&lt;/p&gt;</comment>
                    <comment id="13238454" author="rcmuir" created="Mon, 26 Mar 2012 15:58:58 +0100">&lt;p&gt;Live docs aren't a metadata. I think you are conflating 'tiny' with 'metadata'.&lt;/p&gt;

&lt;p&gt;I'm saying we should declare its metadata, thats all. This is pretty black and white!&lt;/p&gt;

&lt;p&gt;IF a directory wants to, as a heuristic, interpret metadata == tiny, then thats fine,&lt;br/&gt;
but thats separate.&lt;/p&gt;</comment>
                    <comment id="13239407" author="thetaphi" created="Tue, 27 Mar 2012 13:36:01 +0100">&lt;blockquote&gt;
&lt;p&gt;I also don't think RAMFile should be public, ie, the only way to make&lt;br/&gt;
changes to a file stored in a RAMDir is via RAMOutputStream. We can&lt;br/&gt;
do this separately...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;RAMFile's public ctor without directory is only used by PrefixCodedTerms, itself used only by FrozenBufferedDeletes. I don't really see the real use case to do this like that. We can maybe replace that using a FST (it is already sorted by BytesRef) or using PagesBytes? Alternatively replace the whole thing with OutputStreamDataOutput(new ByteArrayOutputStream())?&lt;/p&gt;</comment>
                    <comment id="13239463" author="rcmuir" created="Tue, 27 Mar 2012 14:54:57 +0100">&lt;p&gt;oops: I did that, sorry. &lt;/p&gt;

&lt;p&gt;it just wants a thing that combined byte[] slices that you can &lt;br/&gt;
get datainput from, so it seemed like the right thing?&lt;/p&gt;</comment>
                    <comment id="13418498" author="thetaphi" created="Thu, 19 Jul 2012 18:52:42 +0100">&lt;p&gt;Hi,&lt;br/&gt;
I will soon work again on that. I have some comments:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;We can remove the heavy synchronization bottleneck on RAMFile. RAMFile should only have final fields and should be created after the file is written., This should improve performance alltogether. The current synchronization is needed to "emulate" real file system behaviour (file is visible in directory with 0 bytes once created). This behaviour is not needed at all by Lucene. We should make the file visible in the ConcurrentHashMap of RAMDirectory once the IndexOutput is closed! We should create the RAMFile instance on this stage not before (so all is final). By this all sync on RAMFile can be removed.&lt;/li&gt;
	&lt;li&gt;We should add IOContext.META&lt;/li&gt;
	&lt;li&gt;Maybe we should rename RAMDirectory in trunk / 4.x to HeapDirectory. So we can have other impls like DirectBufferDirectory or whatever (see Shay Bannon's &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2292" title="ByteBuffer Directory - allowing to store the index outside the heap"&gt;LUCENE-2292&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13418509" author="rcmuir" created="Thu, 19 Jul 2012 19:00:05 +0100">&lt;p&gt;There is a patch somewhere to factor out the MMapIndexInput into a general ByteBufferIndexInput if you follow the same rules.&lt;/p&gt;

&lt;p&gt;I think we can just use that? you can have a direct and array-backed version (just have some hook to allocate a new ByteBuffer of some size). I think we should just start with the array-backed one for simplicity. Maybe the direct one can avoid some arrays bounds checks, but otherwise its not really&lt;br/&gt;
related to the stupidity of current ramdirectory. arrays are fine, its just they shouldnt be so tiny etc.&lt;/p&gt;</comment>
                    <comment id="13435915" author="thetaphi" created="Thu, 16 Aug 2012 12:55:12 +0100">&lt;p&gt;Patch updated to trunk. I will work on this soon.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12541206" name="LUCENE-3659.patch" size="15149" author="thetaphi" created="Thu, 16 Aug 2012 12:55:12 +0100"/>
                    <attachment id="12519944" name="LUCENE-3659.patch" size="15666" author="thetaphi" created="Mon, 26 Mar 2012 12:47:01 +0100"/>
                    <attachment id="12519938" name="LUCENE-3659.patch" size="14733" author="thetaphi" created="Mon, 26 Mar 2012 12:12:16 +0100"/>
                    <attachment id="12519809" name="LUCENE-3659.patch" size="10747" author="thetaphi" created="Sat, 24 Mar 2012 17:16:46 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 20 Dec 2011 15:27:14 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>221535</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24033</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3614] Add a JUL/SLF4J example InfoStream implementation so IndexWriter can log to JUL/SLF4J</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3614</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Followup to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3598" title="Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-logging"&gt;&lt;del&gt;LUCENE-3598&lt;/del&gt;&lt;/a&gt;: Hoss suggested to add a default JUL/SLF4J implementation to contrib/misc (that can also be used by SOLR to log IndexWriter verbose messages to its logging framework).&lt;/p&gt;</description>
                <environment/>
            <key id="12533444">LUCENE-3614</key>
            <summary>Add a JUL/SLF4J example InfoStream implementation so IndexWriter can log to JUL/SLF4J</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Dec 2011 12:58:04 +0000</created>
                <updated>Fri, 10 May 2013 00:05:16 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>1</votes>
                        <watches>2</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>219173</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24078</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3610] Revamp spatial APIs that use primitives (or arrays of primitives) in their args/results so that they use strongly typed objects</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3610</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;My "spatial awareness" is pretty meek, but &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3599" title="haversine() is broken / misdocumented"&gt;&lt;del&gt;LUCENE-3599&lt;/del&gt;&lt;/a&gt; seems like a prime example of the types of mistakes that are probably really easy to make with all of the Spatial related APIs that deal with arrays (or sequences) of doubles where specific indexes of those arrays (or sequences) have significant meaning: mainly latitude vs longitude.&lt;/p&gt;

&lt;p&gt;We should probably reconsider any method that takes in double[] or multiple doubles to express latlon pairs and rewrite them to use the existing LatLng class &amp;#8211; or if people think that class is too heavyweight, then add a new lightweight class to handle the strong typing of a basic latlon point instead of just passing around a double&lt;span class="error"&gt;&amp;#91;2&amp;#93;&lt;/span&gt; or two doubles called "x" and "y" ...&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-keyword"&gt;static&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; class SimpleLatLonPointInRadians {
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;double&lt;/span&gt; latitude;
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;double&lt;/span&gt; longitude;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...then all those various methods that expect lat+lon pairs in radians (like DistanceUtils.haversine, DistanceUtils.normLat, DistanceUtils.normLng, DistanceUtils.pointOnBearing, DistanceUtils.latLonCorner, etc...) can start having APIs that don't make your eyes bleed when you start trying to understand what order the args go in.&lt;/p&gt;</description>
                <environment/>
            <key id="12533215">LUCENE-3610</key>
            <summary>Revamp spatial APIs that use primitives (or arrays of primitives) in their args/results so that they use strongly typed objects</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="hossman">Hoss Man</reporter>
                        <labels>
                    </labels>
                <created>Tue, 29 Nov 2011 22:58:16 +0000</created>
                <updated>Fri, 10 May 2013 00:05:16 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/spatial</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13159714" author="yseeley@gmail.com" created="Wed, 30 Nov 2011 00:47:10 +0000">&lt;p&gt;I think we should just avoid using "x" and "y", and use "lat" and "lon".&lt;br/&gt;
The error was in parameter naming only, but was never apparent to me because when I saw "x", I thought "first parameter" which is "lat" in a lat,lon pair (and this is why the params were always passed correctly... it didn't cause confusion until someone with more geo experience looked at it).&lt;/p&gt;</comment>
                    <comment id="13159718" author="hossman" created="Wed, 30 Nov 2011 00:51:24 +0000">&lt;p&gt;That's one example ... some of the other methods i was mentioned deal with (either as an argument or as a result) two element arrays of doubles ... which one comes first in the array? ... you can't fix that by renaming the variable.&lt;/p&gt;

&lt;p&gt;in either case: is there really any downside against having a simple object that wraps the latlon pair so you &lt;b&gt;always&lt;/b&gt; know which one is which?  is the RAM/GC overhead of an object like i described really significant compared to a double&lt;span class="error"&gt;&amp;#91;2&amp;#93;&lt;/span&gt; ?&lt;/p&gt;</comment>
                    <comment id="13159729" author="yseeley@gmail.com" created="Wed, 30 Nov 2011 01:25:47 +0000">&lt;blockquote&gt;&lt;p&gt;in either case: is there really any downside against having a simple object that wraps the latlon pair so you always know which one is which?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We should always do lat first (and I believe we do).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;is the RAM/GC overhead of an object like i described really significant compared to a double&lt;span class="error"&gt;&amp;#91;2&amp;#93;&lt;/span&gt; ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The object (compared to a double[]) could actually be better - no array bounds checks.  If you're only talking about replacing double&lt;span class="error"&gt;&amp;#91;2&amp;#93;&lt;/span&gt; w/ an object, I'm all for it.  Otherwise, it's case by case - it depends if it's going to be used in an inner loop.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 30 Nov 2011 00:47:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>218944</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24082</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3581] IndexReader#isCurrent() should return true on a NRT reader if no deletes are applied and only deletes are present in IW</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3581</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I keep forgetting about this, I better open an issue. If you have a NRT reader without deletes applied it should infact return true on IR#isCurrent() if the IW only has deletes in its buffer ie. no documents where updated / added since the NRT reader was opened. Currently if there is a delete coming in we force a reopen which does nothing since deletes are not applied anyway.&lt;/p&gt;</description>
                <environment/>
            <key id="12531947">LUCENE-3581</key>
            <summary>IndexReader#isCurrent() should return true on a NRT reader if no deletes are applied and only deletes are present in IW</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 18 Nov 2011 15:00:33 +0000</created>
                <updated>Fri, 10 May 2013 00:05:16 +0100</updated>
                                    <version>3.5</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13237926" author="mikemccand" created="Sun, 25 Mar 2012 17:56:36 +0100">&lt;p&gt;This need not block 3.6.0 right?&lt;/p&gt;

&lt;p&gt;We are returning "false" when we could return "true" from isCurrent, but this just means the app will go through the reopen when it didn't have to...?  Ie relatively minor?&lt;/p&gt;</comment>
                    <comment id="13541466" author="markrmiller@gmail.com" created="Mon, 31 Dec 2012 19:13:35 +0000">&lt;p&gt;Fixing this for 4.1 or should we push it?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 25 Mar 2012 16:56:36 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>217684</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24110</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3489] Refactor test classes that use assumeFalse(codec != SimpleText, Memory) to use new annotation and move the expensive methods to separate classes</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3489</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Folloup for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3463" title="Jenkins trunk tests (nightly only) fail quite often with OOM in Automaton/FST tests"&gt;&lt;del&gt;LUCENE-3463&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TODO:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Move test-methods that need the new @UseNoMemoryExpensiveCodec annotation to separate classes&lt;/li&gt;
	&lt;li&gt;Eliminate the assumeFalse-calls that check the current codec and disable the test if SimpleText or Memory is used&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12525818">LUCENE-3489</key>
            <summary>Refactor test classes that use assumeFalse(codec != SimpleText, Memory) to use new annotation and move the expensive methods to separate classes</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Wed, 5 Oct 2011 14:19:58 +0100</created>
                <updated>Fri, 10 May 2013 00:05:17 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13120942" author="rcmuir" created="Wed, 5 Oct 2011 14:25:51 +0100">&lt;p&gt;you know, another similar issue we have is tests that assumeFalse(codec != PreFlex), because of things like new index statistics or byte terms or other features that it doesn't support.&lt;/p&gt;

&lt;p&gt;Maybe there is some way we could generalize the annotation?&lt;/p&gt;

&lt;p&gt;something like @AvoidCodecs("SimpleText", "Memory"), @AvoidCodecs("PreFlex"), and this set would be handled like the boolean today?&lt;/p&gt;</comment>
                    <comment id="13120957" author="thetaphi" created="Wed, 5 Oct 2011 14:31:51 +0100">&lt;p&gt;Nice idea, this can be easily transformed to a annotation with param! Of course it would be per-class, too.&lt;/p&gt;</comment>
                    <comment id="13121020" author="dweiss" created="Wed, 5 Oct 2011 15:57:12 +0100">&lt;p&gt;While working on my presentation I've been trying to generalize the concept of "randomized tests". There's definitely a lot of great concepts here, but they're closely coupled with Lucene and the rest of the Solr/Lucene infrastructure. &lt;/p&gt;

&lt;p&gt;I have a draft code of a RandomizedTesting framework that provides very much similar functionality, although in a slightly different technical way (for example it's based on JUnit @Rules only, not on a custom runner/ base abstract class). I would really like you to peek at this and, perhaps with some effort, generalize the concepts in the test framework instead of introducing more Lucene-specific annotations.&lt;/p&gt;

&lt;p&gt;I'll publish the code tomorrow (it still needs some, ehm, polishing) along with some thoughts that I had about the current code in LuceneTestCase/Runner. &lt;/p&gt;

&lt;p&gt;I'd really like this to evolve into something of a stand-alone project (even if bundled with Lucene) so that other project can benefit without necessarily rely on the rest of Lucene code. We're already using a somewhat decoupled code internally and making it really cross-project applicable is a great way of proving these concepts are generally useful.&lt;/p&gt;

&lt;p&gt;Until tomorrow? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13121077" author="thetaphi" created="Wed, 5 Oct 2011 17:06:19 +0100">&lt;blockquote&gt;&lt;p&gt;(for example it's based on JUnit @Rules only, not on a custom runner/ base abstract class)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is a nice idea, although the reason for the LuceneTestCaseRunner and the abstract base class is more because we hate @Test annotations and dont want to add thousands of stupid Assert-includes (having this is abstract base class is more convenient). Just to mention UweSays on twitter: &lt;a href="http://twitter.com/#!/UweSays" class="external-link"&gt;@UweSays&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13121098" author="dweiss" created="Wed, 5 Oct 2011 17:19:46 +0100">&lt;p&gt;Yeah, I figured that you want to keep it compact. These may be compatible because there's nothing forbidding us to keep LuceneTestCase as a base class (descending from Assert and providing Lucene-related infrastructure). I'm just trying to push all the randomization (seed initialization, reproducibility, thread controls) out of LuceneTestCase and into something more generic. So far it looks good to my eyes, but I'll be looking forward to your strict German opinion, Uwe &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Oh, by the way &amp;#8211; is there any particular reason for so many things to be static (class level)? I get these are fixtures reused by tests but would people scream if they were object-level fixtures rather than class-level fixtures? It'd make things a bit easier... starting with the need for a single initial seed, for example.&lt;/p&gt;</comment>
                    <comment id="13121102" author="rcmuir" created="Wed, 5 Oct 2011 17:27:19 +0100">&lt;blockquote&gt;
&lt;p&gt;Oh, by the way – is there any particular reason for so many things to be static (class level)? I get these are fixtures reused by tests but would people scream if they were object-level fixtures rather than class-level fixtures? It'd make things a bit easier... starting with the need for a single initial seed, for example.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;why we have the different seeds:&lt;br/&gt;
One thing we do is support running a test class (test1(), test2(), test3()). If test2() fails, we want to be able to just run that method and reproduce it.&lt;br/&gt;
So we allow you to specify -Dtestmethod to only run a single method.&lt;/p&gt;

&lt;p&gt;At the same time, we want to support doing things like creating indexes in beforeClass() and afterClass() for efficient tests.&lt;br/&gt;
We also support -Dtests.iter, where you run a single test method over and over... this is often convenient. If we only had 1 class-level seed, this would&lt;br/&gt;
be useless as it would just do the same thing over and over!&lt;/p&gt;

&lt;p&gt;So the need for multiple seeds comes from the fact that some things are random at "class-level" and some things are at "method level". &lt;br/&gt;
If you look at the 3 parts to the random seed, its really part1:part2:part3,&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;part1 = class seed&lt;/li&gt;
	&lt;li&gt;part2 = method seed&lt;/li&gt;
	&lt;li&gt;part3 = runner seed (this is needed for consistent randomization of test methods)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13121105" author="thetaphi" created="Wed, 5 Oct 2011 17:29:41 +0100">&lt;blockquote&gt;&lt;p&gt;Yeah, I figured that you want to keep it compact. These may be compatible because there's nothing forbidding us to keep LuceneTestCase as a base class (descending from Assert and providing Lucene-related infrastructure).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I just wanted to mention this.&lt;/p&gt;

&lt;p&gt;The other stuff in LuceneTestRunner is just to work around some limitations in JUnit's @BeforeClass: @BeforeClass does not pass the Class object to the annotated method, and you cannot find out which child class is initialized. So checking for annotations on the implementation class from the abstract LuceneTestCase base class does not work.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Oh, by the way – is there any particular reason for so many things to be static (class level)? I get these are fixtures reused by tests but would people scream if they were object-level fixtures rather than class-level fixtures? It'd make things a bit easier... starting with the need for a single initial seed, for example.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reason is simple: We want those per test-class lifetime, but JUnit allocates a new class instance for each test method. And lot's of Lucene tests use @BeforeClass to produce indexes (random) static indexes, then used by all test methods in a read-only way. Currently we have 3 seeds, one for class-level stuff, one for instance stuff and a third one for the runner (according to Mister &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rcmuir" class="user-hover" rel="rcmuir"&gt;Robert Muir&lt;/a&gt;: &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3362" title="Initialization error of Junit tests with solr-test-framework with IDEs and Maven"&gt;&lt;del&gt;LUCENE-3362&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The randoms must therefore be static and initialized in @BeforeClass.&lt;/p&gt;</comment>
                    <comment id="13121116" author="rcmuir" created="Wed, 5 Oct 2011 17:37:52 +0100">&lt;p&gt;The need for the runner seed is explained in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3362" title="Initialization error of Junit tests with solr-test-framework with IDEs and Maven"&gt;&lt;del&gt;LUCENE-3362&lt;/del&gt;&lt;/a&gt;. One problem is the "Test lifecyle" of junit is ill-defined, it depends on how you are running tests!&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;The problem is that via ant, tests work like this (e.g. for 3 test classes):
computeTestMethods
beforeClass
afterClass
computeTestMethods
beforeClass
AfterClass
computeTestMethods
beforeClass
afterClass

but via an IDE or maven, if you run it from a folder like you did, then it does this:
computeTestMethods
computeTestMethods
computeTestMethods
beforeClass
afterClass
beforeClass
afterClass
beforeClass
afterClass 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13121182" author="dweiss" created="Wed, 5 Oct 2011 18:32:17 +0100">&lt;p&gt;Thanks. I did go through the code, so I know where the seeds are used and had a pretty much good understanding as to why. &lt;/p&gt;

&lt;p&gt;As for the different lifecycle - this is weird, but isn't it a direct consequence of subclassing BlockJUnit4ClassRunner and relying on what it internally does? This is what's causing the problem (superclass impl. changing over time - I think you just hit two different junit versions in that issue). &lt;/p&gt;

&lt;p&gt;Perhaps I was wrong and a custom runner is indeed needed, but if so then I still think a single seed (logically) would be fine. A custom runner then:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;collects methods to be executed (per-class)&lt;/li&gt;
	&lt;li&gt;initializes the global init seed/ random. This random becomes the initial randomness source for everything that follows to make it repeatable.&lt;/li&gt;
	&lt;li&gt;shuffles methods,&lt;/li&gt;
	&lt;li&gt;execute any @BeforeClass rules (see note below),&lt;/li&gt;
	&lt;li&gt;for each selected method (-Dtestmethod limits the selection and acts as a filter), repeat test.iter-times (seed changes predictably): 
{initialize per-method starting seed based on the current random, create a new test instance, execute}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;execute any @AfterClass rules&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The question how to randomize class-level fixtures could be answered by a static utility method that would return the per-class seed using ThreadLocal or a thread map updated by the runner. Still predictable and repeatable.&lt;/p&gt;

&lt;p&gt;I'll chew a bit on the possibilities and report back tomorrow.&lt;/p&gt;</comment>
                    <comment id="13121189" author="dweiss" created="Wed, 5 Oct 2011 18:35:11 +0100">&lt;p&gt;Re-reading the above algorithm I think I'll make it clearer: my point is that you can write repeatable runner by starting from a single initial seed and assigning initial seeds to all execution start points (tests) regardless of whether they are executed or not (and how many times). Hope I'm a bit clear(er) now.&lt;/p&gt;</comment>
                    <comment id="13121190" author="rcmuir" created="Wed, 5 Oct 2011 18:36:14 +0100">&lt;blockquote&gt;
&lt;p&gt;This is what's causing the problem (superclass impl. changing over time - I think you just hit two different junit versions in that issue). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I disagree. I used the same junit version (4.7) myself in both eclipse and via ant to deal with this problem. It has nothing to do with that.&lt;/p&gt;

&lt;p&gt;The junit test lifecycle is really undefined just as I described, its unfortunate.&lt;/p&gt;</comment>
                    <comment id="13121193" author="rcmuir" created="Wed, 5 Oct 2011 18:38:32 +0100">&lt;p&gt;And just so you know, its not possible i could have used 4.8 here, because all of our tests fail with 4.8&lt;/p&gt;

&lt;p&gt;Thats because of breaks in the lifecycle of TestWatchMan (Its initialized before the @Before's in 4.8, not in 4.7).&lt;br/&gt;
A separate problem, but just something to mention. currently you cannot use junit 4.8 with lucene's tests for this reason.&lt;/p&gt;</comment>
                    <comment id="13121285" author="rcmuir" created="Wed, 5 Oct 2011 18:58:19 +0100">&lt;p&gt;One last thing, thinking thru the simplifications Dawid is looking at doing, &lt;br/&gt;
and knowing how horrible the code currently is, we could consider trying some things like:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;upgrade/fix our tests to work with latest junit? maybe there are less frustrations&lt;/li&gt;
	&lt;li&gt;contribute some of the more general things like assume(String message, xxx) to junit to get them out of our codebase?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13273642" author="rcmuir" created="Fri, 11 May 2012 22:40:45 +0100">&lt;p&gt;attached is a patch generalizing the UseNoExpensiveMemory annotation to @AvoidCodecs that takes a list of codecs to avoid.&lt;/p&gt;

&lt;p&gt;This way, tests that cannot work with Lucene3x codec can just avoid it, using another codec, rather than assuming (in general its bad that many of the tests of actual new functionality often dont run at all because of the current assumes) &lt;/p&gt;</comment>
                    <comment id="13273811" author="thetaphi" created="Sat, 12 May 2012 03:18:50 +0100">&lt;p&gt;I like the annotation. Can we maybe change it to look like @SuppressWarnings? So it does not need codecs={} or if there is only one codec, no {} at all? Should be not too hard?&lt;/p&gt;

&lt;p&gt;Otherwise strong +1!&lt;/p&gt;</comment>
                    <comment id="13273812" author="thetaphi" created="Sat, 12 May 2012 03:20:48 +0100">&lt;p&gt;It's easy, just rename codecs to "String[] value" and you are done. After that you can use @AvoidCodecs("SimpleText") or @AvoidCodecs(&lt;/p&gt;
{"SimpleText","Lucene3x"}
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;See: &lt;a href="http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/SuppressWarnings.html" class="external-link"&gt;http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/SuppressWarnings.html&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13273856" author="rcmuir" created="Sat, 12 May 2012 07:21:33 +0100">&lt;p&gt;I agree, this is the main problem with the current patch. We should fix this before committing.&lt;/p&gt;</comment>
                    <comment id="13273996" author="rcmuir" created="Sat, 12 May 2012 14:18:47 +0100">&lt;p&gt;updated patch using value[], much less wordy. &lt;/p&gt;

&lt;p&gt;I will commit soon.&lt;/p&gt;</comment>
                    <comment id="13273998" author="rcmuir" created="Sat, 12 May 2012 14:29:20 +0100">&lt;p&gt;ok one last change, renamed to SuppressCodecs (it actually is not just funny, but better since it works the same way etc)&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12524570">LUCENE-3463</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12522682">LUCENE-3429</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12526024">LUCENE-3492</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12526637" name="LUCENE-3489.patch" size="34329" author="rcmuir" created="Sat, 12 May 2012 14:29:20 +0100"/>
                    <attachment id="12526636" name="LUCENE-3489.patch" size="34179" author="rcmuir" created="Sat, 12 May 2012 14:18:46 +0100"/>
                    <attachment id="12526575" name="LUCENE-3489.patch" size="34433" author="rcmuir" created="Fri, 11 May 2012 22:40:45 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 5 Oct 2011 13:25:51 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>46432</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24201</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3460] Move handling of query only containing MUST_NOT to QueryParser (and remove QueryUtils.makeQueryable() hack in Solr)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3460</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;With the parent issue, users entering (a -b) into the queryparser can simply fail with an UnsupportedOperationException, if "a" is a stopword.&lt;/p&gt;

&lt;p&gt;Solr already has a hack to add a MatchAllDocsQuery, if a query only contains prohibited clauses.&lt;/p&gt;

&lt;p&gt;The other issue (not affecting prohibited clauses) with stopwords is: If the user enters (a the) into queryparser, the query will return no results, as "a" and "the" are stopwords. This confuses lots of people (not only developers, even ordinary users of our interfaces). If somebody queries for a stopword, the correct way to handle this is to return &lt;b&gt;all&lt;/b&gt; documents (MatchAllDocsQuery).&lt;/p&gt;

&lt;p&gt;A possible solution, as suggested by Chris Male on IRC was: Add a flag to QueryParser to enable a "no-should-or-must-clauses" mode, where this is replaced by MatchAllDocs automatically. This would also solve the prohibited clause case, too.&lt;/p&gt;

&lt;p&gt;The stopword case is bad, but the opposite is as bad as returning all documents.&lt;/p&gt;

&lt;p&gt;At least this issue should somehow handle the only-prohibited case like Solr and remove the hack from Solr.&lt;/p&gt;

&lt;p&gt;Changing this in QueryParser is the more correct solution than doing this hidden in BQ.&lt;/p&gt;</description>
                <environment/>
            <key id="12524564">LUCENE-3460</key>
            <summary>Move handling of query only containing MUST_NOT to QueryParser (and remove QueryUtils.makeQueryable() hack in Solr)</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                    <parent id="12524409">LUCENE-3451</parent>
                        <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Sun, 25 Sep 2011 14:24:21 +0100</created>
                <updated>Fri, 10 May 2013 00:05:17 +0100</updated>
                                    <version>3.4</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13114255" author="mikemccand" created="Sun, 25 Sep 2011 14:25:48 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13114260" author="rcmuir" created="Sun, 25 Sep 2011 14:39:16 +0100">&lt;blockquote&gt;
&lt;p&gt;If the user enters (a the) into queryparser, the query will return no results, as "a" and "the" are stopwords. This confuses lots of people (not only developers, even ordinary users of our interfaces). If somebody queries for a stopword, the correct way to handle this is to return all documents (MatchAllDocsQuery).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;-1.&lt;/p&gt;

&lt;p&gt;There is no terms to search for, and no evidence that all documents are relevant to the query.&lt;/p&gt;</comment>
                    <comment id="13114265" author="cmale" created="Sun, 25 Sep 2011 15:06:52 +0100">&lt;p&gt;I do kind of agree with Robert here.  If there are no terms remaining (after analysis), then there shouldn't really be a Query at all.&lt;/p&gt;</comment>
                    <comment id="13114275" author="thetaphi" created="Sun, 25 Sep 2011 15:54:43 +0100">&lt;p&gt;I also agree partly with Robert; with partly i mean: Returning nothing for stop-words only confuses users, too, so we should do something.&lt;/p&gt;

&lt;p&gt;Just to make it clear, this was on IRC:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="error"&gt;&amp;#91;15:11&amp;#93;&lt;/span&gt;	chrismale: If someones query is all stopwords, then matching everything makes sense.&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;15:11&amp;#93;&lt;/span&gt;	chrismale: every term got filtered out&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;15:12&amp;#93;&lt;/span&gt;	chrismale: Leaving you nothing&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;15:12&amp;#93;&lt;/span&gt;	chrismale: Nothing is parsed to MatchAllDocs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So the issue description is &lt;b&gt;right&lt;/b&gt; &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;My idea was to add a setter to QP:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;the default is to work as it did before: To prevent the UOE, QP should simply parse to an empty BQ in the case of only prohibited clauses. This would preserve backwards compatibility for QP, it will never throw Exception&lt;/li&gt;
	&lt;li&gt;add a "Solr" mode: This would add MatchAllDocsQuery with Occur.MUST, so this would be identical behaviour like Solr does in QueryUtils. The special case in Solr can be removed, QP never throws Exception.&lt;/li&gt;
	&lt;li&gt;my favourite is this mode: If the user enters a query that only has stop-words as positive clauses (we can easily detect this by counting terms), the QP should throw a good ParseException explaining that the entered query string does no produce a meaningful query, as all clauses &lt;b&gt;may&lt;/b&gt; be stop words (it could also be stripped of by other filters, not only stop filter - so the message should be more generic). It would do this also if no negative queries are involved. So a user entering "a the" would get a meaningfull explanation that this is an invalid query.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13114414" author="hossman" created="Mon, 26 Sep 2011 01:32:02 +0100">&lt;p&gt;a) watch out how/if you remove QueryUtils.makeQueryable ... this "hack" isn't just about adding a MatchAllDocsQuery, it's also got logic for dealing with WrappedQuery instances.&lt;/p&gt;

&lt;p&gt;b) the original reason why QueryUtils.makeQueryable was added (instead of putting that logic in SolrQueryParser) was so that Solr plugins could use the underlying lucene parser to parse user input (even if the resulting query parsed to all negative clauses, or no clauses) and then programaticly manipulate that query in a manner of their choosing &amp;#8211; calling makeQueryable if that was the desire.  The three useages Uwe describes would eliminate this possibility (specificly bacause of "QP should simply parse to an empty BQ in the case of only prohibited clauses." statement)  ie: Perhaps i want to allow my users to specify all MUST_NOT clauses, and then i want to add my &lt;b&gt;own&lt;/b&gt; MUST clause instead of a MatchAllDocsQuery.&lt;/p&gt;


&lt;p&gt;...all of that said: i'm definitely in favor of more "signals" coming out of the query parser of things that the caller should be aware of.  particularly if we can help deal with the situation of &lt;b&gt;nested&lt;/b&gt; clauses that are semanticly invalid (ie: "bar +(foo +(a the))"&lt;/p&gt;

</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 25 Sep 2011 13:25:48 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2967</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24230</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3459] Change ChainedFilter to use FixedBitSet</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3459</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;ChainedFilter also uses OpenBitSet(DISI) at the moment. It should also be changed to use FixedBitSet. There are two issues:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;It exposes sometimes OpenBitSetDISI to it's public API - we should remove those methods like in BooleanFilter and break backwards&lt;/li&gt;
	&lt;li&gt;It allows a XOR operation. This is not yet supported by FixedBitSet, but it's easy to add (like for BooleanFilter). On the other hand, this XOR operation is bogus, as it may mark documents in the BitSet that are deleted, breaking new features like applying Filters down-low (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1536" title="if a filter can support random access API, we should use it"&gt;&lt;del&gt;LUCENE-1536&lt;/del&gt;&lt;/a&gt;). We should remove the XOR operation maybe or force it to use IR.validDocs() (trunk) or IR.isDeleted()&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12524560">LUCENE-3459</key>
            <summary>Change ChainedFilter to use FixedBitSet</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Sun, 25 Sep 2011 12:17:52 +0100</created>
                <updated>Fri, 10 May 2013 00:05:17 +0100</updated>
                                    <version>3.4</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13592740" author="dsmiley" created="Mon, 4 Mar 2013 22:25:43 +0000">&lt;p&gt;Ha!  I was just about to report the reverse problem &amp;#8211; make TermsFilter return OpenBitset for optimal compatibility with ChainedFilter.  I'm using TermsFilters with ChainedFilter and the mis-match of OpenBitSet and FixedBitSet is definitely not optimal since I want it to intersect those bitsets together quickly.&lt;/p&gt;

&lt;p&gt;I don't understand why there are both.  My IDE says FixedBitSet has 206 usages compared to OpenBitSet's 230.  That's closer than I thought.  I recall &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thetaphi%E2%80%8D" class="user-hover" rel="thetaphi‍"&gt;Uwe Schindler&lt;/a&gt; saying he would like OBS deprecated, but I pointed out that it is OBS that is fully public and FixedBitSet that is marked as @lucene.internal.&lt;/p&gt;</comment>
                    <comment id="13592746" author="thetaphi" created="Mon, 4 Mar 2013 22:31:16 +0000">&lt;p&gt;If you look into the details of usage, you will see recognize that OpenBitSet is only usd by Solr. Lucene does not use it anymore except at these few places. ChainedFilter needs some additional bit operations, FixedBitSet currently does not support.&lt;/p&gt;

&lt;p&gt;Performance-wise FixedBitSet is better for filters because it has less checks and does not support resize (the bit set size is fixed). &lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 4 Mar 2013 22:25:43 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2966</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24231</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3451] Remove special handling of pure negative Filters in BooleanFilter, disallow pure negative queries in BooleanQuery</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3451</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We should at least in Lucene 4.0 remove the hack in BooleanFilter that allows pure negative Filter clauses. This is not supported by BooleanQuery and confuses users (I think that's the problem in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3450" title="BooleanQuery seems broken on trunk"&gt;&lt;del&gt;LUCENE-3450&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The hack is buggy, as it does not respect deleted documents and returns them in its DocIdSet.&lt;/p&gt;

&lt;p&gt;Also we should think about disallowing pure-negative Queries at all and throw UOE.&lt;/p&gt;</description>
                <environment/>
            <key id="12524409">LUCENE-3451</key>
            <summary>Remove special handling of pure negative Filters in BooleanFilter, disallow pure negative queries in BooleanQuery</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Fri, 23 Sep 2011 13:47:35 +0100</created>
                <updated>Fri, 10 May 2013 00:05:17 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                                                  <comments>
                    <comment id="13113434" author="thetaphi" created="Fri, 23 Sep 2011 14:44:12 +0100">&lt;p&gt;Simple patch for trunk:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Disables the default full-1 bitset if no required/optional clauses&lt;/li&gt;
	&lt;li&gt;Moves the Prohibited clauses at the end. The order is now: SHOULD, MUST, MUST_NOT; I am not sure if this is correct and conforms to BooleanScorer2 (I don't understand BooleanScorer2). Does somebody know in which order clauses are applied in BooleanScorer2?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13113514" author="mikemccand" created="Fri, 23 Sep 2011 16:50:12 +0100">&lt;p&gt;Can we throw an exc if a BQ or BF has only MUST_NOT clauses, since we cannot handle it?&lt;/p&gt;

&lt;p&gt;I think silently doing the wrong thing is awful (eg led to Karl struggling to understand what was wrong).&lt;/p&gt;</comment>
                    <comment id="13113528" author="cmale" created="Fri, 23 Sep 2011 17:11:52 +0100">&lt;blockquote&gt;&lt;p&gt;Can we throw an exc if a BQ or BF has only MUST_NOT clauses, since we cannot handle it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1.  Hit this problem myself many times when first using Solr and Lucene.&lt;/p&gt;</comment>
                    <comment id="13113584" author="thetaphi" created="Fri, 23 Sep 2011 18:30:10 +0100">&lt;p&gt;Patch that also disallows only negative clauses in BooleanQuery and BooleanFilter.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;For BF this check is done in getDocIdSet that throws UOE&lt;/li&gt;
	&lt;li&gt;For BQ this check is done in ctor of BooleanWeight where the clauses are enumerated and counted, so the check is just an additional low-cost check.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Currently 2 core tests with DisjunctionMaxQuery fail (have to look into it, maybe they simply test the broken behaviour). Solr tests pass without problems (as Solr already has a special handling in QueryUtils).&lt;/p&gt;</comment>
                    <comment id="13113595" author="mikemccand" created="Fri, 23 Sep 2011 18:37:24 +0100">&lt;p&gt;Patch looks great!&lt;/p&gt;

&lt;p&gt;I think we should also backport to 3.x?  And advertise the breakage.  If users hit this exception it means their current searches aren't working so it's a service for us to inform them of this.&lt;/p&gt;</comment>
                    <comment id="13113807" author="thetaphi" created="Fri, 23 Sep 2011 23:16:54 +0100">&lt;p&gt;Attached a patch with some changes in the BooleanQuery prohibited-only detection and all failing tests fixed:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;TestBoolean2 had a explicit prohibited-only query -&amp;gt; changed to assert the UOE&lt;/li&gt;
	&lt;li&gt;TestBoolean2 had a random BQ generator. Fixed this generator to not create prohibited-only queries&lt;/li&gt;
	&lt;li&gt;in TestComplexExplanations* the prohibited-only fake clauses were removed&lt;/li&gt;
	&lt;li&gt;MemoryIndexTest used a list of lots of query-strings, some of them looking like that : "a -b". If the test had randomly choosen a MockTokenizer with english stopwords -&amp;gt; peng. Removed that Analyzer&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The last example is one problem of the explicit UOE: Now a user can suddenly get UOE if he uses a query where e.g. all positive clauses are stopwords - ok, users were always confused about that. Any comments about that?&lt;/p&gt;</comment>
                    <comment id="13113808" author="thetaphi" created="Fri, 23 Sep 2011 23:20:33 +0100">&lt;p&gt;There is a debugging relict in last patch: BooleanQuery.this.toString() in BooleanWeights UOE, please remove that.&lt;/p&gt;</comment>
                    <comment id="13113960" author="mikemccand" created="Sat, 24 Sep 2011 13:39:51 +0100">&lt;p&gt;Patch looks great Uwe!&lt;/p&gt;

&lt;p&gt;Nice catch on the analyzers removing stop words and then making an all MUST_NOT BQ.  But, I think we should throw an exception in this case, since it's a horrible trap now?  User will get 0 results but that's flat out silently wrong?&lt;/p&gt;</comment>
                    <comment id="13114237" author="thetaphi" created="Sun, 25 Sep 2011 14:07:41 +0100">&lt;p&gt;Updated patch after committing BF cleanup (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3458" title="Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQuery"&gt;&lt;del&gt;LUCENE-3458&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="13114385" author="sokolov" created="Sun, 25 Sep 2011 23:55:12 +0100">&lt;p&gt;Just wondering if there is a reason not to "fix" the user's query by adding a &lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;*:*&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; (maybe implicitly), rather than throwing an Exception?  This is invariably the fix users are instructed to apply in this case, and it does seem to be the logical implication of a pure not-query.&lt;/p&gt;

&lt;p&gt;Hmm - I just found the discussion over in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3460" title="Move handling of query only containing MUST_NOT to QueryParser (and remove QueryUtils.makeQueryable() hack in Solr)"&gt;LUCENE-3460&lt;/a&gt;, which addresses this point.&lt;/p&gt;</comment>
                    <comment id="13114389" author="thetaphi" created="Mon, 26 Sep 2011 00:18:10 +0100">&lt;p&gt;Mike,&lt;br/&gt;
See Sub-Task &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3460" title="Move handling of query only containing MUST_NOT to QueryParser (and remove QueryUtils.makeQueryable() hack in Solr)"&gt;LUCENE-3460&lt;/a&gt; for an explanation.&lt;/p&gt;</comment>
                    <comment id="13148858" author="thetaphi" created="Fri, 11 Nov 2011 23:51:51 +0000">&lt;p&gt;How should we proceed with this? I would like to commit this, but I am afraid of the consequences for users without a solution in QueryParser.&lt;/p&gt;</comment>
                    <comment id="13148860" author="thetaphi" created="Fri, 11 Nov 2011 23:53:17 +0000">&lt;p&gt;Updated patch for trunk.&lt;/p&gt;</comment>
                    <comment id="13148973" author="yseeley@gmail.com" created="Sat, 12 Nov 2011 03:42:35 +0000">&lt;p&gt;The current handling of boolean queries with only prohibited clauses is not a bug, but working as designed, so this issue is about changing that behavior.  Currently working applications will now start unexpectedly throwing exceptions... now that's trappy.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think we should also backport to 3.x? And advertise the breakage. If users hit this exception it means their current searches aren't working&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We shouldn't make assumptions like that - those applications may be working exactly as designed.&lt;br/&gt;
The issue with stopwords is complex - there is no &lt;b&gt;right&lt;/b&gt; way IMO (returning all documents vs no documents), and some users will continue to be surprised by the results regardless of which you do.&lt;/p&gt;

&lt;p&gt;Changing BQ to prohibit negative queries means that it will immediately mean that our QueryParser (and maybe other code) has a very serious bug.  This issue can't be committed without addressing other parts of Lucene and Solr that can generate negative queries first.&lt;/p&gt;

&lt;p&gt;I'm currently leaning toward the current behavior of BQ.&lt;/p&gt;</comment>
                    <comment id="13149052" author="thetaphi" created="Sat, 12 Nov 2011 11:53:15 +0000">&lt;p&gt;This is qhy I opened the sub-issue to change queryparser.&lt;/p&gt;

&lt;p&gt;At least we should make BoolenFilter in contrib/queries to behave like BQ and not assume all bits set initially when only negative clauses occur. We might only want to add a MatchAllDocumentsFilter to allow the behaviour from before.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310051">
                <name>Supercedes</name>
                                <outwardlinks description="supercedes">
                            <issuelink>
            <issuekey id="12524304">LUCENE-3450</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12503449" name="LUCENE-3451.patch" size="13426" author="thetaphi" created="Fri, 11 Nov 2011 23:53:16 +0000"/>
                    <attachment id="12496392" name="LUCENE-3451.patch" size="13370" author="thetaphi" created="Sun, 25 Sep 2011 14:07:41 +0100"/>
                    <attachment id="12496334" name="LUCENE-3451.patch" size="14036" author="thetaphi" created="Fri, 23 Sep 2011 23:16:54 +0100"/>
                    <attachment id="12496290" name="LUCENE-3451.patch" size="6459" author="thetaphi" created="Fri, 23 Sep 2011 18:30:10 +0100"/>
                    <attachment id="12496271" name="LUCENE-3451.patch" size="3556" author="thetaphi" created="Fri, 23 Sep 2011 14:44:12 +0100"/>
                </attachments>
            <subtasks>
            <subtask id="12524564">LUCENE-3460</subtask>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 23 Sep 2011 15:50:12 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2964</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24239</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3425] NRT Caching Dir to allow for exact memory usage, better buffer allocation and "global" cross indices control</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3425</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;A discussion on IRC raised several improvements that can be made to NRT caching dir. Some of the problems it currently has are:&lt;/p&gt;

&lt;p&gt;1. Not explicitly controlling the memory usage, which can result in overusing memory (for example, large new segments being committed because refreshing is too far behind).&lt;br/&gt;
2. Heap fragmentation because of constant allocation of (probably promoted to old gen) byte buffers.&lt;br/&gt;
3. Not being able to control the memory usage across indices for multi index usage within a single JVM.&lt;/p&gt;

&lt;p&gt;A suggested solution (which still needs to be ironed out) is to have a BufferAllocator that controls allocation of byte[], and allow to return unused byte[] to it. It will have a cap on the size of memory it allows to be allocated.&lt;/p&gt;

&lt;p&gt;The NRT caching dir will use the allocator, which can either be provided (for usage across several indices) or created internally. The caching dir will also create a wrapped IndexOutput, that will flush to the main dir if the allocator can no longer provide byte[] (exhausted).&lt;/p&gt;

&lt;p&gt;When a file is "flushed" from the cache to the main directory, it will return all the currently allocated byte[] to the BufferAllocator to be reused by other "files".&lt;/p&gt;</description>
                <environment/>
            <key id="12522478">LUCENE-3425</key>
            <summary>NRT Caching Dir to allow for exact memory usage, better buffer allocation and "global" cross indices control</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="kimchy">Shay Banon</reporter>
                        <labels>
                    </labels>
                <created>Fri, 9 Sep 2011 22:09:01 +0100</created>
                <updated>Fri, 10 May 2013 00:05:17 +0100</updated>
                                    <version>3.4</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13101567" author="mikemccand" created="Fri, 9 Sep 2011 22:12:39 +0100">&lt;p&gt;Also, a quick win on trunk is to use IOCtx's FlushInfo.estimatedSegmentSize to decide up front whether to try caching or not.&lt;/p&gt;

&lt;p&gt;Ie if the to-be-flushed segment is too large we should not cache it.&lt;/p&gt;</comment>
                    <comment id="13103723" author="mikemccand" created="Tue, 13 Sep 2011 17:15:33 +0100">&lt;p&gt;Actually, NRTCachingDir does explicitly control the RAM usage in that&lt;br/&gt;
if its cache is using too much RAM then the next createOutput will go&lt;br/&gt;
straight to disk.&lt;/p&gt;

&lt;p&gt;The one thing it does not do is evict the created files after they&lt;br/&gt;
close.  So, if you flush a big segment in IW, then NRTCachingDir will&lt;br/&gt;
keep those files in RAM even though its now over-budget.  (But the&lt;br/&gt;
next segment to flush will go straight to disk).&lt;/p&gt;

&lt;p&gt;I think this isn't that big a problem in practice; ie, as long as you&lt;br/&gt;
set your IW RAM buffer to something not too large, or you ensure you&lt;br/&gt;
are opening a new NRT reader often enough that the accumulated docs&lt;br/&gt;
won't create a very large segment, then the excess RAM used by&lt;br/&gt;
NRTCachingDir will be bounded.&lt;/p&gt;

&lt;p&gt;Still it would be nice to fix it so it evicts the files that set it&lt;br/&gt;
over, such that it's always below the budget once the outputs is&lt;br/&gt;
closed.  And I agree we should make it possible to have a single pool&lt;br/&gt;
for accounting purposes, so you can share this pool across multiple&lt;br/&gt;
NRTCachingDirs (and other things that use RAM).&lt;/p&gt;</comment>
                    <comment id="13234770" author="hossman" created="Wed, 21 Mar 2012 18:14:22 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 9 Sep 2011 21:12:39 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4103</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24265</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3424] Return sequence ids from IW update/delete/add/commit to allow total ordering outside of IW</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3424</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Based on the discussion on the &lt;a href="http://mail-archives.apache.org/mod_mbox/lucene-dev/201109.mbox/%3CCAAHmpki-h7LUZGCUX_rfFx=q5-YkLJei+piRG=oic8D1pNRquQ@mail.gmail.com%3E" class="external-link"&gt;mailing list&lt;/a&gt; IW should return sequence ids from update/delete/add and commit to allow ordering of events for consistent transaction logs and recovery.&lt;/p&gt;</description>
                <environment/>
            <key id="12522395">LUCENE-3424</key>
            <summary>Return sequence ids from IW update/delete/add/commit to allow total ordering outside of IW</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 9 Sep 2011 09:59:34 +0100</created>
                <updated>Fri, 10 May 2013 00:05:18 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13134973" author="simonw" created="Tue, 25 Oct 2011 13:23:43 +0100">&lt;p&gt;here is a first patch to add sequence ids to the IndexWriter. Add, Update and Delete methods return a long sequence id which is incremented for each operation. For updates and deletes the sequence ids introduce a small overhead in the DeleteQueue since I have to add a long value to each item . However, for addDocument I now have to add an empty Item in the queue to allow increasing seq ids even when you add a document. Since those queue items are very short living I think this is feasible. &lt;/p&gt;

&lt;p&gt;if that is too much of an overhead we can also disable this by default via IWC and make it optional, this is actually very straight forward.&lt;/p&gt;

&lt;p&gt;reviews &amp;amp; comments are very appreciated.&lt;/p&gt;</comment>
                    <comment id="13136119" author="mikemccand" created="Wed, 26 Oct 2011 18:15:08 +0100">&lt;p&gt;Patch looks great!&lt;/p&gt;

&lt;p&gt;The basic idea is every IW op (add/update/delete) returns a long&lt;br/&gt;
seqID.  This is a "transient" thing (only useful in RAM in your&lt;br/&gt;
current IW session; never stored in the index nor in RAM), and the app&lt;br/&gt;
can use it to know the precise order-of-ops inside IW, to know eg if a&lt;br/&gt;
delete and add happens from two threads at once, which one "took".&lt;/p&gt;

&lt;p&gt;The seqID should never be the same for any 2 ops, even across threads,&lt;br/&gt;
right?  Will it ever have "holes" (ie, skip a given value), or must&lt;br/&gt;
all values be accounted for?&lt;/p&gt;

&lt;p&gt;Commit doesn't incr the seqID right?  It just returns the max seqID&lt;br/&gt;
in the commit point, right?  If you commit having made no "actual"&lt;br/&gt;
changes (eg say you just called optimize), what seqID comes back?&lt;/p&gt;

&lt;p&gt;When an exc occurs is a seqID allocated and then skipped?  (Maybe only&lt;br/&gt;
for certain exceptions?).&lt;/p&gt;

&lt;p&gt;If an aborting-exc is hit... will we "lose" a bunch of seqIDs right?&lt;br/&gt;
Like the next op against the IW will assign a previously used seqID?&lt;/p&gt;

&lt;p&gt;seqIDs have nothing to do with flushing?  Ie, the app sees no change&lt;br/&gt;
in the returned seqIDs just because a flush occurred under the hood?&lt;/p&gt;

&lt;p&gt;Cool that the new test case is able to use the&lt;br/&gt;
ThreadedIndexingAndSearching base class!&lt;/p&gt;

&lt;p&gt;In general can you give a different name if the seqID was "coded" (&amp;lt;&amp;lt;&lt;br/&gt;
1) vs not?  (maybe codedSeqID or something)? Just to reduce chance of&lt;br/&gt;
future errors...&lt;/p&gt;

&lt;p&gt;If the perf hit is negligible I don't think we need to add an IWC&lt;br/&gt;
option?&lt;/p&gt;</comment>
                    <comment id="13136302" author="simonw" created="Wed, 26 Oct 2011 21:14:59 +0100">&lt;p&gt;thanks mike for taking the time, this stuff is hairy.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; The seqID should never be the same for any 2 ops, even across threads,&lt;br/&gt;
right? Will it ever have "holes" (ie, skip a given value), or must&lt;br/&gt;
all values be accounted for?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;one seqID will never be assigned twice. the seq ID is always taken from the current tail of the queue and is final once the tails next pointer is assigned. Yet, in the current patch there is a possibility for holes ie. some seq. ids are not used at all. Currently when I do a full flush (NRT reopen or commit) I need to cut over to the new deletequeue which means that two delete queues are active for a short amount of time. The old queue might be still in use by some DWPT (currently in flight) and the new queue is used for incoming threads. what I do to prevent double assignments is that I use the current old queues max seq id and increment it by the number of active thread states (ie. the max number of possible dwpt in flight). Deletes are no problem at that point since its synced on DW just like flushAllThreads(). I need to think about how we could close those gaps but I think we need to block ie. non-blocking / swap DWPT will not work though.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Commit doesn't incr the seqID right? It just returns the max seqID&lt;br/&gt;
in the commit point, right? If you commit having made no "actual"&lt;br/&gt;
changes (eg say you just called optimize), what seqID comes back?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;right, it would return the the same seq id or possibly a higher one due to the gaps I explained above.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; When an exc occurs is a seqID allocated and then skipped? (Maybe only&lt;br/&gt;
for certain exceptions?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;its allocated as basically the last op in DWPT#updateDocument so yes if an exc occurs after that which breaks the DWPT ie. is aborting the ids are skipped. if an exc happens in the same thread ie. during flush it will stay assigned. This could be a problem though but if an exc occurs we are in invalid state anyway, right?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;if an aborting-exc is hit... will we "lose" a bunch of seqIDs right?&lt;br/&gt;
Like the next op against the IW will assign a previously used seqID?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;no previously assigned seqID should not be assigned again. The del queue is global so once you assigned it its gone - once an item is in the queue it should not change&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;seqIDs have nothing to do with flushing? Ie, the app sees no change&lt;br/&gt;
in the returned seqIDs just because a flush occurred under the hood?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;right, except of the full flush I mentioned above.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; In general can you give a different name if the seqID was "coded" (&amp;lt;&amp;lt;&lt;br/&gt;
1) vs not? (maybe codedSeqID or something)? Just to reduce chance of&lt;br/&gt;
future errors...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah good point. I tried to not introduce a short living object here so I figured piggy-packing the seq. id is fine but yeah we should name that differently. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If the perf hit is negligible I don't think we need to add an IWC&lt;br/&gt;
option?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;its just like an update but we save the delete handling - some extra cpu cycles but since the other work is so much heavier I think its ok though.&lt;/p&gt;


</comment>
                </comments>
                    <attachments>
                    <attachment id="12500663" name="LUCENE-3424.patch" size="59998" author="simonw" created="Tue, 25 Oct 2011 13:23:43 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 26 Oct 2011 17:15:08 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2939</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24266</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3411] TestIndexWriterDelete checkIndex failure</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3411</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;hit this on the flexscoring branch: no indexer code is changed here, but the random seeds won't work in trunk... ill supply a patch with my local mods so we can reproduce.&lt;/p&gt;</description>
                <environment/>
            <key id="12521069">LUCENE-3411</key>
            <summary>TestIndexWriterDelete checkIndex failure</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Sep 2011 16:04:23 +0100</created>
                <updated>Fri, 10 May 2013 00:05:18 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13095333" author="rcmuir" created="Thu, 1 Sep 2011 16:04:37 +0100">&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Testcase: testIndexingThenDeleting(org.apache.lucene.index.TestIndexWriterDelete):	Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:161)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:147)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:488)
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testIndexingThenDeleting(TestIndexWriterDelete.java:941)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 
    [junit] 
    [junit] Tests run: 19, Failures: 0, Errors: 1, Time elapsed: 23.776 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=2 version=4.0 format=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 2: name=_2 docCount=45482
    [junit]     codec=SegmentCodecs [codecs=[Memory], provider=RandomCodecProvider: {field=Memory, content=MockVariableIntBlock(baseBlockSize=20), id=Pulsing(freqCutoff=5 minBlockSize=92 maxBlockSize=263), body=MockSep, contents=MockDocValuesCodec, value=Standard(minBlockSize=65 maxBlockSize=188), city=MockRandom}]
    [junit]     compound=true
    [junit]     hasProx=true
    [junit]     numFiles=2
    [junit]     size (MB)=2,256
    [junit]     diagnostics = {optimize=false, mergeFactor=2, os.version=2.6.38-10-generic, os=Linux, lucene.version=4.0-SNAPSHOT, source=merge, os.arch=amd64, java.version=1.6.0_24, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [1 fields]
    [junit]     test: field norms.........OK [1 fields]
    [junit]     test: terms, freq, prox...ERROR: java.lang.RuntimeException: term [31 39]: doc 45482 &amp;gt;= maxDoc 45482
    [junit] java.lang.RuntimeException: term [31 39]: doc 45482 &amp;gt;= maxDoc 45482
    [junit] 	at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:778)
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:569)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:157)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:147)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:488)
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testIndexingThenDeleting(TestIndexWriterDelete.java:941)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [0 total field count; avg 0 fields per doc]
    [junit]     test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    [junit]     test: DocValues........OK [0 total doc Count; Num DocValues Fields 0
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit] 	at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:584)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:157)
    [junit] 	at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:147)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:488)
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testIndexingThenDeleting(TestIndexWriterDelete.java:941)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit] 
    [junit]   2 of 2: name=_3 docCount=22741
    [junit]     codec=SegmentCodecs [codecs=[Memory], provider=RandomCodecProvider: {field=Memory, content=MockVariableIntBlock(baseBlockSize=20), id=Pulsing(freqCutoff=5 minBlockSize=92 maxBlockSize=263), body=MockSep, contents=MockDocValuesCodec, value=Standard(minBlockSize=65 maxBlockSize=188), city=MockRandom}]
    [junit]     compound=true
    [junit]     hasProx=true
    [junit]     numFiles=2
    [junit]     size (MB)=1,128
    [junit]     diagnostics = {os.version=2.6.38-10-generic, os=Linux, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=amd64, java.version=1.6.0_24, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [1 fields]
    [junit]     test: field norms.........OK [1 fields]
    [junit]     test: terms, freq, prox...OK [21 terms; 477561 terms/docs pairs; 477561 tokens]
    [junit]     test: stored fields.......OK [0 total field count; avg 0 fields per doc]
    [junit]     test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
    [junit]     test: DocValues........OK [0 total doc Count; Num DocValues Fields 0
    [junit] 
    [junit] WARNING: 1 broken segments (containing 45482 documents) detected
    [junit] 
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testIndexingThenDeleting -Dtests.seed=-77a5e53649d91be7:-2fa0c072bec00b06:-29ee9869203d1ce3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {field=Memory, content=MockVariableIntBlock(baseBlockSize=20), id=Pulsing(freqCutoff=5 minBlockSize=92 maxBlockSize=263), body=MockSep, contents=MockDocValuesCodec, value=Standard(minBlockSize=65 maxBlockSize=188), city=MockRandom}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {field=DFR P1, content=DFR I(F)L1, id=DFR I(n)B2, body=IB LL-D2, contents=DFR BeB2, value=DFR I(F)B1, city=DFR I(F)L1, country=DFR I(ne)B1}, locale=fr_LU, timezone=Australia/Victoria
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDemo, TestDocument, Test2BPostings, TestAddIndexes, TestBinaryTerms, TestCompoundFile, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderOnDiskFull, TestIndexWriterDelete]
    [junit] NOTE: Linux 2.6.38-10-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=137491864,total=274792448
    [junit] ------------- ---------------- ---------------
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13095334" author="rcmuir" created="Thu, 1 Sep 2011 16:08:02 +0100">&lt;p&gt;here are my local changes for reference, but i can't even reproduce this one locally....&lt;/p&gt;</comment>
                    <comment id="13149546" author="simonw" created="Mon, 14 Nov 2011 10:35:00 +0000">&lt;p&gt;this is spooky, robert did you ever reproduce this? I am going to mark this as fixVersion 4.0 so we don't forget about it.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12492609" name="LUCENE-3411_localchanges.patch" size="15540" author="rcmuir" created="Thu, 1 Sep 2011 16:08:02 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 14 Nov 2011 10:35:00 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4106</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24279</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3392] Combining analyzers output</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3392</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It should be easy to combine the output of multiple Analyzers, or TokenStreams.&lt;br/&gt;
A ComboAnalyzer and a ComboTokenStream class would take multiple instances, and multiplex their output, keeping a rough order of tokens like increasing position then increasing start offset then increasing end offset.&lt;/p&gt;</description>
                <environment/>
            <key id="12519690">LUCENE-3392</key>
            <summary>Combining analyzers output</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="ofavre">Olivier Favre</reporter>
                        <labels>
                        <label>analysis</label>
                    </labels>
                <created>Mon, 22 Aug 2011 15:11:02 +0100</created>
                <updated>Fri, 10 May 2013 00:05:18 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                          <timeoriginalestimate seconds="172800">48h</timeoriginalestimate>
                    <timeestimate seconds="172800">48h</timeestimate>
                                  <comments>
                    <comment id="13089413" author="ofavre" created="Tue, 23 Aug 2011 13:06:48 +0100">&lt;p&gt;Patch for lucene-3x.&lt;br/&gt;
Tested with Sun's Java 1.6.0_26-b03.&lt;br/&gt;
Uses a special factory for cloning Readers, some implementation use reflection to gain access to private fields in order to reduce the need to read and copy a Readers' content.&lt;/p&gt;</comment>
                    <comment id="13089415" author="ofavre" created="Tue, 23 Aug 2011 13:17:04 +0100">&lt;p&gt;The proposed implementation may a have tight bond with the JVM implementation of some classes (StringReader, BufferedReader and FilterReader), as they rely on a named private field (respectively "str", "in" and "in").&lt;br/&gt;
This can be avoided, but any Reader should then be fully read and stored as a String or a char[], which can have a huge overhead.&lt;br/&gt;
Considering each clone would get read relatively at the same speed (well, only for word delimiting analysis, not for a KeywordAnalyzer) an implementation could only retain in memory the portion read by at least one cloned reader but not all clones, in order to implement a "multi read head" reader.&lt;/p&gt;

&lt;p&gt;Another implementation would be to change the API to give a CloneableReader interface with a "giveAClone()" function instead of a Reader for tokenStream and reusableTokenStream functions.&lt;br/&gt;
But this involves massive refactoring (&amp;gt;13,000 lines) and introduces an important API break.&lt;/p&gt;

&lt;p&gt;The proposed implementation is the best solution I found.&lt;br/&gt;
Any suggestions are welcome!&lt;/p&gt;</comment>
                    <comment id="13089504" author="ofavre" created="Tue, 23 Aug 2011 15:32:25 +0100">&lt;p&gt;Patch for lucene-trunk.&lt;br/&gt;
Tested with sun's Java 1.6.0_26-b03.&lt;br/&gt;
Adds support for Reader cloning in lucene's core, and the analysis stuff in modules/analysis/common&lt;/p&gt;</comment>
                    <comment id="13089522" author="ofavre" created="Tue, 23 Aug 2011 15:58:20 +0100">&lt;p&gt;Moved analysis related changes into contrib/analysers/common, like the patch for the trunk.&lt;/p&gt;

&lt;p&gt;Small changes:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;2 space indentation (was 4 before, my personal default value)&lt;/li&gt;
	&lt;li&gt;removed a few useless imports&lt;/li&gt;
	&lt;li&gt;simplified ComboTokenStream, and fixes, as I saw functions have become final in the trunk.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13234779" author="hossman" created="Wed, 21 Mar 2012 18:14:25 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12491345" name="ComboAnalyzer-lucene3x.patch" size="79513" author="ofavre" created="Tue, 23 Aug 2011 15:58:20 +0100"/>
                    <attachment id="12491336" name="ComboAnalyzer-lucene3x.patch" size="86027" author="ofavre" created="Tue, 23 Aug 2011 13:06:48 +0100"/>
                    <attachment id="12491343" name="ComboAnalyzer-lucene-trunk.patch" size="79584" author="ofavre" created="Tue, 23 Aug 2011 15:32:25 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 21 Mar 2012 18:14:25 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4110</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24298</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3380] enable FileSwitchDirectory randomly in tests and fix compound-file/NoSuchDirectoryException bugs</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3380</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Looks like FileSwitchDirectory has the same bugs in it as &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3374" title="move nrtcachingdir to core in 4.0"&gt;&lt;del&gt;LUCENE-3374&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We should randomly enable this guy in tests and flush them all out the same way.&lt;/p&gt;</description>
                <environment/>
            <key id="12519228">LUCENE-3380</key>
            <summary>enable FileSwitchDirectory randomly in tests and fix compound-file/NoSuchDirectoryException bugs</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 Aug 2011 03:33:00 +0100</created>
                <updated>Fri, 10 May 2013 00:05:18 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13086858" author="thetaphi" created="Thu, 18 Aug 2011 08:30:34 +0100">&lt;p&gt;I assume the bugs in FileSwitchDirectory are the same NotExists Exceptions thrown &lt;/p&gt;

&lt;p&gt;We should maybe also add FileSwitchDirectory to the list of random directories. It could create two random directories (using LTC.newDirectory(false) 2 times with a suffix on the dir name like ".1" and ".2") and combine them with a FileSwitchDirectory. The Set&amp;lt;String&amp;gt; of extensions could be a random list of extensions from the IndexFileNames collection.&lt;/p&gt;</comment>
                    <comment id="13086870" author="rcmuir" created="Thu, 18 Aug 2011 09:05:52 +0100">&lt;p&gt;no, there are problems involving compoundfilewriter as well!&lt;/p&gt;</comment>
                    <comment id="13086872" author="rcmuir" created="Thu, 18 Aug 2011 09:16:03 +0100">&lt;p&gt;the compound file directory can be thought of easily:&lt;br/&gt;
Imagine FileSwitchDirectory (F) which contains two directories (A and B)&lt;br/&gt;
and in the configuration, "cfs" files go to A, everything else to B.&lt;/p&gt;

&lt;p&gt;so currently it calls F.createCompoundOutput("xxxx.cfs") which delegates to A.createCompoundOutput("xxxx.cfs") -&amp;gt;&amp;gt; CompoundFileWriter(A, "xxxx.cfs"), which then, new since &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3218" title="Make CFS appendable  "&gt;&lt;del&gt;LUCENE-3218&lt;/del&gt;&lt;/a&gt;, will create a.createOutput("xxxx.cfe")&lt;/p&gt;

&lt;p&gt;The problem is that this cfe file is created under the wrong directory, and you will get FNFE.&lt;/p&gt;

&lt;p&gt;We can use the solution I provided in  &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3374" title="move nrtcachingdir to core in 4.0"&gt;&lt;del&gt;LUCENE-3374&lt;/del&gt;&lt;/a&gt;, but seriously maybe we should rethink &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3218" title="Make CFS appendable  "&gt;&lt;del&gt;LUCENE-3218&lt;/del&gt;&lt;/a&gt; before releasing, because this could break similar delegators and basically they will experience what is like index corruption.&lt;/p&gt;</comment>
                    <comment id="13086900" author="thetaphi" created="Thu, 18 Aug 2011 10:04:04 +0100">&lt;p&gt;In my opinion, the createCompoundOutput/Input should get both files then the directory can wrap correctly. Maybe the underlying CFWriter/Reader should only get 2 IndexInput/Output, no filenames, so the delegation can completely done in the directory.&lt;/p&gt;</comment>
                    <comment id="13086906" author="rcmuir" created="Thu, 18 Aug 2011 10:10:28 +0100">&lt;p&gt;I think so too, this means that the createCompoundInput for example is going to have to read the version header and deal with the backwards case where there is no CFE file, but we can create a utility for this.&lt;/p&gt;</comment>
                    <comment id="13086917" author="rcmuir" created="Thu, 18 Aug 2011 10:44:22 +0100">&lt;p&gt;and thinking about it more, that makes this approach hard too, because how would the delegation work?&lt;/p&gt;

&lt;p&gt;an alternative is for both these dirs (NRTCachingDIr/FileSwitchDIr) to implement special CompoundFileDirectories, or maybe we make a DelegatingCompoundFileDirectory even that they share, but this still doesn't solve the root problem: that its going to be even more confusing for the future and potentially a problem for any custom directories out there.&lt;/p&gt;</comment>
                    <comment id="13087105" author="rcmuir" created="Thu, 18 Aug 2011 17:34:28 +0100">&lt;p&gt;here's a patch with tests for the problems, and bugfixes.&lt;/p&gt;

&lt;p&gt;I didn't enable it randomly yet as thats more complicated and first I want to get these bugs fixed in trunk/branch.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12490816" name="LUCENE-3380.patch" size="6589" author="rcmuir" created="Thu, 18 Aug 2011 17:34:28 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 18 Aug 2011 07:30:34 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2969</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24310</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3333] Specialize DisjunctionScorer if all clauses are TermQueries</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3333</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;spinnoff from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3328" title="Specialize BooleanQuery if all clauses are TermQueries"&gt;&lt;del&gt;LUCENE-3328&lt;/del&gt;&lt;/a&gt; - since we have a specialized conjunction scorer we should also investigate if this pays off in disjunction scoring&lt;/p&gt;</description>
                <environment/>
            <key id="12515042">LUCENE-3333</key>
            <summary>Specialize DisjunctionScorer if all clauses are TermQueries</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Fri, 22 Jul 2011 17:09:50 +0100</created>
                <updated>Fri, 10 May 2013 00:05:18 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2946</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24357</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3266] Improve FileLocking based on Java 1.6 </title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3266</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Snippet from NativeFSLockFactory:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;/*
* The javadocs for FileChannel state that you should have
* a single instance of a FileChannel (per JVM) for all
* locking against a given file (locks are tracked per 
* FileChannel instance in Java 1.4/1.5). Even using the same 
* FileChannel instance is not completely thread-safe with Java 
* 1.4/1.5 though. To work around this, we have a single (static) 
* HashSet that contains the file paths of all currently 
* locked locks.  This protects against possible cases 
* where different Directory instances in one JVM (each 
* with their own NativeFSLockFactory instance) have set 
* the same lock dir and lock prefix. However, this will not 
* work when LockFactorys are created by different 
* classloaders (eg multiple webapps). 
* 
* TODO: Java 1.6 tracks system wide locks in a thread safe manner 
* (same FileChannel instance or not), so we may want to 
* change this when Lucene moves to Java 1.6.
*/
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;since we are on 1.6 we should improve this if possible.&lt;/p&gt;</description>
                <environment/>
            <key id="12512316">LUCENE-3266</key>
            <summary>Improve FileLocking based on Java 1.6 </summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Thu, 30 Jun 2011 13:18:58 +0100</created>
                <updated>Fri, 10 May 2013 00:05:18 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                        <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12511487">LUCENE-3239</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10738</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24424</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3252] Use single array in fixed straight bytes DocValues if possible</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3252</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;FixedStraightBytesImpl currently uses a straight array only if the byte size is 1 per document we could further optimize this to use a single array if all the values fit in.&lt;/p&gt;</description>
                <environment/>
            <key id="12511962">LUCENE-3252</key>
            <summary>Use single array in fixed straight bytes DocValues if possible</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 28 Jun 2011 09:14:59 +0100</created>
                <updated>Fri, 10 May 2013 00:05:19 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13056378" author="simonw" created="Tue, 28 Jun 2011 09:16:40 +0100">&lt;p&gt;here is a patch that uses a single array if possible. Yet, the problem I have with this patch is that the fallback impl doesn't get any tests anymore so I might need to add a mock impl here somehow that uses the fallback for testing too. Any other ideas?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12484408" name="LUCENE-3252.patch" size="2931" author="simonw" created="Tue, 28 Jun 2011 09:16:40 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10750</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24438</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3236] Make LowerCaseFilter and StopFilter keyword aware, similar to PorterStemFilter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3236</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;PorterStemFilter has functionality to detect if a term has been marked as a "keyword" by the KeywordMarkerFilter (KeywordAttribute.isKeyword() == true), and if so, skip stemming.&lt;/p&gt;

&lt;p&gt;The suggestion is to have the same functionality in other filters where it is applicable. I think it may be particularly applicable to the LowerCaseFilter (ie if it is a keyword, don't mess with the case), and StopFilter (if it is a keyword, then don't filter it out even if it looks like a stop word).&lt;/p&gt;

&lt;p&gt;Backward compatibility is maintained (in both cases) by adding a new constructor which takes an additional boolean parameter ignoreKeyword. The current constructor will call this new constructor with ignoreKeyword = false.&lt;/p&gt;

&lt;p&gt;Patches are attached (for LowerCaseFilter and StopFilter).&lt;/p&gt;

&lt;p&gt;I have verified that the analysis JUnit tests run against the updated code, ie, backward compatibility is maintained.&lt;/p&gt;</description>
                <environment>&lt;p&gt;N/A&lt;/p&gt;</environment>
            <key id="12511406">LUCENE-3236</key>
            <summary>Make LowerCaseFilter and StopFilter keyword aware, similar to PorterStemFilter</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="sujitpal">Sujit Pal</reporter>
                        <labels>
                        <label>analysis</label>
                    </labels>
                <created>Thu, 23 Jun 2011 21:59:49 +0100</created>
                <updated>Fri, 10 May 2013 00:05:19 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13054098" author="sujitpal" created="Thu, 23 Jun 2011 22:09:29 +0100">&lt;p&gt;Patch generated with svn diff from the top level Lucene/Solr trunk. Contains updates to LowerCaseFilter and StopFilter to recognize and NOT operate on terms marked with KeywordAttribute.isKeyword.&lt;br/&gt;
(NOTE: also contains changes to changes2html.pl which seem to have been generated automatically).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12483639" name="lucene-3236-patch.diff" size="7578" author="sujitpal" created="Thu, 23 Jun 2011 22:09:29 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10764</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24455</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3235] TestDoubleBarrelLRUCache hangs under Java 1.5, 3.x and trunk, likely JVM bug</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3235</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Not sure what's going on yet... but under Java 1.6 it seems not to hang bug under Java 1.5 hangs fairly easily, on Linux.  Java is 1.5.0_22.&lt;/p&gt;

&lt;p&gt;I suspect this is relevant: &lt;a href="http://stackoverflow.com/questions/3292577/is-it-possible-for-concurrenthashmap-to-deadlock" class="external-link"&gt;http://stackoverflow.com/questions/3292577/is-it-possible-for-concurrenthashmap-to-deadlock&lt;/a&gt; which refers to this JVM bug &lt;a href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6865591" class="external-link"&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6865591&lt;/a&gt; which then refers to this one &lt;a href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6822370" class="external-link"&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6822370&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It looks like that last bug was fixed in Java 1.6 but not 1.5.&lt;/p&gt;</description>
                <environment/>
            <key id="12511398">LUCENE-3235</key>
            <summary>TestDoubleBarrelLRUCache hangs under Java 1.5, 3.x and trunk, likely JVM bug</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 Jun 2011 20:56:42 +0100</created>
                <updated>Fri, 10 May 2013 00:05:19 +0100</updated>
                                    <version>3.0</version>
                <version>3.1</version>
                <version>3.2</version>
                <version>3.3</version>
                <version>3.4</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13054066" author="rcmuir" created="Thu, 23 Jun 2011 20:57:39 +0100">&lt;p&gt;+1 to drop java 5&lt;/p&gt;</comment>
                    <comment id="13054067" author="thetaphi" created="Thu, 23 Jun 2011 21:01:20 +0100">&lt;p&gt;LOL, no comment.&lt;/p&gt;</comment>
                    <comment id="13054070" author="rcmuir" created="Thu, 23 Jun 2011 21:05:21 +0100">&lt;p&gt;i ran the test with the same version as mike (1.5.0_22) in two ways on windows:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;-Dtests.iter=100&lt;/li&gt;
	&lt;li&gt;in a loop from a script, 100 times with its own ant run.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;i can't reproduce it on windows.&lt;/p&gt;

&lt;p&gt;in my eyes, there isn't even an argument about whether or not we should support java5: its not possible, if bugs are not getting fixed.&lt;/p&gt;</comment>
                    <comment id="13054071" author="mikemccand" created="Thu, 23 Jun 2011 21:06:07 +0100">&lt;p&gt;Still hangs if I run -client; but it looks like -Xint prevents the hang (235 iterations so far on beast).&lt;/p&gt;

&lt;p&gt;3.2 also hangs.&lt;/p&gt;</comment>
                    <comment id="13054336" author="rcmuir" created="Fri, 24 Jun 2011 11:11:57 +0100">&lt;p&gt;Mike, i installed 1.5.0_22 (amd64) on my linux machine, and i can't reproduce there either (i ran like 500 iterations).&lt;/p&gt;

&lt;p&gt;Maybe my hardware isn't concurrent enough? or maybe you should un-overclock? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13054349" author="mikemccand" created="Fri, 24 Jun 2011 11:37:32 +0100">&lt;p&gt;VERY interesting!  Is anyone able to repro this hang besides me...?&lt;/p&gt;</comment>
                    <comment id="13054353" author="dweiss" created="Fri, 24 Jun 2011 11:43:29 +0100">&lt;p&gt;I don't think you can force -client if it's a 64 bit release and you have tons of memory, can you? You can check by running java -client -version &amp;#8211; this is what it tells me, for example:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;dweiss@dweiss-linux:~/work/lucene/lucene-trunk$ java -client -version
java version "1.6.0_16"
Java(TM) SE Runtime Environment (build 1.6.0_16-b01)
Java HotSpot(TM) 64-Bit Server VM (build 14.2-b01, mixed mode)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can you do a remote stack of all the VM (or run it from the console and send it a signal to dump all threads)?&lt;/p&gt;</comment>
                    <comment id="13054362" author="mikemccand" created="Fri, 24 Jun 2011 11:56:58 +0100">&lt;p&gt;Yes the stack looks just like the stack overflow link I posted &amp;#8211; several threads stuck in sun.misc.Unsafe.park &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;java -Xint definitely does not hang... ran for like 4200 iterations.&lt;/p&gt;</comment>
                    <comment id="13054363" author="mikemccand" created="Fri, 24 Jun 2011 11:57:28 +0100">&lt;p&gt;Indeed java -client -version shows it's still using server VM &amp;#8211; you're right!&lt;/p&gt;</comment>
                    <comment id="13054366" author="dweiss" created="Fri, 24 Jun 2011 12:01:22 +0100">&lt;p&gt;I'm same as Robert: +1 to drop 1.5...&lt;/p&gt;</comment>
                    <comment id="13054433" author="markrmiller@gmail.com" created="Fri, 24 Jun 2011 14:14:09 +0100">&lt;blockquote&gt;&lt;p&gt;+1 to drop 1.5...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1.&lt;/p&gt;</comment>
                    <comment id="13149556" author="simonw" created="Mon, 14 Nov 2011 10:53:44 +0000">&lt;p&gt;we moved to 1.6 on trunk seems we can't do much about it on 3.x - folks should run their stuff on 1.6 jvms or newer&lt;/p&gt;</comment>
                    <comment id="13149562" author="rcmuir" created="Mon, 14 Nov 2011 10:58:42 +0000">&lt;p&gt;wait, this statement makes no sense.&lt;/p&gt;

&lt;p&gt;if 1.5 is no longer supported, then 1.5 should no longer be supported, and we should be&lt;br/&gt;
free to use 1.6 code everywhere.&lt;/p&gt;</comment>
                    <comment id="13149565" author="thetaphi" created="Mon, 14 Nov 2011 11:01:37 +0000">&lt;p&gt;I agree with Robert. This issue is still existent in 3.x as we officially support Java 5.&lt;/p&gt;</comment>
                    <comment id="13149577" author="simonw" created="Mon, 14 Nov 2011 11:18:15 +0000">&lt;p&gt;well then we should fix it - I will mark it as 3.5&lt;/p&gt;</comment>
                    <comment id="13149581" author="thetaphi" created="Mon, 14 Nov 2011 11:33:05 +0000">&lt;p&gt;An easy fix would be to use Collections.synchronizedMap(new HashMap()) in the ctor to initializer cache1 and cache2 (if Java 5 is detected)? If people are using Java 5 they get not-the best-performance.&lt;/p&gt;</comment>
                    <comment id="13149584" author="rcmuir" created="Mon, 14 Nov 2011 11:40:10 +0000">&lt;p&gt;I like Uwe's idea: not-the-best-performance is far preferable to a hang/deadlock!!!!!&lt;/p&gt;</comment>
                    <comment id="13149585" author="thetaphi" created="Mon, 14 Nov 2011 11:44:11 +0000">&lt;p&gt;I am currently preparing a patch.&lt;/p&gt;</comment>
                    <comment id="13149592" author="thetaphi" created="Mon, 14 Nov 2011 12:01:40 +0000">&lt;p&gt;Patch.&lt;/p&gt;

&lt;p&gt;We should forward port the deprecation/removal of useless Constants.&lt;/p&gt;</comment>
                    <comment id="13149631" author="simonw" created="Mon, 14 Nov 2011 14:07:53 +0000">&lt;blockquote&gt;&lt;p&gt;An easy fix would be to use Collections.synchronizedMap(new HashMap()) in the ctor to initializer cache1 and cache2 (if Java 5 is detected)? If people are using Java 5 they get not-the best-performance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like that too...&lt;/p&gt;</comment>
                    <comment id="13149672" author="thetaphi" created="Mon, 14 Nov 2011 15:15:47 +0000">&lt;p&gt;Updated patch after &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3574" title="Add some more constants for newer Java versions to Constants.class, remove outdated ones."&gt;&lt;del&gt;LUCENE-3574&lt;/del&gt;&lt;/a&gt; was committed. I also added a System.out.println to the test (VERBOSE only).&lt;/p&gt;</comment>
                    <comment id="13149701" author="thetaphi" created="Mon, 14 Nov 2011 15:49:43 +0000">&lt;p&gt;I wait until tomorrow before I commit this "safe-but-slow" fix.&lt;/p&gt;</comment>
                    <comment id="13149728" author="mikemccand" created="Mon, 14 Nov 2011 16:35:48 +0000">&lt;p&gt;+1 for the safe-but-slow Java 5 only workaround....&lt;/p&gt;</comment>
                    <comment id="13149906" author="hossman" created="Mon, 14 Nov 2011 20:58:53 +0000">
&lt;p&gt;+0&lt;/p&gt;

&lt;p&gt;if anyone else suggested that we should add a "slow hack" to work around a Sun JRE bug i would argue that we were being unfair to people using other JRE (ie: does IBM's JRE have this bug? &amp;#8211; do IBM java 1.5 users deserve slower performance because Sun's JRE has a bug?) but since rmuir is the biggest proponent I know of not assuming everyone on the planet uses Sun JREs, and he's signed off on this, I'll defer.&lt;/p&gt;</comment>
                    <comment id="13149919" author="thetaphi" created="Mon, 14 Nov 2011 21:06:39 +0000">&lt;p&gt;We are using ConcurrentHashMap also at other places, should we replace all of them or where is the bug that this happens only here?&lt;/p&gt;

&lt;p&gt;It also appears to happen on Mike's machine, so maybe its hardware-related (Solaris?) as the Sun bugreport seems to tell us.&lt;/p&gt;

&lt;p&gt;I am also +0 to apply the patch. I just showed one possibility how to fix this.&lt;/p&gt;</comment>
                    <comment id="13149921" author="rcmuir" created="Mon, 14 Nov 2011 21:10:50 +0000">&lt;p&gt;Hoss you are right: we should also check Constants.SUN ?&lt;/p&gt;

&lt;p&gt;otherwise lets not do the hack...&lt;/p&gt;

&lt;p&gt;But i'm for the change because there is nothing slower than a hang/deadlock...&lt;/p&gt;</comment>
                    <comment id="13149941" author="thetaphi" created="Mon, 14 Nov 2011 21:29:03 +0000">&lt;p&gt;...and Solaris? The JVM BUG seems to only affect Solaris (according to the sun reports).&lt;/p&gt;</comment>
                    <comment id="13149954" author="rcmuir" created="Mon, 14 Nov 2011 21:35:11 +0000">&lt;p&gt;and intel cpu&lt;/p&gt;</comment>
                    <comment id="13149967" author="rcmuir" created="Mon, 14 Nov 2011 21:50:17 +0000">&lt;p&gt;I modified the patch, the thing is that Mike was seeing this on Linux I think too...&lt;/p&gt;</comment>
                    <comment id="13149979" author="simonw" created="Mon, 14 Nov 2011 22:02:11 +0000">&lt;p&gt;here is a very detailed writeup for this:&lt;br/&gt;
&lt;a href="http://blogs.oracle.com/dave/entry/a_race_in_locksupport_park" class="external-link"&gt;http://blogs.oracle.com/dave/entry/a_race_in_locksupport_park&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;some interesting facts:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The problem would only manifest when we were using the -UseMembar optimization that lets us remove fences from certain hot thread state transitions paths that need to coordinate safepoints between mutator threads and the JVM. This feature is enabled by default, but we can turn it off with the -XX:+UseMembar switch, which causes the JVM to emit normal fence instructions in the state transitions paths.&lt;/li&gt;
	&lt;li&gt;The bug is a "day-one" bug and present in all versions of HotSpot.&lt;/li&gt;
	&lt;li&gt;Parker::park() and unpark() reside in os_linux.cpp, os_solaris.cpp and os_windows.cpp for Linux, Solaris and Windows, respectively.&lt;/li&gt;
	&lt;li&gt;The built-in synchronized implementation uses a different park mechanism (PlatformPark:&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; whereas the java.util.concurrent infrastructure uses Parker::. Only Parker:: is vulnerable.&lt;/li&gt;
	&lt;li&gt;The bug will not manifest on uniprocessors or environments where threads are otherwise constrained to just a single processor.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think the only reasonable fix for this is to recommend people to use -XX:+UseMembar if they are running on a vulnerable JVM&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13150003" author="thetaphi" created="Mon, 14 Nov 2011 22:24:34 +0000">&lt;p&gt;That blog is as cool as the generics policeman ones... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Thanks Simon, I think we should list this bug and its workaround in the wiki page and close this report.&lt;/p&gt;

&lt;p&gt;Mike can you try -XX:+UseMembar ?&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="13150355" author="simonw" created="Tue, 15 Nov 2011 10:30:47 +0000">&lt;blockquote&gt;&lt;p&gt;Thanks Simon, I think we should list this bug and its workaround in the wiki page and close this report.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1 this is not our problem. if we go and fix all java.util.concurrent uses in solr &amp;amp; lucene we gonna end up in a big mess. According to the oracle blog this is also in 1.6 jvms and you will be vulnderable if you use any CHM like classes in your own code... &lt;/p&gt;</comment>
                    <comment id="13150363" author="simonw" created="Tue, 15 Nov 2011 10:41:45 +0000">&lt;p&gt;I updated the wiki...&lt;/p&gt;</comment>
                    <comment id="13150484" author="mikemccand" created="Tue, 15 Nov 2011 13:34:37 +0000">&lt;p&gt;That's a nice blog post!  What a scary platform-specific JVM bug...&lt;/p&gt;

&lt;p&gt;I still hit hit this hang reasonably often when running 3.x tests.  It's always the DBLRU cache, so far anyway.&lt;/p&gt;

&lt;p&gt;Because this is our most intense use of a CHM... I still think the workaround (scoped down to 1.5, Sun JVM, little endian arch) makes sense?  I agree it won't fully work around the JVM bug, since in theory other uses of java.util.concurrent.* could hit it, but it can prevent the most common occurrence?  The patch seems minimal and worth it... a hang is a truly awful.&lt;/p&gt;</comment>
                    <comment id="13150545" author="markrmiller@gmail.com" created="Tue, 15 Nov 2011 15:13:24 +0000">&lt;blockquote&gt;&lt;p&gt;I still think the workaround (scoped down to 1.5, Sun JVM, little endian arch) makes sense?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 Doesn't hurt other JVMs, improves things on the Sun JVM (something that hangs a lot and then does not hang is a big improvement in my book), and putting a workaround command to use in the wiki just seems a whole lot less user friendly to me. It doesn't mean we have to try and address every use of java.util.concurrent to work around this specific issue, does it?&lt;/p&gt;</comment>
                    <comment id="13150561" author="thetaphi" created="Tue, 15 Nov 2011 15:39:06 +0000">&lt;p&gt;This testcase fails, but we are using concurrent also in ParallelMultiSearcher (die, die, die) and other places (even the indexer was partly upgraded to use ConcurrentLock). In my opinion we should not change our code to work around that issue. Just because one test case hangs its not guaranteed that other uses will work correctly. It brings a false security and slows down VMs that work correctly. And it only affects very modern processors.&lt;/p&gt;

&lt;p&gt;If we would have a logging framework in Lucene (maybe Solr could do this): It could parse the args of Java (from system property) and look for -XX:+UseMembar, if its Java 1.5 it should print a warning to Solr/Lucene log file.&lt;/p&gt;</comment>
                    <comment id="13150775" author="mikemccand" created="Tue, 15 Nov 2011 21:03:04 +0000">
&lt;blockquote&gt;&lt;p&gt;In my opinion we should not change our code to work around that issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In general, I think we should change our code to work around awful JVM&lt;br/&gt;
bugs, as long as 1) it's not so much effort for us to to do so (and as&lt;br/&gt;
always a volunteer steps up to the task), and 2) the change has&lt;br/&gt;
negligible cost to "lucky" users (who use a JVM / the right flags that&lt;br/&gt;
would not have hit the JVM bug).&lt;/p&gt;

&lt;p&gt;I think the last patch fits these criteria, since it's a tiny change&lt;br/&gt;
and it scopes the workaround?&lt;/p&gt;

&lt;p&gt;We've done this many times in the past; if the cost to "lucky" users&lt;br/&gt;
is negligible and the benefit to "unlucky" users (unknowingly using&lt;br/&gt;
the affected JVMs) is immense (not hitting horrific bug), I think the&lt;br/&gt;
tradeoff is worthwhile?  Otherwise users will conclude Lucene (or&lt;br/&gt;
whatever software is embedding it) is buggy.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This testcase fails, but we are using concurrent also in ParallelMultiSearcher (die, die, die) and other places (even the indexer was partly upgraded to use ConcurrentLock).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, we use concurrent* elsewhere, but terms dict is the big&lt;br/&gt;
user... very few apps actually use PMS.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It brings a false security and slows down VMs that work correctly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, we already have "false security" that Lucene won't hang on any&lt;br/&gt;
JVM... we don't claim this patch will fully work around the bug, but&lt;br/&gt;
at least it should reduce it.&lt;/p&gt;

&lt;p&gt;How are we slowing down other VMs...?  We scope the workaround?&lt;/p&gt;

&lt;p&gt;I'm not saying we should go crazy here, making a big patch to avoid&lt;br/&gt;
concurrent* everywhere, but the current patch is minimal, addresses&lt;br/&gt;
the big usage of concurrent* in 3.x, is scoped down well.&lt;/p&gt;

&lt;p&gt;It will avoid hangs for some number unlucky users out there... so why&lt;br/&gt;
not commit it?&lt;/p&gt;</comment>
                    <comment id="13150813" author="rcmuir" created="Tue, 15 Nov 2011 21:56:14 +0000">&lt;blockquote&gt;
&lt;p&gt;How are we slowing down other VMs...? We scope the workaround?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, according to the bug report it seems all 1.5's are affected and some 1.6's?&lt;br/&gt;
Doesn't seem like solaris is related either, especially since Mike hit it on linux&lt;/p&gt;

&lt;p&gt;So the current patch is actually way under-scoped.&lt;/p&gt;

&lt;p&gt;Sure, some 1.6's are affected, and if we want it to be even better,&lt;br/&gt;
we should likely improve Constants a little bit to make the minor&lt;br/&gt;
version more easily accessible, from the bug report it seems we&lt;br/&gt;
should at least consider doing something for &amp;lt; 1.6.0u21 ? And we should&lt;br/&gt;
remove the 'Solaris' check, but keep it little-endian because the bug&lt;br/&gt;
report mentions its way more likely to happen on those cpus.&lt;/p&gt;</comment>
                    <comment id="13150874" author="thetaphi" created="Tue, 15 Nov 2011 23:10:16 +0000">&lt;p&gt;Yes, also 1.6.0_17 is affected. As always, 1.6.0_18 is the last and only good JVM &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;My problem with the patch is that it only affects very few users, most users will have a working environment also with broken JVMs. The fix in the patch is very heavy, as, if we apply it correctly, will also slowdown &amp;lt;1.6.0_18.&lt;/p&gt;

&lt;p&gt;As I said before, we should at least instruct Solr to print a WARN in the log if a JVM &amp;lt; 1.6.0_18 is started and the JVM parameter -XX:+UseMembar is missing. In Lucene we have no way to tell this the user as we have no logging framework, alternatively we could throw an Error is one of the central classes in Lucene is loaded by classloader and the JVM parameter is not given (static initializer e.g. in Constants.java). The same way we could tell the user: Dont use Java 7 GA.&lt;/p&gt;

&lt;p&gt;As far as I know, the JVM command can be checked with a System-property and a simple regex should help.&lt;/p&gt;</comment>
                    <comment id="13150878" author="thetaphi" created="Tue, 15 Nov 2011 23:19:42 +0000">&lt;p&gt;This ois the way to get the runtime args: &lt;a href="http://download.oracle.com/javase/1.5.0/docs/api/java/lang/management/RuntimeMXBean.html#getInputArguments()" class="external-link"&gt;http://download.oracle.com/javase/1.5.0/docs/api/java/lang/management/RuntimeMXBean.html#getInputArguments()&lt;/a&gt;:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
RuntimeMXBean m = ManagementFactory.getRuntimeMXBean();
List&amp;lt;&lt;span class="code-object"&gt;String&lt;/span&gt;&amp;gt; vmargs = m.getInputArguments();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This prints all args for me:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;[junit] vmargs=[-Xmx512M, -Dtests.verbose=false, -Dtests.infostream=false, -Dtests.lockdir=C:\Users\Uwe, Schindler\Projects\lucene\trunk-lusolr1\lucene\build, -Dtests.codec=random, -Dtests.postingsformat=random, -Dtests.locale=random, -Dtests.timezone=random, -Dtests.directory=random, -Dtests.linedocsfile=europarl.lines.txt.gz, -Dtests.iter=1, -Dtests.iter.min=1, -Dtests.seed=random, -Dtests.luceneMatchVersion=4.0, -Dtests.cleanthreads=perMethod, -Djava.util.logging.config.file=/dev/null, -Dtests.nightly=false, -Dtests.asserts.gracious=false, -Dtests.multiplier=1, -DtempDir=C:\Users\Uwe, Schindler\Projects\lucene\trunk-lusolr1\lucene\build\test\1, -Dlucene.version=4.0-SNAPSHOT, -Dtestmethod=, -Djetty.testMode=1, -Djetty.insecurerandom=1, -Dsolr.directoryFactory=org.apache.solr.core.MockDirectoryFactory, -ea:org.apache.lucene..., -ea:org.apache.solr...]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13150896" author="rcmuir" created="Tue, 15 Nov 2011 23:40:36 +0000">&lt;p&gt;Maybe we should remove this global cache completely?&lt;/p&gt;

&lt;p&gt;If the whole point is to prevent double-seeks for queries doing IDF then getting termdocs,&lt;br/&gt;
we can just put a tiny LRU cache in the already-existing threadlocal...&lt;/p&gt;</comment>
                    <comment id="13150904" author="rcmuir" created="Tue, 15 Nov 2011 23:48:52 +0000">&lt;p&gt;actually i think even a tiny queue would work.&lt;/p&gt;

&lt;p&gt;this would prevent the double-seeks. I dont think there is a point to worrying about anything else.&lt;/p&gt;

&lt;p&gt;the LRU-ness seems stupid: for 'common' terms that appear over and over like stopwords, those are gonna be slow anyway.&lt;/p&gt;</comment>
                    <comment id="13150905" author="thetaphi" created="Tue, 15 Nov 2011 23:50:02 +0000">&lt;p&gt;Robert, I agree, a simple LRUCache based on LinkedHashMap with&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
@Override
&lt;span class="code-keyword"&gt;protected&lt;/span&gt; &lt;span class="code-object"&gt;boolean&lt;/span&gt; removeEldestEntry(Map.Entry&amp;lt;K,V&amp;gt; eldest) {
  &lt;span class="code-keyword"&gt;return&lt;/span&gt; size() &amp;gt; MAX_ENTRIES;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;would be fine. Because of this bugs and heavy use in Solr of other concurrent classes, we should maybe add a warning to the startup based on my findings before.&lt;/p&gt;</comment>
                    <comment id="13150917" author="mikemccand" created="Wed, 16 Nov 2011 00:19:31 +0000">&lt;blockquote&gt;
&lt;p&gt;If the whole point is to prevent double-seeks for queries doing IDF then getting termdocs,&lt;br/&gt;
we can just put a tiny LRU cache in the already-existing threadlocal...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In fact this is what we used to do (LinkedHashMap), and then in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2075" title="Share the Term -&amp;gt; TermInfo cache across threads"&gt;&lt;del&gt;LUCENE-2075&lt;/del&gt;&lt;/a&gt; we moved to a cache shareable across threads.&lt;/p&gt;

&lt;p&gt;But I agree &amp;#8211; risk of hangs is not worth it.  Let's just move back to thread-private cache, but let's make it tiny (8? 16? 13?) in size?&lt;/p&gt;</comment>
                    <comment id="13151001" author="rcmuir" created="Wed, 16 Nov 2011 03:43:15 +0000">&lt;p&gt;we can also offer an option... fasterButMoreHangs...&lt;/p&gt;</comment>
                    <comment id="13237031" author="hossman" created="Fri, 23 Mar 2012 20:28:22 +0000">&lt;p&gt;Bulk changing fixVersion 3.6 to 4.0 for any open issues that are unassigned and have not been updated since March 19.&lt;/p&gt;

&lt;p&gt;Email spam suppressed for this bulk edit; search for hoss20120323nofix36 to identify all issues edited&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310040">
                <name>Required</name>
                                <outwardlinks description="requires">
                            <issuelink>
            <issuekey id="12531326">LUCENE-3574</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12503679" name="LUCENE-3235.patch" size="2917" author="rcmuir" created="Mon, 14 Nov 2011 21:48:57 +0000"/>
                    <attachment id="12503626" name="LUCENE-3235.patch" size="2091" author="thetaphi" created="Mon, 14 Nov 2011 15:15:47 +0000"/>
                    <attachment id="12503602" name="LUCENE-3235.patch" size="3030" author="thetaphi" created="Mon, 14 Nov 2011 12:01:40 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 23 Jun 2011 19:57:39 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10765</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24456</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3229] SpanNearQuery: ordered spans should not overlap</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3229</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;While using Span queries I think I've found a little bug.&lt;/p&gt;

&lt;p&gt;With a document like this (from the TestNearSpansOrdered unit test) :&lt;/p&gt;

&lt;p&gt;"w1 w2 w3 w4 w5"&lt;/p&gt;

&lt;p&gt;If I try to search for this span query :&lt;/p&gt;

&lt;p&gt;spanNear([spanNear(&lt;span class="error"&gt;&amp;#91;field:w3, field:w5&amp;#93;&lt;/span&gt;, 1, true), field:w4], 0, true)&lt;/p&gt;

&lt;p&gt;the above document is returned and I think it should not because 'w4' is not after 'w5'.&lt;br/&gt;
The 2 spans are not ordered, because there is an overlap.&lt;/p&gt;

&lt;p&gt;I will add a test patch in the TestNearSpansOrdered unit test.&lt;br/&gt;
I will add a patch to solve this issue too.&lt;br/&gt;
Basicaly it modifies the two docSpansOrdered functions to make sure that the spans does not overlap.&lt;/p&gt;

</description>
                <environment>&lt;p&gt;Windows XP, Java 1.6&lt;/p&gt;</environment>
            <key id="12511231">LUCENE-3229</key>
            <summary>SpanNearQuery: ordered spans should not overlap</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="lboutros">ludovic Boutros</reporter>
                        <labels>
                    </labels>
                <created>Wed, 22 Jun 2011 13:12:43 +0100</created>
                <updated>Fri, 10 May 2013 00:05:19 +0100</updated>
                                    <version>3.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>1</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13053206" author="lboutros" created="Wed, 22 Jun 2011 13:14:27 +0100">&lt;p&gt;Add the Test unit.&lt;/p&gt;</comment>
                    <comment id="13053207" author="lboutros" created="Wed, 22 Jun 2011 13:15:07 +0100">&lt;p&gt;add a Patch.&lt;/p&gt;</comment>
                    <comment id="13053286" author="lboutros" created="Wed, 22 Jun 2011 15:35:37 +0100">&lt;p&gt;testSpanNearUnOrdered unit test does not work anymore.&lt;/p&gt;

&lt;p&gt;The unordered SpanNear class uses the ordering function of the ordered SpanNear class. Perhaps, it should use its own ordering function which allows the span overlaps.&lt;br/&gt;
I will check.&lt;/p&gt;</comment>
                    <comment id="13053309" author="lboutros" created="Wed, 22 Jun 2011 16:30:26 +0100">&lt;p&gt;add a patch for the "SpanNearUnOrdered" class. Everything should be ok now.&lt;/p&gt;</comment>
                    <comment id="13053780" author="paul.elschot@xs4all.nl" created="Thu, 23 Jun 2011 11:24:25 +0100">&lt;p&gt;To reduce surprises like this one when nested spans are used, the ordered case might be changed to require no overlap at all.&lt;br/&gt;
To do that one could compare the end of one spans with the beginning of the next one.&lt;/p&gt;

&lt;p&gt;AFAIK none of the existing test cases uses a nested span query, so more some test cases for that would be good to have.&lt;/p&gt;

&lt;p&gt;The docSpansOrdered method in NearSpansUnordered from the SpanOverLap2.diff patch&lt;br/&gt;
is the same as the existing docSpansOrdered method in NearSpansOrdered. &lt;br/&gt;
That is probably not intended.&lt;/p&gt;

&lt;p&gt;Could you provide patches as decribed here: &lt;a href="http://wiki.apache.org/lucene-java/HowToContribute" class="external-link"&gt;http://wiki.apache.org/lucene-java/HowToContribute&lt;/a&gt; ?&lt;/p&gt;</comment>
                    <comment id="13053817" author="lboutros" created="Thu, 23 Jun 2011 13:29:25 +0100">&lt;p&gt;:To reduce surprises like this one when nested spans are used, the ordered case might be changed to require no overlap at all.&lt;br/&gt;
:To do that one could compare the end of one spans with the beginning of the next one.&lt;br/&gt;
:AFAIK none of the existing test cases uses a nested span query, so more some test cases for that would be good to have.&lt;/p&gt;

&lt;p&gt;The patch does exactly that.&lt;/p&gt;

&lt;p&gt;:The docSpansOrdered method in NearSpansUnordered from the SpanOverLap2.diff patch&lt;br/&gt;
:is the same as the existing docSpansOrdered method in NearSpansOrdered.&lt;br/&gt;
:That is probably not intended.&lt;/p&gt;

&lt;p&gt;It is the same as the actual method because I don't want to modify the current behavior of the NearSpansUnordered class.&lt;br/&gt;
Overlap should be allowed for unordered near span queries. And if I do not do that, unit test is KO for unordered near span queries.&lt;/p&gt;

&lt;p&gt;:Could you provide patches as decribed here: &lt;a href="http://wiki.apache.org/lucene-java/HowToContribute" class="external-link"&gt;http://wiki.apache.org/lucene-java/HowToContribute&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;Sorry for that, sure, I will provide the patch shortly.&lt;/p&gt;

</comment>
                    <comment id="13053823" author="lboutros" created="Thu, 23 Jun 2011 13:45:24 +0100">&lt;p&gt;Here is the patch as described in the wiki.&lt;br/&gt;
Is it ok ?&lt;/p&gt;</comment>
                    <comment id="13054023" author="paul.elschot@xs4all.nl" created="Thu, 23 Jun 2011 19:46:14 +0100">&lt;p&gt;Basically the same functionality as previous patch by Ludovic Boutros.&lt;br/&gt;
Simplified the check for non overlapping spans, this might speed it up somewhat.&lt;br/&gt;
Added javadoc explanations on ordered without overlap and unordered with overlap. &lt;br/&gt;
Minor spelling and indentation changes.&lt;/p&gt;

&lt;p&gt;NearSpansOrdered might be further simplified as not all locals are actually used now because of the simplified check, but for now I prefer to leave that to the JIT to optimize away.&lt;/p&gt;</comment>
                    <comment id="13054026" author="paul.elschot@xs4all.nl" created="Thu, 23 Jun 2011 19:48:53 +0100">&lt;p&gt;Thanks for bringing this up, this has confused more people in the past, and that could well be over now.&lt;/p&gt;</comment>
                    <comment id="13054045" author="lboutros" created="Thu, 23 Jun 2011 20:17:49 +0100">&lt;p&gt;Thanks Paul,&lt;/p&gt;

&lt;p&gt;do you have any idea when this patch will be applied to the branch 3x ?&lt;/p&gt;</comment>
                    <comment id="13054270" author="paul.elschot@xs4all.nl" created="Fri, 24 Jun 2011 08:01:17 +0100">&lt;p&gt;Try and set 3.4 as fix version for this, 3.3 is already on the way out.&lt;br/&gt;
It might also help to add some text for a changes.txt entry.&lt;/p&gt;</comment>
                    <comment id="13237027" author="hossman" created="Fri, 23 Mar 2012 20:28:19 +0000">&lt;p&gt;Bulk changing fixVersion 3.6 to 4.0 for any open issues that are unassigned and have not been updated since March 19.&lt;/p&gt;

&lt;p&gt;Email spam suppressed for this bulk edit; search for hoss20120323nofix36 to identify all issues edited&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12483630" name="LUCENE-3229.patch" size="6482" author="paul.elschot@xs4all.nl" created="Thu, 23 Jun 2011 19:46:14 +0100"/>
                    <attachment id="12483588" name="LUCENE-3229.patch" size="4008" author="lboutros" created="Thu, 23 Jun 2011 13:45:24 +0100"/>
                    <attachment id="12483453" name="SpanOverlap2.diff" size="1697" author="lboutros" created="Wed, 22 Jun 2011 16:30:26 +0100"/>
                    <attachment id="12483433" name="SpanOverlap.diff" size="1358" author="lboutros" created="Wed, 22 Jun 2011 13:15:07 +0100"/>
                    <attachment id="12483432" name="SpanOverlapTestUnit.diff" size="1508" author="lboutros" created="Wed, 22 Jun 2011 13:14:27 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 23 Jun 2011 10:24:25 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10769</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24462</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3161] consider warnings from the source compilation</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3161</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;as Doron mentioned in his review: At compiling there are various warning printed, I think it would be more assuring for downloaders if the build runs without warning. These warnings are not a stopper.&lt;/p&gt;

&lt;p&gt;we could conditionalize these warnings so that they don't "display" when compiling from actual releases, but I have to wonder if we should hide these... being open source I think we should display all our warts, maybe some contributor sees these warnings and decides they want to submit a patch to fix some of them.&lt;/p&gt;</description>
                <environment/>
            <key id="12508762">LUCENE-3161</key>
            <summary>consider warnings from the source compilation</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                        <label>maybe32blocker</label>
                    </labels>
                <created>Mon, 30 May 2011 22:02:31 +0100</created>
                <updated>Fri, 10 May 2013 00:05:19 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13041376" author="rcmuir" created="Tue, 31 May 2011 03:07:53 +0100">&lt;p&gt;There are currently 18 warnings in lucene if you use java 6, all of which are in generated code:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;jflex generates fallthrough warnings&lt;/li&gt;
	&lt;li&gt;javacc generates dep-ann and redundant cast warnings.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Additionally on java 5-only, you will get a dep-ann warning for Version.java, which is actually a JDK bug.&lt;br/&gt;
I'll commit a workaround for this issue, as its the only one in our code.&lt;/p&gt;

&lt;p&gt;In the future, the javacc dep-ann warnings can be reduced by upgrading to the latest javacc, which generates java-5 compatible code.&lt;br/&gt;
I say reduced because on java 5 it hits the same bug as above... and it still has redundant casts.&lt;/p&gt;

&lt;p&gt;I don't think at this time we should be upgrading any of our code generators though, so I think a workaround for our Version.java is the best solution.&lt;br/&gt;
And, I don't think we should in general hide any warnings, even to users for the reasons i mentioned above.&lt;/p&gt;</comment>
                    <comment id="13041379" author="rcmuir" created="Tue, 31 May 2011 03:13:07 +0100">&lt;p&gt;I committed the SuppressWarnings("dep-ann") with a note in revs 1129465, 1129466 (branch3x), 1129467 (branch32).&lt;/p&gt;

&lt;p&gt;I'll keep the issue open for future improvements like possibly upgrading javacc.&lt;/p&gt;</comment>
                    <comment id="13041418" author="doronc" created="Tue, 31 May 2011 05:56:14 +0100">&lt;blockquote&gt;&lt;p&gt;And, I don't think we should in general hide any warnings, even to users for the reasons i mentioned above.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 for not hiding!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 31 May 2011 04:56:14 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2943</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24529</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3153] Adding field w/ norms should fail if same field was added w/o norms already</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3153</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;A spinoff from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3146" title="IndexReader.setNorms is no op if one of the field instances omits norms"&gt;&lt;del&gt;LUCENE-3146&lt;/del&gt;&lt;/a&gt;. Consider the following two scenarios, according to how 4.0 currently works:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Field "a" is added w/ norms. Sometime later field "a" is added to a document w/o norms &amp;#8211; norms are disabled for field "a", for all docs.&lt;/li&gt;
	&lt;li&gt;Field "a" is added w/o norms - norms are disabled for field "a". Sometime later field "a" is added to a document w/ norms &amp;#8211; app thinks norms were added, while in fact they are dropped.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is a bug and case #2 should fail on add/updateDocument - app should know norms were not added. While case #1 isn't great either, it's the only way an app can choose to disable norms for field "a", after instances of it already contain norms, so we should support that scenario.&lt;/p&gt;

&lt;p&gt;In order to detect that early, we should track norms info in .fnx, as Mike describes at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3146" title="IndexReader.setNorms is no op if one of the field instances omits norms"&gt;&lt;del&gt;LUCENE-3146&lt;/del&gt;&lt;/a&gt;. Since this changes the index format, we should also update the "file format" page after we do it.&lt;/p&gt;

&lt;p&gt;Not sure what's the deal w/ 3.x indexes that are read by 4.0 code. Initially they won't have .fnx file, so no central norms information exist to detect the cases I've described above. Over time, as segments are merged, .fnx will include information from more and more segments, but there's always a chance few segments will still contain the norms for field "a". I'm not very familiar w/ that part of the code, but I think that:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;If .fnx says "no norms for field a", the we ignore any norms information that may or may not exist in segments.&lt;/li&gt;
	&lt;li&gt;If .fnx says "norms for field a", then we need to make up some norms values for (old) segments w/ no norms? We need to make up values during segment merge and search?&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12508744">LUCENE-3153</key>
            <summary>Adding field w/ norms should fail if same field was added w/o norms already</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="shaie">Shai Erera</reporter>
                        <labels>
                    </labels>
                <created>Mon, 30 May 2011 20:21:28 +0100</created>
                <updated>Fri, 10 May 2013 00:05:19 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13041403" author="doronc" created="Tue, 31 May 2011 05:05:16 +0100">&lt;p&gt;Can this be checked before any commit (/flush)?&lt;/p&gt;

&lt;p&gt;Assume 10 docs were added without norms to a fresh index, now, without a commit or even a flush, a document is added with norms. Is the info required for checking the "configuration" for that field available at that time?&lt;/p&gt;

&lt;p&gt;If it is not, this is still just a best effort check.&lt;/p&gt;</comment>
                    <comment id="13041448" author="shaie" created="Tue, 31 May 2011 07:01:00 +0100">&lt;p&gt;The difference between the two is that on add/UpdateDocument, we can fail fast. Upon commit, it's a failure that happens too late.&lt;/p&gt;

&lt;p&gt;So I'm not at all convinced now that we should fail on this. Really, apps shouldn't be fiddling w/ norms, at least the apps I know of always index a field the same way. I don't know how common it is for apps to flip the norms bit, and clearly they can only do it one way. So maybe what we should be doing is:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Consolidate norms info in .fnx &amp;#8211; that's a good idea irregardless of the issue.&lt;/li&gt;
	&lt;li&gt;Have javadocs sort out any confusion &amp;#8211; we don't fail add/updateDoc attempts, just follow javadocs semantics&lt;/li&gt;
	&lt;li&gt;Provide API for apps to disable norms for a field, since that practically the only direction we want to allow a/ the aforementioned changed.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Hmm ... another scenario hit me as I wrote the above lines:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;App adds a field w/o norms.&lt;/li&gt;
	&lt;li&gt;App deletes the document w/ the field&lt;/li&gt;
	&lt;li&gt;App adds a field w/ norms &amp;#8211; now what? norms are marked disabled for that field, but the only document that caused that is deleted.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;commit() can be called in between and several documents can be added w/ and w/o norms &amp;#8211; point is, this just gets complicated. This is another reason IMO to let apps manage norms and trust that they don't do fiddle w/ norms. The 'disableNorms' API may still be useful for an app that does not fiddle w/ norms, but decides it does not need norms for a field anymore.&lt;/p&gt;</comment>
                    <comment id="13041461" author="doronc" created="Tue, 31 May 2011 08:02:32 +0100">&lt;p&gt;I was not clear enough.&lt;/p&gt;

&lt;p&gt;I meant that when deciding on consistency of requested NORMS state, if relying only on committed data, then the handling of add/update requests is in a best effort manner, while the handling at commit is complete.&lt;/p&gt;

&lt;p&gt;So, for this example:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Index does not contain field F&lt;/li&gt;
	&lt;li&gt;doc1 is added with F set to NO NORMS&lt;/li&gt;
	&lt;li&gt;doc2 is added with F set to WITH NORMS&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I was not sure about the ability to tell that F in doc2 is inconsistent, because of relying on committed data, and, perhaps, especially with DWPT.&lt;/p&gt;

&lt;p&gt;At commit, it is def possible to check this.&lt;/p&gt;

&lt;p&gt;Similarly this scenario has same problem:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Index contains (committed) field F WITH NORMS&lt;/li&gt;
	&lt;li&gt;doc1 is added with F set to NO NORMS&lt;/li&gt;
	&lt;li&gt;doc2 is added with F set to WITH NORMS&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Again, F in doc2, while consistent with F as committed in the index, is inconsistent with previously added F in doc1.&lt;/p&gt;

&lt;p&gt;In this situation, throwing the exception due to inconsistencies might have to be late in some scenarios (at commit) and hence unacceptable IMO. At the least, such a behavior should be specifically requested by application, e.g. by setting a STRICT_NORMS mode or something like that in iwcfg. &lt;/p&gt;

&lt;p&gt;I am not convinced going that far is justified.&lt;/p&gt;</comment>
                    <comment id="13041608" author="mikemccand" created="Tue, 31 May 2011 15:56:32 +0100">&lt;p&gt;This is quickly getting scary hairy &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Maybe we should not move "omits norms" bit into fnx and continue leaving the checking as "best effort"?&lt;/p&gt;</comment>
                    <comment id="13041611" author="simonw" created="Tue, 31 May 2011 16:02:29 +0100">&lt;blockquote&gt;&lt;p&gt;Maybe we should not move "omits norms" bit into fnx and continue leaving the checking as "best effort"?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;the .fnx file is the way to go here. We can even move hasProx there too eventually.&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13412302" author="hossman" created="Thu, 12 Jul 2012 00:03:45 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429717" author="rcmuir" created="Tue, 7 Aug 2012 04:41:31 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 31 May 2011 04:05:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2949</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24537</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3151] Make all of Analysis completely independent from Lucene Core</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3151</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Lucene's analysis package, including the definitions of Attribute, TokenStream, etc. are quite useful outside of Lucene (for instance, Mahout uses them) for text processing.  I'd like to move the definitions, or at least their packaging, to a separate JAR file so that one can consume them w/o needing Lucene core.  My draft idea is to have a definition area that Lucene core is dependent on and the rest of the analysis package can then be dependent on the definition area.  (I'm open to other ideas as well)&lt;/p&gt;</description>
                <environment/>
            <key id="12508629">LUCENE-3151</key>
            <summary>Make all of Analysis completely independent from Lucene Core</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                    </labels>
                <created>Sat, 28 May 2011 18:48:03 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13040640" author="mikemccand" created="Sat, 28 May 2011 18:52:59 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13040641" author="rcmuir" created="Sat, 28 May 2011 18:54:11 +0100">&lt;p&gt;I agree, and wanted to mention we shouldn't limit ourselves based on packaging.&lt;/p&gt;

&lt;p&gt;for example we can have analyzers-def and analyzers-impl, but actually shove the analyzers-def into the lucene-core jar for simplicity/packaging purposes if we want.&lt;/p&gt;

&lt;p&gt;but this way you could still use the analyzers without the lucene core if you wanted.&lt;/p&gt;</comment>
                    <comment id="13040644" author="gsingers" created="Sat, 28 May 2011 19:07:10 +0100">&lt;p&gt;Analysis could even be released independently.  I've got a start to a patch that I hope to put up today as a POC.&lt;/p&gt;</comment>
                    <comment id="13040668" author="gsingers" created="Sat, 28 May 2011 21:10:49 +0100">&lt;p&gt;I would propose to add:&lt;br/&gt;
lucene/src/analysis-defs that would contain all of the analysis declarations (including attributes) and that the main build would depend on it being built first.  I thought about moving it to modules/analysis, but that makes for some clunky Ant, IMO (although, I'm not sure if this is less clunky.)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but actually shove the analyzers-def into the lucene-core jar for simplicity/packaging purposes if we want.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I'm not sure on shoving them into lucene-core just b/c I wonder if people might think they need both jars then b/c they don't know if it's in core.  Not sure on that one, so I'm not ruling it out.&lt;/p&gt;</comment>
                    <comment id="13040689" author="gsingers" created="Sat, 28 May 2011 23:54:21 +0100">&lt;p&gt;doesn't fully compile yet (but core does) due to our recursive build system, but at least fleshes out the proposed directory layout.  I may, however, change src/declarations to src/common and then we would have lucene-common.jar.  I was surprised by how much I needed to move out of core (e.g. BytesRef)&lt;/p&gt;</comment>
                    <comment id="13040718" author="rcmuir" created="Sun, 29 May 2011 02:35:12 +0100">&lt;p&gt;Looks like it makes sense that we would have to pull out these classes to do it now... but here are a few thoughts maybe for discussion... this stuff certainly should not block this issue, its hard refactorings and a lot of work, but just ideas for the future.&lt;/p&gt;

&lt;p&gt;As far as analyzers:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;does the lucene-core/common jar need to have all the tokenAttributes? Maybe it should only have the ones that the indexer etc actually consume, and things like TypeAttribute, FlagsAttribute, KeywordAttribute, Token, etc should simply be moved to the analysis module?&lt;/li&gt;
	&lt;li&gt;does the lucene-core/common jar need to have Tokenizer/TokenFilter/CharFilter/CharReader/etc. Seems like it really only needs TokenStream and those could also be moved to the analysis module.&lt;/li&gt;
	&lt;li&gt;currently I think its bad that the analyzers depend upon so many of lucene's util package (some internal)... long term we want to get rid of the cumbersome backwards compatibility methods like Version and ideally have a very minimal interface between core and analysis so that you could safely just use your old analyzers jar file, etc... maybe we should see how hard it is to remove some of these util dependencies?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So in a way, this issue is related to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2309" title="Fully decouple IndexWriter from analyzers"&gt;&lt;del&gt;LUCENE-2309&lt;/del&gt;&lt;/a&gt;...&lt;/p&gt;
</comment>
                    <comment id="13044282" author="gsingers" created="Sat, 4 Jun 2011 13:18:47 +0100">


&lt;p&gt;It's not too bad, except for the build system's recursive nature.  Not sure how to get around that yet.&lt;/p&gt;


&lt;p&gt;I did it for Token.  I think the others are useful at the definition layer if someone wants just this piece of analysis, but not all of Lucene's implementations.  But, could be persuaded otherwise.&lt;/p&gt;


&lt;p&gt;QueryParserBase has a dep. here, so if we could fix that, then we might be able to do this.   That being said, they are useful constructs for someone who wants them w/o all of Lucene's implementations.&lt;/p&gt;


&lt;p&gt;I've got a new patch that helps here w/ some, but some of those utils are pretty useful in the context of a common area, I guess.&lt;/p&gt;

</comment>
                    <comment id="13067432" author="lancenorskog" created="Tue, 19 Jul 2011 02:21:35 +0100">&lt;p&gt;&lt;em&gt;&lt;b&gt;Architects remove dependencies&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For external use, this locksteps the external user (Mahout for example) to changes in these data structures. It's a direct coupling. This is how you get conflicting dependencies, what the Linux people call "RPM Hell". &lt;/p&gt;

&lt;p&gt;If you can make a minimal class for export, then have Lucene use a larger class, that might work. Here is a &lt;em&gt;semi-coupled&lt;/em&gt; design:&lt;/p&gt;
&lt;h5&gt;&lt;a name="publicclassITerm"&gt;&lt;/a&gt;public class ITerm&lt;/h5&gt;
&lt;ul&gt;
	&lt;li&gt;A really minimal API that will never be changed, only added onto.&lt;/li&gt;
	&lt;li&gt;Code that uses this API will always work- that is the contract.
	&lt;ul&gt;
		&lt;li&gt;clone() is banned (via UnsupportedOperationException).&lt;/li&gt;
		&lt;li&gt;If a class implements clone(), all subclasses must also implement it.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;I would also ban equals &amp;amp; hashCode- if you want these, make your own subclass that delegates to a real Term subclass.&lt;/li&gt;
&lt;/ul&gt;


&lt;h5&gt;&lt;a name="publicclassTermextendsITerm"&gt;&lt;/a&gt;public class Term extends ITerm&lt;/h5&gt;
&lt;ul&gt;
	&lt;li&gt;This is what Lucene uses.&lt;/li&gt;
	&lt;li&gt;It can be versioned.&lt;/li&gt;
	&lt;li&gt;If you code to this, you lock your binaries to Lucene release jars.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here is a &lt;em&gt;fully-decoupled&lt;/em&gt; design:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Separate suite of main Lucene objects, with minimal features as above.&lt;/li&gt;
	&lt;li&gt;Separate Lucene library that xlates/wraps/etc. between this parallel suite and the Lucene versions. Lucene exports this jar and works very hard to avoid version changes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It's a hard problem all around, and different solutions have failed in their own ways. Error-handling is a particularly big problem. Using these objects in parallel brings its own funkiness.&lt;/p&gt;</comment>
                    <comment id="13417048" author="gsingers" created="Wed, 18 Jul 2012 13:50:00 +0100">&lt;p&gt;I'm resurrecting this.  I'm now thinking that we just put some build targets into Lucene that make it easy to build this, instead of rearranging the packaging.&lt;/p&gt;</comment>
                    <comment id="13417685" author="gsingers" created="Wed, 18 Jul 2012 22:13:24 +0100">&lt;p&gt;Hmm, this gets wonky with some of the dependencies.  Ideally, I'd like to keep this isolated to just the analysis package in core and util (ideally not even that, but do need things like ArrayUtil, BytesRef, etc.), however not sure that can be done w/o some refactoring.  For instance, Analyzer has a dependency on IndexableField, all so that it can check to see whether it is tokenized or not.  Could it just take in a boolean for getOffsetGap indicating whether it is tokenized or not?  It also has a dependency on AlreadyClosedException, which, I suppose could be moved to Util.&lt;/p&gt;

&lt;p&gt;There also a number of imports for Javadocs which are probably useful, but a bit odd in terms of packaging for this particular thing.&lt;/p&gt;</comment>
                    <comment id="13417791" author="rcmuir" created="Wed, 18 Jul 2012 23:48:34 +0100">&lt;p&gt;Grant: I agree.&lt;/p&gt;

&lt;p&gt;I guess it would be good to figure out exactly what the desired goals are:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;is the goal to just use analyzers.jar without having lucene core.jar for aesthetic reasons (no lucene jar file)&lt;/li&gt;
	&lt;li&gt;is instead the goal to be able to depend on an analyzers.jar without causing classpath hell for users that might want to use a different version of lucene in their app, when all you need is the analyzers?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;If its just the second, maybe we just need some fancy jar-jar packing. We would have to target&lt;br/&gt;
a different package name or something like that so there are no conflicts: then again this might be something the end&lt;br/&gt;
user could just do themselves without us doing anything (maven has plugins for this type of thing, etc?). Then&lt;br/&gt;
they could deal with what the renamed packages should be etc?&lt;/p&gt;</comment>
                    <comment id="13418221" author="gsingers" created="Thu, 19 Jul 2012 12:21:29 +0100">&lt;p&gt;My goal is #1 (I have the same goal for the FST package).  I want to be able to use analyzers independently of Lucene and I don't want to have to bring in whatever dependencies other parts of Lucene might have (which is admittedly small at the moment).  Doing this also achieves #2, I suppose.  I've almost got a patch ready that just makes this build sugar, but I wonder if it is better to separate out the code if #1 is the goal.&lt;/p&gt;</comment>
                    <comment id="13418248" author="rcmuir" created="Thu, 19 Jul 2012 13:26:11 +0100">&lt;p&gt;I don think it is from my perspective. Really this isnt a common use case of lucene and &lt;br/&gt;
will make things awkward and confusing (harder to use) to have a lots of jar files.&lt;/p&gt;</comment>
                    <comment id="13419043" author="gsingers" created="Fri, 20 Jul 2012 11:29:41 +0100">&lt;p&gt;Here's a first draft at this.  The packaging looks more or less right, but I haven't fully tested it yet.  The main downsides to this approach are:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Minor loss of Javadoc due to references to things like IndexWriter, DoubleField, etc.  I kept the references, just removed the @link, which allowed me to drop the import statement&lt;/li&gt;
	&lt;li&gt;We need to somehow document that this jar is for standalone use only.  It's probably a minor issue, but going forward, people could get into classloader hell with this if they are mixing versions.  Of course, that's always the case in Java, so caveat emptor.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                    <comment id="13419045" author="gsingers" created="Fri, 20 Jul 2012 11:32:57 +0100">&lt;p&gt;I should add: to run this, for now, do &lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;ant jar-analyzer-definition&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;.  Still need to make sure it fully hooks into the rest of the build correctly, too.&lt;/p&gt;</comment>
                    <comment id="13419106" author="rcmuir" created="Fri, 20 Jul 2012 14:00:55 +0100">&lt;p&gt;Hey Grant: I know it sounds silly but can we split out the getOffsetGap API change into a separate issue?&lt;br/&gt;
This would be nice to fix ASAP.&lt;/p&gt;

&lt;p&gt;I dont understand why it takes IndexableField or took Fieldable. All the other methods here like&lt;br/&gt;
getPositionIncrementGap take "String fieldName". I think this one should too.&lt;/p&gt;

&lt;p&gt;I dont think it needs a boolean for tokenized either: returning a 0 for NOT_ANALYZED fields. &lt;br/&gt;
If you choose NOT_ANALYZED, that should mean the Analyzer is not invoked!&lt;/p&gt;

&lt;p&gt;If you want to do expert stuff control the offset gaps between values for NOT_ANALYZED fields, &lt;br/&gt;
then just analyze it instead, with keyword tokenizer!&lt;/p&gt;
</comment>
                    <comment id="13419163" author="gsingers" created="Fri, 20 Jul 2012 15:05:42 +0100">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-4240" title="Analyzer.getOffsetGap Improvements"&gt;&lt;del&gt;LUCENE-4240&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13419605" author="gsingers" created="Fri, 20 Jul 2012 22:55:57 +0100">&lt;p&gt;Updated version for trunk.  Updated packaging to now include all of Util, as it was getting ridiculous trying to get the exact list.  I tested this setup by taking the jar produced here, plus the common analyzers jar and put them in a standalone project and tested them and it seemed to work.&lt;/p&gt;

&lt;p&gt;Thus, I think this is mostly done and ready to commit.  I'd say the only issue left is to say how we want to document this so that people aren't confused.  My suggestion would be to collocate a file name README-analyzers-def.txt alongside the jar that explains it.  Otherwise, we could just put it in the README.&lt;/p&gt;</comment>
                    <comment id="13419610" author="rcmuir" created="Fri, 20 Jul 2012 23:05:25 +0100">&lt;blockquote&gt;
&lt;p&gt;I tested this setup by taking the jar produced here, plus the common analyzers jar and put them in a standalone project and tested them and it seemed to work.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Personally I dont feel comfortable with that as a testing strategy. There is nothing to prevent someone from breaking this jar in the future (e.g. if i import something from o.a.l.index into an analyzer for some reason).&lt;/p&gt;

&lt;p&gt;If we cannot test this, can we just make it an optional target (e.g. not part of package). Generally this is pretty expert to do (it must be, it has no javadocs, etc etc), so I think its fair the people who need this could just run the ant target from the source release.&lt;/p&gt;


</comment>
                    <comment id="13419621" author="rcmuir" created="Fri, 20 Jul 2012 23:08:45 +0100">&lt;p&gt;For example, I just searched for org.apache.lucene.index in the analyzers-common source and there is code using IndexReader, TermsEnum, etc.&lt;/p&gt;</comment>
                    <comment id="13419790" author="gsingers" created="Sat, 21 Jul 2012 11:13:25 +0100">&lt;blockquote&gt;&lt;p&gt;For example, I just searched for org.apache.lucene.index in the analyzers-common source and there is code using IndexReader, TermsEnum, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ugh.  I wonder if that just makes all of this a moot point.  I'll take a look.  I was thinking about how to more reliably test it yesterday, but didn't implement it.    I guess ideally we could exercise all the analyzers independently on some content, or just run the analyzers test suite somehow.&lt;/p&gt;</comment>
                    <comment id="13419955" author="gsingers" created="Sat, 21 Jul 2012 22:38:29 +0100">&lt;p&gt;For:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;IndexReader &amp;#8211; It's mostly just in tests, except the QueryStopWordAnalyzer&lt;/li&gt;
	&lt;li&gt;TermsEnum &amp;#8211; Same thing, a test and the QueryStopWordAnalyzer&lt;/li&gt;
	&lt;li&gt;Synonym package has dependency on DataOutput and ByteArray* from store (which can be added to the base packaging)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So, basically, the issue would be with the QueryStopWordAnalyzer (the tests aren't an issue)&lt;/p&gt;</comment>
                    <comment id="13419957" author="lancenorskog" created="Sat, 21 Jul 2012 23:00:56 +0100">&lt;p&gt;Is it intended to support jars from different Lucene versions? Would a "unit test" for this project include old versions of jars retained as binaries?&lt;/p&gt;</comment>
                    <comment id="13420146" author="rcmuir" created="Sun, 22 Jul 2012 12:30:36 +0100">&lt;p&gt;Why didn't the compiler catch these things?&lt;/p&gt;</comment>
                    <comment id="13420147" author="thetaphi" created="Sun, 22 Jul 2012 12:38:42 +0100">&lt;p&gt;How about using a tools to collect all realy needed dependencies from core and package it as lucene-core4analysis-min.jar? JARJAR can do this (as ANT task, without renaming classes, just to collect the dependent ones from core). We would then also not need to remove the Javadocs (NRQ,...), Grant's patch removed.&lt;/p&gt;</comment>
                    <comment id="13420248" author="gsingers" created="Sun, 22 Jul 2012 18:31:24 +0100">&lt;blockquote&gt;&lt;p&gt;Why didn't the compiler catch these things?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure I follow.  There really isn't compilation involved at this point and they are runtime dependencies that fail.&lt;/p&gt;

&lt;p&gt;@Uwe:  it's possible, but I suspect the IndexReader dep. is going to bring in a lot, which seems a little silly given it is all just used in the QueryStopWordAnalyzer, which could easily be collapsed into just using the StopFilter and some example code for people.  I'm not that familiar w/ JARJAR, but if you want to try it and we can compare.&lt;/p&gt;</comment>
                    <comment id="13423089" author="gsingers" created="Thu, 26 Jul 2012 15:06:48 +0100">&lt;p&gt;@Robert, @Uwe, any more thoughts on this one?  I hate to see this derailed by one single little used Analyzer that has a workaround solution anyway.  I'm going to try to get more tests in place this weekend, or at least soon.&lt;/p&gt;</comment>
                    <comment id="13423090" author="rcmuir" created="Thu, 26 Jul 2012 15:09:48 +0100">&lt;p&gt;For the long term I like Uwe's idea better I think rather than restricting which javadocs &lt;br/&gt;
in core can link to what and restricting which files the analyzers can use.&lt;/p&gt;

&lt;p&gt;Separately we should fix that Analyzer &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12537417" name="LUCENE-3151.patch" size="11440" author="gsingers" created="Fri, 20 Jul 2012 22:55:57 +0100"/>
                    <attachment id="12537324" name="LUCENE-3151.patch" size="12722" author="gsingers" created="Fri, 20 Jul 2012 11:29:41 +0100"/>
                    <attachment id="12480754" name="LUCENE-3151.patch" size="496169" author="gsingers" created="Sat, 28 May 2011 23:54:21 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 28 May 2011 17:52:59 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2941</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24539</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3150] Wherever we catch &amp; suppress Throwable we should not suppress ThreadInterruptedException</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3150</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In various places we catch Throwable and suppress it, usually in exception handlers where we want to just throw the first exc we had hit.&lt;/p&gt;

&lt;p&gt;But this is dangerous for a thread interrupt since it means we can swallow &amp;amp; ignore the interrupt.&lt;/p&gt;

&lt;p&gt;We should at least catch the interrupt &amp;amp; restore the interrupt bit, if we can't rethrow it.&lt;/p&gt;

&lt;p&gt;One example is in SegmentInfos where we write the segments.gen file... there are many other examples in SegmentInfos too.&lt;/p&gt;</description>
                <environment/>
            <key id="12508624">LUCENE-3150</key>
            <summary>Wherever we catch &amp; suppress Throwable we should not suppress ThreadInterruptedException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sat, 28 May 2011 17:24:25 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13040791" author="shaie" created="Sun, 29 May 2011 12:47:28 +0100">&lt;p&gt;We can fix IOUtils to do that &amp;#8211; now that we call it from many places in the code, it will catch a lot of instances. Also, perhaps we could have an Abortable interface and then call IOUtils from all places that impl abort() today.&lt;/p&gt;</comment>
                    <comment id="13149280" author="simonw" created="Sun, 13 Nov 2011 12:07:21 +0000">&lt;p&gt;here is a first patch for this issue. I integrated another utility into IOUtils that checks if we suppress an InterruptException and resets the interrupt bit on the thread if so. I also call maybeResetInterrupt throughout the code in core where we catch(Throwable) and don't rethrow - its a start....&lt;/p&gt;</comment>
                    <comment id="13237029" author="hossman" created="Fri, 23 Mar 2012 20:28:20 +0000">&lt;p&gt;Bulk changing fixVersion 3.6 to 4.0 for any open issues that are unassigned and have not been updated since March 19.&lt;/p&gt;

&lt;p&gt;Email spam suppressed for this bulk edit; search for hoss20120323nofix36 to identify all issues edited&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12503531" name="LUCENE-3150.patch" size="17224" author="simonw" created="Sun, 13 Nov 2011 12:07:21 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 29 May 2011 11:47:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10802</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24540</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3145] FST APIs should support CharsRef too</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3145</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The Builder API at heart is IntsRef, but we have sugar to pass in BytesRef, CharSequence, etc.  We should add CharsRef too.&lt;/p&gt;

&lt;p&gt;Likewise we have IntsRefFSTEnum, BytesRefFSTEnum; we should add CharsRef there.&lt;/p&gt;

&lt;p&gt;Finally the static Util methods should accept CharsRef.&lt;/p&gt;</description>
                <environment/>
            <key id="12508325">LUCENE-3145</key>
            <summary>FST APIs should support CharsRef too</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Wed, 25 May 2011 17:07:47 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13043557" author="rcmuir" created="Fri, 3 Jun 2011 17:40:46 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13234777" author="hossman" created="Wed, 21 Mar 2012 18:14:25 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 3 Jun 2011 16:40:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2948</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24545</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3133] Fix QueryParser to handle nested fields</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3133</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Once we commit &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2454" title="Nested Document query support"&gt;&lt;del&gt;LUCENE-2454&lt;/del&gt;&lt;/a&gt;, we need to make it easy for apps to enable this with QueryParser.&lt;/p&gt;

&lt;p&gt;It seems like it's a "schema" like behavior, ie we need to be able to express the join structure of the related fields.&lt;/p&gt;

&lt;p&gt;And then whenever QP produces a query that spans fields requiring a join, the NestedDocumentQuery is used to wrap the child fields?&lt;/p&gt;</description>
                <environment/>
            <key id="12508090">LUCENE-3133</key>
            <summary>Fix QueryParser to handle nested fields</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 23 May 2011 19:27:39 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13038148" author="markh" created="Mon, 23 May 2011 20:13:05 +0100">&lt;p&gt;2454 already includes extensions for the XML parser syntax and the standard QueryParser could work the same with some added syntax. I think I've seen other languages use WITH as a keyword e.g.&lt;/p&gt;
&lt;div class="code panel" style="border-style: solid;border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  forename:john surname:smith WITH(employer:google AND date:2009)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example the WITH keyword is used to mark a clause that relates to a child document.&lt;br/&gt;
What is left unsaid here is how parent documents are distinguished from child docs in the index. I guess you could &lt;br/&gt;
1) Extend the WITH syntax to make it part of the query expression or&lt;br/&gt;
2) Make it part of the QueryParser constructor (i.e. tell the query parser what denotes parent docs) or&lt;br/&gt;
3) Have a fixed system for tagging parents enforced by Lucene's IndexWriter when calling the addDocuments API.&lt;/p&gt;

&lt;p&gt;Option 3 seems too restrictive (it may be desirable for example to have multiple levels of hierarchy to roll up to in an index).&lt;/p&gt;

&lt;p&gt;The majority of users we have using this feature currently do so using a form-based query builder which assembles the nested XML syntax behind the scenes so there is no need for extensions to the standard QueryParser.  I can see some power users would want this though.&lt;/p&gt;</comment>
                    <comment id="13038235" author="mikemccand" created="Mon, 23 May 2011 22:15:15 +0100">
&lt;p&gt;I'm confused on why the query parser language would need to be&lt;br/&gt;
extended to handle this...&lt;/p&gt;

&lt;p&gt;Ie, it seems like, for a given index, the assignment of fields to&lt;br/&gt;
parent vs child docs is a global/static decision?  And then any query&lt;br/&gt;
that has clauses against mixed parent/child fields should be "wrapped"&lt;br/&gt;
by NestedDocumentQuery so that the child field/doc matches are&lt;br/&gt;
"translated" to the corresponding parent docs?&lt;/p&gt;

&lt;p&gt;Why should each query be free to change this?&lt;/p&gt;

&lt;p&gt;EG if a user type that same query, but without WITH, then nothing&lt;br/&gt;
would match right?&lt;/p&gt;

&lt;p&gt;I guess this means I'd vote for 2 &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13038270" author="markh" created="Mon, 23 May 2011 23:10:03 +0100">&lt;p&gt;So the 2 reasons I can think of why the WITH construct may be needed are:&lt;br/&gt;
1) If the field names aren't exclusive to a doc type e.g. "name" or "age" is a field found on both parent and child docs&lt;br/&gt;
or&lt;br/&gt;
2) If you want to find a parent with two different children (e.g. a resume of someone who has held a position at Google in 2009 and a different position at LinkedIn during 2010).&lt;/p&gt;

&lt;p&gt;In both cases the WITH clause is needed to set the context around clauses to avoid any ambiguity&lt;/p&gt;</comment>
                    <comment id="13038286" author="mikemccand" created="Mon, 23 May 2011 23:34:53 +0100">&lt;p&gt;Oh I see.  Hmm, is case 1 is going to cause problems?  (Ie if both parent &amp;amp; child docs can come back matching a given field).  Is there a "normal" use case where you would want to put same field name on both parent &amp;amp; child docs?  (I had thought normally the field names would be orthogonal).&lt;/p&gt;

&lt;p&gt;Case 2) I agree needs some special syntax.  In fact, even non-nested docs might want such a query?  Eg if my docs are cars, and each car has a multi-valued field listing its features ("A/C", "Automatic transmission", ...), and I want to find all cars that have both A/C and Automatic transmission.  Boolean AND query won't work correctly for this; I'd need this same extension as your bullet 2 I think?&lt;/p&gt;</comment>
                    <comment id="13038292" author="mikemccand" created="Mon, 23 May 2011 23:41:33 +0100">&lt;p&gt;Duh nevermind on case 2 &amp;#8211; Boolean AND query would work for that example!&lt;/p&gt;</comment>
                    <comment id="13038328" author="sokolov" created="Tue, 24 May 2011 01:06:11 +0100">&lt;p&gt;Mightn't you want to be able to do self-joins?  For example if you want to represent an XML document, and your field is "Element" - it has any number Attribute children and any number of Node children, which in turn may be Elements.  I wonder if &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2454" title="Nested Document query support"&gt;&lt;del&gt;LUCENE-2454&lt;/del&gt;&lt;/a&gt; could be extended to allow recursive ChildDocumentQuery - ie DescendantDocumentQuery?&lt;/p&gt;</comment>
                    <comment id="13038448" author="markh" created="Tue, 24 May 2011 09:42:45 +0100">&lt;blockquote&gt;&lt;p&gt;I wonder if &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2454" title="Nested Document query support"&gt;&lt;del&gt;LUCENE-2454&lt;/del&gt;&lt;/a&gt; could be extended to allow recursive ChildDocumentQuery &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No need to extend. This can be done today by nesting a NestedDocumentQuery inside another.&lt;br/&gt;
The only thing you need to do is set the "ParentsFilter" to roll up results to the appropriate point e.g. parent/child/grandchild&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;is there a "normal" use case where you would want to put same field name on both parent &amp;amp; child docs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wouldn't want to rule that possibility out e.g. a person has a name and age and their sons and daughters have names and ages too.&lt;/p&gt;
</comment>
                    <comment id="13043560" author="rcmuir" created="Fri, 3 Jun 2011 17:40:47 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13234772" author="hossman" created="Wed, 21 Mar 2012 18:14:23 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12464145">LUCENE-2454</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 23 May 2011 19:13:05 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2957</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24557</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3122] Cascaded grouping</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3122</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Similar to &lt;a href="https://issues.apache.org/jira/browse/SOLR-2526" title="Grouping on multiple fields"&gt;SOLR-2526&lt;/a&gt;, in that you are grouping on 2 separate fields, but instead of treating those fields as a single grouping by a compound key, this change would let you first group on key1 for the primary groups and then secondarily on key2 within the primary groups.&lt;/p&gt;

&lt;p&gt;Ie, the result you get back would have groups A, B, C (grouped by key1) but then the documents within group A would be grouped by key 2.&lt;/p&gt;

&lt;p&gt;I think this will be important for apps whose documents are the product of denormalizing, ie where the Lucene document is really a sub-document of a different identifier field.  Borrowing an example from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3097" title="Post grouping faceting"&gt;&lt;del&gt;LUCENE-3097&lt;/del&gt;&lt;/a&gt;, you have doctors but each doctor may have multiple offices (addresses) where they practice and so you index doctor X address as your lucene documents.  In this case, your "identifier" field (that which "counts" for facets, and should be "grouped" for presentation) is doctorid.  When you offer users search over this index, you'd likely want to 1) group by distance (ie, &amp;lt; 0.1 miles, &amp;lt; 0.2 miles, etc., as a function query), but 2) also group by doctorid, ie cascaded grouping.&lt;/p&gt;

&lt;p&gt;I suspect this would be easier to implement than it sounds: the per-group collector used by the 2nd pass grouping collector for key1's grouping just needs to be another grouping collector.  Spookily, though, that collection would also have to be 2-pass, so it could get tricky since grouping is sort of recursing on itself.... once we have &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3112" title="Add IW.add/updateDocuments to support nested documents"&gt;&lt;del&gt;LUCENE-3112&lt;/del&gt;&lt;/a&gt;, though, that should enable efficient single pass grouping by the identifier (doctorid).&lt;/p&gt;</description>
                <environment/>
            <key id="12507739">LUCENE-3122</key>
            <summary>Cascaded grouping</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Thu, 19 May 2011 11:39:32 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/grouping</component>
                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="13043536" author="rcmuir" created="Fri, 3 Jun 2011 17:40:38 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 3 Jun 2011 16:40:38 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10812</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24567</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3120] span query matches too many docs when two query terms are the same unless inOrder=true</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3120</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;spinoff of user list discussion - &lt;a href="http://markmail.org/message/i4cstlwgjmlcfwlc" class="external-link"&gt;SpanNearQuery - inOrder parameter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With 3 documents:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;"a b x c d"&lt;/li&gt;
	&lt;li&gt;"a b b d"&lt;/li&gt;
	&lt;li&gt;"a b x b y d"&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here are a few queries (the number in parenthesis indicates expected #hits):&lt;/p&gt;


&lt;p&gt;These ones work &lt;b&gt;as expected&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(1)  in-order, slop=0, "b", "x", "b"&lt;/li&gt;
	&lt;li&gt;(1)  in-order, slop=0, "b", "b"&lt;/li&gt;
	&lt;li&gt;(2)  in-order, slop=1, "b", "b"&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;These ones match &lt;b&gt;too many&lt;/b&gt; hits:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(1)  any-order, slop=0, "b", "x", "b"&lt;/li&gt;
	&lt;li&gt;(1)  any-order, slop=1, "b", "x", "b"&lt;/li&gt;
	&lt;li&gt;(1)  any-order, slop=2, "b", "x", "b"&lt;/li&gt;
	&lt;li&gt;(1)  any-order, slop=3, "b", "x", "b"&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;These ones match &lt;b&gt;too many&lt;/b&gt; hits as well:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(1)  any-order, slop=0, "b", "b"&lt;/li&gt;
	&lt;li&gt;(2)  any-order, slop=1, "b", "b"&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Each of the above passes when using a phrase query (applying the slop, no in-order indication in phrase query).&lt;/p&gt;

&lt;p&gt;This seems related to a known overlapping spans issue - &lt;a href="http://markmail.org/message/7jxn5eysjagjwlon" class="external-link"&gt;non-overlapping Span queries&lt;/a&gt; - as indicated by Hoss, so we might decide to close this bug after all, but I would like to at least have the junit that exposes the behavior in JIRA.&lt;/p&gt;</description>
                <environment/>
            <key id="12507728">LUCENE-3120</key>
            <summary>span query matches too many docs when two query terms are the same unless inOrder=true</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="doronc">Doron Cohen</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 May 2011 10:13:20 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13036078" author="doronc" created="Thu, 19 May 2011 10:15:53 +0100">&lt;p&gt;Attached test case demonstrating the bug.&lt;/p&gt;</comment>
                    <comment id="13036080" author="gregtarr" created="Thu, 19 May 2011 10:17:46 +0100">&lt;p&gt;Thanks for raising this.&lt;/p&gt;</comment>
                    <comment id="13036429" author="doronc" created="Thu, 19 May 2011 21:24:32 +0100">&lt;p&gt;Updated patch with fixed test to not depend on analysis module.&lt;/p&gt;</comment>
                    <comment id="13036538" author="hossman" created="Thu, 19 May 2011 23:29:03 +0100">&lt;p&gt;comment i made on the mailing list regarding this topic...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;the crux of hte issue (as i recall) is that there is really no conecptual reason to why a query for "'john' near 'john', in any order, with slop of Z" shouldn't match a doc that contains only one instance of "john" ... the first SpanTermQuery says "i found a match at position X" the second SpanTermQuery says "i found a match at position Y" and the SpanNearQuery says "the differnece between X and Y is less then Z" therefore i have a match. (The SpanNearQuery can't fail just because X and Y are the same &amp;#8211; they might be two distinct term instances, with differnet payloads perhaps, that just happen to have the same position).&lt;/p&gt;

&lt;p&gt;However: if true==inOrder case works because the SpanNearQuery enforces that "X must be less then Y" so the same term can't ever match twice. &lt;/p&gt;&lt;/blockquote&gt;</comment>
                    <comment id="13036540" author="hossman" created="Thu, 19 May 2011 23:32:15 +0100">&lt;p&gt;What we might want to consider is a new option on SpanNearQuery that would mandate that the spans not overlap.&lt;/p&gt;

&lt;p&gt;Paul Elschot described the general form of this idea once as an numeric option to specify a minimum distance between the subspans (so the default, as implemented today, for inOrder==true would be minPositionDistance=1; and the default for inOrder==false would be minPositionDistance=0)&lt;/p&gt;

</comment>
                    <comment id="13043547" author="rcmuir" created="Fri, 3 Jun 2011 17:40:42 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13237030" author="hossman" created="Fri, 23 Mar 2012 20:28:21 +0000">&lt;p&gt;Bulk changing fixVersion 3.6 to 4.0 for any open issues that are unassigned and have not been updated since March 19.&lt;/p&gt;

&lt;p&gt;Email spam suppressed for this bulk edit; search for hoss20120323nofix36 to identify all issues edited&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12479826" name="LUCENE-3120.patch" size="3835" author="doronc" created="Thu, 19 May 2011 21:24:32 +0100"/>
                    <attachment id="12479744" name="LUCENE-3120.patch" size="3852" author="doronc" created="Thu, 19 May 2011 10:15:53 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 19 May 2011 09:17:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10814</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24569</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3116] pendingCommit in IndexWriter is not thoroughly tested</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3116</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When working on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3084" title="MergePolicy.OneMerge.segments should be List&amp;lt;SegmentInfo&amp;gt; not SegmentInfos, Remove Vector&amp;lt;SI&amp;gt; subclassing from SegmentInfos &amp;amp; more refactoring"&gt;&lt;del&gt;LUCENE-3084&lt;/del&gt;&lt;/a&gt;, I had a copy-paste error in my patch (see revision 1124307 and corrected in 1124316), I replaced pendingCommit by segmentInfos in IndexWriter, corrected by the following patch:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;--- lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexWriter.java (original)
+++ lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexWriter.java Wed May 18 16:16:29 2011
@@ -2552,7 +2552,7 @@ public class IndexWriter implements Clos
         lastCommitChangeCount = pendingCommitChangeCount;
         segmentInfos.updateGeneration(pendingCommit);
         segmentInfos.setUserData(pendingCommit.getUserData());
-        rollbackSegments = segmentInfos.createBackupSegmentInfos(true);
+        rollbackSegments = pendingCommit.createBackupSegmentInfos(true);
         deleter.checkpoint(pendingCommit, true);
       } finally {
         // Matches the incRef done in startCommit:
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This did not cause any test failure.&lt;/p&gt;

&lt;p&gt;On IRC, Mike said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;span class="error"&gt;&amp;#91;19:21&amp;#93;&lt;/span&gt;	mikemccand: ThetaPh1: hmm&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:21&amp;#93;&lt;/span&gt;	mikemccand: well&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:22&amp;#93;&lt;/span&gt;	mikemccand: pendingCommit and sis only differ while commit() is running&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:22&amp;#93;&lt;/span&gt;	mikemccand: ie if a thread starts commit&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:22&amp;#93;&lt;/span&gt;	mikemccand: but fsync is taking a long time&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:22&amp;#93;&lt;/span&gt;	mikemccand: and another thread makes a change to sis&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:22&amp;#93;&lt;/span&gt;	ThetaPh1: ok so hard to find that bug&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;19:22&amp;#93;&lt;/span&gt;	mikemccand: we need our mock dir wrapper to sometimes take a long time syncing....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe we need such a test, I feel bad when such stupid changes don't make any test fail.&lt;/p&gt;</description>
                <environment/>
            <key id="12507637">LUCENE-3116</key>
            <summary>pendingCommit in IndexWriter is not thoroughly tested</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Wed, 18 May 2011 18:48:10 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                    <version>3.2</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13035520" author="mikemccand" created="Wed, 18 May 2011 18:52:24 +0100">&lt;p&gt;It's great you caught this on backport Uwe!  And, yes, spooky no tests failed...&lt;/p&gt;

&lt;p&gt;It'll be challenging to have a test catch this.  Fixing MockDirWrapper to sometimes take "unusually" long time to do the fsync is a great start.  What this change would have caused is .rollback() would roll back to a wrong copy of the sis, ie not a commit point but rather a commit point plus some additional flushes.&lt;/p&gt;</comment>
                    <comment id="13043543" author="rcmuir" created="Fri, 3 Jun 2011 17:40:41 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13222933" author="rcmuir" created="Tue, 6 Mar 2012 02:27:57 +0000">&lt;p&gt;its easy to add the sleep, but we dont even have good multithreaded tests with rollback() &lt;span class="error"&gt;&amp;#91;except testing how exceptions are handled and not really asserting anything?&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Can we push this out to 4.0?&lt;/p&gt;</comment>
                    <comment id="13223382" author="mikemccand" created="Tue, 6 Mar 2012 16:29:54 +0000">&lt;p&gt;I think we can push to 4.0...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 18 May 2011 17:52:24 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2959</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24573</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3109] Rename FieldsConsumer to InvertedFieldsConsumer</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3109</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The name FieldsConsumer is missleading here it really is an InvertedFieldsConsumer and since we are extending codecs to consume non-inverted Fields we should be clear here. Same applies to Fields.java as well as FieldsProducer.&lt;/p&gt;</description>
                <environment/>
            <key id="12507456">LUCENE-3109</key>
            <summary>Rename FieldsConsumer to InvertedFieldsConsumer</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 May 2011 09:14:27 +0100</created>
                <updated>Fri, 10 May 2013 00:05:20 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13035510" author="mikemccand" created="Wed, 18 May 2011 18:41:42 +0100">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13209305" author="iuliux" created="Thu, 16 Feb 2012 12:19:09 +0000">&lt;p&gt;Is this still valid? (It looks like a good place for me to enter the community)&lt;/p&gt;

&lt;p&gt;Should also the *FieldsReader/Writer classes that derive FieldsProducer/Consumer become *InvertedFieldsReader/Writer?&lt;/p&gt;</comment>
                    <comment id="13209309" author="simonw" created="Thu, 16 Feb 2012 12:30:07 +0000">&lt;blockquote&gt;&lt;p&gt;Is this still valid? (It looks like a good place for me to enter the community)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think so there should also be an InvertedFieldsProducer&lt;/p&gt;</comment>
                    <comment id="13209652" author="iuliux" created="Thu, 16 Feb 2012 19:38:10 +0000">&lt;p&gt;Attached a patch with the refactoring of Fields, FieldsProducer, FieldsConsumer and any other related classes.&lt;br/&gt;
It turned out to be pretty ample (also affected Solr)&lt;/p&gt;

&lt;p&gt;Please give some feedback if something is wrong.&lt;/p&gt;</comment>
                    <comment id="13209816" author="iuliux" created="Thu, 16 Feb 2012 22:27:31 +0000">&lt;p&gt;Repaired a foolish mistake.&lt;br/&gt;
Also limited to the classes sepcified in the ticket.&lt;/p&gt;</comment>
                    <comment id="13248604" author="mikemccand" created="Fri, 6 Apr 2012 20:03:40 +0100">&lt;p&gt;Hi Iulius, this patch is great: this rename is badly needed...&lt;/p&gt;

&lt;p&gt;I was able to apply the patch (resolving a few conflicts since the code has shifted since it was created), but... some things seem to be missing (eg InvertedFieldsProducer rename).  How did you generate the patch?&lt;/p&gt;</comment>
                    <comment id="13249237" author="iuliux" created="Sat, 7 Apr 2012 13:38:49 +0100">&lt;p&gt;Good question. It seems I didn't add the renamed sources.&lt;/p&gt;

&lt;p&gt;Could you please upload the patch with the shifted-code-conflicts you mentioned about solved?&lt;br/&gt;
This would be wonderful, I could much easily redo only the renamed sources.&lt;/p&gt;

&lt;p&gt;Thanks for your feedback.&lt;/p&gt;</comment>
                    <comment id="13249281" author="mikemccand" created="Sat, 7 Apr 2012 16:22:13 +0100">&lt;p&gt;Hi Iulius,&lt;/p&gt;

&lt;p&gt;Here's my current patch &amp;#8211; it doesn't compile because of the missing renamed sources but possibly from other things (eg if I messed up any of the merging).  But hopefully it's close &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  Thanks!&lt;/p&gt;</comment>
                    <comment id="13249331" author="iuliux" created="Sat, 7 Apr 2012 22:01:37 +0100">&lt;p&gt;It was cleaner to redo it from scratch. Hope didn't miss anything this time.&lt;/p&gt;

&lt;p&gt;It built fine and tests got passed.&lt;/p&gt;</comment>
                    <comment id="13249390" author="mikemccand" created="Sat, 7 Apr 2012 23:37:38 +0100">&lt;p&gt;Thanks for the fast turnaround Iulius!&lt;/p&gt;

&lt;p&gt;Did you use "svn mv" to rename the sources?  (I'm guessing not &amp;#8211; I don't see the removed original sources).&lt;/p&gt;

&lt;p&gt;But it's fine: I got this to apply quite easily.  Thanks!  I'll commit shortly...&lt;/p&gt;</comment>
                    <comment id="13249398" author="mikemccand" created="Sat, 7 Apr 2012 23:47:15 +0100">&lt;p&gt;Hmm, one thing: I noticed the imports got changed into wildcards, eg:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;+import org.apache.lucene.index.*;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.MultiInvertedFields;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In general I prefer seeing each import (not the wildcard)... can you redo patch putting them back?  Thanks!&lt;/p&gt;

&lt;p&gt;(I'm assuming/hoping this is a simple setting in your IDE?).&lt;/p&gt;</comment>
                    <comment id="13249454" author="iuliux" created="Sun, 8 Apr 2012 02:17:20 +0100">&lt;blockquote&gt;&lt;p&gt;In general I prefer seeing each import (not the wildcard)... can you redo patch putting them back? Thanks!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Totally agree on that. I should blame IDEA for this one, should I?&lt;/p&gt;

&lt;p&gt;Thanks for all the patience.&lt;/p&gt;</comment>
                    <comment id="13249512" author="mikemccand" created="Sun, 8 Apr 2012 10:57:28 +0100">&lt;p&gt;Thanks Iulius, looks great!  I'll commit...&lt;/p&gt;</comment>
                    <comment id="13249515" author="mikemccand" created="Sun, 8 Apr 2012 11:09:25 +0100">&lt;p&gt;Thanks Iulius!&lt;/p&gt;</comment>
                    <comment id="13249517" author="thetaphi" created="Sun, 8 Apr 2012 11:17:04 +0100">&lt;p&gt;We also changed public APIs (Fields -&amp;gt; InvertedFields). We need to change CHANGES.txt and MIGRATE.txt to the new API, it's now heavily outdated.&lt;/p&gt;

&lt;p&gt;Should we change AtomicReader to have invertedField() instead fields()? Also the name FieldsEnum is now inconsistent.&lt;/p&gt;</comment>
                    <comment id="13249518" author="thetaphi" created="Sun, 8 Apr 2012 11:17:32 +0100">&lt;p&gt;Documentation fixes needed.&lt;/p&gt;</comment>
                    <comment id="13249520" author="mikemccand" created="Sun, 8 Apr 2012 11:36:05 +0100">&lt;blockquote&gt;&lt;p&gt;We need to change CHANGES.txt and MIGRATE.txt to the new API, it's now heavily outdated.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks Uwe, you're right, my bad.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Should we change AtomicReader to have invertedField() instead fields()? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also the name FieldsEnum is now inconsistent.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it should be InvertedFieldsEnum?&lt;/p&gt;

&lt;p&gt;Iulius do you want to make these changes?  Or I can... let me know.&lt;/p&gt;</comment>
                    <comment id="13249524" author="thetaphi" created="Sun, 8 Apr 2012 12:00:12 +0100">&lt;p&gt;I can do it, too (not now). It's 5 minutes work with Eclipse...&lt;/p&gt;</comment>
                    <comment id="13249535" author="iuliux" created="Sun, 8 Apr 2012 13:47:50 +0100">&lt;blockquote&gt;&lt;p&gt;Should we change AtomicReader to have invertedField() instead fields()? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I worked out all the &lt;tt&gt;fields()&lt;/tt&gt; methods that returned &lt;tt&gt;InvertedFields&lt;/tt&gt;.&lt;br/&gt;
Also &lt;tt&gt;MIGRATE.txt&lt;/tt&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also the name FieldsEnum is now inconsistent.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is not included in the patch because I have some difficulty deciding whether or not also rename all the classes derived from &lt;tt&gt;FieldsEnum&lt;/tt&gt; (like &lt;tt&gt;FilterAtomicReader.FilterFieldsEnum&lt;/tt&gt;)&lt;/p&gt;

&lt;p&gt;Also, should &lt;tt&gt;MultiFields&lt;/tt&gt; and &lt;tt&gt;MultiFieldsEnum&lt;/tt&gt; get renamed?&lt;/p&gt;</comment>
                    <comment id="13249544" author="rcmuir" created="Sun, 8 Apr 2012 14:18:19 +0100">&lt;p&gt;Can we please revert the renaming of Fields to InvertedFields?&lt;/p&gt;

&lt;p&gt;The title of this issue made me think it only affects low-level codec apis but now&lt;br/&gt;
we are talking about a massive renaming of postings apis that, in my opinion,&lt;br/&gt;
goes in the wrong direction, and in the least requires more discussion.&lt;/p&gt;</comment>
                    <comment id="13249545" author="rcmuir" created="Sun, 8 Apr 2012 14:21:15 +0100">&lt;p&gt;I don't understand the reasoning to add "Inverted" to all these apis.&lt;/p&gt;

&lt;p&gt;its damaging when we had a perfectly good single-syllable "Fields" before.&lt;br/&gt;
Now we make a harder-to-consume multi-syllable API, for what reason?&lt;br/&gt;
What other kind of Field is there?!&lt;/p&gt;

&lt;p&gt;But, again I'm not gonna spark a huge argument/discussion about this. &lt;br/&gt;
I'm just asking for a revert.&lt;/p&gt;</comment>
                    <comment id="13249546" author="mikemccand" created="Sun, 8 Apr 2012 14:26:41 +0100">&lt;p&gt;OK I'll revert so we can discuss more...&lt;/p&gt;</comment>
                    <comment id="13249549" author="rcmuir" created="Sun, 8 Apr 2012 14:39:19 +0100">&lt;p&gt;Thanks, when looking at naming of apis that users will interact with,&lt;br/&gt;
I think we should go with the simplest possible naming thats easiest to consume.&lt;/p&gt;

&lt;p&gt;For example things like "Term", "Query", "Document", etc. I think this kind of&lt;br/&gt;
naming helps to keep the API consumable: as far as more expert stuff inside codec,&lt;br/&gt;
thats sort of a different story (though we shouldnt just name it whatever, i think&lt;br/&gt;
we don't have to be nearly as picky about names).&lt;/p&gt;

&lt;p&gt;For the core APIs that hook into IndexReader and IndexWriter, and for things in &lt;br/&gt;
the o.a.l.document package, and things like that, I think we should be shooting &lt;br/&gt;
for these super-simplistic names that have worked for lucene all along.&lt;/p&gt;

&lt;p&gt;To me, taking an IndexReader and enumerating Fields-&amp;gt;FieldsEnum-&amp;gt;Terms-&amp;gt;TermsEnum... &lt;br/&gt;
is pretty clear and makes sense.&lt;/p&gt;

&lt;p&gt;Fields and Terms being plural makes sense to me, but one improvement to think of is&lt;br/&gt;
removing the confusing plural ending of these enum classes (FieldEnum, TermEnum). &lt;br/&gt;
It seems this only have existed to not conflict with the pre-flex API before &lt;br/&gt;
(for backwards compatibility). I think that would be an easy improvement to&lt;br/&gt;
those enum classes..., for consistency maybe do the same with DocsEnum, or even&lt;br/&gt;
think of a new name for that one entirely, I'm not sure.&lt;/p&gt;</comment>
                    <comment id="13249551" author="rcmuir" created="Sun, 8 Apr 2012 14:56:20 +0100">&lt;p&gt;Also I think there are other improvements we can do here that would be more natural:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Fields.getUniqueFieldCount() -&amp;gt; Fields.size()&lt;/li&gt;
	&lt;li&gt;Terms.getUniqueTermCount() -&amp;gt; Terms.size()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In general the names of the other statistics could probably use some help.&lt;br/&gt;
When i gave a lucene talk on the new stats i had to add "wtf are these things in english"&lt;br/&gt;
beside the name of each new stat. I can't suggest good java names at this time,&lt;br/&gt;
but these are the english names:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Terms.getDocCount() -&amp;gt; "number of documents with value"&lt;/li&gt;
	&lt;li&gt;Terms.getSumDocFreq() -&amp;gt; "number of postings"&lt;/li&gt;
	&lt;li&gt;Terms.getSumTotalTermFreq() -&amp;gt; "number of tokens"&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TermsEnum.totalTermFreq is probably ok, but maybe it was named that way&lt;br/&gt;
only to be consistent with docFreq? Really something like "number of occurrences"&lt;br/&gt;
is what most people would expect here.&lt;/p&gt;</comment>
                    <comment id="13249556" author="rcmuir" created="Sun, 8 Apr 2012 15:09:24 +0100">&lt;p&gt;Also just another idea to throw out there:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;AtomicReader.termDocsEnum -&amp;gt; AtomicReader.termDocs&lt;/li&gt;
	&lt;li&gt;AtomicReader.termPositionsEnum -&amp;gt; AtomicReader.termPositions&lt;/li&gt;
	&lt;li&gt;TermsEnum.docs -&amp;gt; TermsEnum.termDocs&lt;/li&gt;
	&lt;li&gt;TermsEnum.docsAndPositions -&amp;gt; TermsEnum.termPositions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This terminology would be more consistent with all previous&lt;br/&gt;
lucene APIs and seems like an easy win?&lt;/p&gt;</comment>
                    <comment id="13412286" author="hossman" created="Thu, 12 Jul 2012 00:03:42 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429718" author="rcmuir" created="Tue, 7 Aug 2012 04:41:32 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                    <comment id="13541461" author="markrmiller@gmail.com" created="Mon, 31 Dec 2012 19:04:53 +0000">&lt;p&gt;4.1 or push to 4.2?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12521886" name="LUCENE-3109.patch" size="35781" author="iuliux" created="Sun, 8 Apr 2012 13:47:49 +0100"/>
                    <attachment id="12521870" name="LUCENE-3109.patch" size="137448" author="iuliux" created="Sun, 8 Apr 2012 02:17:20 +0100"/>
                    <attachment id="12521834" name="LUCENE-3109.patch" size="269392" author="iuliux" created="Sat, 7 Apr 2012 22:01:37 +0100"/>
                    <attachment id="12521824" name="LUCENE-3109.patch" size="126369" author="mikemccand" created="Sat, 7 Apr 2012 16:22:13 +0100"/>
                    <attachment id="12514899" name="LUCENE-3109.patch" size="129141" author="iuliux" created="Thu, 16 Feb 2012 22:27:31 +0000"/>
                    <attachment id="12514850" name="LUCENE-3109.patch" size="268930" author="iuliux" created="Thu, 16 Feb 2012 19:38:09 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>6.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 18 May 2011 17:41:42 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10820</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24579</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3069] Lucene should have an entirely memory resident term dictionary</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3069</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;FST based TermDictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. Some environments have enough memory available to keep the entire FST based term dict in memory. We should add a TermDictionary implementation that encodes all needed information for each term into the FST (custom fst.Output) and builds a FST from the entire term not just the delta.&lt;/p&gt;</description>
                <environment/>
            <key id="12506174">LUCENE-3069</key>
            <summary>Lucene should have an entirely memory resident term dictionary</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="billy">Han Jiang</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Wed, 4 May 2011 15:09:51 +0100</created>
                <updated>Sat, 1 Jun 2013 18:22:54 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                <component>core/search</component>
                        <due/>
                    <votes>1</votes>
                        <watches>8</watches>
                                                                                  <comments>
                    <comment id="13051003" author="mikemccand" created="Fri, 17 Jun 2011 12:53:40 +0100">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3209" title="Memory codec"&gt;&lt;del&gt;LUCENE-3209&lt;/del&gt;&lt;/a&gt; is a new codec that puts everything (terms + postings) in RAM.  For this issue I think we should make this controllable, ie so terms can be in RAM but postings remain in the Directory.&lt;/p&gt;</comment>
                    <comment id="13617982" author="dsmiley" created="Sat, 30 Mar 2013 04:50:21 +0000">&lt;p&gt;I'd love to see this come to pass.  I've been thinking about what goes on a layer beneath TermsEnum (i.e. how it is implemented) as I work on spatial stuff. Geohash prefixes are a natural fit for FSTs; it should compress ridiculously well.  There is an approach to building a heatmap (spatial grid faceting) that I'm thinking of that would do 2500 seek()'s for a 50x50 grid; I'd like those seek's to be as fast as possible.  I have another approach in mind requiring a slightly different encoding, but it would do 2500 next()'s which should be faster.  Nonetheless; it's a lot &amp;#8211; ideally the terms dict would be entirely memory resident.&lt;/p&gt;</comment>
                    <comment id="13624919" author="billy" created="Sun, 7 Apr 2013 14:52:39 +0100">&lt;p&gt;This project is quite interesting!&lt;/p&gt;

&lt;p&gt;Since we already have an entirely memory resident PF, the target of this project seems to be as below:&lt;br/&gt;
1. implement a simplified version of BlockTreeTerms*;&lt;br/&gt;
2. change the API of current PostingsBastFormat, so that some non-block-based term dic will be possible to plug in it.(ideally, MemoryPF should work with this)&lt;/p&gt;</comment>
                    <comment id="13642971" author="billy" created="Fri, 26 Apr 2013 16:55:10 +0100">&lt;p&gt;This is my inital proposal for this project: &lt;a href="https://google-melange.appspot.com/gsoc/proposal/review/google/gsoc2013/billybob/34001" class="external-link"&gt;https://google-melange.appspot.com/gsoc/proposal/review/google/gsoc2013/billybob/34001&lt;/a&gt;&lt;br/&gt;
I'm looking forward to your feedbacks. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13643620" author="mikemccand" created="Sat, 27 Apr 2013 11:11:01 +0100">&lt;p&gt;Han would like to tackle this for GSoC 2013...&lt;/p&gt;</comment>
                    <comment id="13672167" author="billy" created="Sat, 1 Jun 2013 18:22:54 +0100">&lt;p&gt;the detail ideas/wild thoughts will be put here: &lt;a href="https://gist.github.com/sleepsort/5642021" class="external-link"&gt;https://gist.github.com/sleepsort/5642021&lt;/a&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12510622">LUCENE-3209</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
            <subtask id="12650461">LUCENE-5029</subtask>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 17 Jun 2011 11:53:40 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10846</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24618</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3060] Revise ThreadAffinityDocumentsWriterThreadPool queue handling</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3060</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spin-off from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3023" title="Land DWPT on trunk"&gt;&lt;del&gt;LUCENE-3023&lt;/del&gt;&lt;/a&gt;... In ThreadAffinityDocumentsWriterThreadPool#getAndLock() we had talked about switching from a per-threadstate queue (safeway model) to a single queue (whole foods)&lt;/p&gt;</description>
                <environment/>
            <key id="12505905">LUCENE-3060</key>
            <summary>Revise ThreadAffinityDocumentsWriterThreadPool queue handling</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Mon, 2 May 2011 09:19:14 +0100</created>
                <updated>Fri, 10 May 2013 00:05:21 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10850</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24627</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3056] Support Query Rewriting Caching</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3056</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Out of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3041" title="Support Query Visting / Walking"&gt;LUCENE-3041&lt;/a&gt;, its become apparent that using a Visitor / Walker isn't right for caching the rewrites of Querys.  Although we still intend to introduce the Query / Walker for advanced query transformations, rewriting still serves a purpose for very specific implementation detail writing.  As such, it can be very expensive.  So I think we should introduce first class support for rewrite caching.  I also feel the key is to make the caching as transparent as possible, to reduce the strain on Query implementors.&lt;/p&gt;

&lt;p&gt;The TermState idea gave me the idea of maybe making a RewriteState / RewriteCache / RewriteInterceptor, which would be consulted for rewritten Querys.  It would then maintain an internal cache that it would check.  If a value wasn't found, it'd then call Query#rewrite, and cache the result.&lt;/p&gt;

&lt;p&gt;By having this external rewrite source, people could 'pre' rewrite Querys if they were particularly expensive but also common.&lt;/p&gt;</description>
                <environment/>
            <key id="12505840">LUCENE-3056</key>
            <summary>Support Query Rewriting Caching</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="cmale">Chris Male</reporter>
                        <labels>
                    </labels>
                <created>Sat, 30 Apr 2011 11:50:56 +0100</created>
                <updated>Fri, 10 May 2013 00:05:21 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13027334" author="cmale" created="Sat, 30 Apr 2011 15:31:58 +0100">&lt;p&gt;Patch implementing what I outlined.  Converted BooleanQuery over to using RewriteState.&lt;/p&gt;</comment>
                    <comment id="13027578" author="simonw" created="Mon, 2 May 2011 09:02:31 +0100">&lt;p&gt;Hey chris,&lt;/p&gt;

&lt;p&gt;here are some comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I like that you only have to change BooleanQuery to enable this!! nice!&lt;/li&gt;
	&lt;li&gt;Can we rename RewriteState into RewriteContext its just more consistent to all the other ctx we pass to query and scorer?&lt;/li&gt;
	&lt;li&gt;Can we rename DefaultRewriteState into CachingRewriteContext and make a RewriteContext that simply does query.rewrite() that way nothing changes by default and we can use a static instance in Query#rewrite(IndexReader) maybe as an anonymous inner class in Query?&lt;/li&gt;
	&lt;li&gt;Can we move CachingRewriteContext into lucene/src/java/org/apache/lucene/util?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This change somewhat depends on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3041" title="Support Query Visting / Walking"&gt;LUCENE-3041&lt;/a&gt; since we might wanna pass that RewriteContext on a per segment level right? So maybe we should link those issues.&lt;/p&gt;</comment>
                    <comment id="13027592" author="cmale" created="Mon, 2 May 2011 09:50:42 +0100">&lt;blockquote&gt;&lt;p&gt;This change somewhat depends on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3041" title="Support Query Visting / Walking"&gt;LUCENE-3041&lt;/a&gt; since we might wanna pass that RewriteContext on a per segment level right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, thats very true.  I'm wondering whether its best to rethink the signatures of the #search methods in IS since we need to incorporate both this and &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3041" title="Support Query Visting / Walking"&gt;LUCENE-3041&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I'll upload a patch shortly addressing the other improvements.&lt;/p&gt;</comment>
                    <comment id="13027601" author="cmale" created="Mon, 2 May 2011 10:07:46 +0100">&lt;p&gt;Patch implementing Simon's suggestions&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;RewriteState -&amp;gt; RewriteContext&lt;/li&gt;
	&lt;li&gt;DefaultRewriteState -&amp;gt; org.apache.lucene.util.CachingRewriteContext&lt;/li&gt;
	&lt;li&gt;Query now has a static anonymous inner class instance which does simple rewrite.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12477936" name="LUCENE-3056.patch" size="10982" author="cmale" created="Mon, 2 May 2011 10:07:46 +0100"/>
                    <attachment id="12477880" name="LUCENE-3056.patch" size="10305" author="cmale" created="Sat, 30 Apr 2011 15:31:58 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 2 May 2011 08:02:31 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2944</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24631</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3041] Support Query Visting / Walking</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3041</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Out of the discussion in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2868" title="It should be easy to make use of TermState; rewritten queries should be shared automatically"&gt;&lt;del&gt;LUCENE-2868&lt;/del&gt;&lt;/a&gt;, it could be useful to add a generic Query Visitor / Walker that could be used for more advanced rewriting, optimizations or anything that requires state to be stored as each Query is visited.&lt;/p&gt;

&lt;p&gt;We could keep the interface very simple:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-keyword"&gt;interface&lt;/span&gt; QueryVisitor {
  Query visit(Query query);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then use a reflection based visitor like Earwin suggested, which would allow implementators to provide visit methods for just Querys that they are interested in.&lt;/p&gt;</description>
                <environment/>
            <key id="12504983">LUCENE-3041</key>
            <summary>Support Query Visting / Walking</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="cmale">Chris Male</reporter>
                        <labels>
                    </labels>
                <created>Fri, 22 Apr 2011 15:11:48 +0100</created>
                <updated>Fri, 10 May 2013 00:05:21 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>2</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="13023526" author="simonw" created="Sat, 23 Apr 2011 09:16:03 +0100">&lt;p&gt;I like the simple interface but the name is somewhat misleading here I think. Either we make this a 'real' visitor pattern and add accept methods to Query which I don't think is necessary or we should make the name specific for the task. Since this is really for walking the Query 'AST' during the rewrite process we should make this very clean in the IF name. QueryRewriter or something like that would make more sense and it would justify the Query return value, no?&lt;/p&gt;</comment>
                    <comment id="13023536" author="cmale" created="Sat, 23 Apr 2011 10:11:31 +0100">&lt;p&gt;I remain weary of calling it QueryRewriter since there is already Query rewriting support through Query#rewrite, but I take your point.  What about QueryOptimizer?&lt;/p&gt;</comment>
                    <comment id="13023558" author="simonw" created="Sat, 23 Apr 2011 14:58:08 +0100">&lt;blockquote&gt;&lt;p&gt;What about QueryOptimizer?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;QueryProcessor or QueryPreProcessor?&lt;/p&gt;</comment>
                    <comment id="13023560" author="cmale" created="Sat, 23 Apr 2011 15:00:53 +0100">&lt;p&gt;I'm happy to settle with QueryProcessor#process&lt;/p&gt;</comment>
                    <comment id="13023561" author="simonw" created="Sat, 23 Apr 2011 15:20:10 +0100">&lt;blockquote&gt;&lt;p&gt;I'm happy to settle with QueryProcessor#process&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 - Chris are you cranking out a patch for this?&lt;/p&gt;

&lt;p&gt;I think if we have a QueryProcessor we should somehow make it possible to optionally hook it into IndexSearcher to essentially replace the direct call to Query#rewrite&lt;br/&gt;
Eventually it should be the QueryProcessor's responsibility to rewrite the query and pass the actual 'primitive' query to the searcher once done. I think its good to keep that interface super lean and let more fancy impl. follow up on it. Stuff like automatic dispatch for certain query types might need some cglib magic or at least req. java 6 to perform so they might need to go to contrib/misc.&lt;/p&gt;</comment>
                    <comment id="13023562" author="cmale" created="Sat, 23 Apr 2011 15:25:29 +0100">&lt;p&gt;Yup I have a patch cooking.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Stuff like automatic dispatch for certain query types might need some cglib magic or at least req. java 6 to perform so they might need to go to contrib/misc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think this will be the case.  I am striving to use Java 5 reflection classes and that seems to be working fine.&lt;/p&gt;</comment>
                    <comment id="13025146" author="cmale" created="Tue, 26 Apr 2011 10:31:52 +0100">&lt;p&gt;Working patch, minus tests at the moment.&lt;/p&gt;

&lt;p&gt;Introduces the QueryProcessor interface like that described above.&lt;/p&gt;

&lt;p&gt;Also introduces a generic InvocationDispatcher that can be used for visitor like multi-dispatch.&lt;/p&gt;</comment>
                    <comment id="13025155" author="simonw" created="Tue, 26 Apr 2011 11:02:45 +0100">&lt;p&gt;Hey chris,&lt;/p&gt;

&lt;p&gt;I have some comments on your patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;s/visitorClass/processorClass/&lt;/li&gt;
	&lt;li&gt;s/visitor/processor/&lt;/li&gt;
	&lt;li&gt;in InvocationDispatcher you might wanna check if the single parameter is Query subclass&lt;/li&gt;
	&lt;li&gt;I am worried about checking which method to dispatch for every query type once per segment as well as that we create an processor instance per segment and not per search. There are actually two problems here. 1. We create a new instance per segment. 2. We can not share the dispatch map, not even across segments. I think we should create one instance per search and pass the IR down together with the query or even better follow the pattern that Collector et al. uses and pass it with a setReader(IR) method. that way we also have a clear way how to tell the processor that we just moved on to a new segment. Regarding sharing the map, I think you should use a prototype pattern that creates a new Processor from an existing one maybe via clone()? In the InvocationDispatcher case we should maybe use a concurrent hash map and share the map across instances for the same dispatcher class.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The process implementation in DefaultQueryProcessor executes query.rewrite before passing the query to the&lt;br/&gt;
dispatcher which is no good since some QueryProcessor impls might not want to rewrite that query at all. In &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2868" title="It should be easy to make use of TermState; rewritten queries should be shared automatically"&gt;&lt;del&gt;LUCENE-2868&lt;/del&gt;&lt;/a&gt; karl tries to find a way to prevent lucene to rewrite one and the same FuzzyQuery since he has them in multiple clauses somewhere down the BQ tree. This is a super expensive operation in his case to rewriting it only once makes sense. I think this should be left to the actual implementation. &lt;/li&gt;
&lt;/ul&gt;

</comment>
                    <comment id="13025173" author="cmale" created="Tue, 26 Apr 2011 12:02:09 +0100">&lt;blockquote&gt;&lt;p&gt;Regarding sharing the map, I think you should use a prototype pattern that creates a new Processor from an existing one maybe via clone()? In the InvocationDispatcher case we should maybe use a concurrent hash map and share the map across instances for the same dispatcher class.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't quite follow you.  Currently DispatchingQueryProcessor caches InvocationDispatchers by concrete impl type.  So we only create a new InvocationDispatcher when we have a new implementation (which means InvocationDispatchers are shared between segments, searches, everything).  In that regard DispatchingQueryProcessor#dispatcherByClass should be a ConcurrentHashMap.  But otherwise, I think we're okay?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The process implementation in DefaultQueryProcessor executes query.rewrite before passing the query to the dispatcher which is no good since some QueryProcessor impls might not want to rewrite that query at all. In &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2868" title="It should be easy to make use of TermState; rewritten queries should be shared automatically"&gt;&lt;del&gt;LUCENE-2868&lt;/del&gt;&lt;/a&gt; karl tries to find a way to prevent lucene to rewrite one and the same FuzzyQuery since he has them in multiple clauses somewhere down the BQ tree. This is a super expensive operation in his case to rewriting it only once makes sense. I think this should be left to the actual implementation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is super tricky.  The question is how to define a base case in #process(Query).  Lets assume DefaultQueryProcessor#process(Query) just dispatched immediately.  It might be a receiver of the same dispatch (perhaps the query is a TermQuery and no #process(TermQuery) is provided, so #process(Query) is chosen).  It then just dispatches again, receives again.. and we're in a loop.&lt;/p&gt;

&lt;p&gt;Any thoughts on how to avoid that?&lt;/p&gt;</comment>
                    <comment id="13025283" author="simonw" created="Tue, 26 Apr 2011 16:38:22 +0100">&lt;blockquote&gt;
&lt;p&gt;I don't quite follow you. Currently DispatchingQueryProcessor caches InvocationDispatchers by concrete impl type. So we only create a new InvocationDispatcher when we have a new implementation (which means InvocationDispatchers are shared between segments, searches, everything). In that regard DispatchingQueryProcessor#dispatcherByClass should be a ConcurrentHashMap. But otherwise, I think we're okay?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;my bad.. I didn't look close enough &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Yet, I was proposing something like what you did though &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; the problem here could be that it is static but for now ConcurrentHashMap would do.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Any thoughts on how to avoid that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;hmm, I think we should try to dispatch first. If there is not specialized method to dispatch we should rewrite and continue.&lt;/p&gt;

&lt;p&gt;I still wonder how this would walk down the query tree so I am happily waiting for the next patch.&lt;/p&gt;</comment>
                    <comment id="13025647" author="cmale" created="Wed, 27 Apr 2011 08:12:48 +0100">&lt;p&gt;Updated patch.&lt;/p&gt;

&lt;p&gt;This simplifies the hierarchy a lot.  DispatchingQueryProcessor is merged into QueryProcessor, which then becomes an abstract class.  QueryProcessor now has #dispatchProcessing(Query) which is the entry point to the dispatching process.  &lt;/p&gt;

&lt;p&gt;DefaultQueryProcessor is changed to RewriteCachingQueryProcessor which caches the rewriting of querys.  This could be extended further to provide special support for BooleanQuery.&lt;/p&gt;

&lt;p&gt;Remaining to do is to provide a test which illustrates walking through a complex class.&lt;/p&gt;</comment>
                    <comment id="13025669" author="simonw" created="Wed, 27 Apr 2011 09:37:11 +0100">&lt;p&gt;Chris, nice simplification. I have one question, lets say we have a boolean query OR(AND(Fuzzy:A, Fuzzy:B), AND(Fuzzy A, Fuzzy:C)) how would it be possible with the current patch to reuse the rewrite for Fuzzy:A? As far as I can see If I don't rewrite the boolean query myself the current patch will rewrite the top level query and returns right? So somehow it must be possible to walk down the query ast.&lt;/p&gt;

&lt;p&gt;or do I miss something?&lt;/p&gt;</comment>
                    <comment id="13025677" author="cmale" created="Wed, 27 Apr 2011 10:03:09 +0100">&lt;p&gt;No, you didn't miss something.  The RewriteCachingQueryProcessor currently only rewrites the top level query.  It needs to be extended to handle BooleanQuerys and any other composite query (BoostingQuery for example).  I might actually add a DefaultQueryProcessor again which walks the full Query AST by default.  Then get RewritingCachingQueryProcessor to extend and cache.&lt;/p&gt;

&lt;p&gt;I'll iterate a new patch.&lt;/p&gt;</comment>
                    <comment id="13026097" author="cmale" created="Thu, 28 Apr 2011 04:38:51 +0100">&lt;p&gt;A much larger patch that implements full query AST walking.&lt;/p&gt;

&lt;p&gt;The problem with having the QueryProcessor fully external to Query#rewrite, is that composite Querys would need to expose their children.  This is a little messy and could be hard with more exotic user-made Querys.  &lt;/p&gt;

&lt;p&gt;So this patch basically expands Query#rewrite to include the QueryProcessor.  Composite queries can then pass their children to the processor during their rewrite.&lt;/p&gt;

&lt;p&gt;For backwards compat, and simplicity, I've created a SimpleQueryProcessor which directly calls rewrite.  This means casual users do not need to concern themselves with processing.&lt;/p&gt;

&lt;p&gt;Overtime we can expose the QueryProcessor API through IndexSearcher and other situations.&lt;/p&gt;</comment>
                    <comment id="13026119" author="cmale" created="Thu, 28 Apr 2011 06:02:53 +0100">&lt;p&gt;Updated patch which removes the stupid test I'd included&lt;/p&gt;</comment>
                    <comment id="13026129" author="lancenorskog" created="Thu, 28 Apr 2011 06:44:17 +0100">&lt;p&gt;This is an excellent opportunity to redefine Queries as immutable, which would make query rewriting an order of magnitude safer.&lt;/p&gt;</comment>
                    <comment id="13026175" author="cmale" created="Thu, 28 Apr 2011 08:37:33 +0100">&lt;blockquote&gt;&lt;p&gt;This is an excellent opportunity to redefine Queries as immutable&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What do you envisage this involving? Although not required by the API, most rewriting implementations make a new Query and add changes there, leaving themselves untouched.  Are you wanting to require this somehow?&lt;/p&gt;</comment>
                    <comment id="13027054" author="dsmiley" created="Fri, 29 Apr 2011 17:06:56 +0100">&lt;p&gt;Yes! I enthusiastically support introducing the visitor design pattern into the Query api.  I've polled the community on this before and got positive responses from a few committers but I haven't yet had the time to do anything.  It's great to see you've gotten the ball rolling Chris.  &lt;/p&gt;

&lt;p&gt;I haven't looked at your patch yet.  Query.rewrite() is definitely a candidate for reworking in terms of this new pattern.&lt;/p&gt;</comment>
                    <comment id="13027080" author="earwin" created="Fri, 29 Apr 2011 18:07:29 +0100">&lt;p&gt;I vehemently oppose introducing the "visitor design pattern" (classic double-dispatch version) into the Query API. It is a badly broken replacement (ie, cannot be easily extended) for multiple dispatch.&lt;/p&gt;

&lt;p&gt;Also, from the looks of it (short IRC discussion), user-written visitors and rewrite() API have totally different aims.&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;rewrite() is very specific (it is a pre-search preparation that produces runnable query, eg expands multi-term queries into OR sequences or wrapped filters), but should work over any kinds of user-written Queries with possibly exotic behaviours (eg, take rewrite from the cache). Consequently, the logic is tightly coupled to each Query-impl innards.&lt;/li&gt;
	&lt;li&gt;user-written visitors on the other hand, may have a multitude of purporses (wildly varying logic for node handling + navigation - eg, some may want to see MTQs expanded, and some may not) over relatively fixed number of possible node types.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So the best possible solution so far is to keep rewrite() asis - it serves its purporse quite well.&lt;br/&gt;
And introduce generic reflection-based multiple-dispatch visitor that can walk any kind of hierarchies (eg, in my project I rewrite ASTs to ASTs, ASTs to Queries, and Queries to bags of Terms) so people can transform their query trees.&lt;br/&gt;
The current patch contains a derivative of &lt;a href="https://gist.github.com/dfebaf79f5524e6ea8b4" class="external-link"&gt;my original version&lt;/a&gt;. And here's a &lt;a href="https://gist.github.com/e5eb67d762be0bce8d28" class="external-link"&gt;test/example&lt;/a&gt;&lt;br/&gt;
This visitor keeps all logic on itself and thus cannot replace rewrite().&lt;/p&gt;</comment>
                    <comment id="13027279" author="cmale" created="Sat, 30 Apr 2011 04:39:48 +0100">&lt;p&gt;To follow up on Earwin's comments, I'm going to do the following:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Leave Query#rewrite out of the walking process.  As Earwin said, rewrite provides vital query optimization / conversion to primitive runnable queries.  Having this method on Query is a good idea since user Queries can simply implement this method and move on.&lt;/li&gt;
	&lt;li&gt;In a separate issue, add a RewriteState like concept which can be used for caching rewrites like that suggested by Simon.  This will have a considerable performance improvement for people doing lots of repeated FuzzyQuerys for example.&lt;/li&gt;
	&lt;li&gt;Change my processing concept into a generic Walker&amp;lt;I, O&amp;gt; system, which can be used for lots of things in Lucene.  Users can implement this Walker to do whatever they want (maybe we can pry Earwin's walker based highlighter from him? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/biggrin.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;)&lt;/li&gt;
	&lt;li&gt;Overload IndexSearcher's methods to support passing in a Walker.  We need this, instead of simply having the Walker external, because we really want to support per-segment Walking.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I'll make a patch for the stuff related to this issue shortly, and spin off the RewriteState stuff.&lt;/p&gt;</comment>
                    <comment id="13027299" author="cmale" created="Sat, 30 Apr 2011 09:10:49 +0100">&lt;p&gt;New patch that implements what I said in the previous comments (except for the IS changes).&lt;/p&gt;

&lt;p&gt;Also a test is now included.&lt;/p&gt;</comment>
                    <comment id="13027575" author="simonw" created="Mon, 2 May 2011 08:46:11 +0100">&lt;blockquote&gt;&lt;p&gt;New patch that implements what I said in the previous comments (except for the IS changes).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Chris, patch looks good! Are you going to add the IS changes here too? I wonder if we could move the MethodDispatchException into InvocationDispatcher as a nested class I don't think we need an extra file for this class.&lt;/p&gt;</comment>
                    <comment id="13027589" author="cmale" created="Mon, 2 May 2011 09:47:45 +0100">&lt;blockquote&gt;&lt;p&gt;Are you going to add the IS changes here too?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yup, I'm just working through the best way to expose the API in the IS while supporting per segment walking.  I'll have something together in the next day or two.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I wonder if we could move the MethodDispatchException into InvocationDispatcher as a nested class &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good call.  I'll make the change and upload something immediately.&lt;/p&gt;</comment>
                    <comment id="13027612" author="earwin" created="Mon, 2 May 2011 11:26:07 +0100">&lt;p&gt;The static cache is now not threadsafe.&lt;br/&gt;
And original had nice diagnostics for ambigous dispatches. Why not just take it and cut over to JDK reflection and CHM?&lt;br/&gt;
Same can be said for tests.&lt;/p&gt;

&lt;p&gt;What about throwing original invocation exception instead of the wrapper? Since we're emulating a language feature, a simple method call, it's logical to only throw custom exceptions in .. well .. exceptional cases, like ambiguity/no matching method. If client code throws Errors/RuntimeExceptions, they should be transparently rethrown.&lt;/p&gt;</comment>
                    <comment id="13064708" author="simonw" created="Wed, 13 Jul 2011 18:20:58 +0100">&lt;p&gt;chris are you going to revisite this? I think we can make good use of this though.&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13064998" author="cmale" created="Thu, 14 Jul 2011 02:24:21 +0100">&lt;p&gt;Simon,&lt;/p&gt;

&lt;p&gt;Yes I definitely am going to revisit this.  I'm still unsure how best to incorporate this into IndexSearcher so that it can work per-segment.  But we can commit the functionality and then deal with the integration separately.&lt;/p&gt;

&lt;p&gt;What plans do you have?&lt;/p&gt;</comment>
                    <comment id="13065320" author="simonw" created="Thu, 14 Jul 2011 16:20:03 +0100">&lt;blockquote&gt;&lt;p&gt;Yes I definitely am going to revisit this. I'm still unsure how best to incorporate this into IndexSearcher so that it can work per-segment. But we can commit the functionality and then deal with the integration separately.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;actually this already works per segment in a way it should. If you look at rewriting this is done based on the IS anyway for a query so we only rewrite once. If you are inside a MTQ and you want to do the rewrite caching or other things you can do this based on the IS. The question is if we should really commit this without any specific usecase or other code using it :/&lt;/p&gt;

&lt;p&gt;are you going to address earwins issue still?&lt;/p&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12477868" name="LUCENE-3041.patch" size="15199" author="cmale" created="Sat, 30 Apr 2011 09:10:49 +0100"/>
                    <attachment id="12477613" name="LUCENE-3041.patch" size="48922" author="cmale" created="Thu, 28 Apr 2011 06:02:53 +0100"/>
                    <attachment id="12477606" name="LUCENE-3041.patch" size="52510" author="cmale" created="Thu, 28 Apr 2011 04:38:51 +0100"/>
                    <attachment id="12477477" name="LUCENE-3041.patch" size="14779" author="cmale" created="Wed, 27 Apr 2011 08:12:48 +0100"/>
                    <attachment id="12477376" name="LUCENE-3041.patch" size="14227" author="cmale" created="Tue, 26 Apr 2011 10:31:52 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 23 Apr 2011 08:16:03 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10862</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24646</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3034] If you vary a setting per round and that setting is a long string, the report padding/columns break down.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3034</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is especially noticeable if you vary a setting where the value is a fully specified class name - in this case, it would be nice if columns in each row still lined up.&lt;/p&gt;</description>
                <environment/>
            <key id="12504490">LUCENE-3034</key>
            <summary>If you vary a setting per round and that setting is a long string, the report padding/columns break down.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="5" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.png">Trivial</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="markrmiller@gmail.com">Mark Miller</assignee>
                                <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Sun, 17 Apr 2011 17:08:54 +0100</created>
                <updated>Fri, 10 May 2013 00:05:21 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/benchmark</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13032453" author="doronc" created="Thu, 12 May 2011 16:17:41 +0100">&lt;p&gt;Hi Mark, could you add an example algorithm with this behavior?&lt;/p&gt;

&lt;p&gt;Also, this is from the package javadocs:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
# multi val params are iterated by NewRound's, added to reports, start with column name.
merge.factor=mrg:10:20
max.buffered=buf:100:1000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is it possible to workaround the problem by specifying a sufficiently long column name as the first value, that is, replacing e.g. 'mrg' or 'buf' in the above?&lt;/p&gt;</comment>
                    <comment id="13032459" author="markrmiller@gmail.com" created="Thu, 12 May 2011 16:28:19 +0100">&lt;p&gt;Hey Doron - I have a patch for this, I've just been too lazy to extract it. I'm not sure if there is anything built-in that is long enough to matter - it comes into play if, for example, if you want to alternate fully qualified class names per round.&lt;/p&gt;

&lt;p&gt;My original workaround was to simply pad the column name - but it was ugly and had it's limitations, so I instead made some modifications to the formatting classes.&lt;/p&gt;</comment>
                    <comment id="13032499" author="doronc" created="Thu, 12 May 2011 17:42:30 +0100">&lt;blockquote&gt;&lt;p&gt;My original workaround was to simply pad the column name&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah that's what I meant, so ok, better formatting will help.&lt;/p&gt;</comment>
                    <comment id="13237929" author="rcmuir" created="Sun, 25 Mar 2012 18:01:06 +0100">&lt;p&gt;Mark, are you planning on working on this one? Is it ok to defer until 4.0?&lt;/p&gt;</comment>
                    <comment id="13238739" author="markrmiller@gmail.com" created="Mon, 26 Mar 2012 20:53:55 +0100">&lt;p&gt;yeah, this is def not that important. Lets move to 4.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 12 May 2011 15:17:41 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10867</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24652</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3022] DictionaryCompoundWordTokenFilter Flag onlyLongestMatch has no affect</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3022</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When using the DictionaryCompoundWordTokenFilter with a german dictionary, I got a strange behaviour:&lt;br/&gt;
The german word "streifenbluse" (blouse with stripes) was decompounded to "streifen" (stripe),"reifen"(tire) which makes no sense at all.&lt;br/&gt;
I thought the flag onlyLongestMatch would fix this, because "streifen" is longer than "reifen", but it had no effect.&lt;br/&gt;
So I reviewed the sourcecode and found the problem:&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;code&amp;#93;&lt;/span&gt;&lt;br/&gt;
protected void decomposeInternal(final Token token) {&lt;br/&gt;
    // Only words longer than minWordSize get processed&lt;br/&gt;
    if (token.length() &amp;lt; this.minWordSize) &lt;/p&gt;
{
      return;
    }&lt;br/&gt;
    &lt;br/&gt;
    char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.buffer());&lt;br/&gt;
    &lt;br/&gt;
    for (int i=0;i&amp;lt;token.length()-this.minSubwordSize;++i) {&lt;br/&gt;
        Token longestMatchToken=null;&lt;br/&gt;
        for (int j=this.minSubwordSize-1;j&amp;lt;this.maxSubwordSize;++j) {&lt;br/&gt;
            if(i+j&amp;gt;token.length()) {
                break;
            }&lt;br/&gt;
            if(dictionary.contains(lowerCaseTermBuffer, i, j)) {&lt;br/&gt;
                if (this.onlyLongestMatch) {&lt;br/&gt;
                   if (longestMatchToken!=null) {&lt;br/&gt;
                     if (longestMatchToken.length()&amp;lt;j) {
                       longestMatchToken=createToken(i,j,token);
                     }&lt;br/&gt;
                   } else {
                     longestMatchToken=createToken(i,j,token);
                   }&lt;br/&gt;
                } else {
                   tokens.add(createToken(i,j,token));
                }&lt;br/&gt;
            } &lt;br/&gt;
        }&lt;br/&gt;
        if (this.onlyLongestMatch &amp;amp;&amp;amp; longestMatchToken!=null) {
          tokens.add(longestMatchToken);
        }&lt;br/&gt;
    }&lt;br/&gt;
  }&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;/code&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;br/&gt;
should be changed to &lt;br/&gt;
&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;code&amp;#93;&lt;/span&gt;&lt;br/&gt;
protected void decomposeInternal(final Token token) {&lt;br/&gt;
    // Only words longer than minWordSize get processed&lt;br/&gt;
    if (token.termLength() &amp;lt; this.minWordSize) {
      return;
    }

&lt;p&gt;    char[] lowerCaseTermBuffer=makeLowerCaseCopy(token.termBuffer());&lt;/p&gt;

&lt;p&gt;    Token longestMatchToken=null;&lt;br/&gt;
    for (int i=0;i&amp;lt;token.termLength()-this.minSubwordSize;++i) {&lt;/p&gt;

&lt;p&gt;        for (int j=this.minSubwordSize-1;j&amp;lt;this.maxSubwordSize;++j) {&lt;br/&gt;
            if(i+j&amp;gt;token.termLength()) &lt;/p&gt;
{
                break;
            }
&lt;p&gt;            if(dictionary.contains(lowerCaseTermBuffer, i, j)) {&lt;br/&gt;
                if (this.onlyLongestMatch) {&lt;br/&gt;
                   if (longestMatchToken!=null) {&lt;br/&gt;
                     if (longestMatchToken.termLength()&amp;lt;j) &lt;/p&gt;
{
                       longestMatchToken=createToken(i,j,token);
                     }
&lt;p&gt;                   } else &lt;/p&gt;
{
                     longestMatchToken=createToken(i,j,token);
                   }
&lt;p&gt;                } else &lt;/p&gt;
{
                   tokens.add(createToken(i,j,token));
                }
&lt;p&gt;            }&lt;br/&gt;
        }&lt;br/&gt;
    }&lt;br/&gt;
    if (this.onlyLongestMatch &amp;amp;&amp;amp; longestMatchToken!=null) &lt;/p&gt;
{
        tokens.add(longestMatchToken);
    }
&lt;p&gt;  }&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;/code&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So, that only the longest token is really indexed and the onlyLongestMatch Flag makes sense.&lt;/p&gt;</description>
                <environment/>
            <key id="12504029">LUCENE-3022</key>
            <summary>DictionaryCompoundWordTokenFilter Flag onlyLongestMatch has no affect</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="bitscorpion">Johann Höchtl</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Tue, 12 Apr 2011 10:15:35 +0100</created>
                <updated>Mon, 13 May 2013 04:09:54 +0100</updated>
                                    <version>2.9.4</version>
                <version>3.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                          <timeoriginalestimate seconds="300">5m</timeoriginalestimate>
                    <timeestimate seconds="300">5m</timeestimate>
                                  <comments>
                    <comment id="13019637" author="rcmuir" created="Thu, 14 Apr 2011 01:39:59 +0100">&lt;p&gt;This sounds like a bug, do you want to try your hand at contributing a patch?&lt;/p&gt;

&lt;p&gt;See &lt;a href="http://wiki.apache.org/lucene-java/HowToContribute" class="external-link"&gt;http://wiki.apache.org/lucene-java/HowToContribute&lt;/a&gt; for some instructions.&lt;/p&gt;</comment>
                    <comment id="13019752" author="bitscorpion" created="Thu, 14 Apr 2011 10:36:20 +0100">&lt;p&gt;Patch fixing this issue&lt;br/&gt;
including JUnitTest&lt;/p&gt;</comment>
                    <comment id="13019885" author="rcmuir" created="Thu, 14 Apr 2011 17:05:58 +0100">&lt;p&gt;Hi Johann, in my opinion your patch is completely correct, thanks for fixing this.&lt;/p&gt;

&lt;p&gt;I noticed though, that a solr test failed because its factory defaults to this value being "on" (and the previous behavior was broken!!!)&lt;/p&gt;

&lt;p&gt;Because of this, I propose we default this behavior to "off" in the Solr factory and add an upgrading note. Previously decompounding in solr defaulted to buggy behavior, but I think by default we should index all compound components (since that seems to be what the desired intended behavior was, which mostly worked, only because of the bug!)&lt;/p&gt;

&lt;p&gt;I'll leave the issue open for a few days to see if anyone objects to this plan.&lt;/p&gt;</comment>
                    <comment id="13022232" author="rcmuir" created="Wed, 20 Apr 2011 18:12:05 +0100">&lt;p&gt;Hi, it seems we need to figure out some things about this compound filter:&lt;/p&gt;

&lt;p&gt;What is the onlyLongestMatch option intended to do? Its description is a bit vague, is it:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Supposed to only extract the longest-matching subword dictionary matches?&lt;/li&gt;
	&lt;li&gt;Supposed to only extract the longest subword from a compound?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;As an example, consider a dictionary containing: so, ft, soft, ball&lt;br/&gt;
How should "softball" be decomposed based on this option?&lt;/p&gt;

&lt;p&gt;If onlyLongestMatch is interpreted to mean only the longest-matching subwords, then i think it should emit "softball, soft, ball". "so" and "ft" would not be emitted, because you set this option.&lt;/p&gt;

&lt;p&gt;However, if its interpreted to mean only the longest matching subword from the compound (like this patch), then it should only emit "softball, soft" and stop.&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;onlyLongestMatch&lt;/th&gt;
&lt;th class='confluenceTh'&gt;trunk&lt;/th&gt;
&lt;th class='confluenceTh'&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3022" title="DictionaryCompoundWordTokenFilter Flag onlyLongestMatch has no affect"&gt;LUCENE-3022&lt;/a&gt;&lt;/th&gt;
&lt;th class='confluenceTh'&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-3038" title="DictionaryCompoundWordTokenFilter fails to create some tokens for final parts of words"&gt;&lt;del&gt;LUCENE-3038&lt;/del&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;false&lt;/td&gt;
&lt;td class='confluenceTd'&gt;softball, so, soft, ft, ball&lt;/td&gt;
&lt;td class='confluenceTd'&gt;softball, so, soft, ft, ball&lt;/td&gt;
&lt;td class='confluenceTd'&gt;softball, so, soft, ft, ball&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;true&lt;/td&gt;
&lt;td class='confluenceTd'&gt;softball, soft, ft, ball&lt;/td&gt;
&lt;td class='confluenceTd'&gt;softball, soft&lt;/td&gt;
&lt;td class='confluenceTd'&gt;softball, soft, ft, ball&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Its definitely clear that the current behavior is wrong, but what should the correct behavior be? Maybe we need two separate options here?&lt;/p&gt;</comment>
                    <comment id="13022351" author="bitscorpion" created="Wed, 20 Apr 2011 21:27:42 +0100">&lt;p&gt;Let's stay with this example:&lt;br/&gt;
Dict: &lt;/p&gt;
{"soft","so","ft"}
&lt;p&gt;Word: "softball"&lt;/p&gt;

&lt;p&gt;The first option onlyLongestMatch, which behaves in the way, that only the longest matching dictionary entry should be returned. (Should this option be modified to keep all of the longest matches? length(soft) == length(ball)?)&lt;br/&gt;
Output: "soft" if true; "so","ft","soft" if false&lt;/p&gt;

&lt;p&gt;The second option should be keepRemain, which makes a term out of the remain after substracting the longestMatch (makes only sense with onlyLongestMatch!?)&lt;br/&gt;
Output: "soft","ball" if keepRemain==onlyLongestMatch==true&lt;/p&gt;

&lt;p&gt;With this second option you could keep the remains, which are not in your dictionary (reduces the complexity of the required dictionary and can improve the compound-splitting-logic)&lt;/p&gt;</comment>
                    <comment id="13043542" author="rcmuir" created="Fri, 3 Jun 2011 17:40:40 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12476350" name="LUCENE-3022.patch" size="6236" author="rcmuir" created="Thu, 14 Apr 2011 17:05:58 +0100"/>
                    <attachment id="12476311" name="LUCENE-3022.patch" size="6236" author="bitscorpion" created="Thu, 14 Apr 2011 10:36:20 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 14 Apr 2011 00:39:59 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10876</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24664</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3005] Define Test Plan for 4.0</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3005</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Before we can release, we need a test plan that defines what a successful release candidate must do to be accepted.&lt;/p&gt;

&lt;p&gt;Test plan should be written at &lt;a href="http://wiki.apache.org/lucene-java/TestPlans" class="external-link"&gt;http://wiki.apache.org/lucene-java/TestPlans&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;See &lt;a href="http://www.lucidimagination.com/search/document/14bd01e519f39aff/brainstorming_on_improving_the_release_process" class="external-link"&gt;http://www.lucidimagination.com/search/document/14bd01e519f39aff/brainstorming_on_improving_the_release_process&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12502894">LUCENE-3005</key>
            <summary>Define Test Plan for 4.0</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Wed, 30 Mar 2011 17:10:18 +0100</created>
                <updated>Mon, 13 May 2013 04:09:28 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13455929" author="steve_rowe" created="Fri, 14 Sep 2012 17:50:52 +0100">&lt;p&gt;I think &lt;tt&gt;smokeTestRelease.py&lt;/tt&gt; does exactly this.&lt;/p&gt;

&lt;p&gt;What it doesn't/can't do is sanity check things to make sure no new problems have cropped up.  But even in this case, new checks can be added to the script once they have been identified.&lt;/p&gt;

&lt;p&gt;So IMHO this should be closed as won't fix.  I'll do so tomorrow if there are no objections.&lt;/p&gt;
</comment>
                    <comment id="13455951" author="jkrupan" created="Fri, 14 Sep 2012 18:13:46 +0100">&lt;p&gt;That Lucid link doesn't work for me. I think it is equivalent to:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.gossamer-threads.com/lists/lucene/java-dev/120890" class="external-link"&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/120890&lt;/a&gt;&lt;br/&gt;
or&lt;br/&gt;
&lt;a href="http://mail-archives.apache.org/mod_mbox/lucene-dev/201103.mbox/%3C5D982587-7316-4CC5-B1A0-2F91E50073B5@apache.org%3E" class="external-link"&gt;http://mail-archives.apache.org/mod_mbox/lucene-dev/201103.mbox/%3C5D982587-7316-4CC5-B1A0-2F91E50073B5@apache.org%3E&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13455960" author="rcmuir" created="Fri, 14 Sep 2012 18:22:45 +0100">&lt;p&gt;Right, i think a lot of this is old news, its not perfect but we are closer.&lt;/p&gt;

&lt;p&gt;As far as what Grant mentioned in the original thread: I think we've tried to automate this as much&lt;br/&gt;
as possible in the smokeTestRelease.py: it takes the actual generated artifacts and does a ton&lt;br/&gt;
of tests on them.&lt;/p&gt;

&lt;p&gt;As i mentioned in the original thread, the worst part of the release process before was that the code&lt;br/&gt;
wasn't releasable until someone cleaned it up: I think this is significantly better. We have a lot&lt;br/&gt;
more checks in the build system: nitpicking over javadocs, 3rd party dependencies, license headers,&lt;br/&gt;
etc, etc. And we run the smokeTestRelease.py as a weekly hudson job to ensure things like packaging&lt;br/&gt;
are not broken.&lt;/p&gt;

&lt;p&gt;In my opinion we should continue down this path, trying to ensure that our code is always releasable&lt;br/&gt;
at any time from this kind of perspective, rather than adding manual steps.&lt;/p&gt;

&lt;p&gt;I think the right way to do this is just like how we handle bugs in our code. If a problem crops up&lt;br/&gt;
in e.g. packaging, don't just fix it, but also add a 'test' (code to smokeTestRelease or whatever) so&lt;br/&gt;
that jenkins is continuously looking for it.&lt;/p&gt;

&lt;p&gt;In my opinion most of the previous problems were all due to:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;not having tests (in this sense 'test' means testing packaging, licensing, etc)&lt;/li&gt;
	&lt;li&gt;not having automated builds running those tests.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 14 Sep 2012 16:50:52 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2882</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24681</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-3004] Define Test Plan for 3.2</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3004</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Before we can release, we need a test plan that defines what a successful release candidate must do to be accepted.&lt;/p&gt;

&lt;p&gt;Test plan should be written at &lt;a href="http://wiki.apache.org/lucene-java/TestPlans" class="external-link"&gt;http://wiki.apache.org/lucene-java/TestPlans&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;See &lt;a href="http://www.lucidimagination.com/search/document/14bd01e519f39aff/brainstorming_on_improving_the_release_process" class="external-link"&gt;http://www.lucidimagination.com/search/document/14bd01e519f39aff/brainstorming_on_improving_the_release_process&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12502893">LUCENE-3004</key>
            <summary>Define Test Plan for 3.2</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                    </labels>
                <created>Wed, 30 Mar 2011 17:09:22 +0100</created>
                <updated>Fri, 10 May 2013 00:05:22 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13041279" author="rcmuir" created="Mon, 30 May 2011 21:32:15 +0100">&lt;p&gt;Personally I don't like the idea of adding more manual checks to our release process, no matter how formal and well defined they are.&lt;/p&gt;

&lt;p&gt;I am, however, +1 to adding any additional checks and tests to our build system that are completely automated. I don't think we need more of these to block a release, instead this should be an area of continual improvement.&lt;/p&gt;

&lt;p&gt;For example, we added "fail on javadocs warnings" to the build instead of "verify there are no javadocs warnings manually".&lt;/p&gt;</comment>
                    <comment id="13041285" author="markrmiller@gmail.com" created="Mon, 30 May 2011 21:41:28 +0100">&lt;p&gt;Def does not seem like a blocker to me is my first thought. Far, far from it.&lt;/p&gt;

&lt;p&gt;Also, I don't like the drive to formality this seems to represent - I understand what defines an acceptable release candidate - 3 PMC +1's. Shebam!&lt;/p&gt;

&lt;p&gt;We should just make more targeted improvement JIRA's as needed/motivated. Growth does not need to mean more formality by default. Please gods...&lt;/p&gt;</comment>
                    <comment id="13043553" author="rcmuir" created="Fri, 3 Jun 2011 17:40:45 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13123110" author="rcmuir" created="Fri, 7 Oct 2011 20:36:44 +0100">&lt;p&gt;Personally I think we could actually mark this issue fixed, a ton of improvements have happened to the release process.&lt;br/&gt;
I'm gonna unset blocker though.&lt;/p&gt;</comment>
                    <comment id="13234775" author="hossman" created="Wed, 21 Mar 2012 18:14:24 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 30 May 2011 20:32:15 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2885</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24682</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2997] PayloadQueryParser addition</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2997</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I recently needed to deploy payloads for my search system and ran into a small wall: there was no query parser available for use with Payloads.  I through this one together, extending out of the new modular QueryParser structure.&lt;/p&gt;

&lt;p&gt;Attached is the class file.  I didn't know what package this would belong in (whether with the query parser or with the rest of the payload functionality in contrib/analyzers), so it's in the default package for now.&lt;/p&gt;

&lt;p&gt;I know this is a little, simple thing, but it seemed like something that should probably be included.&lt;/p&gt;</description>
                <environment>&lt;p&gt;n/a&lt;/p&gt;</environment>
            <key id="12502639">LUCENE-2997</key>
            <summary>PayloadQueryParser addition</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="edwardd">Edward Drapkin</reporter>
                        <labels>
                        <label>dead</label>
                        <label>features</label>
                        <label>patch</label>
                    </labels>
                <created>Tue, 29 Mar 2011 00:54:01 +0100</created>
                <updated>Mon, 13 May 2013 04:09:20 +0100</updated>
                                    <version>3.0.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                <component>modules/other</component>
                        <due/>
                    <votes>1</votes>
                        <watches>1</watches>
                          <timeoriginalestimate seconds="0">0h</timeoriginalestimate>
                    <timeestimate seconds="0">0h</timeestimate>
                                  <comments>
                    <comment id="13012280" author="edwardd" created="Tue, 29 Mar 2011 00:54:50 +0100">&lt;p&gt;PayloadQueryParser implementation.&lt;/p&gt;</comment>
                    <comment id="13234774" author="hossman" created="Wed, 21 Mar 2012 18:14:24 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12474833" name="PayloadQueryParser.java" size="4211" author="edwardd" created="Tue, 29 Mar 2011 00:54:50 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 21 Mar 2012 18:14:24 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10889</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24689</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2971] Auto Generate our LICENSE.txt and NOTICE.txt files</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2971</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Once &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2952" title="Make license checking/maintenance easier/automated"&gt;&lt;del&gt;LUCENE-2952&lt;/del&gt;&lt;/a&gt; is in place, we should be able to automatically generate Lucene and Solr's LICENSE.txt and NOTICE.txt file (without massive duplication)&lt;/p&gt;</description>
                <environment/>
            <key id="12501694">LUCENE-2971</key>
            <summary>Auto Generate our LICENSE.txt and NOTICE.txt files</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Thu, 17 Mar 2011 15:20:42 +0000</created>
                <updated>Mon, 13 May 2013 04:09:09 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="13007964" author="yseeley@gmail.com" created="Thu, 17 Mar 2011 16:01:47 +0000">&lt;p&gt;We need to be careful about an auto generated NOTICE... it's supposed to be minimal and only contain legally required attributions.  &lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.apache.org/legal/src-headers.html#notice" class="external-link"&gt;http://www.apache.org/legal/src-headers.html#notice&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://markmail.org/message/fqnvkvkhk6q5f4uw" class="external-link"&gt;http://markmail.org/message/fqnvkvkhk6q5f4uw&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://markmail.org/message/cxwtnuys65c7hs2y" class="external-link"&gt;http://markmail.org/message/cxwtnuys65c7hs2y&lt;/a&gt;&lt;/p&gt;
</comment>
                    <comment id="13007987" author="gsingers" created="Thu, 17 Mar 2011 16:32:24 +0000">&lt;p&gt;Thanks for the pointers, that should definitely be helpful if and when we add this.&lt;/p&gt;</comment>
                    <comment id="13043549" author="rcmuir" created="Fri, 3 Jun 2011 17:40:43 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13234776" author="hossman" created="Wed, 21 Mar 2012 18:14:24 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12500632">LUCENE-2952</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 17 Mar 2011 16:01:47 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10907</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24715</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2949] FastVectorHighlighter FieldTermStack could likely benefit from using TermVectorMapper</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2949</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Based on my reading of the FieldTermStack constructor that loads the vector from disk, we could probably save a bunch of time and memory by using the TermVectorMapper callback mechanism instead of materializing the full array of terms into memory and then throwing most of them out.&lt;/p&gt;</description>
                <environment/>
            <key id="12500376">LUCENE-2949</key>
            <summary>FastVectorHighlighter FieldTermStack could likely benefit from using TermVectorMapper</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                        <label>FastVectorHighlighter</label>
                        <label>Highlighter</label>
                    </labels>
                <created>Thu, 3 Mar 2011 22:51:44 +0000</created>
                <updated>Fri, 10 May 2013 00:05:22 +0100</updated>
                                    <version>3.0.3</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>1</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13025026" author="koji" created="Tue, 26 Apr 2011 03:09:18 +0100">&lt;p&gt;A first draft patch. All FVH tests pass.&lt;/p&gt;

&lt;p&gt;As it is my first experience of TermVectorMapper, feedback would be appreciated. For example, I left setExpectations() empty, but I'm not sure it is correct.&lt;/p&gt;</comment>
                    <comment id="13025478" author="gsingers" created="Tue, 26 Apr 2011 23:18:11 +0100">&lt;p&gt;I haven't looked at the patch yet, but the setExpectations is basically there in case you wish to pre allocate any structures.&lt;/p&gt;</comment>
                    <comment id="13043545" author="rcmuir" created="Fri, 3 Jun 2011 17:40:41 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13055115" author="sokolov" created="Sun, 26 Jun 2011 17:38:57 +0100">&lt;p&gt;This looks like the same issue as &lt;a href="https://issues.apache.org/jira/browse/LUCENENET-350" title="Performance enhancement in FastVectorHighlighter"&gt;&lt;del&gt;LUCENENET-350&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                    <comment id="13148865" author="simonw" created="Fri, 11 Nov 2011 23:58:04 +0000">&lt;p&gt;anybody working on this? koji, if you are not going to work on this can you unassign you from the issue?&lt;/p&gt;</comment>
                    <comment id="13148937" author="koji" created="Sat, 12 Nov 2011 02:07:05 +0000">&lt;p&gt;Thanks Simon for awaking me on this! I'll commit if nobody objects.&lt;/p&gt;</comment>
                    <comment id="13148944" author="simonw" created="Sat, 12 Nov 2011 02:18:25 +0000">&lt;p&gt;Koji, I wonder if we can make use of:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-keyword"&gt;abstract&lt;/span&gt; void setExpectations(&lt;span class="code-object"&gt;String&lt;/span&gt; field, &lt;span class="code-object"&gt;int&lt;/span&gt; numTerms, &lt;span class="code-object"&gt;boolean&lt;/span&gt; storeOffsets, &lt;span class="code-object"&gt;boolean&lt;/span&gt; storePositions);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we could actually allocate an array for TermInfo instead of a LinkedList which creates tons of object. I'd even go further and say we allocate a parallel array ie. one array for start &amp;amp; end offset, position, term (ByteBlockPool maybe even?) this would reduce the num of objects dramatically.&lt;/p&gt;
</comment>
                    <comment id="13149542" author="koji" created="Mon, 14 Nov 2011 10:31:22 +0000">&lt;p&gt;Cool, I like the idea! But I don't have much time to try it now, I'll unassign myself.&lt;/p&gt;</comment>
                    <comment id="13234781" author="hossman" created="Wed, 21 Mar 2012 18:14:25 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12477353" name="LUCENE-2949.patch" size="3176" author="koji" created="Tue, 26 Apr 2011 03:09:18 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 26 Apr 2011 02:09:18 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10926</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24737</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2929] all postings enums must explicitly declare what they need up-front.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2929</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently, the DocsEnum api assumes you &lt;b&gt;might&lt;/b&gt; consumes freqs at any time.&lt;br/&gt;
Additionally the DocsAndPositionsEnum api assumes you &lt;b&gt;might&lt;/b&gt; consume a payload at any time.&lt;/p&gt;

&lt;p&gt;High level things such as queries know what kinds of data they need from the index up-front,&lt;br/&gt;
and the current APIs are limiting to codecs (other than Standard, which has these intertwined).&lt;/p&gt;

&lt;p&gt;So, we either need DocsAndFreqsEnum, DocsPositionsAndPayloadsEnum, or at least booleans&lt;br/&gt;
in the methods that create these to specify whether you want freqs or payloads.&lt;/p&gt;

&lt;p&gt;we did this for freqs in the bulkpostings API, which is good, but these DocsEnum apis&lt;br/&gt;
are also new in 4.0 and there's no reason to introduce non-performant APIs.&lt;/p&gt;

&lt;p&gt;additionally when/if we add payloads to the bulkpostings API, we should make sure we keep&lt;br/&gt;
the same trend and require you to specify you want payloads or not up-front.&lt;/p&gt;</description>
                <environment/>
            <key id="12499121">LUCENE-2929</key>
            <summary>all postings enums must explicitly declare what they need up-front.</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Sat, 19 Feb 2011 14:09:13 +0000</created>
                <updated>Fri, 10 May 2013 00:05:22 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12996779" author="rcmuir" created="Sat, 19 Feb 2011 14:12:57 +0000">&lt;p&gt;And: if you ask for docs and freqs but the field omitTFAP, we should return&lt;br/&gt;
an error in some fashion similar to what the bulk API does.&lt;/p&gt;

&lt;p&gt;I think we do this now if you ask for a DocsAndPositionsEnum when the field omitTFAP,&lt;br/&gt;
but we continue to lie and return TF=1 freqs in this case, which is not true,&lt;br/&gt;
there is simply no freq data at all.&lt;/p&gt;</comment>
                    <comment id="13152973" author="mikemccand" created="Fri, 18 Nov 2011 17:12:43 +0000">&lt;p&gt;Patch, splitting out DocsAndFreqsEnum from DocsEnum, and fixing all&lt;br/&gt;
places to pull the actual enum they need.&lt;/p&gt;

&lt;p&gt;I also added helper methods in _TestUtil to pull Docs/AndFreqsEnum,&lt;br/&gt;
randomly sometimes "upgrading" the enum pulled (eg, if you wanted&lt;br/&gt;
DocsEnum you might get a DocsAndFreqsEnum instead).&lt;/p&gt;

&lt;p&gt;The codec is allowed to return null from TermsEnum.docsAndFreqs, which&lt;br/&gt;
means term freqs were not indexed for that field (ie IndexOptions.DOCS_ONLY).&lt;/p&gt;

&lt;p&gt;Just like the bulk branch, I broke out a MatchOnlyTermScorer, and also&lt;br/&gt;
MatchOnlyConjunctionTermScorer.&lt;/p&gt;

&lt;p&gt;Still have to make "and payloads" strongly typed as well...&lt;/p&gt;</comment>
                    <comment id="13162193" author="mikemccand" created="Sat, 3 Dec 2011 18:20:45 +0000">&lt;p&gt;New patch; I think it's ready.&lt;/p&gt;

&lt;p&gt;Instead of making a new class for every combination of attrs, I think&lt;br/&gt;
we should have "class per dimension", ie DocsEnum is used if you only&lt;br/&gt;
iterate over docs, no matter which attrs you pull.  So now you pass in&lt;br/&gt;
an up front boolean needsFreqs to TermsEnum.docs.&lt;/p&gt;</comment>
                    <comment id="13162199" author="mikemccand" created="Sat, 3 Dec 2011 19:27:33 +0000">&lt;p&gt;Another patch... last one had "svn rm'd" DocsEnum for some reason...&lt;/p&gt;</comment>
                    <comment id="13162394" author="rcmuir" created="Sun, 4 Dec 2011 13:11:05 +0000">&lt;blockquote&gt;
&lt;p&gt;Instead of making a new class for every combination of attrs, I think&lt;br/&gt;
we should have "class per dimension"&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13233479" author="mikemccand" created="Tue, 20 Mar 2012 15:21:05 +0000">&lt;p&gt;Still need "requiresPayloads" boolean.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                <outwardlinks description="incorporates">
                            <issuelink>
            <issuekey id="12599131">LUCENE-4230</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12506005" name="LUCENE-2929.patch" size="167477" author="mikemccand" created="Sat, 3 Dec 2011 19:27:33 +0000"/>
                    <attachment id="12506004" name="LUCENE-2929.patch" size="165698" author="mikemccand" created="Sat, 3 Dec 2011 18:20:45 +0000"/>
                    <attachment id="12504230" name="LUCENE-2929.patch" size="153732" author="mikemccand" created="Fri, 18 Nov 2011 17:12:43 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 18 Nov 2011 17:12:43 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2889</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24757</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2899] Add OpenNLP Analysis capabilities as a module</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2899</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Now that OpenNLP is an ASF project and has a nice license, it would be nice to have a submodule (under analysis) that exposed capabilities for it. Drew Farris, Tom Morton and I have code that does:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Sentence Detection as a Tokenizer (could also be a TokenFilter, although it would have to change slightly to buffer tokens)&lt;/li&gt;
	&lt;li&gt;NamedEntity recognition as a TokenFilter&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We are also planning a Tokenizer/TokenFilter that can put parts of speech as either payloads (PartOfSpeechAttribute?) on a token or at the same position.&lt;/p&gt;

&lt;p&gt;I'd propose it go under:&lt;br/&gt;
modules/analysis/opennlp&lt;/p&gt;</description>
                <environment/>
            <key id="12497161">LUCENE-2899</key>
            <summary>Add OpenNLP Analysis capabilities as a module</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="gsingers">Grant Ingersoll</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                    </labels>
                <created>Sun, 30 Jan 2011 15:44:03 +0000</created>
                <updated>Mon, 20 May 2013 04:56:58 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>13</votes>
                        <watches>27</watches>
                                                    <comments>
                    <comment id="13036124" author="joern" created="Thu, 19 May 2011 12:44:16 +0100">&lt;p&gt;The first release is now out. I guess you will use maven for dependency management, you can find here how to add the released version as a dependency:&lt;br/&gt;
&lt;a href="http://incubator.apache.org/opennlp/maven-dependency.html" class="external-link"&gt;http://incubator.apache.org/opennlp/maven-dependency.html&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13290007" author="lancenorskog" created="Wed, 6 Jun 2012 09:17:43 +0100">&lt;p&gt;This is a patch for the trunk (as of a few days ago) that supplies the OpenNLP Sentence Detector, Tokenizer, Parts-of-Speech, Chunking and Named Entity Recognition tools.&lt;/p&gt;

&lt;p&gt;This has nothing to do with the code mentioned above.&lt;/p&gt;
</comment>
                    <comment id="13290015" author="lancenorskog" created="Wed, 6 Jun 2012 09:31:05 +0100">&lt;p&gt;Notes for a Wiki page:&lt;/p&gt;

&lt;p&gt;OpenNLP Integration&lt;/p&gt;

&lt;p&gt;What is the integration? The first integration is a Tokenizer and three Filters. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The OpenNLPTokenizer uses the OpenNLP SentenceDetector and Tokenizer tools instead of the standard Lucene Tokenizers.  This requires statistical model files. One quirk of these is that all punctuation is maintained.&lt;/li&gt;
	&lt;li&gt;The OpenNLPFilter implements Parts-of-Speech tagging, Chunking (finding noun/verb phrases), and Named Entity Recognition (tagging people, place names etc.). This filter will add all tags as payload attributes to the tokens.&lt;/li&gt;
	&lt;li&gt;The FilterPayloadsFilter removes tokens by checking the payloads. Given a list of payloads, it will either keep only tokens with one of those payloads, or remove only matching tokens and keep the rest. (This filter maintains position increments correctly.)&lt;/li&gt;
	&lt;li&gt;The StripPayloadsFilter removes payloads from Tokens.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;How do I get going?&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;pull the latest trunk&lt;/li&gt;
	&lt;li&gt;apply the patch&lt;/li&gt;
	&lt;li&gt;download these models to contrib/opennlp/src/test-* files/opennlp/solr/conf/opennlp/
	&lt;ul&gt;
		&lt;li&gt;&lt;a href="http://opennlp.sourceforge.net/models-1.5/" class="external-link"&gt;http://opennlp.sourceforge.net/models-1.5/&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;Everything that starts with 'en'&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;download the OpenNLP distribution from &lt;a href="http://opennlp.apache.org/cgi-bin/download.cgi" class="external-link"&gt;http://opennlp.apache.org/cgi-bin/download.cgi&lt;/a&gt;
	&lt;ul&gt;
		&lt;li&gt;Currently it is apache-opennlp-1.5.2-incubating-bin.tar.gz&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;unpack this and copy the jar files from lib/ to&lt;br/&gt;
solr/contrib/opennlp/lib&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now, go to trunk-dir/solr and run 'ant test-contrib'. It compiles against the libraries and uses the model files. &lt;br/&gt;
Next, run 'ant example', cd to the example directory and run 'java -Dsolr.solr.home=opennlp -jar start.jar'&lt;br/&gt;
You now should start without any Exceptions. At this point, go to the Schema analyzer, pick the 'text_opennlp_pos' field type, and post a sentence or two to the analyzer. You should get text tokenized with payloads. Unfortunately, the analysis page shows them as bytes instead of text. If you would like this, then go vote on &lt;a href="https://issues.apache.org/jira/browse/SOLR-3493" title="Show payloads in different formats in Analysis page"&gt;SOLR-3493&lt;/a&gt;.&lt;/p&gt;
</comment>
                    <comment id="13290020" author="lancenorskog" created="Wed, 6 Jun 2012 09:35:46 +0100">&lt;p&gt;About the build-&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;This should be a Lucene module. I got lost trying to make the build work copying jars around, so it will ended up in Solr/contrib.&lt;/li&gt;
	&lt;li&gt;Downloading the jars. I don't know how to put together license validation with the OpenNLP Maven build. I think it takes some upgrading in the OpenNLP project.&lt;/li&gt;
	&lt;li&gt;Why download the models from a separate place? The models are not Apache licensed. They are binaries derived from GNU- and otherwise licensed training data. The OpenNLP people archived them on Sourceforge.&lt;/li&gt;
&lt;/ol&gt;



</comment>
                    <comment id="13290023" author="lancenorskog" created="Wed, 6 Jun 2012 09:44:01 +0100">&lt;p&gt;I consider the code and feature set mostly cooked as a first release. The toolkit as is lets you do two things:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Do named entity recognition and filter out names for an autosuggest dictionary&lt;/li&gt;
	&lt;li&gt;Pick nouns and verbs out of text and only index those. This gives you a field with a smaller more focused set of terms. MoreLikeThis might work better.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Please review it for design, bugs, code nits, whatever.&lt;/p&gt;</comment>
                    <comment id="13290040" author="lancenorskog" created="Wed, 6 Jun 2012 10:08:36 +0100">&lt;p&gt;An explanation about the OpenNLPUtil factory class: the statistical models are several megabytes apiece. This class loads them and caches them by file name. It does not reload them across commits. &lt;/p&gt;

&lt;p&gt;The models are immutable objects. The factory class creates another object that consults the model. There is one of these for each field analysis. &lt;/p&gt;

&lt;p&gt;The models are large enough that if the different unit tests load them all at once, it needs more than the default ram. Therefore, the unit tests unload all models between tests, and only run single-threaded.&lt;/p&gt;
</comment>
                    <comment id="13292182" author="lancenorskog" created="Sat, 9 Jun 2012 04:24:47 +0100">&lt;p&gt;License-ready.&lt;br/&gt;
Ivy-ready.&lt;br/&gt;
OpenNLP libraries available through Ivy.&lt;br/&gt;
You still have to download jwnl-1.3.3 from &lt;a href="http://sourceforge.net/projects/jwordnet/files/" class="external-link"&gt;http://sourceforge.net/projects/jwordnet/files/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And of course download the model files. But this is committable to the Solr side.&lt;/p&gt;</comment>
                    <comment id="13293526" author="gsingers" created="Tue, 12 Jun 2012 12:36:32 +0100">&lt;p&gt;Very cool Lance.  The models are indeed tricky and I wonder how we can properly hook them into the tests, if at all.  I wonder how hard it would be to create much smaller ones based on training just a few things.&lt;/p&gt;</comment>
                    <comment id="13293529" author="teofili" created="Tue, 12 Jun 2012 12:40:48 +0100">&lt;blockquote&gt;&lt;p&gt;I wonder how hard it would be to create much smaller ones based on training just a few things.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;there was the idea of using the OpenNLP CorpusServer with some wikinews articles to train them (back to &lt;a href="https://issues.apache.org/jira/browse/OPENNLP-385" title="OpenNLP UIMA integration module should have unit tests"&gt;OPENNLP-385&lt;/a&gt;)&lt;/p&gt;</comment>
                    <comment id="13293560" author="joern" created="Tue, 12 Jun 2012 13:08:04 +0100">&lt;p&gt;I am using this mentioned Corpus Server together with the Apache UIMA Cas Editor for labeling projects. If someone wants to set something up to label data we (OpenNLP people) are happy to help with that!&lt;/p&gt;</comment>
                    <comment id="13293562" author="gsingers" created="Tue, 12 Jun 2012 13:13:12 +0100">&lt;p&gt;Cool!  &lt;/p&gt;

&lt;p&gt;I think if we could just get a very small model that can be checked in and used for testing purposes, that is all that would be needed.  We don't really need to test OpenNLP, we just need to test that the code properly interfaces with OpenNLP, so a really small model should be fine.  &lt;/p&gt;</comment>
                    <comment id="13293582" author="gsingers" created="Tue, 12 Jun 2012 13:46:51 +0100">&lt;p&gt;This really should just be a part of the analysis modules (with the exception of the Solr example parts).  I don't know exactly how we are handling Solr examples anymore, but I seem to recall the general consensus was to not proliferate them.  Can we just expose the functionality in the main one?&lt;/p&gt;

&lt;p&gt;I'll update the patch to move this to the module for starters.  Not sure on what to do w/ the example part.&lt;/p&gt;</comment>
                    <comment id="13293589" author="joern" created="Tue, 12 Jun 2012 14:03:36 +0100">&lt;p&gt;For a test you can run OpenNLP just over a piece of training data, even when trained on a tiny amount of data this will give good results. It does not test OpenNLP, but is sufficient for the desired interface testing.&lt;/p&gt;</comment>
                    <comment id="13294113" author="lancenorskog" created="Wed, 13 Jun 2012 04:28:53 +0100">&lt;blockquote&gt;&lt;p&gt;This really should just be a part of the analysis modules (with the exception of the Solr example parts). I don't know exactly how we are handling Solr examples anymore, but I seem to recall the general consensus was to not proliferate them. Can we just expose the functionality in the main one?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;A lot of Solr/Lucene features are only demoed in solrconfig/schema unit test files (DIH for example). That is fine.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The models are indeed tricky and I wonder how we can properly hook them into the tests, if at all.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;D'oh! Forgot about that. If we have tagged data in the project, it helps show the other parts of an NLP suite. It's hard to get a full picture of the jigsaw puzzle if you don't know NLP software.&lt;/p&gt;
</comment>
                    <comment id="13295570" author="lancenorskog" created="Fri, 15 Jun 2012 11:23:12 +0100">&lt;p&gt;Wiki page is up! &lt;a href="http://wiki.apache.org/solr/OpenNLP" class="external-link"&gt;http://wiki.apache.org/solr/OpenNLP&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also, the Solr fancy toolkits had no links from the Solr front page, so I added 'Advanced Tools' with links to UIMA and this.&lt;/p&gt;</comment>
                    <comment id="13393485" author="lancenorskog" created="Sun, 17 Jun 2012 08:59:54 +0100">&lt;blockquote&gt;&lt;p&gt;The models are indeed tricky and I wonder how we can properly hook them into the tests, if at all.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have mini training data for sentence detection, tokenization, POS and chunking. The purpose is to make the matching unit tests pass. The data and build script are in a new (unattached) patch. &lt;/p&gt;

&lt;p&gt;NER is proving a tougher nut to crack. I tried annotating several hundred lines of Reuters but no go. &lt;/p&gt;

&lt;p&gt;How would I make an NER dataset that will make OpenNLP spit out one or two tags? Is there a large NER dataset that is Apache-friendly?&lt;/p&gt;</comment>
                    <comment id="13395787" author="joern" created="Mon, 18 Jun 2012 11:15:41 +0100">&lt;p&gt;For NER you should try the perceptron and a cutoff of zero. For NER with a cutoff of 5 you need otherwise much more training data.&lt;/p&gt;</comment>
                    <comment id="13399243" author="lancenorskog" created="Fri, 22 Jun 2012 11:08:49 +0100">&lt;blockquote&gt;&lt;p&gt;For NER you should try the perceptron and a cutoff of zero. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks! This patch generates all models needed by tests, and the tests are rewritten to use the poor quality data from the models. To make the models, go to &lt;tt&gt;solr/contrib/opennlp/src/test-files/training&lt;/tt&gt; and run &lt;tt&gt;bin/training.sh&lt;/tt&gt;. This populates &lt;tt&gt;solr/contrib/opennlp/src/test-files/opennlp/conf/opennlp&lt;/tt&gt;. I don't have windows anymore so I can't make a .bat version.&lt;/p&gt;
</comment>
                    <comment id="13399245" author="lancenorskog" created="Fri, 22 Jun 2012 11:10:55 +0100">&lt;p&gt;General status:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;At this point you have to download 1 library (jwnl) and run a script to make the unit tests work.&lt;/li&gt;
	&lt;li&gt;You have to download several model files from sourceforge to do real work. There is no script to help.&lt;/li&gt;
	&lt;li&gt;The tokenizer and filter are in solr/ not lucene/&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What is missing to make this a full package:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Payload handling
	&lt;ul&gt;
		&lt;li&gt;TokenFilter to parse TAG/term or term_TAG into term/payload.&lt;/li&gt;
		&lt;li&gt;Output code in Solr for the reverse.&lt;/li&gt;
		&lt;li&gt;Payload query for tags.&lt;/li&gt;
		&lt;li&gt;Similarity scoring algorithms for tags.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Tag handling
	&lt;ul&gt;
		&lt;li&gt;There is a universal set of 12 parts-of-speech tags, with mappings for many language tagsets (Treebank etc.) into 12 common tags. Multi-language sites would benefit from this. I persuaded the authors to switch from GNU to Apache licensing.
		&lt;ul&gt;
			&lt;li&gt;&lt;a href="http://arxiv.org/abs/1104.2086" class="external-link"&gt;A Universal Part-of-Speech Tagset&lt;/a&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What NLP apps would be useful for search? Coordinate expansion, for example.&lt;/p&gt;</comment>
                    <comment id="13404899" author="lancenorskog" created="Mon, 2 Jul 2012 07:10:20 +0100">&lt;p&gt;This is about finished. The Tokenizer and TokenFilters are moved over into lucene/analysis/opennlp. They do not have unit tests in lucene/ because of the difficulty in supplying model data. They are unit-tested by the factories in solr/contrib/opennlp.&lt;/p&gt;

&lt;p&gt;The solr/example/opennlp directory is gone, as per request. Possible field types are documented in the solrconfig.xml in the unit test resources.&lt;/p&gt;

&lt;p&gt;All jars are downloaded via ivy. The jwnl library is one rev after what this was compiled with. It is only used in collocation, which is not exposed in this release.&lt;/p&gt;

&lt;p&gt;To build, test and commit, there is a boostrap sequence. In the top-level directory:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  ant clean compile
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This downloads the OpenNLP jars&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
cd solr/contrib/opennlp/test-files/training
sh bin/training.sh
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This creates low-quality model files in &lt;tt&gt;solr/contrib/opennlp/src/test-files/opennlp/solr/collection1/conf/opennlp&lt;/tt&gt;. In the trunk/solr directory, run&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt; 
ant example test-contrib
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You now have committable binary models. They are small, and only there to run the OpenNLP unit tests. They generate results that are objectively bogus, but the unit tests are matched to the results. If you want real models, you have to download them from sourceforge.&lt;/p&gt;</comment>
                    <comment id="13404904" author="lancenorskog" created="Mon, 2 Jul 2012 07:29:44 +0100">&lt;p&gt;Oops- remove &lt;tt&gt;solr/contrib/opennlp/src/test-files/opennlp/solr/collection1/conf/opennlp/.gitignore&lt;/tt&gt;. This will prevent you from committing the models.&lt;/p&gt;</comment>
                    <comment id="13404915" author="lancenorskog" created="Mon, 2 Jul 2012 08:24:03 +0100">&lt;p&gt;&lt;tt&gt;dev-tools&lt;/tt&gt; needs updating. I don't have IntelliJ and don't feel comfortable making the right Eclipse files. &lt;/p&gt;

&lt;p&gt;This patch works on both trunk and 4.x. I made a few changes in the build files where modules were out of alphabetic order. Also, the reams of copied code in module-build.xml had blocks out of order. I can't easily see where, but it seems like some of them are missing a few lines that others have.&lt;/p&gt;</comment>
                    <comment id="13406804" author="lancenorskog" created="Thu, 5 Jul 2012 02:07:44 +0100">&lt;p&gt;The Wiki is updated for testing and committing this patch: &lt;a href="http://wiki.apache.org/solr/OpenNLP" class="external-link"&gt;http://wiki.apache.org/solr/OpenNLP&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="13414820" author="lancenorskog" created="Mon, 16 Jul 2012 02:41:54 +0100">&lt;p&gt;There is a regression in Solr which causes this to not work in a Solr example: &lt;a href="https://issues.apache.org/jira/browse/SOLR-3625" title="Solr conf class loader does not find indirect jars - regression"&gt;&lt;del&gt;SOLR-3625&lt;/del&gt;&lt;/a&gt;. Until this is fixed, you have to copy the Lucene opennlp jar, the Solr opennlp jar, and the solr/contrib/opennlp/lib jars into the solr war. &lt;/p&gt;</comment>
                    <comment id="13419732" author="lancenorskog" created="Sat, 21 Jul 2012 04:29:54 +0100">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/SOLR-3623" title="inconsistent treatment of lucene jars &amp;amp; third-party deps in analysis-extras &amp;amp; uima (in war and in lucene-libs)"&gt;&lt;del&gt;SOLR-3623&lt;/del&gt;&lt;/a&gt; should give a final answer for how to build contribs and Lucene libraries and external dependencies. I've found it a little confusing.&lt;/p&gt;</comment>
                    <comment id="13428808" author="lancenorskog" created="Sun, 5 Aug 2012 06:53:52 +0100">&lt;p&gt;New patch for current build system on trunk &amp;amp; 4.x.&lt;/p&gt;</comment>
                    <comment id="13428809" author="lancenorskog" created="Sun, 5 Aug 2012 06:55:51 +0100">&lt;p&gt;As it turns out, building is still confused: solr/example/solr-webapps comes and goes. &lt;/p&gt;

&lt;p&gt;This build parks the lucene-analyzer-opennlp jar in solr/contrib/opennlp/lucene-libs. example/..../solrconfig.xml includes a reference to ../....../contrib/opennlp/lib and lucene-libs and ../...../dist.&lt;/p&gt;

&lt;p&gt;A jar-of-jars or a fully repacked jar in dist/ is the best way to ship this. &lt;/p&gt;

&lt;p&gt;Bug status: payloads added by this filter do not get written to the index!&lt;/p&gt;

&lt;p&gt;Build-fiddling status: forbidden api checks fail. checksums and licenses validate. rat-sources validate. No dev-tools changes. &lt;/p&gt;

&lt;p&gt;If you want this committed, I'm quite happy to do the last mile. &lt;/p&gt;</comment>
                    <comment id="13440345" author="alexey_kozhemiakin" created="Thu, 23 Aug 2012 15:44:25 +0100">&lt;p&gt;Yes, please, it would be awesome if someone could make this last effort and commit this issue. Many thanks!&lt;/p&gt;</comment>
                    <comment id="13442250" author="lancenorskog" created="Mon, 27 Aug 2012 03:39:06 +0100">&lt;p&gt;Committable except for dev-tools/ and production builds. I've updated dev-tools/eclipse, I don't have IntelliJ. These dev-tools build files contain 'uima' and so need parallel work for 'opennlp':&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;dev-tools/maven/lucene/analysis/pom.xml.template
dev-tools/maven/lucene/analysis/uima/pom.xml.template
dev-tools/maven/pom.xml.template
dev-tools/maven/solr/contrib/pom.xml.template
dev-tools/maven/solr/contrib/uima/pom.xml.template
dev-tools/scripts/SOLR-2452.patch.hack.pl
  - this one seems to be dead
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                    <comment id="13442251" author="lancenorskog" created="Mon, 27 Aug 2012 03:41:09 +0100">&lt;p&gt;The latest patch is tested fully and painfully in trunk. I'm sure it works as-is in 4.x, but it is not going into 4.0, so I'm not spending time on that&lt;/p&gt;</comment>
                    <comment id="13466478" author="em" created="Sun, 30 Sep 2012 14:34:31 +0100">&lt;p&gt;Could you please create a new Patch for the current Trunk? I had some problems on applying it to my working copy...&lt;/p&gt;

&lt;p&gt;I am not entirely sure whether its the Trunk or your Code, but it seems like your OpenNLP-code only works for the first request.&lt;/p&gt;

&lt;p&gt;As far as I was able to debug, the create()-method of the TokenFilterFactory is only called every now and again (are created TokenFilters reused for longer than one call in Solr?).&lt;/p&gt;

&lt;p&gt;If create() of your FilterFactory was called, everything works. However if the TokenFilter is somehow reused, it fails. &lt;/p&gt;

&lt;p&gt;Is this a bug of Solr or of your Patch?&lt;/p&gt;</comment>
                    <comment id="13466527" author="em" created="Sun, 30 Sep 2012 18:20:31 +0100">&lt;p&gt;Some Attributes were not reset (i.e. "first"-Attribute in OpenNLPTokenizer and "indexToken" in OpenNLPFilter) correctly.&lt;/p&gt;

&lt;p&gt;Since I had trouble applying your patch, I'd like to provide the working source code. Please, create a patch for the current Trunk. &lt;/p&gt;</comment>
                    <comment id="13466565" author="lancenorskog" created="Sun, 30 Sep 2012 21:41:31 +0100">&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;This worked when I posted it. There have been many changes in 4.x and trunk since then. For example, all of the tokenizer and filter factories moved to Lucene from Solr. I'm waiting until 4.0 is finished before I redo this patch. &lt;/p&gt;

</comment>
                    <comment id="13486242" author="vempap" created="Mon, 29 Oct 2012 18:36:44 +0000">&lt;p&gt;Would there be a patch for 4.0 as it is released.&lt;/p&gt;</comment>
                    <comment id="13500698" author="pgorla" created="Mon, 19 Nov 2012 22:33:01 +0000">&lt;p&gt;Thanks for this patch!&lt;/p&gt;

&lt;p&gt;I'm able to get the posTagger working, yet I still have not found a way to incorporate either the Chunker or the NER Models into my Solr project.&lt;/p&gt;

&lt;p&gt;Setting posTagger by itself works, but when I add a link to the chunkerModel (or even just the chunkerModel by itself), I obtain only the tokenized text.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&amp;lt;fieldType name=&lt;span class="code-quote"&gt;"text_opennlp_pos"&lt;/span&gt; class=&lt;span class="code-quote"&gt;"solr.TextField"&lt;/span&gt; positionIncrementGap=&lt;span class="code-quote"&gt;"100"&lt;/span&gt;&amp;gt;
&amp;lt;analyzer&amp;gt;
   &amp;lt;tokenizer class=&lt;span class="code-quote"&gt;"solr.OpenNLPTokenizerFactory"&lt;/span&gt;
      tokenizerModel=&lt;span class="code-quote"&gt;"opennlp/en-token.bin"&lt;/span&gt; /&amp;gt;
   &amp;lt;filter class=&lt;span class="code-quote"&gt;"solr.OpenNLPFilterFactory"&lt;/span&gt; 
      chunkerModel=&lt;span class="code-quote"&gt;"opennlp/en-chunking.bin"&lt;/span&gt;/&amp;gt;
&amp;lt;/analyzer&amp;gt;
&amp;lt;/fieldType&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I'm new to OpenNLP, so any pointers in the right direction would be greatly appreciated.&lt;/p&gt;</comment>
                    <comment id="13541285" author="lancenorskog" created="Mon, 31 Dec 2012 05:30:35 +0000">&lt;p&gt;Wow, someone tried it! I apologize for not noticing your question.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I'm able to get the posTagger working, yet I still have not found a way to incorporate either the Chunker or the NER Models into my Solr project.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The schema.xml file includes samples for all of the models:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;/lusolr_4x_opennlp/solr/contrib/opennlp/src/test-files/opennlp/solr/collection1/conf/schema.xml&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;This is for the chunker. The chunker works from parts-of-speech tags, not the original words. The chunker needs a parts-of-speech model as well as a chunker model. This should throw an error if the parts-of-speech model is not there. I will fix this.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-xml"&gt; &amp;lt;filter class=&lt;span class="code-quote"&gt;"solr.OpenNLPFilterFactory"&lt;/span&gt; 
          posTaggerModel=&lt;span class="code-quote"&gt;"opennlp/en-test-pos-maxent.bin"&lt;/span&gt;
          chunkerModel=&lt;span class="code-quote"&gt;"opennlp/en-test-chunker.bin"&lt;/span&gt;
        /&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is the NER configuration still not working?&lt;/p&gt;</comment>
                    <comment id="13567605" author="kguelzau@novomind.com" created="Thu, 31 Jan 2013 13:05:39 +0000">&lt;p&gt;The patch seems to be a bit out of date.&lt;br/&gt;
Applying it to branch_4x or trunk fails (build scripts).&lt;/p&gt;</comment>
                    <comment id="13567775" author="kguelzau@novomind.com" created="Thu, 31 Jan 2013 16:30:00 +0000">&lt;p&gt;End of OpenNLPTokenizer.fillBuffer() should be:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;while&lt;/span&gt;(length == size) {
  offset += size;
  fullText = Arrays.copyOf(fullText, offset + size);
  length = input.read(fullText, offset, size);
}
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (length == -1) {
  length = 0;
}
fullText = Arrays.copyOf(fullText, offset + length);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13568150" author="lancenorskog" created="Thu, 31 Jan 2013 21:50:09 +0000">&lt;p&gt;Thank you. Have you tried this on the trunk? The Solr components did not work, they could not find the OpenNLP jars.&lt;/p&gt;</comment>
                    <comment id="13568575" author="kguelzau@novomind.com" created="Fri, 1 Feb 2013 08:25:00 +0000">&lt;p&gt;I have applied the Patch to trunk, modified the build scripts manually (ignoring javadoc tasks) and built the opennlp jars.&lt;br/&gt;
Jars are running in a vanilla Solr 4.1 environment.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;solr_server4.1\solr\lib\opennlp\
	&lt;ul class="alternate" type="square"&gt;
		&lt;li&gt;&lt;del&gt;jwnl-1.4_rc3.jar&lt;/del&gt;&lt;/li&gt;
		&lt;li&gt;lucene-analyzers-opennlp-5.0-SNAPSHOT.jar (build with patch)&lt;/li&gt;
		&lt;li&gt;opennlp-maxent-3.0.2-incubating.jar&lt;/li&gt;
		&lt;li&gt;opennlp-tools-1.5.2-incubating.jar&lt;/li&gt;
		&lt;li&gt;solr-opennlp-5.0-SNAPSHOT.jar (build with patch)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;with &amp;lt;lib dir="../lib/opennlp" /&amp;gt; in solrconfig.xml&lt;/p&gt;

&lt;p&gt;Works for me: &lt;a href="http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201301.mbox/%3CB65DA877C3F93B4FB39EA49A1A03C95CC27AB1%40email.novomind.com%3E" class="external-link"&gt;http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201301.mbox/%3CB65DA877C3F93B4FB39EA49A1A03C95CC27AB1%40email.novomind.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;: removed jwnl*.jar as stated by Joern&lt;/p&gt;</comment>
                    <comment id="13568640" author="joern" created="Fri, 1 Feb 2013 10:32:09 +0000">&lt;p&gt;The jwnl library is only needed if you use the OpenNLP Coreference component, otherwise its safe to exclude it. The 1.4_rc3 version is not tested anyway and its likely that the Coreferencer does not probably run with it.&lt;/p&gt;</comment>
                    <comment id="13588854" author="nederhrj" created="Wed, 27 Feb 2013 22:24:11 +0000">&lt;p&gt;New patch for both trunk and 4.1 stable. Tested on revision 1450998.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
ant compile
cd solr/contrib/src/test-files/training
sh bin/trainall.sh
cd ../../../../../../solr
ant example test-contrib
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Hope this helps more people in testing OpenNLP integration with Solr.&lt;/p&gt;

&lt;p&gt;TODO: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Implementing dev-tools&lt;/li&gt;
	&lt;li&gt;Include references to javadocs&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13588855" author="nederhrj" created="Wed, 27 Feb 2013 22:24:44 +0000">&lt;p&gt;New patch for both trunk and 4.1 stable. Tested on revision 1450998.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;ant compile
cd solr/contrib/src/test-files/training
sh bin/trainall.sh
cd ../../../../../../solr
ant example test-contrib
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Hope this helps more people in testing OpenNLP integration with Solr.&lt;/p&gt;

&lt;p&gt;TODO: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Implementing dev-tools&lt;/li&gt;
	&lt;li&gt;Include references to javadocs&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13633907" author="redguy666" created="Wed, 17 Apr 2013 09:49:18 +0100">&lt;p&gt;why don't you prepare this as separate project that produces some jars and config files with instructions on how to add it in solr configuration instead of publishing all changes as patches to solr sources? I am interested in doing some tests with your library but setting all things up seems quite complicated and hard to maintain in future... it is just a thought.&lt;/p&gt;</comment>
                    <comment id="13641739" author="zzullick" created="Thu, 25 Apr 2013 14:02:01 +0100">&lt;p&gt;Some information for those wanting to try this after fighting it for a day: the latest patch posted, &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2899" title="Add OpenNLP Analysis capabilities as a module"&gt;LUCENE-2899&lt;/a&gt;-RJN.patch for 4.1 does not have Em's OpenNLPFilter.java and OpenNLPTokenizer.java fixed applied. So after applying the patch, make sure to replace those classes with Em's version or the bug that causes the NLP system to only be utilized on the first request will still be present. I was also able to successfully apply this patch to 4.2.1 with minor modification (mostly to the build/ivy xml files).&lt;/p&gt;</comment>
                    <comment id="13641968" author="lancenorskog" created="Thu, 25 Apr 2013 18:20:17 +0100">&lt;p&gt;Maciej- This is a good point. This package needs changes in a lot of places and it might be easier to package it the way you say. &lt;/p&gt;

&lt;p&gt;Zack- The "churn" in the APIs is a major problem in the Lucene code management. The original patch worked in the 4.x branch and trunk when it was posted. What Em fixed is in an area which is very very basic to Lucene. The API changed with no notice and no change in versions or method names. &lt;/p&gt;

&lt;p&gt;Everyone- It's great that this has gained some interest. Please create a new master patch with whatever changes are needed for the current code base.&lt;/p&gt;

&lt;p&gt;Lucene grand masters- Please don't say "hey kids, write plugins, they're cool!" and then make subtle incompatible changes in APIs. &lt;/p&gt;</comment>
                    <comment id="13661758" author="lancenorskog" created="Mon, 20 May 2013 04:55:58 +0100">&lt;p&gt;I'm updating the patches for 4.x and trunk. Kai's fix works. The unit tests did not attempt to analyse text that is longer than the fixed size temp buffer, and thus the code for copying successive buffers was never exercised. Kai's fix handles this problem. I've added a unit test. &lt;/p&gt;

&lt;p&gt;Em: the Lucene Tokenizer lifecyle is that the Tokenizer is created with a Reader, and each call to incrementToken() walks the input. When incrementToken() returns false, that is all- the Tokenizer is finished. TokenStream can support a 'stateful' token stream: with OpenNLPFilter, you call incrementToken() until it returns false, and then you can call 'reset' and it will start over from the beginning. The unit tests include a check that reset() works. The changes you made support a feature that is not supported by Lucene. Also, the changes break most of the unit tests. Please create a unit test that shows the bug, and fix the existing unit tests. No unit test = no bug report.&lt;/p&gt;

&lt;p&gt;I'm posting a patch for the current 4.x and trunk. It includes some changes for TokenStream/TokenFilter method signatures, some refactoring in the unit tests, a little tightening in the Tokenizer &amp;amp; Filter, and Kai's fix. There are unit tests for the problem Kai found, and also a test that has TokenizerFactory create multiple Tokenizer streams. If there is a bug in this patch, please write a unit test which demonstrates it.&lt;/p&gt;

&lt;p&gt;The patch is called &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2899" title="Add OpenNLP Analysis capabilities as a module"&gt;LUCENE-2899&lt;/a&gt;-current.patch. It is tested against the current 4.x branch and the current trunk.&lt;/p&gt;

&lt;p&gt;Thanks for your interest and hard work- I know it is really tedious to understand this code &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Lance Norskog&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                <outwardlinks description="depends upon">
                            <issuelink>
            <issuekey id="12598865">SOLR-3623</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12583790" name="LUCENE-2899-current.patch" size="490948" author="lancenorskog" created="Mon, 20 May 2013 04:56:58 +0100"/>
                    <attachment id="12542565" name="LUCENE-2899.patch" size="339462" author="lancenorskog" created="Mon, 27 Aug 2012 02:27:44 +0100"/>
                    <attachment id="12539194" name="LUCENE-2899.patch" size="308751" author="lancenorskog" created="Sun, 5 Aug 2012 06:53:52 +0100"/>
                    <attachment id="12534184" name="LUCENE-2899.patch" size="394115" author="lancenorskog" created="Mon, 2 Jul 2012 09:05:46 +0100"/>
                    <attachment id="12534178" name="LUCENE-2899.patch" size="345949" author="lancenorskog" created="Mon, 2 Jul 2012 07:10:20 +0100"/>
                    <attachment id="12533029" name="LUCENE-2899.patch" size="691276" author="lancenorskog" created="Fri, 22 Jun 2012 11:14:29 +0100"/>
                    <attachment id="12531501" name="LUCENE-2899.patch" size="536994" author="lancenorskog" created="Sat, 9 Jun 2012 04:24:46 +0100"/>
                    <attachment id="12571282" name="LUCENE-2899-RJN.patch" size="324975" author="nederhrj" created="Wed, 27 Feb 2013 22:24:11 +0000"/>
                    <attachment id="12547169" name="OpenNLPFilter.java" size="7930" author="em" created="Sun, 30 Sep 2012 18:20:31 +0100"/>
                    <attachment id="12547168" name="OpenNLPTokenizer.java" size="5890" author="em" created="Sun, 30 Sep 2012 18:20:31 +0100"/>
                    <attachment id="12531076" name="opennlp_trunk.patch" size="498471" author="lancenorskog" created="Wed, 6 Jun 2012 09:17:43 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 19 May 2011 11:44:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10969</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24787</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2886] Adaptive Frame Of Reference </title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2886</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We could test the implementation of the Adaptive Frame Of Reference &lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; on the lucene-4.0 branch.&lt;br/&gt;
I am providing the source code of its implementation. Some work needs to be done, as this implementation is working on the old lucene-1458 branch. &lt;br/&gt;
I will attach a tarball containing a running version (with tests) of the AFOR implementation, as well as the implementations of PFOR and of Simple64 (simple family codec working on 64bits word) that has been used in the experiments in &lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href="http://www.deri.ie/fileadmin/documents/deri-tr-afor.pdf" class="external-link"&gt;http://www.deri.ie/fileadmin/documents/deri-tr-afor.pdf&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12496582">LUCENE-2886</key>
            <summary>Adaptive Frame Of Reference </summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="renaud.delbru">Renaud Delbru</reporter>
                        <labels>
                    </labels>
                <created>Mon, 24 Jan 2011 18:31:25 +0000</created>
                <updated>Fri, 10 May 2013 00:05:22 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/codecs</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12985880" author="renaud.delbru" created="Mon, 24 Jan 2011 18:34:45 +0000">&lt;p&gt;tarball containing a maven project with source code and unit tests for:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;AFOR1&lt;/li&gt;
	&lt;li&gt;AFOR2&lt;/li&gt;
	&lt;li&gt;FOR&lt;/li&gt;
	&lt;li&gt;PFOR Non Compulsive&lt;/li&gt;
	&lt;li&gt;Simple64&lt;/li&gt;
	&lt;li&gt;a basic tool for debugging IntBlock codecs.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It includes also the lucene-1458 snapshot dependencies that are necessary to compile the code and run the tests.&lt;/p&gt;</comment>
                    <comment id="12989644" author="rcmuir" created="Wed, 2 Feb 2011 14:20:01 +0000">&lt;p&gt;I pulled out the simple64 implementation here and adapted it to the bulkpostings branch.&lt;/p&gt;

&lt;p&gt;Thanks for uploading this code Renaud, its great and the code is easy to work with. I hope to get some more of the codecs you wrote into the branch for testing.&lt;/p&gt;

&lt;p&gt;I changed a few things that helped in benchmarking:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;the decoder uses relative gets instead of absolute&lt;/li&gt;
	&lt;li&gt;we write #longs in the block header instead of #bytes (as its always long aligned, but smaller numbers)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But mainly, for this one I think we should change it to be a VariableIntBlock codec... right now it packs 128 integers into as few longs as possible, but this is wasteful for two reasons: it has to write a per-block byte header, and also wastes bits (e.g. in the case of a block of 128 1's).&lt;/p&gt;

&lt;p&gt;With variableintblock, we could do this differently, e.g. read a fixed number of longs per-block (say 4 longs), and our block would then be variable between 4 and 240 integers depending upon data.&lt;/p&gt;</comment>
                    <comment id="12990137" author="mikemccand" created="Thu, 3 Feb 2011 15:45:52 +0000">&lt;p&gt;New patch, including Robert's patch, and also adding Simple64 as a VarInt codec.  We badly need more testing of the VarInt cases, so it's great Simple64 came along (thanks Renaud!).&lt;/p&gt;

&lt;p&gt;All tests pass w/ Simple64VarInt codec.&lt;/p&gt;</comment>
                    <comment id="12990143" author="mikemccand" created="Thu, 3 Feb 2011 15:52:52 +0000">&lt;p&gt;OK I committed the two new Simple64 codecs (to bulk branch).&lt;/p&gt;</comment>
                    <comment id="12990439" author="rcmuir" created="Fri, 4 Feb 2011 03:22:07 +0000">&lt;p&gt;We are still not using 2 of the simple64 selectors, but in the simple64 paper it discusses&lt;br/&gt;
using these wasted bits to represent 120 and 240 "all ones"... I think we should do this?&lt;/p&gt;</comment>
                    <comment id="12990509" author="renaud.delbru" created="Fri, 4 Feb 2011 10:40:58 +0000">&lt;p&gt;Hi Michael, Robert,&lt;br/&gt;
great to hear that the code is useful, looking forward to see some benchmark.&lt;br/&gt;
I think the VarIntBlock approach is a good idea. Concerning the two unused "frame" codes, it will not cost too much to add them. This might be useful for the frequency inverted lists. However, I am not sure they will be used that much. In our experiments, we had a version of AFOR allowing frames of size 8, 16 and 32 integers with allOnes and allZeros. The gain was very minimal, in the order to 0.x% index size reduction, because these cases were occurring very rarely. But, this is still better than nothing. However, in the case of simple64, we are not talking about small frame (up to 32 integers), but frame of 120 to 240 integers. Therefore, I expect to see a drop of probability to encounter 120 or 240 consecutive ones. Maybe we can use them for more clever configurations such as&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;inter-leaved sequences of 1 bit and 2 bits integers&lt;/li&gt;
	&lt;li&gt;inter-leaved sequences of 2 bits and 3 bits integers&lt;br/&gt;
or something like this.&lt;br/&gt;
The best will be to do some tests to see which new configurations will make sense, like how many times a allOnes config is selected, or other configs, and choose which one to add. But this can be tedious task with only a limited benefit.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12990534" author="rcmuir" created="Fri, 4 Feb 2011 11:32:18 +0000">&lt;blockquote&gt;&lt;p&gt;Hi Michael, Robert,&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;great to hear that the code is useful, looking forward to see some benchmark.&lt;br/&gt;
I think the VarIntBlock approach is a good idea. Concerning the two unused "frame" codes, it will not cost too much to add them. This might be useful for the frequency inverted lists. However, I am not sure they will be used that much. &lt;/p&gt;

&lt;p&gt;In the case of 240 1's, i was surprised to see this selector was used over 2% of the time&lt;br/&gt;
for the gov collection's doc file?&lt;/p&gt;

&lt;p&gt;But still, for the all 1's case I'm not actually thinking about unstructured text so much... &lt;br/&gt;
in this case I am thinking about metadata fields and more structured data?&lt;/p&gt;</comment>
                    <comment id="12990535" author="renaud.delbru" created="Fri, 4 Feb 2011 11:46:09 +0000">&lt;blockquote&gt;
&lt;p&gt;In the case of 240 1's, i was surprised to see this selector was used over 2% of the time&lt;br/&gt;
for the gov collection's doc file?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;our results were performed on the wikipedia dataset and blogs dataset. I don;t know what was our selection rate, I was just referring to the gain in overall compression rate.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But still, for the all 1's case I'm not actually thinking about unstructured text so much...&lt;br/&gt;
in this case I am thinking about metadata fields and more structured data?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, this makes sense. In the context of SIREn (kind of simple xml node based inverted index) which is meant for indexing semi-structured data, the difference was more observable (mainly on the frequency and position files, as well as other structure node files).&lt;br/&gt;
This might be also useful on the document id file for very common terms (maybe for certain type of facets, with a very few number of values covering a large portion of the document collection).&lt;/p&gt;</comment>
                    <comment id="12990538" author="renaud.delbru" created="Fri, 4 Feb 2011 12:01:04 +0000">&lt;p&gt;Just an additional comment on semi-structured data indexing. AFOR-2 and AFOR-3 (AFOR-3 refers to AFOR-2 with special code for allOnes frames), was able to beat Rice on two datasets, and S-64 on one (but it was very close to Rice on the others):&lt;/p&gt;

&lt;p&gt;DBpedia dataset: (structured version of wikipedia)&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Method&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Ent&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Frq&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Att&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Val&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pos&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-1&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.246&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.043&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.141&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.065&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.180&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.816&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-2&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.229&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.039&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.132&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.059&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.167&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.758&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.229&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.031&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.131&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.054&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.159&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.736&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;FOR&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.315&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.061&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.170&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.117&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.216&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.049&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;PFOR&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.317&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.044&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.155&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.070&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.205&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.946&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;Rice&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.240&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.029&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.115&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.057&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.152&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.708&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;S-64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.249&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.041&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.133&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.062&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.171&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.791&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;VByte&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.264&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.162&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.222&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.222&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.245&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.335&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Geonames Dataset: &lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Method&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Ent&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Frq&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Att&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Val&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pos&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-1&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.129&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.058&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.025&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.025&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.318&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-2&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.123&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.057&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.024&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.024&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.307&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.114&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.006&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.056&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.016&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.008&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.256&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;FOR&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.150&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.021&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.065&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.025&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.349&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;PFOR&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.154&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.019&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.057&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.022&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.332&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;Rice&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.133&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.019&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.063&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.029&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.021&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.327&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;S-64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.147&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.021&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.058&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.329&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;VByte&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.216&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.142&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.143&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.143&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.143&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.929&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Sindice Dataset: Very heterogeneous dataset containing hundred of thousands of web dataset&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Method&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Ent&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Frq&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Att&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Val&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pos&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-1&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.578&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.395&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.942&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.665&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.014&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.537&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-2&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.361&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.380&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.908&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.619&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.906&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.082&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;AFOR-3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.297&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.176&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.876&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.530&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.722&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.475&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;FOR&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.506&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.506&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.121&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.916&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.440&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.611&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;PFOR&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.221&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.374&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.153&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.795&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.227&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.924&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;Rice&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.721&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.314&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.958&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.714&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.941&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.605&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;S-64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.581&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.370&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.917&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.621&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.908&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.313&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;VByte&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.287&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.106&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.411&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.430&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.488&lt;/td&gt;
&lt;td class='confluenceTd'&gt;15.132&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Here, Ent refers to entity id (similar to doc id), Att and Val are structural node ids.&lt;/p&gt;</comment>
                    <comment id="12990543" author="rcmuir" created="Fri, 4 Feb 2011 12:26:01 +0000">&lt;p&gt;Thanks for those numbers Renaud... yes the cases you see in e.g. Geonames&lt;br/&gt;
was one example of what I was thinking: in general you might say someone&lt;br/&gt;
should be using "omitTFAP" to omit freqs and positions for these fields,&lt;br/&gt;
but they might not be able to do this, if they want to support e.g. phrase&lt;br/&gt;
queries like "washington hill". So if we can pack long streams of 1s with &lt;br/&gt;
freqs and positions I think this is probably useful for a lot of people.&lt;/p&gt;

&lt;p&gt;Additionally for the .doc, i see its smaller in the AFOR-3 case too. Is&lt;br/&gt;
your "Ent" basically a measure of doc deltas? I'm confused exactly&lt;br/&gt;
what it is &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Because I would think if you take e.g. Geonames, the place &lt;br/&gt;
names in the dataset are not in random order but actually "batched" by&lt;br/&gt;
country for example, so you would have long streams of docdelta=1 for&lt;br/&gt;
country=Germany's postings. &lt;/p&gt;

&lt;p&gt;I'm not saying we could rely upon this, but i do think in general lots&lt;br/&gt;
of people's docs aren't in completely random order, and its probably&lt;br/&gt;
common to see long streams of docdelta=1 in structured data for this reason?&lt;/p&gt;

</comment>
                    <comment id="12990548" author="renaud.delbru" created="Fri, 4 Feb 2011 12:52:41 +0000">&lt;blockquote&gt;
&lt;p&gt;So if we can pack long streams of 1s with&lt;br/&gt;
freqs and positions I think this is probably useful for a lot of people.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, if the overhead is minimal, it might not be an issue in certain cases.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Additionally for the .doc, i see its smaller in the AFOR-3 case too. Is&lt;br/&gt;
your "Ent" basically a measure of doc deltas? I'm confused exactly&lt;br/&gt;
what it is &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, Ent is jsut a delta representation of the id of the entity (which can be considered as the document id). It is just that I have changed the name of the concept, as SIREn is manipulating principally entity and not document. In my case, an entity is just a set of attribute-value pairs, similarly to a document in Lucene.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Because I would think if you take e.g. Geonames, the place&lt;br/&gt;
names in the dataset are not in random order but actually "batched" by&lt;br/&gt;
country for example, so you would have long streams of docdelta=1 for&lt;br/&gt;
country=Germany's postings. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I checked, and Geonames dataset was alphabetically sorted by url names:&lt;br/&gt;
&lt;a href="http://sws.geonames.org/1/" class="external-link"&gt;http://sws.geonames.org/1/&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://sws.geonames.org/10/" class="external-link"&gt;http://sws.geonames.org/10/&lt;/a&gt;&lt;br/&gt;
...&lt;br/&gt;
as well as dbpedia and sindice.&lt;/p&gt;

&lt;p&gt;So, yes, this might have (good) consequences on the docdelta list for certain datasets such as geonames. And especially when indexing semi-structured data, as the schema of the data in one dataset is generally identical across entities/documents. therefore it is likely to see long runs of 1 for certain terms or schema terms.&lt;/p&gt;</comment>
                    <comment id="12990555" author="mikemccand" created="Fri, 4 Feb 2011 13:36:48 +0000">&lt;p&gt;I test perf of BulkVInt vs Simple64VarInt (Linux, 10M wikipedia index, NIOFSDir, multi-seg, java -server -Xbatch -Xmx2g -Xms2g):&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS bulkvint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS simple64varint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.47&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-15.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.92&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.09&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-15.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.96&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.70&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.24&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.31&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.36&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.48&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.45&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;27.37&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.08&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;156.48&lt;/td&gt;
&lt;td class='confluenceTd'&gt;162.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;15.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.07&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;42.84&lt;/td&gt;
&lt;td class='confluenceTd'&gt;46.11&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;7.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;48.13&lt;/td&gt;
&lt;td class='confluenceTd'&gt;52.03&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;8.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;32.80&lt;/td&gt;
&lt;td class='confluenceTd'&gt;37.07&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;13.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.38&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;14.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;18.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.53&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;15.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.17&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;15.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19.00&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.04&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;16.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;101.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;141.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;39.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12990560" author="mikemccand" created="Fri, 4 Feb 2011 13:48:54 +0000">&lt;p&gt;Same as last perf test, except MMapDir instead:&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS bulkvint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS simple64varint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.81&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-22.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.04&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-11.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19.18&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.25&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-10.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.25&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-0.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.33&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-0.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.18&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.24&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;36.69&lt;/td&gt;
&lt;td class='confluenceTd'&gt;37.20&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.71&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.92&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.15&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.55&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.27&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.73&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;166.18&lt;/td&gt;
&lt;td class='confluenceTd'&gt;171.96&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.24&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;34.18&lt;/td&gt;
&lt;td class='confluenceTd'&gt;35.49&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;47.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;49.53&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;50.71&lt;/td&gt;
&lt;td class='confluenceTd'&gt;53.52&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.51&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.96&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.18&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.22&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;9.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;127.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;161.43&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;26.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12990568" author="renaud.delbru" created="Fri, 4 Feb 2011 14:42:57 +0000">&lt;p&gt;Hi Michael, &lt;br/&gt;
the first results are not that impressive. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Could you tell me what is BulkVInt ? Is it the simple VInt codec implemented on top of the Bulk branch ?&lt;/li&gt;
	&lt;li&gt;What the difference between '+united +states' and '+nebraska +states' ? Is nebraska a low frequency term ?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12990570" author="rcmuir" created="Fri, 4 Feb 2011 14:54:15 +0000">&lt;p&gt;Hi Renaud:&lt;/p&gt;

&lt;p&gt;The BulkVInt codec is VInt implemented as a FixedIntBlock codec.&lt;br/&gt;
So it reads a single numBytes Vint header, then a byte[], and decodes BLOCKSIZE vints out of it.&lt;br/&gt;
The reason for this, is it has much different performance than "StandardCodec",&lt;br/&gt;
due to the fact StandardCodec has to readByte() readByte() readByte() ...&lt;/p&gt;

&lt;p&gt;You can see the code here: &lt;a href="http://svn.apache.org/repos/asf/lucene/dev/branches/bulkpostings/lucene/src/java/org/apache/lucene/index/codecs/bulkvint/BulkVIntCodec.java" class="external-link"&gt;http://svn.apache.org/repos/asf/lucene/dev/branches/bulkpostings/lucene/src/java/org/apache/lucene/index/codecs/bulkvint/BulkVIntCodec.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One reason for this, is to differentiate performance improvements of actual compression&lt;br/&gt;
algorithms from the way that they do their underlying I/O... previously various codecs&lt;br/&gt;
looked much faster than Vint but a lot of the reason for this is due to the way Vint&lt;br/&gt;
was implemented...&lt;/p&gt;

&lt;p&gt;And yes, you are correct nebraska is a lower freq term. the +united +states is a more &lt;br/&gt;
"normal" phrase query, but +nebraska +states is a phrase query intended to do a lot &lt;br/&gt;
of advance()'ing... &lt;/p&gt;</comment>
                    <comment id="12990573" author="rcmuir" created="Fri, 4 Feb 2011 15:04:31 +0000">&lt;p&gt;Here are my performance results, same setup (-server, etc)/index etc as Mike except&lt;br/&gt;
I'm using windows 64-bit (1.6.0_23/Vista), and I do not have an SSD, just a normal hard disk.&lt;br/&gt;
Using SimpleFS:&lt;/p&gt;
&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS bulkVint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS simple64Varint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.32&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-5.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.67&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.81&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.71&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-1.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.76&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.81&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.42&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.92&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.22&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;24.89&lt;/td&gt;
&lt;td class='confluenceTd'&gt;25.86&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.06&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.36&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.11&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.73&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.34&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.53&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.55&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;29.99&lt;/td&gt;
&lt;td class='confluenceTd'&gt;31.85&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;88.27&lt;/td&gt;
&lt;td class='confluenceTd'&gt;93.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.97&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.60&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.34&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.49&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.54&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.05&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;9.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.36&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.59&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;9.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;47.58&lt;/td&gt;
&lt;td class='confluenceTd'&gt;62.53&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;31.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Using MMap:&lt;/p&gt;
&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS bulkVint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS simple64Varint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.88&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-13.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;123.50&lt;/td&gt;
&lt;td class='confluenceTd'&gt;106.84&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-13.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.17&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-10.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.90&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.19&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-8.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.98&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-7.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;33.70&lt;/td&gt;
&lt;td class='confluenceTd'&gt;31.63&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-6.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.30&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-5.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;37.14&lt;/td&gt;
&lt;td class='confluenceTd'&gt;35.40&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-4.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;29.96&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.57&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-4.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.52&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.22&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.87&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-1.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.69&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.68&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-0.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.91&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.52&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.05&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.51&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.08&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;26.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.93&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;7.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;83.69&lt;/td&gt;
&lt;td class='confluenceTd'&gt;113.19&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;35.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

</comment>
                    <comment id="12990574" author="rcmuir" created="Fri, 4 Feb 2011 15:08:23 +0000">&lt;p&gt;based on these results, I think a good experiment would be for&lt;br/&gt;
us to "group" Simple64, so that we arent reading blocks of just a single long value.&lt;/p&gt;

&lt;p&gt;With NIOFS/SimpleFS, the reads are buffered so i think some of the overhead&lt;br/&gt;
is taken care of, but it doesn't tend to work so well with MMap.&lt;/p&gt;

&lt;p&gt;So, if we 'group' the long values so we are e.g. reading say N long values&lt;br/&gt;
at once in a single internal 'block', I think we might get more efficiency&lt;br/&gt;
via the I/O system, and also less overhead from the bulkpostings apis.&lt;/p&gt;

&lt;p&gt;I would hope this would speed up the other queries, and likely slow down&lt;br/&gt;
the +nebraska +states type thing, but I think thats ok.&lt;/p&gt;</comment>
                    <comment id="12990611" author="renaud.delbru" created="Fri, 4 Feb 2011 16:42:03 +0000">&lt;blockquote&gt;
&lt;p&gt;The BulkVInt codec is VInt implemented as a FixedIntBlock codec.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I saw the code, it is a similar implementation of the VInt we used in our experiments.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;previously various codecs&lt;br/&gt;
looked much faster than Vint but a lot of the reason for this is due to the way Vint&lt;br/&gt;
was implemented...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is odd, because we observed the contrary (on the lucene-1458 branch). The standard codec was by an order of magnitude faster than any other codec. We discovered that this was due to the IntBlock interface implementation that:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;was copying the buffer bytearray two times (one time from the disk to the buffer, then another time from the buffer to the IntBlock codec).&lt;/li&gt;
	&lt;li&gt;had to perform more work wrt to check each of the buffer (IntBlock buffer, IndexInput buffer).&lt;br/&gt;
But this might have been improved since then. Michael told me he worked on a new version of the IntBlock interface which was more performant.&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;
&lt;p&gt;So, if we 'group' the long values so we are e.g. reading say N long values&lt;br/&gt;
at once in a single internal 'block', I think we might get more efficiency&lt;br/&gt;
via the I/O system, and also less overhead from the bulkpostings apis.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If I understand, this is similar to increasing the boundaries of the variable block size. Indeed, it incurs some non-negligible overhead to perform a block read for each simple64 long word (simple64 frame), and this might be better to read more than one per block read.&lt;/p&gt;</comment>
                    <comment id="12990634" author="paul.elschot@xs4all.nl" created="Fri, 4 Feb 2011 17:22:46 +0000">&lt;p&gt;On the facility for allOnes in AFOR-3: one of the reasons why this appears to be of rather little use is that current analyzers do not index stop words. They do this for two reasons, index size and query time, both based on VByte. With an allOnes facility the first reason disappears almost completely, and query times can be also be guarded in other ways, for example by checking for document frequency and then trying to fall back on digrams.&lt;br/&gt;
So the message is: please keep this in.&lt;/p&gt;</comment>
                    <comment id="12990665" author="mikemccand" created="Fri, 4 Feb 2011 18:23:36 +0000">&lt;p&gt;Attached patch, adding block multiplier to Simple64VarInt.&lt;/p&gt;

&lt;p&gt;I haven't tested perf impact yet...&lt;/p&gt;</comment>
                    <comment id="12990674" author="mikemccand" created="Fri, 4 Feb 2011 18:45:24 +0000">&lt;p&gt;New patch, cuts over to bulk-reading the byte[] and then pulling a LongBuffer from that.&lt;/p&gt;</comment>
                    <comment id="12993021" author="rcmuir" created="Thu, 10 Feb 2011 12:19:13 +0000">&lt;p&gt;I spent some more time on Simple64, took Mike's previous patch and added some minor improvements:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Switched the decoding logic to the "Simple-8-4b" referred to in the paper. This is the same encoding, but we process with ints instead of longs.&lt;/li&gt;
	&lt;li&gt;Because our buffers are so tiny (for example 32 bytes), the overhead of NIO hurts rather than helps, so I switched to native arrays.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The performance is looking much more reasonable. Here's my tests on windows, maybe i can convince Mike to sanity-check it on linux.&lt;/p&gt;

&lt;p&gt;64-bit SimpleFS&lt;/p&gt;
&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS BulkVInt&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS Simple64VarInt4&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.79&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.67&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-3.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.45&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.13&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.04&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.43&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.04&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;25.39&lt;/td&gt;
&lt;td class='confluenceTd'&gt;26.21&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.47&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.74&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.88&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.50&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.86&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.21&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.11&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.85&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.17&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.60&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.70&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.05&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;30.02&lt;/td&gt;
&lt;td class='confluenceTd'&gt;31.90&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;86.56&lt;/td&gt;
&lt;td class='confluenceTd'&gt;94.15&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;8.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.10&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.55&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;13.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;46.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;57.90&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;23.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;32-bit SimpleFS&lt;/p&gt;
&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS BulkVInt&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS Simple64VarInt4&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;95.67&lt;/td&gt;
&lt;td class='confluenceTd'&gt;91.02&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-4.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.47&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.25&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-4.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.37&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.32&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-1.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20.45&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20.33&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-0.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.10&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.06&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-0.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.15&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.27&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.73&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.77&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.80&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.87&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.77&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.97&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.97&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.41&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.10&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.91&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.10&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20.50&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;30.00&lt;/td&gt;
&lt;td class='confluenceTd'&gt;33.15&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;10.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.71&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;11.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.65&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.96&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;13.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;43.93&lt;/td&gt;
&lt;td class='confluenceTd'&gt;54.38&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;23.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;64-bit MMap&lt;/p&gt;
&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS BulkVInt&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS Simple64VarInt4&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.99&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.41&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-6.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;38.21&lt;/td&gt;
&lt;td class='confluenceTd'&gt;36.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-5.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;118.11&lt;/td&gt;
&lt;td class='confluenceTd'&gt;112.19&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-5.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-4.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;33.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;32.51&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-3.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.68&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.54&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-3.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;30.00&lt;/td&gt;
&lt;td class='confluenceTd'&gt;29.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.48&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.06&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.60&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.08&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.76&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.87&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.88&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.85&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.98&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;27.00&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.08&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.68&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.31&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;7.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;83.30&lt;/td&gt;
&lt;td class='confluenceTd'&gt;96.53&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;15.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

</comment>
                    <comment id="12993048" author="mikemccand" created="Thu, 10 Feb 2011 13:32:52 +0000">&lt;p&gt;Linux results, NIOFSDir:&lt;/p&gt;


&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS bulkvint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS simple64x4&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.50&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-12.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19.45&lt;/td&gt;
&lt;td class='confluenceTd'&gt;18.94&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;18.20&lt;/td&gt;
&lt;td class='confluenceTd'&gt;17.85&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-1.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.38&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.40&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;43.59&lt;/td&gt;
&lt;td class='confluenceTd'&gt;44.31&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.48&lt;/td&gt;
&lt;td class='confluenceTd'&gt;8.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;5.96&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.14&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;27.58&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.84&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;157.05&lt;/td&gt;
&lt;td class='confluenceTd'&gt;165.43&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;15.41&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;5.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;48.10&lt;/td&gt;
&lt;td class='confluenceTd'&gt;51.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;8.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;32.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;36.24&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;10.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;18.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20.97&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;12.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.63&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.86&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;12.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;103.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;117.66&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;13.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;18.91&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.45&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;13.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.33&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.99&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;13.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;And MMapDir:&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS bulkvint&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS simple64x4&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.86&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.01&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-14.4%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;129.23&lt;/td&gt;
&lt;td class='confluenceTd'&gt;126.21&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="red"&gt;-2.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"united states"~3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.05&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.06&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;0.1%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.6&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.55&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.72&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;1.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.7&lt;/td&gt;
&lt;td class='confluenceTd'&gt;38.24&lt;/td&gt;
&lt;td class='confluenceTd'&gt;39.00&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+united +states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19.29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19.71&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.2%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~0.75&lt;/td&gt;
&lt;td class='confluenceTd'&gt;15.08&lt;/td&gt;
&lt;td class='confluenceTd'&gt;15.52&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.47&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;2.9%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctitle:.&lt;b&gt;&lt;span class="error"&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.23&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;3.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;47.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;49.44&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;36.28&lt;/td&gt;
&lt;td class='confluenceTd'&gt;38.01&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;4.7%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;doctimesecnum:&lt;span class="error"&gt;&amp;#91;10000 TO 60000&amp;#93;&lt;/span&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.02&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.90&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.3%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.08&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;169.47&lt;/td&gt;
&lt;td class='confluenceTd'&gt;180.70&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;6.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~0.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;23.03&lt;/td&gt;
&lt;td class='confluenceTd'&gt;24.79&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;7.6%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.40&lt;/td&gt;
&lt;td class='confluenceTd'&gt;23.64&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;10.5%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;states&lt;/td&gt;
&lt;td class='confluenceTd'&gt;50.60&lt;/td&gt;
&lt;td class='confluenceTd'&gt;56.17&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;11.0%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.16&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.59&lt;/td&gt;
&lt;td class='confluenceTd'&gt;&lt;font color="green"&gt;12.8%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12993053" author="rcmuir" created="Thu, 10 Feb 2011 13:47:29 +0000">&lt;p&gt;Thanks Mike... looks improved over your previous results.&lt;/p&gt;

&lt;p&gt;Should we commit this to bulk branch? I think we should also remove the "fixed-int" version 1.0 of Simple64 we have there...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12470773" name="LUCENE-2886.patch" size="41774" author="rcmuir" created="Thu, 10 Feb 2011 12:19:13 +0000"/>
                    <attachment id="12470270" name="LUCENE-2886.patch" size="4701" author="mikemccand" created="Fri, 4 Feb 2011 18:45:24 +0000"/>
                    <attachment id="12470267" name="LUCENE-2886.patch" size="4064" author="mikemccand" created="Fri, 4 Feb 2011 18:23:36 +0000"/>
                    <attachment id="12470047" name="LUCENE-2886_simple64.patch" size="50782" author="rcmuir" created="Wed, 2 Feb 2011 14:20:01 +0000"/>
                    <attachment id="12470155" name="LUCENE-2886_simple64_varint.patch" size="82482" author="mikemccand" created="Thu, 3 Feb 2011 15:45:52 +0000"/>
                    <attachment id="12469182" name="lucene-afor.tar.gz" size="2695041" author="renaud.delbru" created="Mon, 24 Jan 2011 18:34:44 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>6.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 2 Feb 2011 14:20:01 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10982</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24800</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2880] SpanQuery scoring inconsistencies</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2880</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2879" title="MultiPhraseQuery sums its own idf instead of Similarity."&gt;&lt;del&gt;LUCENE-2879&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can see a full description there, but the gist is that SpanQuery sums up freqs with "sloppyFreq".&lt;br/&gt;
However this slop is simply spans.end() - spans.start()&lt;/p&gt;

&lt;p&gt;For a SpanTermQuery for example, this means its scoring 0.5 for TF versus TermQuery's 1.0.&lt;br/&gt;
As you can imagine, I think in practical situations this would make it difficult for SpanQuery users to&lt;br/&gt;
really use SpanQueries for effective ranking, especially in combination with non-Spanqueries (maybe via DisjunctionMaxQuery, etc)&lt;/p&gt;

&lt;p&gt;The problem is more general than this simple example: for example SpanNearQuery should be consistent with PhraseQuery's slop.&lt;/p&gt;</description>
                <environment/>
            <key id="12496438">LUCENE-2880</key>
            <summary>SpanQuery scoring inconsistencies</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Sun, 23 Jan 2011 15:08:13 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12985339" author="rcmuir" created="Sun, 23 Jan 2011 15:10:57 +0000">&lt;p&gt;Here's a quickly hacked up patch (core tests pass, but i didnt go fixing contrib, etc yet).&lt;/p&gt;

&lt;p&gt;Its just to get ideas.&lt;/p&gt;

&lt;p&gt;The approach I took was for SpanQuery to have a new method:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;  /** 
   * Returns the length (number of positions) in the query.
   * &amp;lt;p&amp;gt;
   * For example, for a simple Term this is 1.
   * For a NEAR of "foo" and "bar" this is 2.
   * This is used by SpanScorer to compute the appropriate slop factor,
   * so that SpanQueries score consistently with other queries.
   */
  public abstract int getLength();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is called once by the Weight, and passed to SpanScorer.&lt;/p&gt;

&lt;p&gt;Then SpanScorer computes the slop as:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;int matchLength = (spans.end() - spans.start()) - queryLength;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;instead of:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;int matchLength = spans.end() - spans.start();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12985347" author="rcmuir" created="Sun, 23 Jan 2011 15:41:36 +0000">&lt;p&gt;thinking about this one, for this to really work correctly with the current setup (e.g. with SpanOrQuery), &lt;br/&gt;
this length might have to be in the Spans class...&lt;/p&gt;

&lt;p&gt;but with &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2878" title="Allow Scorer to expose positions and payloads aka. nuke spans "&gt;LUCENE-2878&lt;/a&gt; we nuke this class, so we can keep the issue open to think about how&lt;br/&gt;
the slop should be computed for these queries, i think just using the end - start is not the best.&lt;/p&gt;</comment>
                    <comment id="12985387" author="paul.elschot@xs4all.nl" created="Sun, 23 Jan 2011 18:16:15 +0000">&lt;p&gt;A related problem is that Spans does not have a weight (or whatever factor) of its own.&lt;br/&gt;
Currently Spans can only be scored at the top level (by SpanScorer) and not when they are nested.&lt;br/&gt;
In the nested case the only way to affect to score value is via the length.&lt;/p&gt;</comment>
                    <comment id="12985388" author="paul.elschot@xs4all.nl" created="Sun, 23 Jan 2011 18:30:52 +0000">&lt;p&gt;The getLength() method may not be straightforward.&lt;/p&gt;

&lt;p&gt;Does the getLength() method in SpanQuery also work in the nested case when there is an spanOr over two spanQueries of different length?&lt;/p&gt;

&lt;p&gt;It may be necessary to add this length to Spans because of this.&lt;/p&gt;

&lt;p&gt;Some reasons for a negative match length:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;multiple terms indexed at the same position,&lt;/li&gt;
	&lt;li&gt;span distance queries with the same subqueries.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I wish I had a good solution for this, but I did not find one yet.&lt;/p&gt;</comment>
                    <comment id="12985407" author="rcmuir" created="Sun, 23 Jan 2011 20:51:47 +0000">&lt;p&gt;Paul I agree, I think the only way it would work is to be in Spans itself,&lt;br/&gt;
which is the real 'Scorer' for spanqueries. Because its wrong for SpanOrQuery&lt;br/&gt;
to have a getLength() really... just like it would be wrong for BooleanQuery&lt;br/&gt;
to know anything about phrase slops of its subqueries!&lt;/p&gt;

&lt;p&gt;we can just leave this issue open and see what happens with&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2878" title="Allow Scorer to expose positions and payloads aka. nuke spans "&gt;LUCENE-2878&lt;/a&gt;, and maybe a good solution will then be more obvious.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12469082" name="LUCENE-2880.patch" size="13511" author="rcmuir" created="Sun, 23 Jan 2011 15:10:57 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 23 Jan 2011 18:16:15 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10987</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24806</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2859] Move Multi* and SlowMultiReaderWrapper to contrib</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2859</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We should move SlowMultiReaderWrapper and all Multi* classes to contrib as it should not be used anymore.&lt;/p&gt;</description>
                <environment/>
            <key id="12495283">LUCENE-2859</key>
            <summary>Move Multi* and SlowMultiReaderWrapper to contrib</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Tue, 11 Jan 2011 19:07:44 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12980296" author="thetaphi" created="Tue, 11 Jan 2011 19:53:13 +0000">&lt;p&gt;One use case is still there: Merging of segments needs Multi* so we cannot remove them from core. But we should hide as it's not public API.&lt;/p&gt;

&lt;p&gt;In all cases, SlowMultiReaderWrapper should be moved to contrib.&lt;/p&gt;</comment>
                    <comment id="12980309" author="jasonrutherglen" created="Tue, 11 Jan 2011 20:15:23 +0000">&lt;blockquote&gt;&lt;p&gt;Merging of segments needs Multi*&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why's this?&lt;/p&gt;</comment>
                    <comment id="12980667" author="mikemccand" created="Wed, 12 Jan 2011 11:14:36 +0000">&lt;p&gt;We could fix that merging uses MultiDocs/AndPositionsEnum.  It's not particularly clean because we make Mapping* subclasses to remap the docIDs around deletions.  If, instead, we fixed PostingsConsumer.merge to take the subs' enums, instead of a single multi enum, then that method could go segment by segment.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 11 Jan 2011 20:15:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2930</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24827</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2853] Improve random seed portability for test cases</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2853</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently, tests get a random seed, and several possibilities can be randomly assigned, or fixed to&lt;br/&gt;
specific values with -Dtests.codec or -Dtests.directory.&lt;/p&gt;

&lt;p&gt;The default values for these sort of things is 'random'.&lt;/p&gt;

&lt;p&gt;I think it would be a great improvement if we:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;improved our code here so that we always consume the same amount from the Random. this means for example, if you specific -Dtests.directory=RAMDirectory, we should consume a wasted int, where we would normally pick one from random.&lt;/li&gt;
	&lt;li&gt;in 3.x, i think its worth it to actually consume wasted ints where 4.0 picks the codec.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With these changes, you could ideally (in many situations situations)&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;take a failure and modify parameters to see if its specific to a particular codec or directory, or a general problem&lt;/li&gt;
	&lt;li&gt;take a failure from trunk and see if it affects 3.x&lt;/li&gt;
&lt;/ol&gt;


</description>
                <environment/>
            <key id="12494964">LUCENE-2853</key>
            <summary>Improve random seed portability for test cases</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Fri, 7 Jan 2011 16:39:47 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2927</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24833</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2844] benchmark geospatial performance based on geonames.org</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2844</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Until now (with this patch), the benchmark contrib module did not include a means to test geospatial data.  This patch includes some new files and changes to existing ones.  Here is a summary of what is being added in this patch per file (all files below are within the benchmark contrib module) along with my notes:&lt;/p&gt;

&lt;p&gt;Changes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;build.xml &amp;#8211; Add dependency on Lucene's spatial module and Solr.
	&lt;ul&gt;
		&lt;li&gt;It was a real pain to figure out the convoluted ant build system to make this work, and I doubt I did it the proper way.&lt;/li&gt;
		&lt;li&gt;Rob Muir thought it would be a good idea to make the benchmark contrib module be top level module (i.e. be alongside analysis) so that it can depend on everything.  &lt;a href="http://lucene.472066.n3.nabble.com/Re-Geospatial-search-in-Lucene-Solr-tp2157146p2157824.html" class="external-link"&gt;http://lucene.472066.n3.nabble.com/Re-Geospatial-search-in-Lucene-Solr-tp2157146p2157824.html&lt;/a&gt;  I agree&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;ReadTask.java &amp;#8211; Added a search.useHitTotal boolean option that will use the total hits number for reporting purposes, instead of the existing behavior.
	&lt;ul&gt;
		&lt;li&gt;The existing behavior (i.e. when search.useHitTotal=false) doesn't look very useful since the response integer is the sum of several things instead of just one thing.  I don't see how anyone makes use of it.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Note that on my local system, I also changed ReportTask &amp;amp; RepSelectByPrefTask to not include the '-' every other line, and also changed Format.java to not use commas in the numbers.  These changes are to make copy-pasting into excel more streamlined.&lt;/p&gt;

&lt;p&gt;New Files:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;geoname-spatial.alg &amp;#8211; my algorithm file.
	&lt;ul&gt;
		&lt;li&gt;Note the ":0" trailing the Populate sequence.  This is a trick I use to skip building the index, since it takes a while to build and I'm not interested in benchmarking index construction.  You'll want to set this to :1 and then subsequently put it back for further runs as long as you keep the doc.geo.schemaField or any other configuration elements affecting index the same.&lt;/li&gt;
		&lt;li&gt;In the patch, doc.geo.schemaField=geohash but unless you're tinkering with &lt;a href="https://issues.apache.org/jira/browse/SOLR-2155" title="Geospatial search using geohash prefixes"&gt;&lt;del&gt;SOLR-2155&lt;/del&gt;&lt;/a&gt;, you'll probably want to set this to "latlon"&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;GeoNamesContentSource.java &amp;#8211; a ContentSource for a geonames.org data file (either a single country like US.txt or allCountries.txt).
	&lt;ul&gt;
		&lt;li&gt;Uses a subclass of DocData to store all the fields.  The existing DocData wasn't very applicable to data that is not composed of a title and body.&lt;/li&gt;
		&lt;li&gt;Doesn't reuse the docdata parameter to getNextDocData(); a new one is created every time.&lt;/li&gt;
		&lt;li&gt;Only supports content.source.forever=false&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;GeoNamesDocMaker.java &amp;#8211; a subclass of DocMaker that works very differently than the existing DocMaker.
	&lt;ul&gt;
		&lt;li&gt;Instead of assuming that each line from geonames.org will correspond to one Lucene document, this implementation supports, via configuration, creating a variable number of documents, each with a variable number of points taken randomly from a GeoNamesContentSource.&lt;/li&gt;
		&lt;li&gt;doc.geo.docsToGenerate:  The number of documents to generate.  If blank it defaults to the number of rows in GeoNamesContentSource.&lt;/li&gt;
		&lt;li&gt;doc.geo.avgPlacesPerDoc: The average number of places to be added to a document.  A random number between 0 and one less than twice this amount is chosen on a per document basis.  If this is set to 1, then exactly one is always used.  In order to support a value greater than 1, use the geohash field type and incorporate &lt;a href="https://issues.apache.org/jira/browse/SOLR-2155" title="Geospatial search using geohash prefixes"&gt;&lt;del&gt;SOLR-2155&lt;/del&gt;&lt;/a&gt; (geohash prefix technique).&lt;/li&gt;
		&lt;li&gt;doc.geo.oneDocPerPlace: Whether at most one document should use the same place.  In other words, Can more than one document have the same place?  If so, set this to false.&lt;/li&gt;
		&lt;li&gt;doc.geo.schemaField: references a field name in schema.xml.  The field should implement SpatialQueryable.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;GeoPerfData.java: This class is a singleton storing data in memory that is shared by GeoNamesDocMaker.java and GeoQueryMaker.java.
	&lt;ul&gt;
		&lt;li&gt;content.geo.zeroPopSubst: if a population is encountered that is &amp;lt;= 0, then use this population value instead.  Default is 100.&lt;/li&gt;
		&lt;li&gt;content.geo.maxPlaces: A limit on the number of rows read in from GeoNamesContentSource.java can be set here.  Defaults to Integer.MAX_VALUE.&lt;/li&gt;
		&lt;li&gt;GeoPerfData is primarily responsible for reading in data from GeoNamesContentSource into memory to store the lat, lon, and population.  When a random place is asked for, you get one weighted according to population.  The idea is to skew the data towards more referenced places, and a population number is a decent way of doing it.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;GeoQueryMaker.java &amp;#8211; returns random queries from GeoPerfData by taking a random point and using a particular configured radius.  A pure lat-lon bounding box query is ultimately done.
	&lt;ul&gt;
		&lt;li&gt;query.geo.radiuskm: The radius of the query in kilometers.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;schema.xml &amp;#8211; a Solr schema file to configure SpatialQueriable fields referenced by doc.geo.schemaField.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;When I run this algorithm as provided with the file in the patch, I get this result:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;Operation   round ____km   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
Search_40       0    350        1      4811687 1,206,541.38        3.99   117,722,664    191,934,464
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key metrics I use are the average milliseconds per query, and the average places per query.  The number of queries performed is the trailing numeric suffix to Operation.  The Formulas:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;avg ms/query:  elapsedSec*1000/queries  == 98.8&lt;/li&gt;
	&lt;li&gt;avg places / query:  recsPerRun/queries == 120,292&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12494501">LUCENE-2844</key>
            <summary>benchmark geospatial performance based on geonames.org</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="dsmiley">David Smiley</reporter>
                        <labels>
                    </labels>
                <created>Mon, 3 Jan 2011 16:58:10 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/benchmark</component>
                        <due/>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12976823" author="rcmuir" created="Mon, 3 Jan 2011 17:10:35 +0000">&lt;p&gt;David, I'll first create an issue to propose moving benchmark/ to modules.&lt;/p&gt;

&lt;p&gt;I've personally been frustrated by this before (just simple stuff like wanting to benchmark some analysis &lt;br/&gt;
definition in a schema.xml for ReadTokens/indexing speed and having to actually write an Analyzer.java to do it)&lt;/p&gt;</comment>
                    <comment id="12983402" author="dsmiley" created="Tue, 18 Jan 2011 21:56:13 +0000">&lt;p&gt;This is an update to the patch which considers the move of the benchmark contrib to /modules/benchmark.  It also includes GeoNamesSetSolrAnalyzerTask which will use Solr's field-specific analyzer.  It's very much tied to these set of classes in the patch.  There are ASF headers now too.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                <outwardlinks description="depends upon">
                            <issuelink>
            <issuekey id="12494504">LUCENE-2845</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12468689" name="benchmark-geo.patch" size="27479" author="dsmiley" created="Tue, 18 Jan 2011 21:56:13 +0000"/>
                    <attachment id="12467326" name="benchmark-geo.patch" size="25734" author="dsmiley" created="Mon, 3 Jan 2011 16:59:07 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 3 Jan 2011 17:10:35 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11017</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24842</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2841] CommonGramsFilter improvements</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2841</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently CommonGramsFilter expects users to remove the common words around which output token ngrams are formed, by appending a StopFilter to the analysis pipeline.  This is inefficient in two ways: captureState() is called on (trailing) stopwords, and then the whole stream has to be re-examined by the following StopFilter.&lt;/p&gt;

&lt;p&gt;The current ctor should be deprecated, and another ctor added with a boolean option controlling whether the common words should be output as unigrams.&lt;/p&gt;

&lt;p&gt;If common words &lt;b&gt;are&lt;/b&gt; configured to be output as unigrams, captureState() will still need to be called, as it is now.&lt;/p&gt;

&lt;p&gt;If the common words are &lt;b&gt;not&lt;/b&gt; configured to be output as unigrams, rather than calling captureState() for the trailing token in each output token ngram, the term text, position and offset can be maintained in the same way as they are now for the leading token: using a System.arrayCopy()'d term buffer and a few ints for positionIncrement and offsetd.  The user then no longer would need to append a StopFilter to the analysis chain.&lt;/p&gt;

&lt;p&gt;An example illustrating both possibilities should also be added.&lt;/p&gt;</description>
                <environment/>
            <key id="12494388">LUCENE-2841</key>
            <summary>CommonGramsFilter improvements</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="steve_rowe">Steve Rowe</reporter>
                        <labels>
                    </labels>
                <created>Fri, 31 Dec 2010 19:26:20 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                    <version>3.1</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>1</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12976338" author="rcmuir" created="Fri, 31 Dec 2010 19:36:41 +0000">&lt;p&gt;+1, this would be a great improvement.&lt;/p&gt;

&lt;p&gt;there are two basic use cases (that I see):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;you still aren't ever removing any stopwords, but using this solely to speed up phrase queries by forming bigrams around the common terms.&lt;/li&gt;
	&lt;li&gt;you are using commongrams+stopfilter as a "stopfilter replacement", which gives a more reasonable index size, the relevance benefits of stopwords, but a user can always refine the query with double quotes and the stopwords are taken into consideration, and fast.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;the latter case currently requires you to also use a stopfilter, but it means we are doing needless captureState very very often (by definition, on common terms!). It also means you are specifying your stopwords list twice, and hashing two chararraysets, etc. So it would be nice to add the boolean and accelerate case #2.&lt;/p&gt;</comment>
                    <comment id="12976341" author="jasonrutherglen" created="Fri, 31 Dec 2010 19:45:02 +0000">&lt;blockquote&gt;&lt;p&gt;you still aren't ever removing any stopwords, but using this solely to speed up phrase queries by forming bigrams around the common terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Isn't ShingleFilter for that case?&lt;/p&gt;</comment>
                    <comment id="12976343" author="rcmuir" created="Fri, 31 Dec 2010 20:00:50 +0000">&lt;p&gt;No, ShingleFilter forms bigrams around all terms, not just common ones.&lt;/p&gt;</comment>
                    <comment id="12976344" author="steve_rowe" created="Fri, 31 Dec 2010 20:04:39 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;you still aren't ever removing any stopwords, but using this solely to speed up phrase queries by forming bigrams around the common terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Isn't ShingleFilter for that case?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;On the index side: ShingleFilter generates token ngrams for all input tokens, not just those around and including common words, so although it &lt;b&gt;could&lt;/b&gt; be used to speed up phrase queries, it would be at the expense of a much larger term dicitionary.&lt;/p&gt;

&lt;p&gt;On the query side: ShingleFilter could be a useful replacement for CommonGramsQueryFilter if you don't have access to the list of words used by CommonGramsFilter on the index side.&lt;/p&gt;</comment>
                    <comment id="13539309" author="itamar" created="Mon, 24 Dec 2012 19:22:43 +0000">&lt;p&gt;Adding option to CommonGramsFilter to not unigram common words&lt;/p&gt;</comment>
                    <comment id="13539310" author="itamar" created="Mon, 24 Dec 2012 19:24:03 +0000">&lt;p&gt;Attached is a patch to fix this, including tests. There is no regression, and the new behavior when keepOrig is set to true is as described in the comments here.&lt;/p&gt;

&lt;p&gt;The only thing I wasn't sure about was CommonGramsQueryFilter - should it be deprecated? or how should it be made to work with this change?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12562327" name="commit-6402a55.patch" size="23199" author="itamar" created="Mon, 24 Dec 2012 19:22:43 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 31 Dec 2010 19:36:41 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11020</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24845</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2840] Multi-Threading in IndexSearcher (after removal of MultiSearcher and ParallelMultiSearcher)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2840</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spin-off from parent issue:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We should discuss about how many threads should be spawned. If you have an index with many segments, even small ones, I think only the larger segments should be separate threads, all others should be handled sequentially. So maybe add a maxThreads cound, then sort the IndexReaders by maxDoc and then only spawn maxThreads-1 threads for the bigger readers and then one additional thread for the rest?&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment/>
            <key id="12494299">LUCENE-2840</key>
            <summary>Multi-Threading in IndexSearcher (after removal of MultiSearcher and ParallelMultiSearcher)</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                    <parent id="12494204">LUCENE-2837</parent>
                        <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Thu, 30 Dec 2010 11:58:45 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>7</watches>
                                                    <comments>
                    <comment id="12976027" author="earwin" created="Thu, 30 Dec 2010 12:36:46 +0000">&lt;p&gt;I use the following scheme:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;There is a fixed pool of threads shared by all searches, that limits total concurrency.&lt;/li&gt;
	&lt;li&gt;Each new search apprehends at most a fixed number of threads from this pool (say, 2-3 of 8 in my setup),&lt;/li&gt;
	&lt;li&gt;and these threads churn through segments as through a queue (in maxDoc order, but I think even that is unnecessary).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;No special smart binding between threads and segments (eg. 1 thread for each biggie, 1 thread for all of the small ones) -&lt;br/&gt;
means simpler code, and zero possibility of stalling, when there are threads to run, segments to search, but binding policy does not connect them.&lt;br/&gt;
Using fewer threads per-search than total available is a precaution against biggie searches blocking fast ones.&lt;/p&gt;</comment>
                    <comment id="12976928" author="mikemccand" created="Mon, 3 Jan 2011 20:05:17 +0000">&lt;blockquote&gt;&lt;p&gt;Using fewer threads per-search than total available is a precaution against biggie searches blocking fast ones.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But doesn't that mean that an app w/ rare queries but each query is massive fails to use all available concurrency?&lt;/p&gt;</comment>
                    <comment id="12979276" author="earwin" created="Sun, 9 Jan 2011 08:50:09 +0000">&lt;blockquote&gt;&lt;p&gt;But doesn't that mean that an app w/ rare queries but each query is massive fails to use all available concurrency?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes. But that's not my case. And likely not someone else's.&lt;/p&gt;

&lt;p&gt;I think if you want to be super-generic, it's better to defer exact threading to the user, instead of doing a one-size-fits-all solution. Else you risk conjuring another ConcurrentMergeScheduler.&lt;br/&gt;
While we're at it, we can throw in some sample implementation, which can satisfy some of the users, but not everyone.&lt;/p&gt;</comment>
                    <comment id="12979284" author="doronc" created="Sun, 9 Jan 2011 10:06:31 +0000">&lt;p&gt;Is it a possible that with this, searching a large optimized index (single segment) might be slower than searching an un-optimzed index of the same size, since the latter enjoys concurrency? If so, is it too wild for more than one thread to handle that single segment?&lt;/p&gt;</comment>
                    <comment id="12979293" author="mikemccand" created="Sun, 9 Jan 2011 11:10:24 +0000">&lt;blockquote&gt;&lt;p&gt;I think if you want to be super-generic, it's better to defer exact threading to the user, instead of doing a one-size-fits-all solution. Else you risk conjuring another ConcurrentMergeScheduler.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think something like CMS (basically a custom ES w/ proper thread prio/scheduling) will be necessary here.&lt;/p&gt;

&lt;p&gt;Until Java can schedule threads the way an OS schedules processes we'll need to emulate it ourselves.&lt;/p&gt;

&lt;p&gt;You want long running queries (or, merges) to be gracefully down prioritized so that new/fast queries (merges) finish quickly.&lt;/p&gt;

&lt;p&gt;And you want searches (merges) to use the allowed concurrency fully.&lt;/p&gt;</comment>
                    <comment id="12979306" author="earwin" created="Sun, 9 Jan 2011 11:51:56 +0000">&lt;p&gt;A lot of fork-join type frameworks don't even care. Even though scheduling threads is something people supposedly use them for.&lt;br/&gt;
Why? I guess that's due to low yield/cost ratio.&lt;br/&gt;
You frequently quote "progress, not perfection" in relation to the code, but why don't we apply this same principle to our threading guarantees?&lt;br/&gt;
I don't want to use allowed concurrency fully. That's not realistic. I want 85% of it. That's already a huge leap ahead of single-threaded searches.&lt;/p&gt;</comment>
                    <comment id="12979337" author="mikemccand" created="Sun, 9 Jan 2011 14:48:37 +0000">&lt;blockquote&gt;&lt;p&gt;You frequently quote "progress, not perfection" in relation to the code, but why don't we apply this same principle to our threading guarantees?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh we should definitely apply progress not perfection here &amp;#8211; in fact we already are: for starters (today), we bind concurrency to segments (so eg an "optimized" index has no concurrency), and we just use an ES (punt this thread scheduling problem to the caller).  This is better than nothing, but not good enough &amp;#8211; we can do better.&lt;/p&gt;

&lt;p&gt;There's another quote that applies here: "big dreams, small steps".  My comment above is "dreaming" but when it comes time to actually get the real work done / making progress towards that dream, of course we take baby steps / progress not perfection.&lt;/p&gt;

&lt;p&gt;Design discussions should start w/ the big dreams but then once you've got a rough sense of where you want to get to in the future you shift back to the baby steps you do today, in the direction of that future goal.&lt;/p&gt;

&lt;p&gt;Maybe I should wrap my comments in &amp;lt;/dream&amp;gt; tags and &amp;lt;/babysteps&amp;gt; tags!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 30 Dec 2010 12:36:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11021</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24846</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2832] on Windows 64-bit, maybe we should default to a better maxBBufSize in MMapDirectory</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2832</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently the default max buffer size for MMapDirectory is 256MB on 32bit and Integer.MAX_VALUE on 64bit:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;public static final int DEFAULT_MAX_BUFF = Constants.JRE_IS_64BIT ? Integer.MAX_VALUE : (256 * 1024 * 1024);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But, in windows on 64-bit, you are practically limited to 8TB. This can cause problems in extreme cases, such as: &lt;a href="http://www.lucidimagination.com/search/document/7522ee54c46f9ca4/map_failed_at_getsearcher" class="external-link"&gt;http://www.lucidimagination.com/search/document/7522ee54c46f9ca4/map_failed_at_getsearcher&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Perhaps it would be good to change this default such that its 256MB on 32Bit &lt;b&gt;OR&lt;/b&gt; windows, but leave it at Integer.MAX_VALUE&lt;br/&gt;
on other 64-bit and "64-bit" (48-bit) systems.&lt;/p&gt;</description>
                <environment/>
            <key id="12494045">LUCENE-2832</key>
            <summary>on Windows 64-bit, maybe we should default to a better maxBBufSize in MMapDirectory</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Fri, 24 Dec 2010 17:53:56 +0000</created>
                <updated>Fri, 10 May 2013 00:05:23 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12982675" author="thetaphi" created="Mon, 17 Jan 2011 15:49:54 +0000">&lt;p&gt;I would suggest to use a different default for Win64, as the adress space is not as small as with 32 bit. How about something like 4 GB or 16 GB?&lt;/p&gt;

&lt;p&gt;Also, for 32bit we use 1/8 of possible address space, so why not the same (1/8) for win64?&lt;/p&gt;</comment>
                    <comment id="12982682" author="thetaphi" created="Mon, 17 Jan 2011 15:55:55 +0000">&lt;p&gt;Sorry my last comment was stupid, as 1/8 of 8TB is still larger as Integer.MAX_VALUE (I was thinking of Long.MAX_VALUE).&lt;/p&gt;

&lt;p&gt;I still have no idea why this fails, as 8 TB of address space should be enough for thousands of 2 GB blocks.&lt;/p&gt;</comment>
                    <comment id="12982692" author="rcmuir" created="Mon, 17 Jan 2011 16:07:26 +0000">&lt;p&gt;In this case, its very extreme. the user had 1.1 billion documents on one windows server.&lt;/p&gt;

&lt;p&gt;I am not sure if this issue will even help anyone at all: will a smaller buffer really help fragmentation in these cases?&lt;br/&gt;
The user never responded to my suggestion to change the buffer size.&lt;/p&gt;

&lt;p&gt;I think a good option here is to do nothing at all, but I'm not opposed to reducing the buffer &lt;b&gt;if&lt;/b&gt; it will actually help,&lt;br/&gt;
mainly because the MultiMMapIndexInput is sped up and it shouldn't cause as much slowdown as before.&lt;/p&gt;</comment>
                    <comment id="12982701" author="rcmuir" created="Mon, 17 Jan 2011 16:44:42 +0000">&lt;p&gt;I am removing 3.1 as I think its the safest option.&lt;/p&gt;

&lt;p&gt;We can revisit if someone is willing to test parameters on enormous indexes (200GB, 500GB, 1TB, ...)&lt;br/&gt;
otherwise we are just guessing.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12468518" name="LUCENE-2832.patch" size="1610" author="rcmuir" created="Sun, 16 Jan 2011 21:33:40 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 17 Jan 2011 15:49:54 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2938</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24854</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2798] Randomize indexed collation key testing</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2798</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Robert Muir noted on #lucene IRC channel today that Lucene's indexed collation key testing is currently fragile (for example, they had to be revisited when Robert upgraded the ICU dependency in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2797" title="upgrade icu to 4.6"&gt;&lt;del&gt;LUCENE-2797&lt;/del&gt;&lt;/a&gt; because of Unicode 6.0 collation changes) and coverage is trivial (only 5 locales tested, and no collator options are exercised).  This affects both the JDK implementation in &lt;tt&gt;modules/analysis/common/&lt;/tt&gt; and the ICU implementation under &lt;tt&gt;modules/icu/&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The key thing to test is that the order of the indexed terms is the same as that provided by the Collator itself.  Instead of the current set of static tests, this could be achieved via indexing randomly generated terms' collation keys (and collator options) and then comparing the index terms' order to the order provided by the Collator over the original terms.&lt;/p&gt;

&lt;p&gt;Since different terms may produce the same collation key, however, the order of indexed terms is inherently unstable.  When performing runtime collation, the Collator addresses the sort stability issue by adding a secondary sort over the normalized original terms.  In order to directly compare Collator's sort with Lucene's collation key sort, a secondary sort will need to be applied to Lucene's indexed terms as well. Robert has suggested indexing the original terms in addition to their collation keys, then using a Sort over the original terms as the secondary sort.&lt;/p&gt;

&lt;p&gt;Another complication: Lucene 3.X uses Java's UTF-16 term comparison, and trunk uses UTF-8 order, so the implemented secondary sort will need to respect that.&lt;/p&gt;

&lt;p&gt;From #lucene:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;rmuir__: so i think we have to on 3.x, sort the 'expected list' with Collator.compare, if thats equal, then as a tiebreak use String.compareTo&lt;br/&gt;
rmuir__: and in the index sort on the collated field, followed by the original term&lt;br/&gt;
rmuir__: in 4.x we do the same thing, but dont use String.compareTo as the tiebreak for the expected list&lt;br/&gt;
rmuir__: instead compare codepoints (iterating character.codepointAt, or comparing .getBytes("UTF-8"))&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment/>
            <key id="12492071">LUCENE-2798</key>
            <summary>Randomize indexed collation key testing</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="steve_rowe">Steve Rowe</assignee>
                                <reporter username="steve_rowe">Steve Rowe</reporter>
                        <labels>
                    </labels>
                <created>Sat, 4 Dec 2010 16:57:38 +0000</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                    <version>3.1</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12966933" author="rcmuir" created="Sun, 5 Dec 2010 12:33:49 +0000">&lt;p&gt;Steven, before working too hard on the jdk collation tests, i just had this idea:&lt;/p&gt;

&lt;p&gt;Are we sure we shouldn't deprecate the jdk collation functionality (remove in trunk) and only offer ICU?&lt;/p&gt;

&lt;p&gt;I was just thinking that the JDK Collator integration is basically a RAM trap due to its aweful keysize, etc:&lt;br/&gt;
&lt;a href="http://site.icu-project.org/charts/collation-icu4j-sun" class="external-link"&gt;http://site.icu-project.org/charts/collation-icu4j-sun&lt;/a&gt;&lt;/p&gt;
</comment>
                    <comment id="12969395" author="steve_rowe" created="Wed, 8 Dec 2010 17:53:48 +0000">&lt;blockquote&gt;
&lt;p&gt;Are we sure we shouldn't deprecate the jdk collation functionality (remove in trunk) and only offer ICU?&lt;/p&gt;

&lt;p&gt;I was just thinking that the JDK Collator integration is basically a RAM trap due to its aweful keysize, etc:&lt;br/&gt;
&lt;a href="http://site.icu-project.org/charts/collation-icu4j-sun" class="external-link"&gt;http://site.icu-project.org/charts/collation-icu4j-sun&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't like this idea, because it removes the choice.&lt;/p&gt;

&lt;p&gt;If there were some way to perform deprecation without eventual removal, I'd be okay with it.  The issue, as I see it, is documentaiton.  Here is an excerpt from the current class-level javadoc for &lt;tt&gt;CollationKeyFilter&lt;/tt&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The &amp;lt;code&amp;gt;ICUCollationKeyFilter&amp;lt;/code&amp;gt; in the icu package of Lucene's contrib area uses ICU4J's Collator, which makes its version available, thus allowing collation to be versioned independently from the JVM.  ICUCollationKeyFilter is also significantly faster and generates significantly shorter keys than CollationKeyFilter.  See &lt;a href="http://site.icu-project.org/charts/collation-icu4j-sun" class="external-link"&gt;http://site.icu-project.org/charts/collation-icu4j-sun&lt;/a&gt; for key generation timing and key length comparisons between ICU4J and java.text.Collator over several languages.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So an attempt is already being made to inform potential victims of the choice they're making - it even links to the same web page you mentioned.&lt;/p&gt;

&lt;p&gt;Maybe if we move the JDK variant out of core and into a module, rather than on trunk, it would at least send a message that it's on par with the ICU variant.&lt;/p&gt;</comment>
                    <comment id="13018230" author="steve_rowe" created="Mon, 11 Apr 2011 08:25:45 +0100">&lt;p&gt;work in progress: JDK-only Analyzer-only test: &lt;tt&gt;TestCollationKeyAnalyzer.testRandomizedCollationKeySort()&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The test succeeds most of the times I run it, but sometimes fails, e.g. for these seeds:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;3253903552510972177:-5236779063463918718&lt;/li&gt;
	&lt;li&gt;1469913545269555695:-7929666046197505961&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Robert, would you please take a look at the code and see if you can figure out why the test fails?&lt;/p&gt;</comment>
                    <comment id="13018283" author="rcmuir" created="Mon, 11 Apr 2011 12:08:32 +0100">&lt;p&gt;just a glance: &lt;/p&gt;

&lt;p&gt;it may be the use of _TestUtil.randomUnicodeString here.&lt;br/&gt;
it is not just avoiding supplementaries, but also avoiding things like U+FFFF&lt;/p&gt;

&lt;p&gt;bottom line: there are serious bugs in this stuff, and even my current "testThreadSafe" i think is not completely avoiding them (I seem to trigger a OOM from the jre impl every few days)&lt;/p&gt;

&lt;p&gt;I've thought about @Ignore'ing our current testThreadSafe for this reason... I don't like dancing around known bugs in a test like this, it makes the test stupid. Somehow this stuff needs to get fixed in ICU/OpenJDK.&lt;/p&gt;</comment>
                    <comment id="13018353" author="steve_rowe" created="Mon, 11 Apr 2011 15:02:47 +0100">&lt;blockquote&gt;&lt;p&gt;it may be the use of _TestUtil.randomUnicodeString here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It may, but the first above-listed seed produces this mismatch (strings are printed out as arrays of codepoints):&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;java.lang.AssertionError: -----------
Indexed string #45: [141]
 Sorted string #45: [141]
-----------
Indexed string #46: [32]
 Sorted string #46: [28, 777]
-----------
Indexed string #47: [28, 777]
 Sorted string #47: [32]

Collator strength: SECONDARY  Collator decomposition: CANONICAL_DECOMPOSITION
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;#46 and #47 include neither supplementary chars nor problematic BMP chars.&lt;/p&gt;

&lt;p&gt;I wrote a test including just &lt;span class="error"&gt;&amp;#91;32&amp;#93;&lt;/span&gt; and &lt;span class="error"&gt;&amp;#91;28,777&amp;#93;&lt;/span&gt; as indexed strings, and the same mismatch occurs for random locales, regardless of collator decomposition, and for all collator strengths except PRIMARY.&lt;/p&gt;</comment>
                    <comment id="13018365" author="rcmuir" created="Mon, 11 Apr 2011 15:16:45 +0100">&lt;blockquote&gt;
&lt;p&gt;I wrote a test including just &lt;span class="error"&gt;&amp;#91;32&amp;#93;&lt;/span&gt; and &lt;span class="error"&gt;&amp;#91;28,777&amp;#93;&lt;/span&gt; as indexed strings, and the same mismatch occurs for random locales, regardless of collator decomposition, and for all collator strengths except PRIMARY.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Without looking too hard (are these hex values?) in your debugging it would be useful to print the sort key as well. Are the sort keys the same?&lt;/p&gt;

&lt;p&gt;But FYI the bugs i found in collation, somehow corrupted the internal state of RuleBasedCollator, so the exact strings you are looking at might simply be a symptom.&lt;/p&gt;</comment>
                    <comment id="13018374" author="steve_rowe" created="Mon, 11 Apr 2011 15:49:29 +0100">&lt;blockquote&gt;&lt;p&gt;Without looking too hard (are these hex values?) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, it's just the output from Arrays.toString(int[]), which outputs decimal.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;in your debugging it would be useful to print the sort key as well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed. Here's the output:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;java.lang.AssertionError: -----------&lt;br/&gt;
Indexed string #0: &lt;span class="error"&gt;&amp;#91;32&amp;#93;&lt;/span&gt;&lt;br/&gt;
Indexed collation key: &lt;span class="error"&gt;&amp;#91;0, 0, 0, 119, 0, 0&amp;#93;&lt;/span&gt;&lt;br/&gt;
 Sorted string #0: &lt;span class="error"&gt;&amp;#91;28, 777&amp;#93;&lt;/span&gt;&lt;br/&gt;
Sorted collation key: &lt;span class="error"&gt;&amp;#91;0, 0, 0, -101, 0, 0&amp;#93;&lt;/span&gt;&lt;br/&gt;
-----------&lt;br/&gt;
Indexed string #1: &lt;span class="error"&gt;&amp;#91;28, 777&amp;#93;&lt;/span&gt;&lt;br/&gt;
Indexed collation key: &lt;span class="error"&gt;&amp;#91;0, 0, 0, -101, 0, 0&amp;#93;&lt;/span&gt;&lt;br/&gt;
 Sorted string #1: &lt;span class="error"&gt;&amp;#91;32&amp;#93;&lt;/span&gt;&lt;br/&gt;
Sorted collation key: &lt;span class="error"&gt;&amp;#91;0, 0, 0, 119, 0, 0&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Collator strength: SECONDARY  Collator decomposition: NO_DECOMPOSITION&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;(again with the Arrays.toString() for the byte array from the collation keys - obviously not ideal in that they're first converted to signed integers...)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Are the sort keys the same?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No.&lt;/p&gt;</comment>
                    <comment id="13018383" author="rcmuir" created="Mon, 11 Apr 2011 16:10:26 +0100">&lt;p&gt;also i don't see any check that preflex codec isn't in use for this test?&lt;/p&gt;
</comment>
                    <comment id="13018386" author="steve_rowe" created="Mon, 11 Apr 2011 16:16:59 +0100">&lt;blockquote&gt;&lt;p&gt;also i don't see any check that preflex codec isn't in use for this test?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;tt&gt;TestCollationKeyAnalyzer.setUp()&lt;/tt&gt; handles it:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  @Override
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; void setUp() &lt;span class="code-keyword"&gt;throws&lt;/span&gt; Exception {
    &lt;span class="code-keyword"&gt;super&lt;/span&gt;.setUp();
    assumeFalse(&lt;span class="code-quote"&gt;"preflex format only supports UTF-8 encoded bytes"&lt;/span&gt;, &lt;span class="code-quote"&gt;"PreFlex"&lt;/span&gt;.equals(CodecProvider.getDefault().getDefaultFieldCodec()));
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And in practice, the test gets skipped 25% of the time as a result of this.&lt;/p&gt;</comment>
                    <comment id="13018388" author="steve_rowe" created="Mon, 11 Apr 2011 16:19:57 +0100">&lt;p&gt;Added two-term collation sort test; added collation key debug printing.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12492054">LUCENE-2797</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12476008" name="LUCENE-2798.patch" size="17616" author="steve_rowe" created="Mon, 11 Apr 2011 16:19:57 +0100"/>
                    <attachment id="12475983" name="LUCENE-2798.patch" size="13440" author="steve_rowe" created="Mon, 11 Apr 2011 08:25:45 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 5 Dec 2010 12:33:49 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11058</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24888</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2780] optimize spantermquery</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2780</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Looking at &lt;a href="http://www.lucidimagination.com/search/document/c2c6f660ddde4f7f/dismaxqparserplugin_and_tokenization" class="external-link"&gt;http://www.lucidimagination.com/search/document/c2c6f660ddde4f7f/dismaxqparserplugin_and_tokenization&lt;/a&gt; ,&lt;br/&gt;
I saw a user building DisjunctionMaxQuery / BooleanQuery with SpanTermQuerys.&lt;/p&gt;

&lt;p&gt;I wonder if users know that doing this is much slower than just using TermQuery?&lt;br/&gt;
I agree it makes little sense to use SpanTermQuery if you arent going to use it inside a SpanNear etc,&lt;br/&gt;
but on the other hand, I think its a little non-intuitive that it wouldnt be just as fast in a case like this.&lt;/p&gt;

&lt;p&gt;I could see this complicating queryparsing etc for users that want to sometimes use positions etc.&lt;/p&gt;

&lt;p&gt;SpanTermQuery is the same as TermQuery, except tf is computed as (#of spans * sloppyFreq(spanLength)&lt;br/&gt;
For this case, #ofspans = tf and spanLength for a single term is always 1.&lt;/p&gt;

&lt;p&gt;Maybe we should optimize SpanTermQuery to return TermScorer, with just this special tf computation.&lt;br/&gt;
This would avoid reading positions for anyone that does this.&lt;/p&gt;
</description>
                <environment/>
            <key id="12480948">LUCENE-2780</key>
            <summary>optimize spantermquery</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 25 Nov 2010 20:28:32 +0000</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12935866" author="rcmuir" created="Thu, 25 Nov 2010 20:29:30 +0000">&lt;p&gt;patch&lt;/p&gt;</comment>
                    <comment id="12935972" author="mikemccand" created="Fri, 26 Nov 2010 10:56:07 +0000">&lt;p&gt;Looks good Robert!  It's a sneaky trap.  Maybe add a comment to createWeight explaining that this is only used when a "normal" (non-span) Query embeds a SpanTermQuery?&lt;/p&gt;

&lt;p&gt;Someday we need to merge Span* back into the "normal" queries.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12460474" name="LUCENE-2780.patch" size="4325" author="rcmuir" created="Thu, 25 Nov 2010 20:29:29 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 26 Nov 2010 10:56:07 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2933</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24906</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2765] Optimize scanning in DocsEnum</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2765</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Similar to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2761" title="specialize payload processing from of DocsAndPositionsEnum"&gt;&lt;del&gt;LUCENE-2761&lt;/del&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;when we call advance(), after skipping it scans, but this can be optimized better than calling nextDoc() like today&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;      // scan for the rest:
      do {
        nextDoc();
      } while (target &amp;gt; doc);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in particular, the freq can be "skipVinted" and the skipDocs (deletedDocs) don't need to be checked during this scanning.&lt;/p&gt;</description>
                <environment/>
            <key id="12480011">LUCENE-2765</key>
            <summary>Optimize scanning in DocsEnum</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Mon, 15 Nov 2010 22:51:33 +0000</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12932247" author="rcmuir" created="Mon, 15 Nov 2010 22:55:58 +0000">&lt;p&gt;Also, another idea like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2761" title="specialize payload processing from of DocsAndPositionsEnum"&gt;&lt;del&gt;LUCENE-2761&lt;/del&gt;&lt;/a&gt; is to specialize the omitTF case here...&lt;/p&gt;
</comment>
                    <comment id="12932281" author="rcmuir" created="Tue, 16 Nov 2010 00:35:02 +0000">&lt;p&gt;here's a patch, maybe can be beautified/optimized further.&lt;/p&gt;

&lt;p&gt;needs benchmarking.&lt;/p&gt;</comment>
                    <comment id="12932283" author="rcmuir" created="Tue, 16 Nov 2010 00:37:52 +0000">&lt;p&gt;my mistake, i left an extra check in the code... here's the updated one.&lt;/p&gt;</comment>
                    <comment id="12932291" author="rcmuir" created="Tue, 16 Nov 2010 00:51:47 +0000">&lt;p&gt;i ran a quick very rough check, with AND query (3149 results for this query)... &lt;br/&gt;
i didnt benchmark the omitTF case (but it should be better too)&lt;/p&gt;

&lt;p&gt;all times in milliseconds&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    QueryParser qp = &lt;span class="code-keyword"&gt;new&lt;/span&gt; QueryParser(Version.LUCENE_CURRENT, &lt;span class="code-quote"&gt;"body"&lt;/span&gt;, &lt;span class="code-keyword"&gt;new&lt;/span&gt; MockAnalyzer());
    Query query = qp.parse(&lt;span class="code-quote"&gt;"+the +america"&lt;/span&gt;);
    &lt;span class="code-object"&gt;System&lt;/span&gt;.out.println(searcher.search(query, 10).totalHits);
    &lt;span class="code-object"&gt;long&lt;/span&gt; ms = &lt;span class="code-object"&gt;System&lt;/span&gt;.currentTimeMillis();
    &lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; i = 0; i &amp;lt; 1000; i++) {
      searcher.search(query, 10);
    }
    &lt;span class="code-object"&gt;long&lt;/span&gt; ms2 = &lt;span class="code-object"&gt;System&lt;/span&gt;.currentTimeMillis();
    &lt;span class="code-object"&gt;System&lt;/span&gt;.out.println(&lt;span class="code-quote"&gt;"time = "&lt;/span&gt; + (ms2 - ms));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;setup&lt;/th&gt;
&lt;th class='confluenceTh'&gt;run1&lt;/th&gt;
&lt;th class='confluenceTh'&gt;run2&lt;/th&gt;
&lt;th class='confluenceTh'&gt;run3&lt;/th&gt;
&lt;th class='confluenceTh'&gt;run4&lt;/th&gt;
&lt;th class='confluenceTh'&gt;run5&lt;/th&gt;
&lt;th class='confluenceTh'&gt;run6&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;trunk&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1707&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1706&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1709&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1704&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1704&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1703&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2765" title="Optimize scanning in DocsEnum"&gt;LUCENE-2765&lt;/a&gt;&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1628&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1623&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1641&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1624&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1627&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1628&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;seems worth it to me.&lt;/p&gt;</comment>
                    <comment id="12932920" author="rcmuir" created="Wed, 17 Nov 2010 12:07:14 +0000">&lt;p&gt;here is Mike's results on his wikipedia index (multi-segment, 5% deletions) with the patch.&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Query&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS base&lt;/th&gt;
&lt;th class='confluenceTh'&gt;QPS spec&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Pct diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;"unit state"&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.94&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.84&lt;/td&gt;
&lt;td class='confluenceTd'&gt;-1.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;state&lt;/td&gt;
&lt;td class='confluenceTd'&gt;36.15&lt;/td&gt;
&lt;td class='confluenceTd'&gt;35.81&lt;/td&gt;
&lt;td class='confluenceTd'&gt;-1.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanNear(&lt;span class="error"&gt;&amp;#91;unit, state&amp;#93;&lt;/span&gt;, 10, true)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.46&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.42&lt;/td&gt;
&lt;td class='confluenceTd'&gt;-0.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;spanFirst(unit, 5)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.51&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.45&lt;/td&gt;
&lt;td class='confluenceTd'&gt;-0.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit state&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.76&lt;/td&gt;
&lt;td class='confluenceTd'&gt;10.78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;0.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~2.0&lt;/td&gt;
&lt;td class='confluenceTd'&gt;13.83&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.06 &lt;/td&gt;
&lt;td class='confluenceTd'&gt;1.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit~1.0&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.36&lt;/td&gt;
&lt;td class='confluenceTd'&gt;14.69 &lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;uni*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;15.57&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16.02&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;unit*&lt;/td&gt;
&lt;td class='confluenceTd'&gt;27.29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.26&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+unit +state&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.73&lt;/td&gt;
&lt;td class='confluenceTd'&gt;12.31&lt;/td&gt;
&lt;td class='confluenceTd'&gt;4.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~1.0&lt;/td&gt;
&lt;td class='confluenceTd'&gt;29.01&lt;/td&gt;
&lt;td class='confluenceTd'&gt;30.86&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;un*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;66.52&lt;/td&gt;
&lt;td class='confluenceTd'&gt;70.99&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;u*d&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21.29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;22.98&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;united~2.0&lt;/td&gt;
&lt;td class='confluenceTd'&gt;6.48&lt;/td&gt;
&lt;td class='confluenceTd'&gt;7.07&lt;/td&gt;
&lt;td class='confluenceTd'&gt;9.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;+nebraska +state&lt;/td&gt;
&lt;td class='confluenceTd'&gt;169.87&lt;/td&gt;
&lt;td class='confluenceTd'&gt;188.95&lt;/td&gt;
&lt;td class='confluenceTd'&gt;11.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12459667" name="LUCENE-2765.patch" size="10777" author="rcmuir" created="Tue, 16 Nov 2010 00:37:52 +0000"/>
                    <attachment id="12459666" name="LUCENE-2765.patch" size="10786" author="rcmuir" created="Tue, 16 Nov 2010 00:35:02 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2928</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24921</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2749] Co-occurrence filter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2749</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The co-occurrence filter to be developed here will output sets of tokens that co-occur within a given window onto a token stream.  &lt;/p&gt;

&lt;p&gt;These token sets can be ordered either lexically (to allow order-independent matching/counting) or positionally (e.g. sliding windows of positionally ordered co-occurring terms that include all terms in the window are called n-grams or shingles). &lt;/p&gt;

&lt;p&gt;The parameters to this filter will be: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;window size: this can be a fixed sequence length, sentence/paragraph context (these will require sentence/paragraph segmentation, which is not in Lucene yet), or over the entire token stream (full field width)&lt;/li&gt;
	&lt;li&gt;minimum number of co-occurring terms: &amp;gt;= 2&lt;/li&gt;
	&lt;li&gt;maximum number of co-occurring terms: &amp;lt;= window size&lt;/li&gt;
	&lt;li&gt;token set ordering (lexical or positional)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;One use case for co-occurring token sets is as candidates for collocations.&lt;/p&gt;</description>
                <environment/>
            <key id="12479418">LUCENE-2749</key>
            <summary>Co-occurrence filter</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="steve_rowe">Steve Rowe</reporter>
                        <labels>
                    </labels>
                <created>Mon, 8 Nov 2010 21:27:20 +0000</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                    <version>3.1</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="13006222" author="epitschke" created="Sun, 13 Mar 2011 17:04:53 +0000">&lt;p&gt;Hi,&lt;br/&gt;
i am fairly new to Lucene development, but i have plenty experience using it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. I would like to make some contribution and think this would be a good task for me to start, as i am fairly interested in the analysis part. Can i work on this task or has there been any work done on this yet?&lt;br/&gt;
Regards&lt;br/&gt;
   Elmar&lt;/p&gt;</comment>
                    <comment id="13006254" author="steve_rowe" created="Sun, 13 Mar 2011 19:06:55 +0000">&lt;p&gt;Hi Elmar,&lt;/p&gt;

&lt;p&gt;I haven't had a chance to do more than an hour or two of work on this, and that was a while back, so please feel free to run with it.&lt;/p&gt;

&lt;p&gt;You should know, though, that Robert Muir and Yonik Seeley (both Lucene/Solr developers) expressed skepticism (on #lucene IRC) about whether this filter belongs in Lucene itself, because in their experience, collocations are used by non-search software, and they believe that Lucene should remain focused exclusively on search.  &lt;/p&gt;

&lt;p&gt;Robert Muir also thinks that components that support Boolean search (i.e., not ranked search) should go elsewhere.  &lt;/p&gt;

&lt;p&gt;I personally disagree with these restrictions in general, and I think that a co-occurrence filter could directly support search.  See this solr-user@lucene.apache.org mailing list discussion for an example I gave (and one of the reasons I made this issue): &lt;a href="http://www.lucidimagination.com/search/document/f69f877e0fa05d17/how_do_i_this_in_solr#d9d5932e7074d356" class="external-link"&gt;http://www.lucidimagination.com/search/document/f69f877e0fa05d17/how_do_i_this_in_solr#d9d5932e7074d356&lt;/a&gt; . In this thread, I described a way to solve the original poster's problem using a co-occurrence filter exactly like the one proposed here.&lt;/p&gt;

&lt;p&gt;I mention all this to caution you that work you put in here may never be committed to Lucene itself.&lt;/p&gt;

&lt;p&gt;The mailing list thread I mentioned above describes the main limitations a filter like this will have: combinatoric explosion of generated terms.  I haven't figured out how to manage this, but it occurs to me that the two-term-collocation case is less problematic in this regard than the generalized case (whole-field window, all possible combinations).  I had a vague implementation conception of incrementing a fixed-width integer to iterate over the combinations, using the integer's bits to include/exclude input terms in the output "termset" tokens.  Using a 32-bit integer to track combinations would limit the length of an input token stream to 32 tokens, but in the generalized case of all combinations, I'm pretty sure that the number of bits available would not be the limiting factor, but rather the number of generated terms.  I guess the question is how to handle cases that produce fewer terms than all combinations of terms from an input token stream, e.g. the two-term-collocation case, without imposing the restrictions necessary in the generalized case.&lt;/p&gt;

&lt;p&gt;Here are a couple of recent information retrieval papers using "termset" to mean "indexed token containing multiple input terms":&lt;/p&gt;

&lt;p&gt;"TSS: Efficient Term Set Search in Large Peer-to-Peer Textual Collections"&lt;br/&gt;
&lt;a href="http://www.cs.ust.hk/~liu/TSS-TC.pdf" class="external-link"&gt;http://www.cs.ust.hk/~liu/TSS-TC.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;"Termset-based Indexing and Query Processing in P2P Search"&lt;br/&gt;
&lt;a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=5384831" class="external-link"&gt;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=5384831&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Sorry, I couldn't find a free public location for the second paper.)&lt;/p&gt;</comment>
                    <comment id="13006812" author="epitschke" created="Tue, 15 Mar 2011 06:43:49 +0000">&lt;p&gt;Hi Steven,&lt;br/&gt;
thanks for the info, i will work through it and get back here with some questions.&lt;br/&gt;
As i have a lot to do with Lucene at my work, this filter would definitely something that i could use. So the work would not be lost &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;br/&gt;
Regards&lt;br/&gt;
   Elmar&lt;/p&gt;</comment>
                    <comment id="13007229" author="steve_rowe" created="Tue, 15 Mar 2011 22:12:40 +0000">&lt;blockquote&gt;&lt;p&gt;this filter would definitely something that i could use&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What use case(s) are you thinking of?&lt;/p&gt;</comment>
                    <comment id="13007837" author="epitschke" created="Thu, 17 Mar 2011 06:52:40 +0000">&lt;p&gt;The first use case that comes into my mind is the filtering of possible names. One of the request i always get is the automatic generation of tag-clouds with a consideration in the search results. I think this would be one possibility to get names without the need to maintain a word list.&lt;br/&gt;
Another thing of course would be to get some kind of semantic combination of words. So you could get to more "natural" search experience. I think if a user search for two words and these are quite near in a text it may be more useful than a lot of occurances of the two words but with no combination.&lt;br/&gt;
Which use cases do you have in mind?&lt;/p&gt;</comment>
                    <comment id="13008353" author="steve_rowe" created="Fri, 18 Mar 2011 07:05:01 +0000">&lt;blockquote&gt;&lt;p&gt;Which use cases do you have in mind? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So far just the solution I proposed in the email thread mentioned in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2749?focusedCommentId=13006254&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13006254" class="external-link"&gt;my previous comment&lt;/a&gt; and the P2P distributed search use case described in the two papers mentioned in the same comment.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 13 Mar 2011 17:04:53 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11102</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24937</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2738] improve test coverage for omitNorms and omitTFAP</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2738</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;just expands on what lucenetestcase does already...&lt;/p&gt;

&lt;p&gt;if you say Analyzed_NO_NORMS, we might set norms anyway.&lt;br/&gt;
in the same sense, if you say Index.NO, we might index it anyway, and might set omitTFAP etc.&lt;/p&gt;</description>
                <environment/>
            <key id="12479077">LUCENE-2738</key>
            <summary>improve test coverage for omitNorms and omitTFAP</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 4 Nov 2010 10:54:51 +0000</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12928176" author="rcmuir" created="Thu, 4 Nov 2010 10:56:25 +0000">&lt;p&gt;here's the start to a patch.&lt;/p&gt;

&lt;p&gt;worried about one fail, either i made a mistake here i don't see, or the test shouldn't be failing:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;ant test-core -Dtestcase=TestIndexWriter -Dtestmethod=testTermVectorCorruption2 -Dtests.seed=8395558104679823604:-6279799097172774748
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12928177" author="rcmuir" created="Thu, 4 Nov 2010 10:58:43 +0000">&lt;p&gt;nevermind, that was my problem.... heres a fixed patch.&lt;/p&gt;

&lt;p&gt;my coffee IV is not fully running yet.&lt;/p&gt;</comment>
                    <comment id="12928189" author="rcmuir" created="Thu, 4 Nov 2010 12:06:22 +0000">&lt;p&gt;ok, here's a final patch... all tests pass (at least a few times).&lt;/p&gt;

&lt;p&gt;I also improved some of the better tests, if they dont need norms to use _NO_NORMS,&lt;br/&gt;
and to explicitly randomly set OmitTFAP&lt;/p&gt;</comment>
                    <comment id="12982607" author="rcmuir" created="Mon, 17 Jan 2011 11:47:53 +0000">&lt;p&gt;Mike just reminded me about this one:&lt;br/&gt;
My concern for not committing is that we would actually reduce test coverage,&lt;br/&gt;
because most tests will create say field "foobar" in a loop like this:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;for (....) {
   newField("foobar"....);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So because removing norms/omitTFAP is infectious, i think we will end out&lt;br/&gt;
only testing certain cases... unless we change the patch so that this random value&lt;br/&gt;
is "remembered" per field name during the length of the test... i think thats the&lt;br/&gt;
right solution (adding hashmap)&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12458810" name="LUCENE-2738.patch" size="15400" author="rcmuir" created="Thu, 4 Nov 2010 12:06:22 +0000"/>
                    <attachment id="12458806" name="LUCENE-2738.patch" size="3355" author="rcmuir" created="Thu, 4 Nov 2010 10:58:43 +0000"/>
                    <attachment id="12458805" name="LUCENE-2738.patch" size="2709" author="rcmuir" created="Thu, 4 Nov 2010 10:56:25 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2919</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24948</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2735] First Cut at GroupVarInt with FixedIntBlockIndexInput / Output</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2735</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I have hacked together a FixedIntBlockIndex impl with Group VarInt encoding - this does way worse than standard codec in benchmarks but I guess that is mainly due to the FixedIntBlockIndex limitations. Once &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2723" title="Speed up Lucene&amp;#39;s low level bulk postings read API"&gt;&lt;del&gt;LUCENE-2723&lt;/del&gt;&lt;/a&gt; is in / or builds with trunk again I will update and run some tests. The isolated microbenchmark shows that there could be improvements over vint even in java though and I am sure we can make it faster impl. wise.&lt;/p&gt;</description>
                <environment/>
            <key id="12478915">LUCENE-2735</key>
            <summary>First Cut at GroupVarInt with FixedIntBlockIndexInput / Output</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 2 Nov 2010 18:06:11 +0000</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12927515" author="simonw" created="Tue, 2 Nov 2010 18:06:53 +0000">&lt;p&gt;here is a rough patch&lt;/p&gt;</comment>
                    <comment id="12927565" author="yseeley@gmail.com" created="Tue, 2 Nov 2010 19:41:10 +0000">&lt;p&gt;Here's an update that removes array lookups in favor of calculating the lengths, in addition to a bunch of other little optimizations that may or may not matter (such as removing unnecessary masks, and checking single-byte values first rather than 4 byte values during encoding).&lt;/p&gt;

&lt;p&gt;The lookup table would have taken substantial memory: 256*(64+4*4) == 20K and would have taken up a good fraction of L1 cache (perhaps not detectable in a micro-benchmark, but perhaps significant in a full application).&lt;/p&gt;

&lt;p&gt;Anyway - I made no attempt at benchmarking it, so YMMV.&lt;/p&gt;</comment>
                    <comment id="12927597" author="mikemccand" created="Tue, 2 Nov 2010 21:08:11 +0000">&lt;p&gt;I made a codec for GVint (attached) but a few tests fail with spooky exceptions, eg TestPhraseQuery.testRandomPhrases and TestCodecs.testRandomPostings and TestIndicesEquals.testInstantiatedIndexWriter (in contrib/instantiated which, somehow, is really good at finding sneaky codec problems!).&lt;/p&gt;</comment>
                    <comment id="12927635" author="toke" created="Tue, 2 Nov 2010 21:51:36 +0000">&lt;p&gt;I tried making an extra test in GVintMicroBenchmark that created the same output as groupintsTest but just read all the bytes directly back, no decoding, using IndexOutput. It's called GroupVarIntRead in the output below and as can be seen, most of the processing seems to take place outside of GVint decoding. Sorry no patch, as I messed up the formatting.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
Running 4 Million random ints with max value: 64
GroupVarInt time per value: 4ns - time to decode 4M ints: 18ms
GroupVarIntRead time per value: 3ns - time to decode 4M ints: 15ms
Vint time per value: 6ns - time to decode 4M ints: 25ms

Running 4 Million random ints with max value: 1024
GroupVarInt time per value: 9ns - time to decode 4M ints: 39ms
GroupVarIntRead time per value: 5ns - time to decode 4M ints: 23ms
Vint time per value: 11ns - time to decode 4M ints: 44ms

Running 4 Million random ints with max value: 524288
GroupVarInt time per value: 12ns - time to decode 4M ints: 51ms
GroupVarIntRead time per value: 9ns - time to decode 4M ints: 38ms
Vint time per value: 14ns - time to decode 4M ints: 56ms

Running 4 Million random ints with max value: 67108864
GroupVarInt time per value: 14ns - time to decode 4M ints: 59ms
GroupVarIntRead time per value: 11ns - time to decode 4M ints: 45ms
Vint time per value: 15ns - time to decode 4M ints: 61ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12927783" author="simonw" created="Wed, 3 Nov 2010 08:35:14 +0000">&lt;blockquote&gt;&lt;p&gt;The lookup table would have taken substantial memory: 256*(64+4*4) == 20K and would have taken up a good fraction of L1 cache (perhaps not detectable in a micro-benchmark, but perhaps significant in a full application).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks yonik, I had a similar version before without a table and the perf was somewhat the same. I agree that this is unnecessary! &lt;/p&gt;

&lt;p&gt;Yet, some of you "cleanups" didn't do any good though &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; that (b=(byte)(current&amp;gt;&amp;gt;&amp;gt;16))!=0 only checks if there is a bit set between bit 16 and 24 due to the case. I will upload a new version in a second with a better test.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; I made a codec for GVint (attached) but a few tests fail with spooky exceptions, eg TestPhraseQuery.testRandomPhrases and TestCodecs.testRandomPostings and TestIndicesEquals.testInstantiatedIndexWriter (in contrib/instantiated which, somehow, is really good at finding sneaky codec problems!).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess that is because the second patch didn't really work though. &lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I tried making an extra test in GVintMicroBenchmark that created the same output as groupintsTest but just read all the bytes directly back, no decoding, using IndexOutput. It's called GroupVarIntRead in the output below and as can be seen, most of the processing seems to take place outside of GVint decoding. Sorry no patch, as I messed up the formatting.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Toke thanks for bringing this up. I run a slightly modified benchmark with a profiler attached to it using IntIndexInput directly one with GVint and one with VInt and guess what the damn hottest method is? Thread.interrupt() takes 77% of the time.&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;Name&lt;/td&gt;
&lt;td class='confluenceTd'&gt;Time (ms)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.index.codecs.gvint.GVintMicroBenchmark.benchRead(int[][], IntStreamFactory)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;218692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;java.lang.Thread.isInterrupted(boolean)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;78749&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.index.codecs.gvint.GVintMicroBenchmark$VIntFactory$2.flushBlock()&lt;/td&gt;
&lt;td class='confluenceTd'&gt;59180&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;&lt;span class="error"&gt;&amp;#91;Wall Time&amp;#93;&lt;/span&gt;  java.io.RandomAccessFile.writeBytes(byte[], int, int)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;57237&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.index.codecs.gvint.GVintMicroBenchmark$VIntFactory$1$1.readBlock()&lt;/td&gt;
&lt;td class='confluenceTd'&gt;55721&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.store.DataInput.readVInt()&lt;/td&gt;
&lt;td class='confluenceTd'&gt;54652&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.store.DataOutput.writeBytes(byte[], int)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;49749&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.index.codecs.gvint.GVintIndexInput$GVintBlockReader.readBlock()&lt;/td&gt;
&lt;td class='confluenceTd'&gt;45956&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.store.DataOutput.writeVInt(int)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;43402&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;org.apache.lucene.index.codecs.gvint.GVintIndexInput.readGroupInt(int, IndexInput)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;39851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;sun.nio.ch.FileDispatcher.pread0(FileDescriptor, long, int, long)&lt;/td&gt;
&lt;td class='confluenceTd'&gt;16054&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I also run the updated benchmarks - here are some numbers:&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;Max random value&lt;/td&gt;
&lt;td class='confluenceTd'&gt;GVint ns / value&lt;/td&gt;
&lt;td class='confluenceTd'&gt;Vint ns/value&lt;/td&gt;
&lt;td class='confluenceTd'&gt;GVint total in ms&lt;/td&gt;
&lt;td class='confluenceTd'&gt;Vint total in ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;15&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21&lt;/td&gt;
&lt;td class='confluenceTd'&gt;80&lt;/td&gt;
&lt;td class='confluenceTd'&gt;86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;31&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20&lt;/td&gt;
&lt;td class='confluenceTd'&gt;81&lt;/td&gt;
&lt;td class='confluenceTd'&gt;82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;63&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19&lt;/td&gt;
&lt;td class='confluenceTd'&gt;21&lt;/td&gt;
&lt;td class='confluenceTd'&gt;79&lt;/td&gt;
&lt;td class='confluenceTd'&gt;84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;127&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19&lt;/td&gt;
&lt;td class='confluenceTd'&gt;20&lt;/td&gt;
&lt;td class='confluenceTd'&gt;78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;255&lt;/td&gt;
&lt;td class='confluenceTd'&gt;19&lt;/td&gt;
&lt;td class='confluenceTd'&gt;35&lt;/td&gt;
&lt;td class='confluenceTd'&gt;78&lt;/td&gt;
&lt;td class='confluenceTd'&gt;141&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;511&lt;/td&gt;
&lt;td class='confluenceTd'&gt;34&lt;/td&gt;
&lt;td class='confluenceTd'&gt;29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;138&lt;/td&gt;
&lt;td class='confluenceTd'&gt;118&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;1023&lt;/td&gt;
&lt;td class='confluenceTd'&gt;37&lt;/td&gt;
&lt;td class='confluenceTd'&gt;29&lt;/td&gt;
&lt;td class='confluenceTd'&gt;151&lt;/td&gt;
&lt;td class='confluenceTd'&gt;118&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

</comment>
                    <comment id="12927784" author="simonw" created="Wed, 3 Nov 2010 08:39:29 +0000">&lt;p&gt;here is a new patch with fixed testcase and updated micro benchmark&lt;/p&gt;</comment>
                    <comment id="12927837" author="rcmuir" created="Wed, 3 Nov 2010 12:49:42 +0000">&lt;blockquote&gt;&lt;p&gt;(in contrib/instantiated which, somehow, is really good at finding sneaky codec problems!).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;we should really try to 'capture' the logic this thing uses, in the core tests.&lt;br/&gt;
I assume its TestIndicesEquals?&lt;/p&gt;

&lt;p&gt;Maybe we can write a similar test: build two "equivalent" indexes (one with TestExternalCodecs.RAMOnlyCodec?)&lt;br/&gt;
and compare them like this test does.&lt;/p&gt;</comment>
                    <comment id="12927896" author="mikemccand" created="Wed, 3 Nov 2010 16:41:45 +0000">&lt;p&gt;OK w/ the latest patch all tests pass for me!  Great &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12927902" author="simonw" created="Wed, 3 Nov 2010 16:56:49 +0000">&lt;blockquote&gt;&lt;p&gt;OK w/ the latest patch all tests pass for me! Great &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;awesome! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12972993" author="rcmuir" created="Sun, 19 Dec 2010 14:24:42 +0000">&lt;p&gt;Simon, maybe you want to commit this to the bulkpostings branch with the other block codecs?&lt;/p&gt;</comment>
                    <comment id="12972996" author="simonw" created="Sun, 19 Dec 2010 14:52:54 +0000">&lt;blockquote&gt;&lt;p&gt;Simon, maybe you want to commit this to the bulkpostings branch with the other block codecs?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;yeah I should do so - I hope I get to it during the week...&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12458666" name="LUCENE-2735_alt.patch" size="26806" author="yseeley@gmail.com" created="Tue, 2 Nov 2010 19:41:10 +0000"/>
                    <attachment id="12458723" name="LUCENE-2735.patch" size="25196" author="simonw" created="Wed, 3 Nov 2010 08:39:29 +0000"/>
                    <attachment id="12458676" name="LUCENE-2735.patch" size="6580" author="mikemccand" created="Tue, 2 Nov 2010 21:08:10 +0000"/>
                    <attachment id="12458659" name="LUCENE-2735.patch" size="31869" author="simonw" created="Tue, 2 Nov 2010 18:06:53 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 2 Nov 2010 19:41:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11115</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24951</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2726] simulate disk fulls in copyBytes</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2726</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2637" title="FSDirectory.copyBytes isn&amp;#39;t safe for SimpleFSDirectory"&gt;&lt;del&gt;LUCENE-2637&lt;/del&gt;&lt;/a&gt;, i disabled copyBytes optimization (so it just calls writeBytes), but i mentioned there,&lt;br/&gt;
that I think it would be good to try to beef up tests if we ever wanted to have an optimization like this.&lt;/p&gt;

&lt;p&gt;the problem was that when there was an index corruption bug, it was very difficult to detect with our tests.&lt;br/&gt;
So I think for safety, we should do this, even though its redundant with our current impl, since we look&lt;br/&gt;
for this in writeBytes.&lt;/p&gt;</description>
                <environment/>
            <key id="12478572">LUCENE-2726</key>
            <summary>simulate disk fulls in copyBytes</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 28 Oct 2010 14:24:49 +0100</created>
                <updated>Fri, 10 May 2013 00:05:24 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12925800" author="rcmuir" created="Thu, 28 Oct 2010 14:26:31 +0100">&lt;p&gt;here's a patch, uses the same logic to check as writeBytes does.&lt;/p&gt;</comment>
                    <comment id="12925806" author="rcmuir" created="Thu, 28 Oct 2010 14:34:09 +0100">&lt;p&gt;So the question here, is how to account for the fact that copyBytes might call writeBytes (like it does today), and not double-count the bytes for disk full.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12458248" name="LUCENE-2726.patch" size="1758" author="rcmuir" created="Thu, 28 Oct 2010 14:26:31 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2924</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>24960</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2684] it's not possible to access sub-query's freq information if BooleanScorer is use</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2684</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2590" title="Enable access to the freq information in a Query&amp;#39;s sub-scorers"&gt;&lt;del&gt;LUCENE-2590&lt;/del&gt;&lt;/a&gt; added an advanced feature, allowing an app to gather all sub-scorers for any Query.&lt;/p&gt;

&lt;p&gt;This is powerful because then, during collection, the app can get some details about how each sub-query "participated" in the overall match for the given document.&lt;/p&gt;

&lt;p&gt;However, I think this is completely broken if the BooleanQuery uses BooleanScorer, because that scorer is not doc-at-once.  Instead, it batch processes chunks of 2048 sequential docIDs per scorer.  This is a big performance gain, but it means that the sub scorers will all be positioned to the end of the 2048 doc chunk while the docs that matched within that chunk are collected.&lt;/p&gt;

&lt;p&gt;I don't think we can easily fix this... likely the "fix" is to make it easy(ier) to force BQ to use BooleanScorer2 (which is doc-at-once)?  It is actually possible to force this, today, by having your collector return false from acceptDocsOutOfOrder...&lt;/p&gt;</description>
                <environment/>
            <key id="12475745">LUCENE-2684</key>
            <summary>it's not possible to access sub-query's freq information if BooleanScorer is use</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 4 Oct 2010 10:41:03 +0100</created>
                <updated>Fri, 10 May 2013 00:05:25 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12985322" author="mikemccand" created="Sun, 23 Jan 2011 13:39:47 +0000">&lt;p&gt;Clearing 3.1 fix version... it's not clear how we can fix this w/o drastic API changes...&lt;/p&gt;</comment>
                    <comment id="13453199" author="rcmuir" created="Tue, 11 Sep 2012 18:24:30 +0100">&lt;blockquote&gt;
&lt;p&gt;It is actually possible to force this, today, by having your collector return false from acceptDocsOutOfOrder...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well you are using a custom collector anyway if you are doing this, so can't we just add a sentence to that&lt;br/&gt;
method's javadocs indicating that you should return false if you want to use the scorer navigation apis?&lt;/p&gt;</comment>
                    <comment id="13453218" author="thetaphi" created="Tue, 11 Sep 2012 18:41:29 +0100">&lt;p&gt;I think this issue is fixed already? VisitSubScorers works in 3.6.2 (if it gets released, Robert backported) and in 4.0 its working, too?&lt;/p&gt;

&lt;p&gt;As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?&lt;/p&gt;</comment>
                    <comment id="13453223" author="rcmuir" created="Tue, 11 Sep 2012 18:43:50 +0100">&lt;blockquote&gt;
&lt;p&gt;As you need a custom collector anyway to make use of Scorer.getChildren(), we should maybe make BS1 throw UOE on getChildren() in 4.0 (explaining that you need inOrder) and visitSubScorers in 3.6.2?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, i think for freq() and getChildren() we should throw UOE with text like this. But we can also do the javadocs too.&lt;/p&gt;

&lt;p&gt;Then i think there would be a lot less surprises.&lt;/p&gt;</comment>
                    <comment id="13453238" author="mikemccand" created="Tue, 11 Sep 2012 19:07:17 +0100">&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;But we should word it as a "workaround" ... ie, it's sort of strange that returning false from this unrelated method means suddenly scorer.freq() works: that's really an implementation detail.  EG someday we could make BS1 score docs in order (it is possible, just not sure it'd be performant), and then this workaround no longer works.&lt;/p&gt;</comment>
                    <comment id="13453245" author="thetaphi" created="Tue, 11 Sep 2012 19:13:58 +0100">&lt;p&gt;It does not only affect freq(). In my case it was "retrieving the subquery score"...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;EG someday we could make BS1 score docs in order (it is possible, just not sure it'd be performant), and then this workaround no longer works.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But with in-order scoring we are in all cases use correctly positioned scorers, otherwise it is a bug (like the DisjunctionSumScorer bug in 3.6 and 4.0 we fixed recently). So returning "false" works around the issue currently, but it would not hurt if somebody would return false, although our new BS1 can handle in order. But on the other hand, if BS1 would score in order, but not position sub-scorers correctly it is clearly a bug!&lt;/p&gt;</comment>
                    <comment id="13453246" author="rcmuir" created="Tue, 11 Sep 2012 19:14:05 +0100">&lt;blockquote&gt;
&lt;p&gt;But we should word it as a "workaround" ... ie, it's sort of strange that returning false from this unrelated method means suddenly scorer.freq() works: that's really an implementation detail. EG someday we could make BS1 score docs in order (it is possible, just not sure it'd be performant), and then this workaround no longer works.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't agree: the strangeness is the two booleans toplevelScorer and scoreDocsInOrder. If we wanted to do this in the future, we could just rename scoreDocsInOrder&lt;br/&gt;
to needsNavigation. &lt;/p&gt;

&lt;p&gt;Or we could just fold both the booleans into 'BS1 is ok' ... are they used anywhere else? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13453259" author="thetaphi" created="Tue, 11 Sep 2012 19:25:49 +0100">&lt;p&gt;An idea (separate issue!) would be:&lt;br/&gt;
BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should &lt;b&gt;not&lt;/b&gt; implement the Scorer interface, that the whole bug! It should maybe some separate class like OutOfOrderDocIdReporter (name is just an example) that only implements collect(Collector). And the navigation api (advance, next) should be separated from score() and freq() - a simple java interface Scorer. So the current in-order scorer would be a simple DocIdSetIterator that additionally implements the Scorer interface (to provide score() and freq()) and current out-of-order scorers would implement only the OutOfOrderDocIdReporter API and pass a inlined Scorer interface (without advance and next) to the setScorer() method (like BucketScorer currently).&lt;/p&gt;</comment>
                    <comment id="13453261" author="rcmuir" created="Tue, 11 Sep 2012 19:26:58 +0100">&lt;p&gt;Collectible... (not serious)&lt;/p&gt;</comment>
                    <comment id="13453262" author="mikemccand" created="Tue, 11 Sep 2012 19:28:00 +0100">&lt;p&gt;The problem is that "scoresDocsInOrder" doesn't really capture what's necessary here (yes, it works today, but, not necessarily tomorrow....).&lt;/p&gt;

&lt;p&gt;I agree Uwe: if we add a Collector.needsNavigation() then even a "fixed" BS1 that sorted the docIDs before collection would not be usable since the subs will not be "on" the doc during collect().&lt;/p&gt;

&lt;p&gt;And I agree Robert: the current booleans "topLevelScorer" and "scoreDocsInOrder", and then a new "needsNavigation", will make things rather confusing.  Really I think topLevelScorer should be strongly typed: the intent is to declare whether you will call Scorer.score(Collector) or whether you will call .nextDoc()/.score() ... they really should be different classes.&lt;/p&gt;

&lt;p&gt;If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation?  (Or scoreDocAtOnce, scoreDocAtATime, something else...).&lt;/p&gt;</comment>
                    <comment id="13453269" author="rcmuir" created="Tue, 11 Sep 2012 19:32:27 +0100">&lt;blockquote&gt;
&lt;p&gt;If we don't think any other future scorer would want to score docs NOT in order ... then maybe we should simple rename scoreDocsInOrder to needsNavigation? (Or scoreDocAtOnce, scoreDocAtATime, something else...).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I actually just remembered the query-time join i think does this too?&lt;/p&gt;

&lt;p&gt;But yeah, if we are going to have booleans, i would prefer something more along the lines of document-at-a-time since its less confusing than&lt;br/&gt;
scoreDocsInOrder (its standard IR terminology and less confusing).&lt;/p&gt;</comment>
                    <comment id="13453274" author="mikemccand" created="Tue, 11 Sep 2012 19:37:10 +0100">&lt;blockquote&gt;&lt;p&gt;BS1 completely violates the scorer interface, the only method you can call is the one taking a Collector. In my opinion, BS1 should not implement the Scorer interface, that the whole bug!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well let's remember that the "must have doc-at-once scoring, for all subs too" is a very rare use-case.&lt;/p&gt;

&lt;p&gt;The vast majority of users just need a fast .score(Collector) interface.&lt;/p&gt;

&lt;p&gt;But yeah I agree: it should be strongly typed, and BS1 should only implement the .score(Collector) interface.  The ScoresDocAtOnce interface can easily implement the .score(Collector) interface (as Scorer does today...).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 11 Sep 2012 17:24:30 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2936</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25002</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2679] IndexWriter.deleteDocuments should have option to not apply to docs indexed in the current IW session</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2679</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2655" title="Get deletes working in the realtime branch"&gt;&lt;del&gt;LUCENE-2655&lt;/del&gt;&lt;/a&gt; we are struggling with how to handle buffered deletes,&lt;br/&gt;
with the new per-thread RAM buffers (DWPT).&lt;/p&gt;

&lt;p&gt;But, the only reason why we must maintain a map of del term -&amp;gt; current&lt;br/&gt;
docID (or sequence ID) is to correctly handle the interleaved adds &amp;amp;&lt;br/&gt;
deletes case.&lt;/p&gt;

&lt;p&gt;However, I suspect that for many apps that interleaving never happens.&lt;br/&gt;
Ie, most apps delete only docs from &lt;b&gt;before&lt;/b&gt; the last commit or NRT&lt;br/&gt;
reopen.  For such apps, we don't need a Map... we just need a Set of&lt;br/&gt;
all del terms to apply to past segments but not to the currently&lt;br/&gt;
buffered docs.&lt;/p&gt;

&lt;p&gt;And, importantly, with &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2655" title="Get deletes working in the realtime branch"&gt;&lt;del&gt;LUCENE-2655&lt;/del&gt;&lt;/a&gt;, this would be a single Set, not&lt;br/&gt;
one per DWPT.  It should be a a healthy RAM reduction on buffered&lt;br/&gt;
deletes, and should make the deletes call faster (add to one set instead of&lt;br/&gt;
N maps).&lt;/p&gt;

&lt;p&gt;We of course must still support the interleaved case, and I think it&lt;br/&gt;
should be the default, but I think we should provide the option for&lt;br/&gt;
the common-case apps to take advantage of much less RAM usage.&lt;/p&gt;</description>
                <environment/>
            <key id="12475635">LUCENE-2679</key>
            <summary>IndexWriter.deleteDocuments should have option to not apply to docs indexed in the current IW session</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Oct 2010 17:21:58 +0100</created>
                <updated>Fri, 10 May 2013 00:05:25 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2926</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25007</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2661] fold in test cases from Lucene in Action 2nd edition</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2661</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Manning Publications Co., publisher of Lucene in Action, 2nd edition&lt;br/&gt;
(&lt;a href="http://manning.com/lucene" class="external-link"&gt;http://manning.com/lucene&lt;/a&gt;) wishes to donate all of the book's source&lt;br/&gt;
code, to fold into Lucene's tests!&lt;/p&gt;

&lt;p&gt;It'll take some iterating to get the tests folded in... I'll attach&lt;br/&gt;
the initial patch.&lt;/p&gt;</description>
                <environment/>
            <key id="12474740">LUCENE-2661</key>
            <summary>fold in test cases from Lucene in Action 2nd edition</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 Sep 2010 19:54:16 +0100</created>
                <updated>Fri, 10 May 2013 00:05:25 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/test</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12913175" author="mikemccand" created="Tue, 21 Sep 2010 19:59:37 +0100">&lt;p&gt;Initial raw patch, just puts the sources under src/liatests, as a placeholder.  We need to move these under the src/test/* tree...&lt;/p&gt;</comment>
                    <comment id="12913185" author="paul.elschot@xs4all.nl" created="Tue, 21 Sep 2010 20:06:50 +0100">&lt;p&gt;The surround test cases have already been done in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1563" title="Add example test case for surround query language"&gt;&lt;del&gt;LUCENE-1563&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12913215" author="mikemccand" created="Tue, 21 Sep 2010 21:10:42 +0100">&lt;p&gt;Forgot to include the books data (used to create the test index used by many tests).&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12474959">LUCENE-2663</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12455184" name="LUCENE-2661-books.patch" size="9840" author="mikemccand" created="Tue, 21 Sep 2010 21:10:42 +0100"/>
                    <attachment id="12455175" name="LUCENE-2661.patch" size="672213" author="mikemccand" created="Tue, 21 Sep 2010 19:59:36 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 21 Sep 2010 19:06:50 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2907</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25025</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2605] queryparser parses on whitespace</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2605</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The queryparser parses input on whitespace, and sends each whitespace separated term to its own independent token stream.&lt;/p&gt;

&lt;p&gt;This breaks the following at query-time, because they can't see across whitespace boundaries:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;n-gram analysis&lt;/li&gt;
	&lt;li&gt;shingles&lt;/li&gt;
	&lt;li&gt;synonyms (especially multi-word for whitespace-separated languages)&lt;/li&gt;
	&lt;li&gt;languages where a 'word' can contain whitespace (e.g. vietnamese)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Its also rather unexpected, as users think their charfilters/tokenizers/tokenfilters will do the same thing at index and querytime, but&lt;br/&gt;
in many cases they can't. Instead, preferably the queryparser would parse around only real 'operators'.&lt;/p&gt;</description>
                <environment/>
            <key id="12471774">LUCENE-2605</key>
            <summary>queryparser parses on whitespace</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 Aug 2010 04:30:26 +0100</created>
                <updated>Fri, 10 May 2013 00:05:25 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>12</votes>
                        <watches>20</watches>
                                                    <comments>
                    <comment id="13105979" author="shenzhuxi" created="Fri, 16 Sep 2011 11:18:13 +0100">&lt;p&gt;subscribed &lt;/p&gt;</comment>
                    <comment id="13139429" author="hossman" created="Sat, 29 Oct 2011 21:31:50 +0100">&lt;p&gt;since (unescaped, unquoted) whitespace characters is the syntax that QueryParser uses to indicate the transition between clauses in a BooleanQuery, changing this (either in QueryParser or in some new query parser) would require coming up with some new syntax.  (or in the case of a special case query parser like the FieldQParser in Solr, eliminating the possibility of expressing multi-clause queries)&lt;/p&gt;</comment>
                    <comment id="13292620" author="berryman" created="Mon, 11 Jun 2012 03:44:48 +0100">&lt;p&gt;subscribed - Current client has index full of clothing - a search for "dress shoes" will return results containing womens' dresses and running shoes. That's not really acceptable.&lt;/p&gt;</comment>
                    <comment id="13293722" author="berryman" created="Tue, 12 Jun 2012 17:08:38 +0100">&lt;p&gt;There is somewhat of a workaround for this for defType=lucene. Just escape every whitespace with a slash. So instead of &lt;b&gt;&lt;tt&gt;new dress shoes&lt;/tt&gt;&lt;/b&gt; search for &lt;b&gt;&lt;tt&gt;new\ dress\ shoes&lt;/tt&gt;&lt;/b&gt;. Of course you lose the ability to use normal lucene syntax.&lt;/p&gt;

&lt;p&gt;I was hoping that this workaround would also work for defType=dismax, but with or without the escaped whitespace, queries get interpreted the same, incorrect way. For instance, assume I have the following line in my synonyms.txt: &lt;b&gt;&lt;tt&gt;dress shoes =&amp;gt; dress_shoes&lt;/tt&gt;&lt;/b&gt;. Further assume that I have a field &lt;b&gt;&lt;tt&gt;experiment&lt;/tt&gt;&lt;/b&gt; that gets analysed with synonyms. A search for &lt;b&gt;&lt;tt&gt;new dress shoes&lt;/tt&gt;&lt;/b&gt; (with or without escaped spaces) will be interpreted as &lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;tt&gt;+((experiment:new)~0.01 (experiment:dress)~0.01 (experiment:shoes)~0.01) (experiment:"new dress_shoes"~3)~0.01&lt;/tt&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The first clause is manditory and contains independently analysed tokens, so this will only match documents that contain "dress", "new", or "shoes", but never "dress shoes" because analysis takes place as expected at index time.&lt;/p&gt;</comment>
                    <comment id="13293748" author="jkrupan" created="Tue, 12 Jun 2012 17:33:52 +0100">&lt;p&gt;My thought on the original issue is that most query parsers should accumulate adjacent terms without intervening operators as a "term list" (quoted phrases would be a second level of term list) and that there needs to be a "list" interface for query term analysis.&lt;/p&gt;

&lt;p&gt;Rather than simply present a raw text stream for the sequence/list of terms, each term would be fed into the token stream with an attribute that indicates which source term it belongs to.&lt;/p&gt;

&lt;p&gt;The synonym processor would see a clean flow of terms and do its processing, but would also need to associate an id with each term of a multi-term synonym phrase so that multiple multi-word synonym choices for the same input term(s) don't get mixed up (i.e., multiple tokens at the same position with no indication of which original synonym phrase they came from).&lt;/p&gt;

&lt;p&gt;By having those ID's for each multi-term synonym phrase, the caller of the list analyzer could then recontruct the tree of "OR" expressions for the various multi-term synonym phrases.&lt;/p&gt;</comment>
                    <comment id="13293940" author="berryman" created="Tue, 12 Jun 2012 22:41:21 +0100">&lt;p&gt;(How's it going Jack) Interesting idea, though I really need to crack into the QueryParser and play around a little bit before I have a strong opinion myself.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12629811">SOLR-4381</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 16 Sep 2011 10:18:13 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11229</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25081</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2585] DirectoryReader.isCurrent might fail to see the segments file during concurrent index changes</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2585</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I could reproduce the issue several times but only by running long and stressfull benchmarks, the high number of files is likely part of the scenario.&lt;br/&gt;
All tests run on local disk, using ext3.&lt;/p&gt;

&lt;p&gt;Sample stacktrace:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;java.io.FileNotFoundException: no segments* file found in org.apache.lucene.store.NIOFSDirectory@/home/sanne/infinispan-41/lucene-directory/tempIndexName: files:
_2l3.frq _uz.fdt _1q4.fnm _1q0.fdx _4bc.fdt _v2.tis _4ll.fdx _2l8.tii _ux.fnm _3g7.fdx _4bb.tii _4bj.prx _uy.fdx _3g7.prx _2l7.frq _2la.fdt _3ge.nrm _2l6.prx 
_1py.fdx _3g6.nrm _v0.prx _4bi.tii _2l2.tis _v2.fdx _2l3.nrm _2l8.fnm _4bg.tis _2la.tis _uu.fdx _3g6.fdx _1q3.frq _2la.frq _4bb.tis _3gb.tii _1pz.tis 
_2lb.nrm _4lm.nrm _3g9.tii _v0.fdt _2l5.fnm _v2.prx _4ll.tii _4bd.nrm _2l7.fnm _2l4.nrm _1q2.tis _3gb.fdx _4bh.fdx _1pz.nrm _ux.fdx _ux.tii _1q6.nrm 
_3gf.fdx _4lk.fdt _3gd.nrm _v3.fnm _3g8.prx _1q2.nrm _4bh.prx _1q0.frq _ux.fdt _1q7.fdt _4bb.fnm _4bf.nrm _4bc.nrm _3gb.fdt _4bh.fnm _2l5.tis 
_1pz.fnm _1py.fnm _3gc.fnm _2l2.prx _2l4.frq _3gc.fdt _ux.tis _1q3.prx _2l7.fdx _4bj.nrm _4bj.fdx _4bi.tis _3g9.prx _1q4.prx _v3.fdt _1q3.fdx _2l9.fdt 
_4bh.tis _3gb.nrm _v2.nrm _3gd.tii _2l7.nrm _2lb.tii _4lm.tis _3ga.fdx _1pz.fdt _3g7.fnm _2l3.fnm _4lk.fnm _uz.fnm _2l2.frq _4bd.fdx _1q2.fdt _3g7.tis 
_4bi.frq _4bj.frq _2l7.prx _ux.prx _3gd.fnm _1q4.fdt _1q1.fdt _v1.fnm _1py.nrm _3gf.nrm _4be.fdt _1q3.tii _1q1.prx _2l3.fdt _4lk.frq _2l4.fdx _4bd.fnm 
_uw.frq _3g8.fdx _2l6.tii _1q5.frq _1q5.tis _3g8.nrm _uw.nrm _v0.tii _v2.fdt _2l7.fdt _v0.tis _uy.tii _3ge.tii _v1.tii _3gb.tis _4lm.fdx _4bc.fnm _2lb.frq 
_2l6.fnm _3g6.tii _3ge.prx _uu.frq _1pz.fdx _1q2.fnm _4bi.prx _3gc.frq _2l9.tis _3ge.fdt _uy.fdt _4ll.fnm _3gc.prx _1q7.tii _2l5.nrm _uy.nrm _uv.frq 
_1q6.frq _4ba.tis _3g9.tis _4be.nrm _4bi.fnm _ux.frq _1q1.fnm _v0.fnm _2l4.fnm _4ba.fnm _4be.tis _uz.prx _1q6.fdx _uw.tii _2l6.nrm _1pz.prx _2l7.tis 
_1q7.fdx _2l9.tii _4lk.tii _uz.frq _3g8.frq _4bb.prx _1q5.tii _1q5.prx _v2.frq _4bc.tii _1q7.prx _v2.tii _2lb.tis _4bi.fdt _uv.nrm _2l2.fnm _4bd.tii _1q7.tis 
_4bg.fnm _3ga.frq _uu.fnm _2l9.fnm _3ga.fnm _uw.fnm _1pz.frq _1q1.fdx _3ge.fdx _2l3.prx _3ga.nrm _uv.fdt _4bb.nrm _1q7.fnm _uv.tis _3gb.fnm 
_2l6.tis _1pz.tii _uy.fnm _3gf.fdt _3gc.nrm _4bf.tis _1q5.fnm _uu.tis _4bh.tii _2l5.fdt _1q6.tii _4bc.tis _3gc.tii _3g9.fnm _2l6.fdt _4bj.fnm _uu.tii _v3.frq 
_3g9.fdx _v0.nrm _2l7.tii _1q0.fdt _3ge.fnm _4bf.fdt _1q6.prx _uz.nrm _4bi.fdx _3gf.fnm _4lm.frq _v0.fdx _4ba.fdt _1py.tii _4bf.tii _uw.fdx _2l5.frq 
_3g9.nrm _v1.fdt _uw.fdt _4bd.frq _4bg.prx _3gd.tis _1q4.tis _2l9.nrm _2la.nrm _v3.tii _4bf.prx _1q1.nrm _4ba.tii _3gd.fdx _1q4.tii _4lm.tii _3ga.tis 
_4bf.fnm write.lock _2l8.prx _2l8.fdt segments.gen _2lb.fnm _2l4.fdt _1q2.prx _4be.fnm _3gf.prx _2l6.fdx _3g6.fnm _4bb.fdt _4bd.tis _4lk.nrm _2l5.fdx 
_2la.tii _4bd.prx _4ln.fnm _3gf.tis _4ba.nrm _v3.prx _uv.prx _1q3.fnm _3ga.tii _uz.tii _3g9.frq _v0.frq _3ge.tis _3g6.tis _4ln.prx _3g7.tii _3g8.fdt 
_3g7.nrm _3ga.prx _2l2.fdx _2l8.fdx _4ba.prx _1py.frq _uz.fdx _2l3.tii _3g6.prx _v3.fdx _1q6.fdt _v1.nrm _2l2.tii _1q0.tis _4ba.fdx _4be.tii _4ba.frq 
_4ll.fdt _4bh.nrm _4lm.fdt _1q7.frq _4lk.tis _4bc.frq _1q6.fnm _3g7.frq _uw.tis _3g8.tis _2l9.fdx _2l4.tii _1q4.fdx _4be.prx _1q3.nrm _1q0.tii _1q0.fnm 
_v3.nrm _1py.tis _3g9.fdt _4bh.fdt _4ll.nrm _4lk.prx _3gd.prx _1q3.tis _1q2.tii _2l2.nrm _3gd.fdt _2l3.fdx _3g6.fdt _3gd.frq _1q1.tis _4bb.fdx _1q2.frq 
_1q3.fdt _v1.tis _2l8.frq _3gc.fdx _1q1.frq _4bg.frq _4bb.frq _2la.fdx _2l9.frq _uy.tis _uy.prx _4bg.fdx _3gb.prx _uy.frq _1q2.fdx _4lm.prx _2la.prx 
_2l4.prx _4bg.fdt _4be.frq _1q7.nrm _2l5.prx _4bf.frq _v1.prx _4bd.fdt _2l9.prx _1q6.tis _3g8.fnm _4ln.tis _2l3.tis _4bc.fdx _2lb.prx _3gb.frq _3gf.frq 
_2la.fnm _3ga.fdt _uz.tis _4bg.nrm _uv.tii _4bg.tii _3g8.tii _4ll.frq _uv.fnm _2l8.tis _2l8.nrm _2l2.fdt _4bj.tis _4lk.fdx _uw.prx _4bc.prx _4bj.fdt _4be.fdx 
_1q4.frq _uu.fdt _1q1.tii _2l5.tii _2lb.fdt _4bh.frq _3ge.frq _1py.prx _1q5.nrm _v1.fdx _3g7.fdt _4ln.fdt _1q4.nrm _1py.fdt _3gc.tis _4ll.prx _v3.tis _4bf.fdx 
_1q5.fdx _1q0.prx _4bi.nrm _4ll.tis _2l4.tis _3gf.tii _v2.fnm _uu.nrm _1q0.nrm _4lm.fnm _uu.prx _2l6.frq _4ln.nrm _ux.nrm _3g6.frq _1q5.fdt _4bj.tii 
_2lb.fdx _uv.fdx _v1.frq
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:634)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:517)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:306)
        at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:408)
        at org.apache.lucene.index.DirectoryReader.isCurrent(DirectoryReader.java:797)
        at org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:407)
        at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:386)
        at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:348)
        at org.infinispan.lucene.profiling.LuceneReaderThread.refreshIndexReader(LuceneReaderThread.java:79)
        at org.infinispan.lucene.profiling.LuceneReaderThread.testLoop(LuceneReaderThread.java:60)
        at org.infinispan.lucene.profiling.LuceneUserThread.run(LuceneUserThread.java:60)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment/>
            <key id="12470757">LUCENE-2585</key>
            <summary>DirectoryReader.isCurrent might fail to see the segments file during concurrent index changes</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="sanne">Sanne Grinovero</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 Aug 2010 21:28:38 +0100</created>
                <updated>Fri, 10 May 2013 00:05:26 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12895027" author="sanne" created="Tue, 3 Aug 2010 21:32:17 +0100">&lt;p&gt;I'm going to see if I can contribute a patch myself, but I don't think I'll be able to provide a unit test.&lt;/p&gt;</comment>
                    <comment id="12895046" author="yseeley@gmail.com" created="Tue, 3 Aug 2010 22:29:10 +0100">&lt;p&gt;Background: via irc we brainstormed that the most likely cause of the exception was that listing the files&lt;br/&gt;
 in a directory is probably not atomic (at the JVM level) - hence it's possible to miss the segments file in a rapidly changing index.&lt;br/&gt;
The simplest fix would seem to be to retry the directory listing a few times if the segments file isn't found.&lt;/p&gt;</comment>
                    <comment id="12895047" author="mikemccand" created="Tue, 3 Aug 2010 22:30:57 +0100">&lt;p&gt;Man it's hard to add a comment here.  Had to scroll way to the right... silly Jira.&lt;/p&gt;

&lt;p&gt;The best guess here is that this is due to non-atomicity of listing a directory right?  Ie, Lucene, in order to find the most recent segments_N file, lists the directory.  But if, as the listing is happening, a commit is done from IndexWriter, writing a new segments_N+1 and removing the old one, it's possible that the directory listing would show no segments file.&lt;/p&gt;

&lt;p&gt;Lucene then falls back to reading segments.gen, but somehow this is also stale/unusable (probably because another commit kicked off after the dir listing and before we could read segments.gen).&lt;/p&gt;

&lt;p&gt;I'm not sure offhand how we can fix this...&lt;/p&gt;

&lt;p&gt;Can you describe the stress test you're running?&lt;/p&gt;</comment>
                    <comment id="12895059" author="sanne" created="Tue, 3 Aug 2010 23:16:30 +0100">&lt;p&gt;sure, the test is totally open source; the directory implementation based on Infinispan is hosted as submodule of Infinispan:&lt;br/&gt;
&lt;a href="http://anonsvn.jboss.org/repos/infinispan/branches/4.1.x/lucene-directory/" class="external-link"&gt;http://anonsvn.jboss.org/repos/infinispan/branches/4.1.x/lucene-directory/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The test is&lt;br/&gt;
org.infinispan.lucene.profiling.PerformanceCompareStressTest&lt;/p&gt;

&lt;p&gt;it is included in the default test suite but disabled in Maven's configuration, so you should run it manually&lt;br/&gt;
mvn clean test -Dtest=PerformanceCompareStressTest&lt;br/&gt;
(running it requires the jboss.org repositories to be enabled in maven settings)&lt;/p&gt;

&lt;p&gt;To describe it at higher level: there are 5 IndexRead-ing threads using reopen() before each search, 2 threads writing to the index, 1 additional thread as a coordinator and asserting that readers find what they expect to see in the index.&lt;br/&gt;
Exactly the same test scenario is then applied in sequence to RAMDirectory (not having issues), NIOFSDirectory, and 4 differently configured Infinispan directories.&lt;br/&gt;
Only the FSDirectory is affected by the issue, and it can never complete the full hour of stresstest succesfully, while all other implementations behave fine.&lt;/p&gt;

&lt;p&gt;IndexWriter is set to MaxMergeDocs(5000) and setUseCompoundFile(false); the issue is reveled both using SerialMergeScheduler and while using the default merger.&lt;/p&gt;

&lt;p&gt;During the last execution the test managed to perform 22,192,006 searches and 26,875 writes before hitting the exceptional case.&lt;/p&gt;

&lt;p&gt;If you deem it useful I'd be happy in contributing a similar testcase to Lucene, but I assume you won't be excited in having such a long running test. Open to ideas to build a simpler one.&lt;/p&gt;</comment>
                    <comment id="12895060" author="sanne" created="Tue, 3 Aug 2010 23:20:59 +0100">&lt;p&gt;reformatted the description: all filenames where on the same line making this page hard to use.&lt;/p&gt;</comment>
                    <comment id="12895236" author="mikemccand" created="Wed, 4 Aug 2010 13:36:28 +0100">&lt;p&gt;Thanks for the details Sanne!  Your Infinispan directories sounds interesting.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://fixunix.com/linux/356378-opendir-readdir-atomicity.html" class="external-link"&gt;http://fixunix.com/linux/356378-opendir-readdir-atomicity.html&lt;/a&gt; is relevant, assuming the JVM is using opendir/readdir on Linux (which I assume it is?).&lt;/p&gt;

&lt;p&gt;Basically Posix makes no guarantee that opendir/readir will see a "point in time" directory listing.  Ie, file add/deletes can be seen out-of-order, much like write operations in different threads in Java if you don't sync.&lt;/p&gt;

&lt;p&gt;So maybe we should add an additional retry cycle in the case that we don't find a segments file?  (We already have various retries if we do see a segments file in the listing, but, we hit an IOExc when trying to load it).&lt;/p&gt;

&lt;p&gt;Sanne do you want to work out a patch?&lt;/p&gt;</comment>
                    <comment id="12922751" author="sanne" created="Tue, 19 Oct 2010 23:22:16 +0100">&lt;p&gt;Hello, sorry for the late answer, for some reason I didn't see the notification.&lt;/p&gt;

&lt;p&gt;Sure I'm very interested in providing a patch for this.&lt;br/&gt;
I've to say I was only able to reproduce this issue in synthetic benchmarks, so I thought it might be &lt;b&gt;very&lt;/b&gt; unlikely in real world scenarios, but now I actually received reports of people having issues with this during real use cases, so I'll definitely give another look.&lt;/p&gt;

&lt;p&gt;Thanks for the pointers!&lt;/p&gt;</comment>
                    <comment id="12926249" author="rcmuir" created="Fri, 29 Oct 2010 13:47:14 +0100">&lt;p&gt;moving out... there is no patch&lt;/p&gt;</comment>
                    <comment id="12983618" author="shaie" created="Wed, 19 Jan 2011 08:50:35 +0000">&lt;p&gt;There is no patch, moving to 3.2&lt;/p&gt;</comment>
                    <comment id="13043540" author="rcmuir" created="Fri, 3 Jun 2011 17:40:40 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13237028" author="hossman" created="Fri, 23 Mar 2012 20:28:20 +0000">&lt;p&gt;Bulk changing fixVersion 3.6 to 4.0 for any open issues that are unassigned and have not been updated since March 19.&lt;/p&gt;

&lt;p&gt;Email spam suppressed for this bulk edit; search for hoss20120323nofix36 to identify all issues edited&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 3 Aug 2010 21:29:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2935</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25101</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2545] improve uses of StringHelper.intern in Field,AbstractField, NumericField etc.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2545</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;There are many times for certain field types that intern is still called when it is not needed. We can improve this with pretty simple open up of the constructors etc.&lt;/p&gt;</description>
                <environment/>
            <key id="12469549">LUCENE-2545</key>
            <summary>improve uses of StringHelper.intern in Field,AbstractField, NumericField etc.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="woody.anderson@gmail.com">Woody Anderson</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Sun, 18 Jul 2010 04:09:09 +0100</created>
                <updated>Mon, 13 May 2013 04:05:40 +0100</updated>
                                    <version>3.0.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12889563" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 06:23:16 +0100">&lt;p&gt;added more constructors, so any existing use will still intern the string name.&lt;br/&gt;
updated FieldsReader to bypass interning for binary and lazy fields.&lt;/p&gt;</comment>
                    <comment id="12889566" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 06:26:12 +0100">&lt;p&gt;some related tickets for intern issues.&lt;/p&gt;</comment>
                    <comment id="12889586" author="thetaphi" created="Sun, 18 Jul 2010 09:01:03 +0100">&lt;p&gt;For trunk the usage of intern() may be removed completely from Lucene, as fields are now decoupled from terms (its a separate thing in index).&lt;/p&gt;

&lt;p&gt;Some pachanges in the patch may help for 3.x branch (not 3.0!), but we should not expose APIs to the outside that accept a "intern" boolean parameter (e.g. Field, AbstractField, NumericField).&lt;/p&gt;</comment>
                    <comment id="12889651" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 19:42:53 +0100">&lt;p&gt;What is the working definition of "outside"?&lt;br/&gt;
public methods?&lt;br/&gt;
if those internName constructors are protected is that ok?&lt;/p&gt;

&lt;p&gt;I dont' know enough about the 4.0 branch yet to comment about how this relates or doesn't, but for the 3.x branch, when making a lot of fields, and i know they have to be intern'd anyway, i can and do have strings that are already intern'd. I basically would have to do this anyway, but given that i can, i intern them with StringHelper and avoid the extra intern in xxxField. I would like for this to be possible, even if i am using "expert" annotated constructors etc.&lt;/p&gt;

&lt;p&gt;I prefer not to write all my code in the lucene.document package in order to use pkg protection, but I'm sure i can write some helper methods for myself and call from my other code. Would you be more open to having this kind of exposure limited to package scope?&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12469557">LUCENE-2548</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12398451">LUCENE-1308</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12422715">LUCENE-1600</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12449781" name="LUCENE-2545.patch" size="13630" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 06:23:16 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 18 Jul 2010 08:01:03 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2929</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25141</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2543] expose position information in SegmentReader, add method getPosition(term)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2543</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;add public long getPosition(Term) to SegmentReader.&lt;br/&gt;
Also, update the impl of getPosition in TermInfosReader (called by new method) to use enumerator.scanTo() to avoid creating many intermediate Term objects.&lt;/p&gt;</description>
                <environment/>
            <key id="12469547">LUCENE-2543</key>
            <summary>expose position information in SegmentReader, add method getPosition(term)</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="woody.anderson@gmail.com">Woody Anderson</reporter>
                        <labels>
                    </labels>
                <created>Sun, 18 Jul 2010 01:01:02 +0100</created>
                <updated>Fri, 10 May 2013 00:05:26 +0100</updated>
                                    <version>3.0.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12889552" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 01:17:37 +0100">&lt;p&gt;adds the getPosition(Term) method, 'tests' it by iterating an enum and validating that the positions match and are strict monotonic etc.&lt;/p&gt;</comment>
                    <comment id="12889572" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 07:09:52 +0100">&lt;p&gt;i took a quite peek at making a patch for this for the lucene/trunk branch. SegmentReader has changed quite a lot with the codec stuff. If someone familiar with that new code knows the quick/easy way to have a... i guess a FieldProducer impl for the SegmentReader that can expose position information, that'd be great.&lt;br/&gt;
I would imagine that for a single segment we're still very capable of knowing the position of term.&lt;/p&gt;

&lt;p&gt;I have code that depends on this functionality, i would like to be able to try it all out against lucene/trunk as it moves forward.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12449776" name="LUCENE-2543.patch" size="2526" author="woody.anderson@gmail.com" created="Sun, 18 Jul 2010 01:17:37 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2891</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25143</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2540] Document. add get(i) and addAll to make interacting with fieldables and documents easier/faster and more readable</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2540</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Working with Document Fieldables is often a pain.&lt;br/&gt;
getting the ith involves chained method calls and is not very readable:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-comment"&gt;// nice
&lt;/span&gt;doc.getFieldable(i);

&lt;span class="code-comment"&gt;// not nice
&lt;/span&gt;doc.getFields().get(i);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;also, when combining documents, or otherwise aggregating multiple fields into a single document,&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-comment"&gt;// nice
&lt;/span&gt;doc.addAll(fieldables);

&lt;span class="code-comment"&gt;// note nice: less readable and more error prone
&lt;/span&gt;List&amp;lt;Fieldable&amp;gt; fields = ...;
&lt;span class="code-keyword"&gt;for&lt;/span&gt; (Fieldable field : fields) {
  result.add(field);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment/>
            <key id="12469408">LUCENE-2540</key>
            <summary>Document. add get(i) and addAll to make interacting with fieldables and documents easier/faster and more readable</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="woody.anderson@gmail.com">Woody Anderson</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Fri, 16 Jul 2010 05:23:22 +0100</created>
                <updated>Mon, 13 May 2013 05:05:28 +0100</updated>
                                    <version>3.0.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12889008" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 05:39:30 +0100">&lt;p&gt;added the methods, test cases and updated ParallelReader&lt;/p&gt;</comment>
                    <comment id="13655743" author="gbowyer@fastmail.co.uk" created="Mon, 13 May 2013 05:05:28 +0100">&lt;p&gt;Outside of batch adding fields it looks like this issue is somewhat dead since we can now address the field(s) by name, and have sensible iterators on them?&lt;/p&gt;

&lt;p&gt;Anyone opposed to closing this ?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12449627" name="LUCENE-2540.patch" size="3374" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 05:39:29 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 13 May 2013 04:05:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2932</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25146</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2539] add initial capacity based Document constructor</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2539</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When loading field from the index without a FieldSelector, and often in user code we know exactly how many fields a Document should have. This patch will simply add the ability to allocate Document memory for precisely.&lt;/p&gt;

&lt;p&gt;I will include as a separate patch where this would be useful in conjunction with patch &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2276" class="external-link"&gt;LUCENE-2276&lt;/a&gt;.&lt;/p&gt;</description>
                <environment/>
            <key id="12469402">LUCENE-2539</key>
            <summary>add initial capacity based Document constructor</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="woody.anderson@gmail.com">Woody Anderson</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Fri, 16 Jul 2010 03:39:19 +0100</created>
                <updated>Mon, 13 May 2013 04:05:23 +0100</updated>
                                    <version>3.0.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12888992" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 03:48:32 +0100">&lt;p&gt;2 new constructors.&lt;br/&gt;
initialCapacity, and actual List to use. Document fields list is also final, thus verifying it is correctly initialized and not overwritten later.&lt;/p&gt;</comment>
                    <comment id="12888995" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 04:03:07 +0100">&lt;p&gt;didn't get the FieldsReader change in the previous patch. it's kinda the whole point.&lt;/p&gt;</comment>
                    <comment id="12889000" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 04:36:02 +0100">&lt;p&gt;unit test added, with IllegalArgumentException thrown if the given fields list is null, or capacity is negative.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12449622" name="LUCENE-2539.patch" size="3980" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 04:36:02 +0100"/>
                    <attachment id="12449619" name="LUCENE-2539.patch" size="2261" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 04:03:07 +0100"/>
                    <attachment id="12449616" name="LUCENE-2539.patch" size="1265" author="woody.anderson@gmail.com" created="Fri, 16 Jul 2010 03:48:32 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11288</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25147</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2535] update FieldSelectorResult with a BREAK only result option. consolidate some of the state for break/load</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2535</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;some field storage strategies can be much improved by immediate break vs. the break-and-load currently provided.&lt;br/&gt;
this adds another break option, which makes it more advantageous to have member variables of the enum that indicate it's a break/load etc. vs. doing a bunch of grouped == checks.&lt;/p&gt;

&lt;p&gt;The BREAK option should cause an early termination with include == false in the ParallelReader when the BREAK before any other field, then the document should not be loaded from that reader. This is fairly easy to validate with a test, which is included in the patch.&lt;/p&gt;</description>
                <environment/>
            <key id="12469140">LUCENE-2535</key>
            <summary>update FieldSelectorResult with a BREAK only result option. consolidate some of the state for break/load</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="woody.anderson@gmail.com">Woody Anderson</reporter>
                        <labels>
                    </labels>
                <created>Tue, 13 Jul 2010 05:18:15 +0100</created>
                <updated>Fri, 10 May 2013 00:05:26 +0100</updated>
                                    <version>3.0.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12887650" author="woody.anderson@gmail.com" created="Tue, 13 Jul 2010 05:21:20 +0100">&lt;p&gt;patch file for FieldSelectorResult&lt;/p&gt;</comment>
                    <comment id="12887651" author="woody.anderson@gmail.com" created="Tue, 13 Jul 2010 05:27:06 +0100">&lt;p&gt;path file for lucene-trunk dev branch intended for 4.0&lt;/p&gt;

&lt;p&gt;this includes the LATENT result type.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12449317" name="LUCENE-2535.patch" size="11914" author="woody.anderson@gmail.com" created="Tue, 13 Jul 2010 05:21:20 +0100"/>
                    <attachment id="12449318" name="LUCENE_4.0-2535.patch" size="12611" author="woody.anderson@gmail.com" created="Tue, 13 Jul 2010 05:27:05 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11292</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25151</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2527] FieldCache.getTermsIndex should cache fasterButMoreRAM=true|false to the same cache key</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2527</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When we cutover FieldCache to use shared byte[] blocks, we added the boolean fasterButMoreRAM option, so you could tradeoff time/space.&lt;/p&gt;

&lt;p&gt;It defaults to true.&lt;/p&gt;

&lt;p&gt;The thinking is that an expert user, who wants to use false, could pre-populate FieldCache by loading the field with false, and then later when sorting on that field it'd use that same entry.&lt;/p&gt;

&lt;p&gt;But there's a bug &amp;#8211; when sorting, it then loads a 2nd entry with "true".  This is because the Entry.custom in FieldCache participates in equals/hashCode.&lt;/p&gt;</description>
                <environment/>
            <key id="12468558">LUCENE-2527</key>
            <summary>FieldCache.getTermsIndex should cache fasterButMoreRAM=true|false to the same cache key</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 5 Jul 2010 13:35:29 +0100</created>
                <updated>Fri, 10 May 2013 00:05:27 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12914386" author="mikemccand" created="Fri, 24 Sep 2010 10:23:40 +0100">&lt;p&gt;I wonder if the pending improvements to FieldCache will handle preventing this FC insanity?&lt;/p&gt;</comment>
                    <comment id="12914990" author="mikemccand" created="Sun, 26 Sep 2010 17:09:36 +0100">&lt;p&gt;I believe &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2649" title="FieldCache should include a BitSet for matching docs"&gt;&lt;del&gt;LUCENE-2649&lt;/del&gt;&lt;/a&gt; has fixed this, in that you will no longer get a double entry, and so whoever first populates this key "wins".&lt;/p&gt;

&lt;p&gt;I think this is the right policy (vs a 2nd requires upgrading to fasterButMoreRAM=true, in place).&lt;/p&gt;

&lt;p&gt;So I think all we should do here is add a test case that asserts that you don't get a double entry.&lt;/p&gt;</comment>
                    <comment id="12915058" author="ryantxu" created="Sun, 26 Sep 2010 23:26:10 +0100">&lt;p&gt;Yes, &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2649" title="FieldCache should include a BitSet for matching docs"&gt;&lt;del&gt;LUCENE-2649&lt;/del&gt;&lt;/a&gt; puts fasterButMoreRAM= true or false in the same entry.  Whatever is called first is what gets used (without warning)&lt;/p&gt;

&lt;p&gt;One option is to 'upgrade' the cache value &amp;#8211; but I'm not sure which one is the upgrade.  Perhaps the last one that you ask for?  If that is the case, would we need another option that says "I would like it fasterButMoreRAM unless you already have it cached different"&lt;/p&gt;

&lt;p&gt;I think documenting that the setting needs to be used consistently is good.  If we have an error stream, then this would be an appropriate place to log an error/warning.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12474983">LUCENE-2665</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 26 Sep 2010 22:26:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11300</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25159</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2525] make MultiPhraseQuery's UnionDocsAndPositionsEnum public</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2525</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Somehow during flex development, we (unnecessarily, I think) lost the public oal.index.MultiTermPositions, absorbing it into MultiPhraseQuery as a private class.  We should move it back to oal.index, and make it public again.&lt;/p&gt;</description>
                <environment/>
            <key id="12468518">LUCENE-2525</key>
            <summary>make MultiPhraseQuery's UnionDocsAndPositionsEnum public</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 4 Jul 2010 18:06:49 +0100</created>
                <updated>Fri, 10 May 2013 00:05:27 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12885053" author="mikemccand" created="Sun, 4 Jul 2010 18:17:16 +0100">&lt;p&gt;Attached simple patch; I'll commit shortly.&lt;/p&gt;</comment>
                    <comment id="12885056" author="thetaphi" created="Sun, 4 Jul 2010 18:37:06 +0100">&lt;p&gt;As far as I remember we left out those combined enums for now, as we have no sultion, how to handle Attributes with it. This is the same as MultiTermsEnum &amp;amp; Co. See &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2154" title="Need a clean way for Dir/MultiReader to &amp;quot;merge&amp;quot; the AttributeSources of the sub-readers"&gt;LUCENE-2154&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12885058" author="mikemccand" created="Sun, 4 Jul 2010 18:47:55 +0100">&lt;p&gt;Ahh so that was a reason...&lt;/p&gt;

&lt;p&gt;But: I don't think this is a reason to make this class private, now?  First off, we have at least one user who was using it and now misses it... second off, we are uncertain still how &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2154" title="Need a clean way for Dir/MultiReader to &amp;quot;merge&amp;quot; the AttributeSources of the sub-readers"&gt;LUCENE-2154&lt;/a&gt; will/should be solved, or even when it will be.&lt;/p&gt;

&lt;p&gt;In the worst case, we would simply declare (in the future) that this UnionDocsAndPositions cannot handle attrs at all, and it'll still be a useful class, right?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12448650" name="LUCENE-2525.patch" size="12826" author="mikemccand" created="Sun, 4 Jul 2010 18:17:15 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 4 Jul 2010 17:37:06 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2901</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25161</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2522] add simple japanese tokenizer, based on tinysegmenter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2522</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;TinySegmenter (&lt;a href="http://www.chasen.org/~taku/software/TinySegmenter/" class="external-link"&gt;http://www.chasen.org/~taku/software/TinySegmenter/&lt;/a&gt;) is a tiny japanese segmenter.&lt;/p&gt;

&lt;p&gt;It was ported to java/lucene by Kohei TAKETA &amp;lt;k-tak@void.in&amp;gt;, &lt;br/&gt;
and is under friendly license terms (BSD, some files explicitly disclaim copyright to the source code, giving a blessing instead)&lt;/p&gt;

&lt;p&gt;Koji knows the author, and already contacted about incorporating into lucene:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;I've contacted Takeda-san who is the creater of Java version of
TinySegmenter. He said he is happy if his program is part of Lucene.
He is a co-author of my book about Solr published in Japan, BTW. ;-)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment/>
            <key id="12468363">LUCENE-2522</key>
            <summary>add simple japanese tokenizer, based on tinysegmenter</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Jul 2010 17:47:20 +0100</created>
                <updated>Fri, 10 May 2013 00:05:27 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12884348" author="rcmuir" created="Thu, 1 Jul 2010 17:50:55 +0100">&lt;p&gt;here is a really quickly done patch, just to get started (not really for committing)&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;converted their tests to basetokenstream tests,&lt;/li&gt;
	&lt;li&gt;changed it to use CharTermAttribute instead of TermAttribute,&lt;/li&gt;
	&lt;li&gt;added clearAttributes()&lt;/li&gt;
	&lt;li&gt;made class final.&lt;/li&gt;
	&lt;li&gt;added solr factory.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The code is nice, it is setup to work on unicode codepoints etc, but i think we can improve&lt;br/&gt;
it by using CharArrayMaps for speed and by using lucene's codepoint i/o stuff in CharUtils.&lt;/p&gt;</comment>
                    <comment id="12884839" author="rcmuir" created="Fri, 2 Jul 2010 23:41:08 +0100">&lt;p&gt;i refactored the TinySegmenterConstants to use ints/switch statements instead of all the hashmaps.&lt;/p&gt;

&lt;p&gt;this creates a larger .java file, but its a smaller .class, and scoring no longer has to create 24 strings per character&lt;/p&gt;</comment>
                    <comment id="13023617" author="rcmuir" created="Sun, 24 Apr 2011 00:28:12 +0100">&lt;p&gt;attached is an updated patch, its still a work in progress (needs some more tests and benchmarking and some other things little fixes).&lt;/p&gt;

&lt;p&gt;Theres a general pattern for these segmenters (this one, smartchinese, sen) thats a little tricky, that is they want to really look at sentences to determine how to segment.&lt;/p&gt;

&lt;p&gt;So, I added a base class for this to make writing these segmenters easier, and also to hopefully improve segmentation accuracy. (I would like to switch smartchinese over to it) This class makes it easy to segment sentences with a Sentence BreakIterator... in my opinion it doesnt matter how theoretically good the word tokenization is for these things, if the sentence tokenizer is really bad (I found this issue with both sen and smartchinese).&lt;/p&gt;

&lt;p&gt;hope to get it committable soon&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12477230" name="LUCENE-2522.patch" size="128270" author="rcmuir" created="Sun, 24 Apr 2011 00:28:12 +0100"/>
                    <attachment id="12448604" name="LUCENE-2522.patch" size="96066" author="rcmuir" created="Fri, 2 Jul 2010 23:41:08 +0100"/>
                    <attachment id="12448504" name="LUCENE-2522.patch" size="57328" author="rcmuir" created="Thu, 1 Jul 2010 17:50:55 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11304</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25164</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2508] Consolidate Highlighter implementations and a major refactor of the non-termvector highlighter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2508</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Originally, I had planned to create a contrib module to allow people to highlight multiple documents in parallel, but after talking to Uwe in IRC about it, I realized that it was pretty useless.  However, I was already sitting on an iterative highlighting algorithm that was much faster (my tests show 20% - 40%) and more accurate and, based on that same IRC conversation, I decided to not let all the work that I had done go to waste and try to contribute it back again.  Uwe had mentioned that "More like this" detected term vectors when called and use the term vector implementation when possible, if I recall correctly, so I decided to do that.&lt;/p&gt;

&lt;p&gt;The patch that I've attached is my first stab at this.  It's not nearly complete and full disclosure dictates that I say that it's not fully documented and there are not any unit tests written.  I wanted to go ahead and open an issue to get some feedback on the approach that I've taken as well as the fact that it exists will be a proverbial kick in my pants to continue working on it.&lt;/p&gt;

&lt;p&gt;In short, what I've changed:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Completely rewritten the non-tv highlighter to be faster and cleaner.  There is some small loss in functionality for now, namely the loss of the GradientHighlighter (I just haven't done this yet) and the lack of exposure of TermFragments and their scores (I can expose this if it is deemed necessary, this is one of the things I'd like feedback on).&lt;/li&gt;
	&lt;li&gt;Moved org.apache.lucene.search.vectorhighlight and org.apache.lucene.search.highlight to a single package with a unified interface, search.highlight (with two sub-packages: search.highlight.termvector and search.highlight.iterative, respectively).&lt;/li&gt;
	&lt;li&gt;Unified the highlighted term formatting into a single interface: highlighter/Formatter and both highlighters use this now.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What I need to do before I personally would consider this finished:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Finish documentation, most specifically on TermVectorHighlighter.  I haven't done this now as I expect things to change up quite a bit before they're finalized and I really hate writing documentation that goes to waste, but I do intend to complete this bullet &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/li&gt;
	&lt;li&gt;"Flesh out" the API of search.highlight.Highlighter as it's very barebones right now&lt;/li&gt;
	&lt;li&gt;Continue removing and consolidating duplicate functionality, like I've done with the highlighted word tag generation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What I think I need feedback on, before I can proceed:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;FastTermVectorHighlighter and the iterative highlighters need completely different sets of information in order to work.  The approach I've taken is exposing a vectorHighlight method in the unified interface and a iterativeHighlight method, as well as a single highlight method that takes all the information needed for either of them and I'm unsure if this is the best way to do this.&lt;/li&gt;
	&lt;li&gt;The naming of things; I'm not sure if this is a big issue, or even an issue at all, but I'd like to not break any conventions that may exist that I'm unaware of.&lt;/li&gt;
	&lt;li&gt;How big of a deal is exposing the particular score of a segment from the highlighting interface and does this need to be extended into the term vector highlighting as well?&lt;/li&gt;
	&lt;li&gt;There are a lot of methods in the tv implementation that are marked depracted; since this release will almost definitely break backwards compatibility anyway, are these safe to remove?&lt;/li&gt;
	&lt;li&gt;Any other input anyone else may have &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I'm going to continue to work on things that I can work on, at least unless someone tells me I'm wasting my time and will look forward to hearing you guys' feedback! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;As a sidenote because it does seem rather random that I would arbitrarily re-write a working algorithm in the non-tv highlighter, I did it originally because I wanted to parallelize the highlighting (which was a failed experiment) and simply to see if I could make the algorithm faster, as I find that sort of thing particularly fun &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;As a second sidenote, if anyone would like an explanation of the algorithm for the highlighting I devised, and why I feel that it's more accurate, I'd be happy to provide them with one (and benchmarks as well).&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Eddie&lt;/p&gt;</description>
                <environment>&lt;p&gt;irrelevant&lt;/p&gt;</environment>
            <key id="12467541">LUCENE-2508</key>
            <summary>Consolidate Highlighter implementations and a major refactor of the non-termvector highlighter</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="edwardd">Edward Drapkin</reporter>
                        <labels>
                        <label>highlight</label>
                        <label>search</label>
                    </labels>
                <created>Tue, 22 Jun 2010 08:11:58 +0100</created>
                <updated>Fri, 10 May 2013 00:05:27 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/highlighter</component>
                        <due/>
                    <votes>2</votes>
                        <watches>2</watches>
                          <timeoriginalestimate seconds="1209600">336h</timeoriginalestimate>
                    <timeestimate seconds="1209600">336h</timeestimate>
                                  <comments>
                    <comment id="12881112" author="edwardd" created="Tue, 22 Jun 2010 08:12:38 +0100">&lt;p&gt;The "first stab" patch.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12447672" name="LUCENE-2508.patch" size="297717" author="edwardd" created="Tue, 22 Jun 2010 08:12:38 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11316</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25178</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2485] IndexWriter should also warm flushed segments</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2485</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2311" title="Pass potent SR to IRWarmer.warm(), and also call warm() for new segments"&gt;&lt;del&gt;LUCENE-2311&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can now set a mergedSegmentWarmer on IW, which warms only newly merged segments.&lt;/p&gt;

&lt;p&gt;But for consistency maybe we should change this to warm all new segments (ie, also flushed ones).  We should rename it to something "setSegmentWarmer".&lt;/p&gt;

&lt;p&gt;Really, the reader pool should be pulled out of IndexWriter, be externally provided, and be responsible for doing warming of new segments.&lt;/p&gt;</description>
                <environment/>
            <key id="12465972">LUCENE-2485</key>
            <summary>IndexWriter should also warm flushed segments</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Wed, 2 Jun 2010 16:17:03 +0100</created>
                <updated>Fri, 10 May 2013 00:05:27 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12874611" author="yseeley@gmail.com" created="Wed, 2 Jun 2010 16:23:37 +0100">&lt;blockquote&gt;&lt;p&gt;But for consistency maybe we should change this to warm all new segments&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As long as warming a new segment doesn't block that new segment from being exposed via getReader()?&lt;/p&gt;</comment>
                    <comment id="12874613" author="yseeley@gmail.com" created="Wed, 2 Jun 2010 16:33:18 +0100">&lt;p&gt;I'm not sure how practical this is or not... but in general, more context would enable a broader range of applications.&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Passing in the complete index (in addition to just the new segment) would allow incremental updating of an index-wide data structure&lt;/li&gt;
	&lt;li&gt;If the new segment was the result of a merge of existing segments, passing in those existing segments could allow more efficient generation of cached items from the cached items of the old segments.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12874650" author="earwin" created="Wed, 2 Jun 2010 17:39:28 +0100">&lt;blockquote&gt;&lt;p&gt;As long as warming a new segment doesn't block that new segment from being exposed via getReader()?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If an application needs warming, it will need to warm up new segments exposed through getReader() anyway. If you're bent on fast turnaround, you're probably not relying on things being warmed up (or okay with the costs).&lt;br/&gt;
Add to this the thing that for realtime-hungry deployments the size of newly-created (not merged) segments is likely smallish, and any warmup (if present) will take negligible time.&lt;/p&gt;

&lt;p&gt;I think you're going to do a bit of overoptimizing here.&lt;/p&gt;</comment>
                    <comment id="12874652" author="yseeley@gmail.com" created="Wed, 2 Jun 2010 17:45:39 +0100">&lt;blockquote&gt;&lt;p&gt;If an application needs warming, it will need to warm up new segments exposed through getReader() anyway.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But it's very different... the advantage to warming new segments is that the warm step was considered part of the merge by getReader() - if the whole thing hadn't completed yet, getReader() would still immediately return with the old segments pre-merge.  w/o this ability, there's no advantage to warming in a hook vs warming explicitly after getReader().&lt;/p&gt;</comment>
                    <comment id="12874687" author="earwin" created="Wed, 2 Jun 2010 18:31:41 +0100">&lt;blockquote&gt;&lt;p&gt;w/o this ability, there's no advantage to warming in a hook vs warming explicitly after getReader()&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There is. Consistency. I understand that this word is not in high regard amongst Luceners (progress, not perfection!), but still.&lt;br/&gt;
It is logical to have all your warming happen in one defined place. If Lucene does magic for you, and biggest part of said warming happens in a separate thread without making you wait - that's very nice! But that's just a sideffect, like compiler optimizations that may or may not happen.&lt;br/&gt;
Also, if your app requires warming for each segment, having a single callback frees you from the need to determine for a given new segment returned from getReader(), if it is a product of merge and thus already warm, or is it a still-cold newly-flushed segment.&lt;/p&gt;</comment>
                    <comment id="12875027" author="mikemccand" created="Thu, 3 Jun 2010 10:58:54 +0100">&lt;p&gt;In addition to the "more context" that Yonik proposed (which I like), we could also pass to the warmer whether the segment was created by flush or by merge or by addIndexes.&lt;/p&gt;

&lt;p&gt;This way the app could have a single place for all warming, but if necessary can pick &amp;amp; choose how it warms the difference cases separately, since warming after a merge is done in the background (won't block an NRT reopen).&lt;/p&gt;

&lt;p&gt;I'd really like to first factor the ReaderPool out of IW though...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 2 Jun 2010 15:23:37 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2904</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25201</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2483] When loading FieldCache terms index, make terms data optional</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2483</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2380" title="Add FieldCache.getTermBytes, to load term data as byte[]"&gt;&lt;del&gt;LUCENE-2380&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, when you load the terms index (FC.getTermsIndex), it loads two&lt;br/&gt;
arrays, ord (maps docID -&amp;gt; ord) and lookup (maps ord -&amp;gt; term).&lt;/p&gt;

&lt;p&gt;But sometimes you don't need the lookup map (and, it's often very&lt;br/&gt;
costly in RAM usage, much moreso than the ord map).&lt;/p&gt;

&lt;p&gt;EG if your index is a single segment, and your app doesn't need the&lt;br/&gt;
values (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2335" title="optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache"&gt;LUCENE-2335&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Or, if you use a sort comparator that resolves ord -&amp;gt; term and v/v (eg&lt;br/&gt;
using terms dict).&lt;/p&gt;

&lt;p&gt;So we should make it optional...&lt;/p&gt;

&lt;p&gt;Also, similarly, we could merge getTerms/getTermsIndex.  It's&lt;br/&gt;
dangerous today if you load terms and then termsIndex because you're&lt;br/&gt;
wasting tons of RAM; it'd be nicer if we could have a single cache&lt;br/&gt;
entry that'd "upgrade" itself to be an index (have the ords).  This&lt;br/&gt;
single entry could then serve ords, ords+terms, or just terms.&lt;/p&gt;</description>
                <environment/>
            <key id="12465671">LUCENE-2483</key>
            <summary>When loading FieldCache terms index, make terms data optional</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Fri, 28 May 2010 21:17:24 +0100</created>
                <updated>Fri, 10 May 2013 00:05:27 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2906</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25203</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2471] Supporting bulk copies in Directory</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2471</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;A method can be added to IndexOutput that accepts IndexInput, and writes bytes using it as a source.&lt;br/&gt;
This should be used for bulk-merge cases (offhand - norms, docstores?). Some Directories can then override default impl and skip intermediate buffers (NIO, MMap, RAM?).&lt;/p&gt;</description>
                <environment/>
            <key id="12464933">LUCENE-2471</key>
            <summary>Supporting bulk copies in Directory</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="earwin">Earwin Burrfoot</reporter>
                        <labels>
                    </labels>
                <created>Wed, 19 May 2010 19:55:41 +0100</created>
                <updated>Fri, 10 May 2013 00:05:28 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12869470" author="shaie" created="Thu, 20 May 2010 05:28:24 +0100">&lt;p&gt;On &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1585" title="Allow to control how payloads are merged"&gt;&lt;del&gt;LUCENE-1585&lt;/del&gt;&lt;/a&gt; I'm already introducing a copy(Dir, File, File) which is overridden in FSDirectory to implement using ByteBuffers (like you did on copyTo(Dir). So which directories would benefit from that? RAM only (because NIO and MMap already use FSDir's impl)?&lt;/p&gt;

&lt;p&gt;I'm generally +1 for adding such API, just wandering who's the immediate consumer of it.&lt;/p&gt;</comment>
                    <comment id="12869504" author="earwin" created="Thu, 20 May 2010 08:03:09 +0100">&lt;p&gt;Bad link? The issue is closed already and no mentions of Directory in the patches.&lt;/p&gt;

&lt;p&gt;Immediate consumer, just as I said - is all bulk-merging code. I.e. - instead of loading norms to a byte array and then writing them out, you do, roughly:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
IndexInput normFile = ...;
IndexOutput newNormFile = ...;
newNormFile.write(normFile, offset, length);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I looked at FSDir and refreshed my memory. copyTo is implemented with channels and transferTo, I think new method will look quite similar.&lt;/p&gt;</comment>
                    <comment id="12869516" author="shaie" created="Thu, 20 May 2010 09:17:22 +0100">&lt;p&gt;Sorry, too many issue these days. I meant &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt;. I've removed FSDir.copyTo mehods and instead created Dir.copy(Dir, File, File). Still need to upload the patch, with those changes.&lt;/p&gt;

&lt;p&gt;But aside from that, I think the API you're talking about is good.&lt;/p&gt;</comment>
                    <comment id="12869522" author="earwin" created="Thu, 20 May 2010 09:24:25 +0100">&lt;p&gt;Ahem. Why did you remove them? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;br/&gt;
The point was to have default impl on Directory and transferTo-optimized one on FSDirectory.&lt;/p&gt;

&lt;p&gt;Ok, let's wait for your patch.&lt;/p&gt;</comment>
                    <comment id="12869524" author="shaie" created="Thu, 20 May 2010 09:59:39 +0100">&lt;p&gt;The default impl still exist, only in the form of a single file copy instead of an entire directory. There were a couple of reasons to replace them:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;They didn't take a target-name API. when I'm copying the segments in addIndexes over on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt;, I need to rename then in the process (to reflect their new segment name), and the API did not exist.&lt;/li&gt;
	&lt;li&gt;The API was very dangerous as it overwrote thr target files, no questions asked. So you could very easily overwrite one of the segments.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;You can still accomplish that by iterating on the dir yourself and copy the files that you want, only you can do that selectively, leas risky and rename them in the process.&lt;/p&gt;</comment>
                    <comment id="12869537" author="earwin" created="Thu, 20 May 2010 10:29:59 +0100">&lt;p&gt;Ah. Actually there was two methods, one that copies entire directory, and another - selected files.&lt;br/&gt;
The former is a legacy &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Only there for back-compat-loving folk to accept the patch.&lt;/p&gt;</comment>
                    <comment id="12869556" author="shaie" created="Thu, 20 May 2010 11:44:54 +0100">&lt;p&gt;Yes, I'm aware of the two methods. The one which accepts a Collection of files is better, but it still didn't allow you to rename them in the process. And adding another Collection argument, and require that the two will align seemed unnecessary. So src.copy(dest, from, to) seemed to be enough.&lt;/p&gt;

&lt;p&gt;Copying an entire Directory is used by Lucene code only in RAMDir when it's init'ed w/ a Directory. Besides that, the scenario of copying an entire Dir is not really clear when it's useful. So the single file copy gives you as much flexibility as you need, and less chances of making crucial mistakes.&lt;/p&gt;</comment>
                    <comment id="12869558" author="mikemccand" created="Thu, 20 May 2010 11:49:12 +0100">&lt;p&gt;I agree: we should only expose the per-file copyTo, ie, the Directory shouldn't "own" the iteration through a collection of files; the caller can do that.&lt;/p&gt;</comment>
                    <comment id="12869579" author="earwin" created="Thu, 20 May 2010 13:04:14 +0100">&lt;p&gt;The only reason for keeping that iteration within Directory was to reuse the buffer. The savings are neglectable, I think. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Copying an entire Directory is used by Lucene code only in RAMDir when it's init'ed w/ a Directory.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So what? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I used copying an entire Directory for backup purporses, then switched to copyTo(collection), to cherry-pick a single commit.&lt;/p&gt;

&lt;p&gt;Still I agree with switching to single file copy+rename. Back-compat luckily went out of the window, so we can design better APIs &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;br/&gt;
Can we do this in a separate issue from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                    <comment id="12869586" author="shaie" created="Thu, 20 May 2010 13:33:56 +0100">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt; depends on this, and the changes are very minor. If we do this in a separate issue, it will block my progress on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt;. Let me post a patch there today or tomorrow, and if we won't have consensus on the change, I'll open a separate issue, or reopen &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2339" title="Allow Directory.copy() to accept a collection of file names to be copied"&gt;&lt;del&gt;LUCENE-2339&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                    <comment id="12869595" author="earwin" created="Thu, 20 May 2010 13:48:50 +0100">&lt;p&gt;I actually suggested separating so this minor patch goes in without being blocked by your progress on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt; &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12870560" author="mikemccand" created="Mon, 24 May 2010 11:32:24 +0100">&lt;p&gt;I think this issue makes sense, separate from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt;?  Ie this issue is for bulk copying when you have IndexInput/Output already open (I don't think &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt; covers this?).  Whereas &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2455" title="Some house cleaning in addIndexes*"&gt;&lt;del&gt;LUCENE-2455&lt;/del&gt;&lt;/a&gt; is operating on file names...&lt;/p&gt;</comment>
                    <comment id="12966358" author="earwin" created="Fri, 3 Dec 2010 00:02:13 +0000">&lt;p&gt;Hmmm. Are we going to do this?&lt;/p&gt;

&lt;p&gt;Optimized bulk copies IndexInput -&amp;gt; IndexOutput for merges.&lt;/p&gt;

&lt;p&gt;I currently see II.copyBytes(IndexOutput out, long numBytes) method in trunk,&lt;br/&gt;
but it's a little bit of a mess (II.copyBytes calls IO.copyBytes, strange overrides doing the same thing in various ways),&lt;br/&gt;
no optimizations for FSDirectory (or at least NIOFSdirectory) case,&lt;br/&gt;
no offset parameter?&lt;br/&gt;
not used when bulk-merging? (well, DataOutput.copyBytes &lt;em&gt;is&lt;/em&gt; used, but there's a single inefficient version of it)&lt;/p&gt;</comment>
                    <comment id="12967100" author="shaie" created="Mon, 6 Dec 2010 06:40:49 +0000">&lt;p&gt;At some point IndexInput/Output.copyBytes did use FileChannel optimization in FSDirectory, but that caused troubles I think when the copying thread was interrupted. So it was removed and we were left w/ the default impl.&lt;/p&gt;</comment>
                    <comment id="12967205" author="rcmuir" created="Mon, 6 Dec 2010 14:50:52 +0000">&lt;p&gt;I think the problem actually wasn't interrupting but some sort of race condition?&lt;/p&gt;

&lt;p&gt;Either way, I think its sad we had to disable the optimization, and it would be nice if we put it back (safely). But some notes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;an NIO-based optimization shouldnt be in FSDirectory, instead in NIOFSDirectory! I don't think SimpleFS should use NIO methods.&lt;/li&gt;
	&lt;li&gt;the real problem is our tests. we unfortunately don't have many tests that search with multiple threads. This is why it was so hard to reproduce the bug.&lt;/li&gt;
	&lt;li&gt;we should make sure we benchmark any change to ensure its faster because there is unfortunately a lot of risk in optimizing this method.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12967210" author="shaie" created="Mon, 6 Dec 2010 15:02:21 +0000">&lt;blockquote&gt;&lt;p&gt;I think the problem actually wasn't interrupting but some sort of race condition? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could be, I don't remember the exact details.&lt;/p&gt;

&lt;p&gt;I totally agree with you, though it's like a "hen and egg" situation - we cannot develop anything safe until we have good threaded unit tests, and we can never know we have those until we have any implementation that might break. So I personally don't mind if we pursue implementation of FileChannel copying, in NIOFSDirectory only, and then investigate the current threaded indexing/search tests and add some if we think something's missing. But currently we're in sort of a limbo &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, I don't think it's related to that issue and can be handled in a separate issue. If you agree, and assuming nothing more should be done here, we can close this one.&lt;/p&gt;</comment>
                    <comment id="12967232" author="rcmuir" created="Mon, 6 Dec 2010 15:55:07 +0000">&lt;p&gt;OK I opened &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2804" title="check all tests that use FSDirectory.open"&gt;&lt;del&gt;LUCENE-2804&lt;/del&gt;&lt;/a&gt;, this will improve our tests by ensuring they use different implementations and are completely reproducable&lt;br/&gt;
across different operating systems. Hopefully there aren't many tests left to fix.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 20 May 2010 04:28:24 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2931</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25215</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2460] Search Results Filtering Based on Bitwise Operations on Integer Fields</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2460</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This package makes it possible to filter results returned from a query based on the results of a bitwise operation on an integer field in the documents returned from the pre-constructed query.&lt;/p&gt;

&lt;p&gt;You can perform three basic types of operations on these integer fields&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;BitwiseOperation.BITWISE_AND (bitwise AND)&lt;/li&gt;
	&lt;li&gt;BitwiseOperation.BITWISE_OR (bitwise inclusive OR)&lt;/li&gt;
	&lt;li&gt;BitwiseOperation.BITWISE_XOR (bitwise exclusive OR)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;You can also negate the results of these operations.&lt;/p&gt;

&lt;p&gt;For example, imagine there is an integer field in the index named "flags" with the a value 8 (1000 in binary). The following results will be expected :&lt;/p&gt;

&lt;p&gt;   1. A source value of 8 will match during a BitwiseOperation.BITWISE_AND operation, with negate set to false.&lt;br/&gt;
   2. A source value of 4 will match during a BitwiseOperation.BITWISE_AND operation, with negate set to true.&lt;/p&gt;

&lt;p&gt;The BitwiseFilter constructor accepts the following values&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The name of the integer field (A string)&lt;/li&gt;
	&lt;li&gt;The BitwiseOperation object. Example BitwiseOperation.BITWISE_XOR&lt;/li&gt;
	&lt;li&gt;The source value (an integer)&lt;/li&gt;
	&lt;li&gt;A boolean value indicating whether or not to negate the results of the operation&lt;/li&gt;
	&lt;li&gt;A pre-constructed org.apache.lucene.search.Query&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12464495">LUCENE-2460</key>
            <summary>Search Results Filtering Based on Bitwise Operations on Integer Fields</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="iekpo">Israel Ekpo</reporter>
                        <labels>
                    </labels>
                <created>Fri, 14 May 2010 02:30:16 +0100</created>
                <updated>Fri, 10 May 2013 00:05:28 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>3</watches>
                          <timeoriginalestimate seconds="7200">2h</timeoriginalestimate>
                    <timeestimate seconds="7200">2h</timeestimate>
                                  <comments>
                    <comment id="12867353" author="iekpo" created="Fri, 14 May 2010 02:47:29 +0100">&lt;p&gt;Attaching the package containing the BitwiseFilter class&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12444459" name="LUCENE-2460-bitwise.tar.gz" size="7311" author="iekpo" created="Fri, 14 May 2010 02:47:29 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11359</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25226</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2457] QueryNode implementors should override equals method</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2457</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Discussed on thread: &lt;a href="http://markmail.org/thread/gjqk35t7e3y4fo5j" class="external-link"&gt;http://markmail.org/thread/gjqk35t7e3y4fo5j&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;"QueryNode(s) are data objects, and it makes sense to override&lt;br/&gt;
their equals method. But before, we need to define what is a QueryNode&lt;br/&gt;
equality. Should two nodes be considered equal if they represent&lt;br/&gt;
syntactically or semantically the same query? e.g. an ORQueryNode created&lt;br/&gt;
from the query &amp;lt;a OR b OR c&amp;gt; will not have the same children ordering as the&lt;br/&gt;
query &amp;lt;b OR c OR a&amp;gt;, so they are syntactically not equal, but they are&lt;br/&gt;
semantically equal, because the order of the OR operands (usually) does not&lt;br/&gt;
matter when the query is executed. I say it usually does not matter, because&lt;br/&gt;
it's up to the Query object implementation built from that ORQueryNode&lt;br/&gt;
object, for this reason, I vote for defining that two query nodes should be&lt;br/&gt;
equals if they are syntactically equal.&lt;/p&gt;

&lt;p&gt;I also vote for excluding query node tags from the equality check, because&lt;br/&gt;
they are not meant to represent the query structure, but to attach extra&lt;br/&gt;
info to the node, which is usually used for communication between&lt;br/&gt;
processors."&lt;/p&gt;</description>
                <environment/>
            <key id="12464274">LUCENE-2457</key>
            <summary>QueryNode implementors should override equals method</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="adriano_crestani">Adriano Crestani</reporter>
                        <labels>
                    </labels>
                <created>Tue, 11 May 2010 19:35:23 +0100</created>
                <updated>Fri, 10 May 2013 00:05:28 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>2</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12985268" author="shaie" created="Sun, 23 Jan 2011 07:47:31 +0000">&lt;p&gt;Open for 8 months with no patch or comments. Moving to 3.2 (though I think we can close it for "lack of interest", if we had such category).&lt;/p&gt;</comment>
                    <comment id="12986116" author="adriano_crestani" created="Tue, 25 Jan 2011 00:03:06 +0000">&lt;p&gt;Hi Shai,&lt;/p&gt;

&lt;p&gt;Please, don't close it, this is a nice feature, mainly for automated tests. I haven't had time to give attention to it yet, but keep it in the open lis for now,t so we don't forget it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13043541" author="rcmuir" created="Fri, 3 Jun 2011 17:40:40 +0100">&lt;p&gt;bulk move 3.2 -&amp;gt; 3.3&lt;/p&gt;</comment>
                    <comment id="13234778" author="hossman" created="Wed, 21 Mar 2012 18:14:25 +0000">&lt;p&gt;Bulk of fixVersion=3.6 -&amp;gt; fixVersion=4.0 for issues that have no assignee and have not been updated recently.&lt;/p&gt;

&lt;p&gt;email notification suppressed to prevent mass-spam&lt;br/&gt;
psuedo-unique token identifying these issues: hoss20120321nofix36&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 23 Jan 2011 07:47:31 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11362</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25229</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2408] Add Document.set/getSourceID, as an optional hint to IndexWriter to improve indexing performance</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2408</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;(Spinoff from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2324" title="Per thread DocumentsWriters that write their own private segments"&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The internal indexer (currently DocumentsWriter &amp;amp; its full indexing&lt;br/&gt;
chain) has separate *PerThread objects holding buffered postings in&lt;br/&gt;
RAM until flush.&lt;/p&gt;

&lt;p&gt;The RAM efficiency of these buffers is very dependent on the term&lt;br/&gt;
distributions sent to each.&lt;/p&gt;

&lt;p&gt;As an optimization, today, we use thread affinity (ie we try to assign&lt;br/&gt;
the same thread to the same *PerThread classes), on the assumption&lt;br/&gt;
that sometimes that thread may be indexing from its own source of&lt;br/&gt;
docs.  When the assumption applies it means we can have much better&lt;br/&gt;
overall RAM efficiency since a single *PerThread set of classes handles&lt;br/&gt;
the term distribution for that source.&lt;/p&gt;

&lt;p&gt;In the extreme case (many threads, each doing completely orthogonal&lt;br/&gt;
terms, eg say different languages) this should be a sizable&lt;br/&gt;
performance gain.&lt;/p&gt;

&lt;p&gt;But really this is a hack &amp;#8211; eg if you index using a dedicated&lt;br/&gt;
indexing thread pool, then thread binding has nothing to do with&lt;br/&gt;
source, and you have no way to get this optimization (even though&lt;br/&gt;
it's still "there").&lt;/p&gt;

&lt;p&gt;To fix this, we should add an optional get/setSourceID to Document.&lt;br/&gt;
It's completely optional for an app to set this... and if they do,&lt;br/&gt;
it'd be a hint which IW can make use of (in an impl private manner).&lt;br/&gt;
If they don't we should just fallback to the "best guess" we use today&lt;br/&gt;
(each thread is its own source).&lt;/p&gt;

&lt;p&gt;The javadoc would be something like "as a hint to IW, to possibly&lt;br/&gt;
improve its indexing performance, if you have docs from difference&lt;br/&gt;
sources you should set the source ID on your Document". And&lt;br/&gt;
how/whether IW makes use of this information is "under the hood"...&lt;/p&gt;</description>
                <environment/>
            <key id="12462666">LUCENE-2408</key>
            <summary>Add Document.set/getSourceID, as an optional hint to IndexWriter to improve indexing performance</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Apr 2010 15:50:11 +0100</created>
                <updated>Fri, 10 May 2013 00:05:28 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11394</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25277</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2407] make CharTokenizer.MAX_WORD_LEN parametrizable</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2407</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;as discussed here &lt;a href="http://n3.nabble.com/are-long-words-split-into-up-to-256-long-tokens-tp739914p739914.html" class="external-link"&gt;http://n3.nabble.com/are-long-words-split-into-up-to-256-long-tokens-tp739914p739914.html&lt;/a&gt; it would be nice to be able to parametrize that value. &lt;/p&gt;</description>
                <environment/>
            <key id="12462662">LUCENE-2407</key>
            <summary>make CharTokenizer.MAX_WORD_LEN parametrizable</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jmwap">javi</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Wed, 21 Apr 2010 15:30:31 +0100</created>
                <updated>Mon, 13 May 2013 04:04:46 +0100</updated>
                                    <version>3.0.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12859373" author="thetaphi" created="Wed, 21 Apr 2010 15:38:09 +0100">&lt;p&gt;This is also a problem for some asian languaes. If ThaiAnalyzer would use CharTokenizer, very long passages could get lost, as ThatWordFilter would not get the complete string (thai is not tokenized by the tokenizer, but later in the filter)&lt;/p&gt;

&lt;p&gt;This also applies to StandardTokenizer, maybe we should set a good default when analyzing Thai text (ThaiAnalyzer should init StandardTokenizer with a large/infinite value).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 21 Apr 2010 14:38:09 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11395</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25278</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2403] contrib/HighFreqTerms should use ByteRefs but provide human-readable output</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2403</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;contrib HighFreqTerms was upgraded to use the flex APIs but currently displays hex code if you do not give a field argument and strings if you do.&lt;/p&gt;

&lt;p&gt;The conversion to a string should be consistent and should occur just before output rather than when loading the priority queue&lt;br/&gt;
See: &lt;a href="http://n3.nabble.com/Bug-in-contrib-misc-HighFreqTerms-java-tc719202.html#a719202" class="external-link"&gt;http://n3.nabble.com/Bug-in-contrib-misc-HighFreqTerms-java-tc719202.html#a719202&lt;/a&gt;&lt;/p&gt;
</description>
                <environment/>
            <key id="12462453">LUCENE-2403</key>
            <summary>contrib/HighFreqTerms should use ByteRefs but provide human-readable output</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="5" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.png">Trivial</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="tburtonwest">Tom Burton-West</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Mon, 19 Apr 2010 17:52:28 +0100</created>
                <updated>Mon, 13 May 2013 04:04:56 +0100</updated>
                                    <version>3.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13135522" author="steve_rowe" created="Tue, 25 Oct 2011 23:56:10 +0100">&lt;p&gt;The nabble link in the description is dead.  Here is the Lucid version: &lt;a href="http://www.lucidimagination.com/search/document/4bcb8f5f5ba4dd8/bug_in_contrib_misc_highfreqterms_java" class="external-link"&gt;http://www.lucidimagination.com/search/document/4bcb8f5f5ba4dd8/bug_in_contrib_misc_highfreqterms_java&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13458062" author="steve_rowe" created="Tue, 18 Sep 2012 20:03:25 +0100">&lt;p&gt;The Lucid link is now dead too &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;

&lt;p&gt;Here's a MarkMail.org link: &lt;a href="http://markmail.org/thread/2nl4jhegxbnal4fe" class="external-link"&gt;http://markmail.org/thread/2nl4jhegxbnal4fe&lt;/a&gt; &lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 25 Oct 2011 22:56:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11399</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25282</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2395] Add a scoring DistanceQuery that does not need caches and separate filters</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2395</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In a chat with Chris Male and my own ideas when implementing for PANGAEA, I thought about the broken distance query in contrib. It lacks the following features:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;It needs a query/filter for the enclosing bbox (which is constant score)&lt;/li&gt;
	&lt;li&gt;It needs a separate filter for filtering out hits to far away (inside bbox but outside distance limit)&lt;/li&gt;
	&lt;li&gt;It has no scoring, so if somebody wants to sort by distance, he needs to use the custom sort. For that to work, spatial caches distance calculation (which is broken for multi-segment search)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The idea is now to combine all three things into one query, but customizeable:&lt;/p&gt;

&lt;p&gt;We first thought about extending CustomScoreQuery and calculate the distance from FieldCache in the customScore method and return a score of 1 for distance=0, score=0 on the max distance and score&amp;lt;0 for farer hits, that are in the bounding box but not in the distance circle. To filter out such negative scores, we would need to override the scorer in CustomScoreQuery which is priate.&lt;/p&gt;

&lt;p&gt;My proposal is now to use a very stripped down CustomScoreQuery (but not extend it) that does call a method getDistance(docId) in its scorer's advance and nextDoc that calculates the distance for the current doc. It stores this distance also in the scorer. If the distance &amp;gt; maxDistance it throws away the hit and calls nextDoc() again. The score() method will reurn per default weight.value*(maxDistance - distance)/maxDistance and uses the precalculated distance. So the distance is only calculated one time in nextDoc()/advance().&lt;/p&gt;

&lt;p&gt;To be able to plug in custom scoring, the following methods in the query can be overridden:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;float getDistanceScore(double distance) - returns per default: (maxDistance - distance)/maxDistance; allows score customization&lt;/li&gt;
	&lt;li&gt;DocIdSet getBoundingBoxDocIdSet(Reader, LatLng sw, LatLng ne) - returns an DocIdSet for the bounding box. Per default it returns e.g. the docIdSet of a NRF or a cartesian tier filter. You can even plug in any other DocIdSet, e.g. wrap a Query with QueryWrapperFilter&lt;/li&gt;
	&lt;li&gt;support a setter for the GeoDistanceCalculator that is used by the scorer to get the distance.&lt;/li&gt;
	&lt;li&gt;a LatLng provider (similar to CustomScoreProvider/ValueSource) that returns for a given doc id the lat/lng. This method is called per IndexReader one time in scorer creation and will retrieve the coordinates. By that we support FieldCache or whatever.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This query is almost finished in my head, it just needs coding &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12462134">LUCENE-2395</key>
            <summary>Add a scoring DistanceQuery that does not need caches and separate filters</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Thu, 15 Apr 2010 12:08:45 +0100</created>
                <updated>Fri, 10 May 2013 00:05:29 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/spatial</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12857278" author="cmale" created="Thu, 15 Apr 2010 12:13:31 +0100">&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;This will replace the work I was doing on improving the DistanceFilter and the DistanceSortSource.  Instead we will have a proper DistanceQuery where the sorting is done through the existing sorting by score functionality in Lucene.  The CartesianShapeFilter will then be able to be used as a Filter with the new Query.&lt;/p&gt;

&lt;p&gt;This also addresses the current problems with caching calculated distances and means that Spatial will work with per segment.&lt;/p&gt;</comment>
                    <comment id="12857282" author="cmale" created="Thu, 15 Apr 2010 12:26:53 +0100">&lt;p&gt;Linked issue for GeoDistanceCalculator which abstracts away the algorithm for calculating the distance.&lt;/p&gt;</comment>
                    <comment id="12857283" author="cmale" created="Thu, 15 Apr 2010 12:31:27 +0100">&lt;p&gt;Linked issue which contains LocationDataSet.  Need to extract the exact code out of the patch, but the LocationDataSet can then be built by the Query per segment to retrieve the lat/lng Points for documents, independent of whether they are distinct fields or geohashes.&lt;/p&gt;

&lt;p&gt;This might depend on the change to Point that I did in another patch, I'll hunt for that now.&lt;/p&gt;</comment>
                    <comment id="12857284" author="cmale" created="Thu, 15 Apr 2010 12:32:29 +0100">&lt;p&gt;Linked issue which improves the Point class.&lt;/p&gt;</comment>
                    <comment id="12857367" author="thetaphi" created="Thu, 15 Apr 2010 17:00:30 +0100">&lt;p&gt;A first idea of the Query, it does not even compile as some classes are missing (coming with Chris' later patches), but it shows how it should work and how its customizeable.&lt;/p&gt;</comment>
                    <comment id="12857593" author="thetaphi" created="Fri, 16 Apr 2010 00:29:55 +0100">&lt;p&gt;small updates to Chris' patches.&lt;/p&gt;</comment>
                    <comment id="12857611" author="thetaphi" created="Fri, 16 Apr 2010 01:16:27 +0100">&lt;p&gt;Added Weight.explain() and fixed a missing replacement.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                <outwardlinks description="depends upon">
                            <issuelink>
            <issuekey id="12443088">LUCENE-2148</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12443101">LUCENE-2151</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12443109">LUCENE-2152</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12441892" name="ASF.LICENSE.NOT.GRANTED--DistanceQuery.java" size="7545" author="thetaphi" created="Fri, 16 Apr 2010 01:16:27 +0100"/>
                    <attachment id="12441887" name="ASF.LICENSE.NOT.GRANTED--DistanceQuery.java" size="6928" author="thetaphi" created="Fri, 16 Apr 2010 00:29:55 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 15 Apr 2010 11:13:31 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11407</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25290</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2394] Factories for cache creation</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2394</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;I've seen the &lt;a href="https://issues.apache.org/jira/browse/LUCENE-831" title="Complete overhaul of FieldCache API/Implementation"&gt;LUCENE-831&lt;/a&gt; (Complete overhaul of FieldCache API/Implementation) targeted for version 3.1 and I think that maybe, before this overhaul, it would be good to have a more cirurgical change, that would need smaller effort in new unit tests, without behavior changes and almost no performance impact.&lt;br/&gt;
One way to achieve that is inserting strategically positioned calls to a factory structure that would allow every already developed code to continue working without changes, at the same time giving the opportunity to put alternative factories to work.&lt;br/&gt;
Focusing on the cache idea (not specifically the FieldCache, that has it's own specific responsabilities, but in the key/value structure that will ultimately hold the cached objects) i've done the small change contained in the patch I'm attaching to this.&lt;br/&gt;
It has default implementations that encapsulate what was being originally used in FieldCache, so all current test cases passes, and creates the possibility to create a EHCacheFactory or InfinispanCacheFactory, or even MyOwnCachingStructureFactory.&lt;br/&gt;
With this, it would be easy to take advantage of the features provided by this kind of project in a uniform way and rapidly allowing new possibilities in scalability and tuning.&lt;br/&gt;
The code in the patch is small (16kb file is small if compared to the hundreds of kbs in other patchs) and even though it doesn't have javadoc right now (sorry) I hope it can be easly understood. So, if Lucene maintainers see that this contribution could be used (in a 2.9.n+1 and 3.0.n+1 and maybe influencing future versions) we could put some more effort in it, documenting, adding necessary unit tests and maybe contributing other factory implementations.&lt;br/&gt;
What do you think?&lt;/p&gt;</description>
                <environment/>
            <key id="12462079">LUCENE-2394</key>
            <summary>Factories for cache creation</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="oswaldodantas">Oswaldo Dantas</reporter>
                        <labels>
                    </labels>
                <created>Wed, 14 Apr 2010 20:46:09 +0100</created>
                <updated>Fri, 10 May 2013 00:05:29 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12857053" author="oswaldodantas" created="Wed, 14 Apr 2010 20:51:18 +0100">&lt;p&gt;Attaching factory suggestion (patch for changes to &lt;a href="https://svn.apache.org/repos/asf/lucene/java/tags/lucene_2_9_2" class="external-link"&gt;https://svn.apache.org/repos/asf/lucene/java/tags/lucene_2_9_2&lt;/a&gt;) focusing in cache, being used in specifically in FieldCacheImpl and TermInfosReader to exemplify.&lt;/p&gt;</comment>
                    <comment id="12874136" author="mikemccand" created="Tue, 1 Jun 2010 19:21:31 +0100">&lt;p&gt;Moving fix version -&amp;gt; 4.0.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12441763" name="ASF.LICENSE.NOT.GRANTED--factoriesPatch.patch" size="16140" author="oswaldodantas" created="Wed, 14 Apr 2010 20:51:18 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 1 Jun 2010 18:21:31 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2905</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25291</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2345] Make it possible to subclass SegmentReader</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2345</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I would like the ability to subclass SegmentReader for numerous reasons:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;to capture initialization/close events&lt;/li&gt;
	&lt;li&gt;attach custom objects to an instance of a segment reader (caches, statistics, so on and so forth)&lt;/li&gt;
	&lt;li&gt;override methods on segment reader as needed&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;currently this isn't really possible&lt;/p&gt;

&lt;p&gt;I propose adding a SegmentReaderFactory that would allow creating custom subclasses of SegmentReader&lt;/p&gt;

&lt;p&gt;default implementation would be something like:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;public&lt;/span&gt; class SegmentReaderFactory {
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; SegmentReader get(&lt;span class="code-object"&gt;boolean&lt;/span&gt; readOnly) {
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; readOnly ? &lt;span class="code-keyword"&gt;new&lt;/span&gt; ReadOnlySegmentReader() : &lt;span class="code-keyword"&gt;new&lt;/span&gt; SegmentReader();
  }

  &lt;span class="code-keyword"&gt;public&lt;/span&gt; SegmentReader reopen(SegmentReader reader, &lt;span class="code-object"&gt;boolean&lt;/span&gt; readOnly) {
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; newSegmentReader(readOnly);
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It would then be made possible to pass a SegmentReaderFactory to IndexWriter (for pooled readers) as well as to SegmentReader.get() (DirectoryReader.open, etc)&lt;/p&gt;

&lt;p&gt;I could prepare a patch if others think this has merit&lt;/p&gt;

&lt;p&gt;Obviously, this API would be "experimental/advanced/will change in future"&lt;/p&gt;

</description>
                <environment/>
            <key id="12460147">LUCENE-2345</key>
            <summary>Make it possible to subclass SegmentReader</summary>
                <type id="5" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Wish</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="tsmith">Tim Smith</reporter>
                        <labels>
                    </labels>
                <created>Wed, 24 Mar 2010 18:39:34 +0000</created>
                <updated>Fri, 10 May 2013 00:05:29 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12849450" author="mikemccand" created="Wed, 24 Mar 2010 21:26:23 +0000">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12849455" author="tsmith" created="Wed, 24 Mar 2010 21:28:51 +0000">&lt;p&gt;that's the reassurance i needed &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;will start working on a patch tomorrow &lt;br/&gt;
will take a few days as i'll start with a 3.0 patch (which i use), then will create a 3.1 patch once i've got that all flushed out&lt;/p&gt;</comment>
                    <comment id="12849475" author="earwin" created="Wed, 24 Mar 2010 21:56:37 +0000">&lt;p&gt;Ahem, that directly clashes with my inflight patch?&lt;/p&gt;

&lt;p&gt;I'm going to land create/reopen/close refactoring pretty soon, and then finish plugins.&lt;br/&gt;
(Which was an arduous task before, as initialization paths for SegmentReaders are really, really insane)&lt;/p&gt;

&lt;p&gt;I'm on this patch for quite some time, I understand that, so maybe we can settle on a deadline? Don't like to see the effort vaporate, also merging is gonna be hell with flex branch alone, don't want to double it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12849497" author="tsmith" created="Wed, 24 Mar 2010 22:20:31 +0000">&lt;p&gt;i'll do my initial work on 3.0 so i can absorb the changes now and will post that patch&lt;/p&gt;

&lt;p&gt;at which point, i can wait for you to finish whatever you need, or we can just incorporate the same ability into your patch for the other ticket&lt;br/&gt;
i would just like to see the ability to subclass SegmentReader's on 3.1 so i don't have to port a patch when i absorb 3.1 (just use the "finalized" apis)&lt;/p&gt;
</comment>
                    <comment id="12849703" author="tsmith" created="Thu, 25 Mar 2010 14:01:01 +0000">&lt;p&gt;Here's a patch against 3.0 that provides the SegmentReaderFactory ability&lt;br/&gt;
(not tested yet, but i'll be doing that shortly as i integrate this functionality)&lt;/p&gt;

&lt;p&gt;It adds a SegmentReaderFactory.&lt;/p&gt;

&lt;p&gt;The IndexWriter now has a getter and setter for setting this&lt;/p&gt;

&lt;p&gt;SegmentReader has a new protected method init() which is called after the segment reader has been initialized (to allow subclasses to hook this action and do additional initialization, etc&lt;/p&gt;

&lt;p&gt;added 2 new IndexReader.open() calls that allow specifying the SegmentReaderFactory&lt;/p&gt;
</comment>
                    <comment id="12849728" author="shaie" created="Thu, 25 Mar 2010 15:42:40 +0000">&lt;blockquote&gt;&lt;p&gt;The IndexWriter now has a getter and setter for setting this&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If this is not expected to change during the lifetime of IW, I think it should be added to IWC when you upgrade the patch to 3.1.&lt;/p&gt;</comment>
                    <comment id="12849731" author="tsmith" created="Thu, 25 Mar 2010 15:44:09 +0000">&lt;p&gt;that was my plan&lt;/p&gt;</comment>
                    <comment id="12850062" author="mikemccand" created="Fri, 26 Mar 2010 10:03:23 +0000">&lt;p&gt;I think we should only commit this only on 3.1 (new feature)?&lt;/p&gt;

&lt;p&gt;Earwin: will this change really conflict w/ your ongoing refactoring (to have DirReader subclass MultiReader)?  It seems somewhat orthogonal?&lt;/p&gt;</comment>
                    <comment id="12850067" author="earwin" created="Fri, 26 Mar 2010 10:29:00 +0000">&lt;blockquote&gt;&lt;p&gt;will this change really conflict w/ your ongoing refactoring (to have DirReader subclass MultiReader)? It seems somewhat orthogonal?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ongoing contains a massive rework of how open/reopen/clone/close is done. Folding copypaste between DirReader and MultiReader is a byproduct.&lt;br/&gt;
The aim is to have clean initialization, get rid of of init/reinit/moreinit methods, moving the code to constructors.&lt;br/&gt;
This alone plays bad with factories.&lt;/p&gt;

&lt;p&gt;The next step, plugins - conflicts ideologically. Plugins allow extension by composition, which is (in my view) much more clean than subclassing.&lt;br/&gt;
Landing them without refactoring all this mess beforehand is dangerous, I already hit some places where reopen/clone were done wrong in respect to existing reader fields. Even more - NRT reader lifecycle is really messy - it beginnings as a reader aimed only for merging, and then being upgraded in two separate stages is finally wrapped in DirReader to be used for actual searching. The factory/init() approach ignores this, and each user of this API will be on his own to separate lightweight readers from full-fledged ones.&lt;/p&gt;</comment>
                    <comment id="12850075" author="shaie" created="Fri, 26 Mar 2010 10:47:52 +0000">&lt;p&gt;Earwin, w/o knowing too much about the details of your work, I wanted to comment on "get rid of of init/reinit/moreinit methods, moving the code to constructors". I work now on Parallel Index and one of the things I do is extend IW. Currently, IW's ctor code performs the initialization, however I'm thinking to move that code to an init method. The reason is to allow easy extensions of IW, such as &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2330" title="Allow easy extension of IndexWriter"&gt;&lt;del&gt;LUCENE-2330&lt;/del&gt;&lt;/a&gt;. There I'm going to add a default ctor to IW, accompanied by an init method the extending class can call if needed. So what I'm trying to say is that init methods are not always bad, and sometimes ctors limit you. Perhaps it would make sense though in what you're trying to do ...&lt;/p&gt;</comment>
                    <comment id="12850079" author="thetaphi" created="Fri, 26 Mar 2010 10:55:30 +0000">&lt;p&gt;Non-final/private method calls from ctors is discouraged, as it creates problems:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;If you override such a init method, you can change the behaviour of the super class' ctor&lt;/li&gt;
	&lt;li&gt;During the time the init() method is called, the fields are not yet initialized in the subclass and also the subclass' ctor was not yet called! This leads to unexspected behaviour (like final fields changing its contents during the lifetime of the class: during the overridden init() method they are null and suddenly change to non-null after the subclass' ctor was executed, which is after the init() method)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Because of this, such init() methods should be private or at least final and never public!&lt;/p&gt;</comment>
                    <comment id="12850081" author="thetaphi" created="Fri, 26 Mar 2010 11:02:43 +0000">&lt;p&gt;Here a good explanation: &lt;a href="http://www.informit.com/articles/article.aspx?p=20521" class="external-link"&gt;http://www.informit.com/articles/article.aspx?p=20521&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12850082" author="mikemccand" created="Fri, 26 Mar 2010 11:08:46 +0000">&lt;p&gt;OK, I guess for now let's hold off on this issue while we iterate at least with Earwin's first refactoring effort...?&lt;/p&gt;

&lt;p&gt;Earwin, can you post a patch w/ your current state, even if it's "rough"?  There seems to be alot of interest/opinions on how to "fix" things here &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  Both IndexWriter &amp;amp; SegmentReader are "needing" extensibility... but this is a big change so I'm hoping we can first just get your refactoring done before adding extensibility.&lt;/p&gt;

&lt;p&gt;Tim, do you think the plugin model ("extension by composition") would be workable for your use case?  Ie, instead of a factory enabling subclasses of SegmentReader?&lt;/p&gt;</comment>
                    <comment id="12850083" author="shaie" created="Fri, 26 Mar 2010 11:10:26 +0000">&lt;p&gt;Thanks Uwe, I know that ctor is the preferred way, and in the process of introducing IWC I delete IW.init which all ctors called and pulled all the code to IW ctor. I will make that init() on IW final. But sometimes putting code in init() is not bad (and it's used in Lucene elsewhere too (e.g. PQ and up until recently IW).&lt;/p&gt;</comment>
                    <comment id="12850084" author="earwin" created="Fri, 26 Mar 2010 11:12:57 +0000">&lt;p&gt;More often than not init() methods are a sign of bad design.&lt;br/&gt;
I.e. in your case extending IW is crazy.&lt;br/&gt;
You should have an interface capturing IW methods and two implementations - one writing to the index and another delegating to its subwriters. You don't do DirectoryReader extends SegmentReader, do you? They both extend lucene-style-interface IndexReader.&lt;/p&gt;

&lt;p&gt;Lucene's back compat policy got people used to writing and digesting freaky code, and I'm not going to fight generally against it, that's a lost cause : )&lt;br/&gt;
But in this exact case (readers) stuffing everything in constructors, defining all fields I can final, replacing SR.openDocStores/loadTermsIndex with reopen()-like method allows me to tackle (at least somewhat) lifecycle complexity. Javac will force me to either initialize something or explicitly leave it null.&lt;br/&gt;
Besides some real bugs in this code, I cleaned up cases where a field was inited twice, just in case! Poor developers got lost in init() methods : }&lt;/p&gt;</comment>
                    <comment id="12850088" author="earwin" created="Fri, 26 Mar 2010 11:16:31 +0000">&lt;blockquote&gt;&lt;p&gt;Earwin, can you post a patch w/ your current state, even if it's "rough"?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ahem. Right now it's more or less finished for Multi/Directory/MutableDirectory/WriterBackedDirectory-readers.&lt;br/&gt;
But the SegmentReader is in shambles (i.e. does not compile yet).&lt;br/&gt;
Should I post asis?&lt;/p&gt;</comment>
                    <comment id="12850094" author="shaie" created="Fri, 26 Mar 2010 11:29:20 +0000">&lt;p&gt;Earwin, I wholeheartedly agree with what you wrote. If we could refactor IW and extract it to a set of interfaces, then I agree (and Michael B. has an issue open for that). I think though that IW's API is already that interface (give or take few methods). So perhaps this can be an easy refactoring - introduce an Indexer (a la Searcher) class (or interface) w/ all of IW public methods, and then let PW extend/impl that class/interface as well as IW. We can also consider making IW itself final this way (though bw police will prevent it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;).&lt;/p&gt;

&lt;p&gt;Then when PW sets up the slices, it can create them as IW or any other IW-like implementation it needs them to impl. If it sounds good enough to become its own issue, I can open one and we can continue discussing it there (and leave that issue focused on extending SR). Then I'll hold off w/ &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2330" title="Allow easy extension of IndexWriter"&gt;&lt;del&gt;LUCENE-2330&lt;/del&gt;&lt;/a&gt;, or simply rename it to reflect that Indexer API.&lt;/p&gt;</comment>
                    <comment id="12850127" author="tsmith" created="Fri, 26 Mar 2010 13:12:25 +0000">&lt;blockquote&gt;&lt;p&gt;I think we should only commit this only on 3.1 (new feature)? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;3.1 only of course (just posted a 3.0 patch now as that's what i'm using and i need the functionality now)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Tim, do you think the plugin model ("extension by composition") would be workable for your use case? Ie, instead of a factory enabling subclasses of SegmentReader?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As long as the plugin model allows the same capabilities, that could work just fine and could be the final solution for this ticket.&lt;/p&gt;

&lt;p&gt;I mainly need the ability to add data structures to a SegmentReader that will be shared for all SegmentReader's for a segment, and then add some extra meta information on a per instance basis&lt;/p&gt;

&lt;p&gt;Is there a ticket or wiki page that details the "plugin" architecture/design so i could take a look?&lt;/p&gt;

&lt;p&gt;However, would the plugins allow overriding specific IndexReader methods?&lt;/p&gt;

&lt;p&gt;I still would see the need to be able to override specific methods for a SegmentReader (in order to track statistics/provide changed/different/faster/more feature rich implementations)&lt;br/&gt;
I don't have a direct need for this right now, however i could envision needing this in the future&lt;/p&gt;

&lt;p&gt;Here's a few requirements i would pose for the plugin model (maybe they are already though of):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Plugins have hooks to "reopen" themselves (some plugins can be shared across all instances of a SegmentReader)
	&lt;ul&gt;
		&lt;li&gt;These reopen hooks would be called during SegmentReader.reopen()&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Plugins are initialized during "SegmentReader.get/SegmentReader.reopen"
	&lt;ul&gt;
		&lt;li&gt;plugins should not have to be added after the fact, as this would not allow proper warming/initializing of plugins inside the NRT indexing&lt;/li&gt;
		&lt;li&gt;i assume this would need be added as some list of PluginFactories added to the IndexWriter/IndexReader.open()?&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Plugins should have a "close" method that is called in SegmentReader.close()
	&lt;ul&gt;
		&lt;li&gt;This will allow proper release of any resources&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Plugins are passed an instance of the SegmentReader they are for
	&lt;ul&gt;
		&lt;li&gt;Plugins should be able to access all methods on a SegmentReader&lt;/li&gt;
		&lt;li&gt;This would effectively allow overriding a SegmentReader by having a plugin provide the functionality instead (however only people explicitly calling the plugin would get this benefit)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



</comment>
                    <comment id="12850140" author="tsmith" created="Fri, 26 Mar 2010 13:58:33 +0000">&lt;p&gt;Here's a patch (again, against 3.0) showing the minimal API i would like to see from the plugin model&lt;/p&gt;</comment>
                    <comment id="12850323" author="tsmith" created="Fri, 26 Mar 2010 19:42:52 +0000">&lt;p&gt;found one issue with the plugins patch&lt;/p&gt;

&lt;p&gt;With NRT indexing, if the SegmentReader is opened with no TermInfosReader (for merging), then the plugins will be initialized with a SegmentReader that has no ability to walk the TermsEnum.&lt;/p&gt;

&lt;p&gt;I guess SegmentPlugin initialization should wait until after the terms index is loaded or have another method for catching this event to the SegmentPlugin interface&lt;/p&gt;</comment>
                    <comment id="12850348" author="earwin" created="Fri, 26 Mar 2010 21:01:35 +0000">&lt;p&gt;I mentioned this issue earlier. My patch removes loadTermsIndex method from SegmentReader and requires you to reopen it. At that moment you can stuff it with a set of plugins without leaving 'final' paradise.&lt;/p&gt;

&lt;p&gt;There are still things to consider. Some SR guts could be converted to plugins themselves, so you can override them with your implementation if you wish. If that is so, there should be a way to decide which of the plugins should be loaded for current SR mode. My previous design loaded plugins only for full-fledged readers.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is there a ticket or wiki page that details the "plugin" architecture/design so i could take a look?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The design itself was never published, rather it was discussed several times on a mailing list, and simmered inside my head for some time. I have a first impl running at my workplace, but it is really fugly &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I will flesh out a proper description after the refactoring, but it has all your points in one or another form. Biggest differences are - they are keyed by Class not by String, they declare multiple interfaces they provide, they declare dependencies, they don't have to implement/extend anything, as all the hooks are on the factory classes. Also, they extend not only SegmentReader, but the whole hierarchy - SR, MR, DR, whatever.&lt;/p&gt;

&lt;p&gt;The problem of overriding SR methods is solved by delegating these methods to plugins, which can be either default or user-provided. (But remember the question of which subset should be initialized for which SR mode)&lt;/p&gt;</comment>
                    <comment id="12850356" author="creamyg" created="Fri, 26 Mar 2010 21:20:35 +0000">&lt;p&gt;&amp;gt; Is there a ticket or wiki page that details the "plugin" architecture/design&lt;br/&gt;
&amp;gt; so i could take a look?&lt;/p&gt;

&lt;p&gt;FWIW, KinoSearch has a complete prototype implementation of this design, &lt;br/&gt;
based loosely on the mailing list conversations that Earwin referred to.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;SegReader and SegWriter are both composites with minimal APIs.&lt;/li&gt;
	&lt;li&gt;All subcomponents subclass either DataWriter or DataReader.&lt;/li&gt;
	&lt;li&gt;The Architecture class (under KinoSearch::Plan) determines which plugins&lt;br/&gt;
    get loaded.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;a href="http://www.rectangular.com/svn/kinosearch/trunk/core/" class="external-link"&gt;http://www.rectangular.com/svn/kinosearch/trunk/core/&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12850361" author="tsmith" created="Fri, 26 Mar 2010 21:30:12 +0000">&lt;blockquote&gt;&lt;p&gt;My patch removes loadTermsIndex method from SegmentReader and requires you to reopen it. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;that's definitely much cleaner and would solve the issue in my current patch (sadly i'm on 3.0 and want to keep my patch there at a minimum until i can port to all the goodness on 3.1).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, they extend not only SegmentReader, but the whole hierarchy - SR, MR, DR, whatever.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;i just wussed out and just did only the SegmentReader case as thats all i need right now&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;as all the hooks are on the factory classes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;could you post your factory class interface?&lt;br/&gt;
If i base my 3.0 patch off that i can reduce my 3.1 port overhead.&lt;/p&gt;


&lt;p&gt;are there any tickets tracking your reopen refactors or your plugin model?&lt;br/&gt;
If not, feel free to retool this ticket for your plugin model for Index Readers as that will solve my use cases (and then some)&lt;/p&gt;</comment>
                    <comment id="12850726" author="earwin" created="Sun, 28 Mar 2010 20:42:59 +0100">&lt;p&gt;Just created &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2355" title="Refactor Directory/Multi/SegmentReader creation/reopening/cloning/closing"&gt;LUCENE-2355&lt;/a&gt; to track reopen stuff. No issue for plugin model yet, but I'll probably create it, can't edit this one, I'm no committer.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12439777" name="LUCENE-2345_3.0.patch" size="18545" author="tsmith" created="Thu, 25 Mar 2010 14:01:01 +0000"/>
                    <attachment id="12439879" name="LUCENE-2345_3.0.plugins.patch" size="19541" author="tsmith" created="Fri, 26 Mar 2010 13:58:33 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 24 Mar 2010 21:26:23 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11451</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25339</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2340] FixedIntBlockIndexOutput encodes unnecessary integers at the end of a list</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2340</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;At closing time, the current FixedIntBlockIndexOutput flushes blocks of blockSize even if there is only a few integers in the block.&lt;br/&gt;
This can be problematic and causes a big overhead when using large blockSize (e.g., 1024), on small segments or on rare term posting list. &lt;/p&gt;

&lt;p&gt;One solution will be to have a secondary flushBlock method with an additional paramter: the valid length of a buffer. This method will be only called in the FixedIntBlockIndexOutput#close() method.&lt;br/&gt;
The way this particular block of integers are encoded are left to subclasses.&lt;/p&gt;</description>
                <environment/>
            <key id="12459891">LUCENE-2340</key>
            <summary>FixedIntBlockIndexOutput encodes unnecessary integers at the end of a list</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="renaud.delbru">Renaud Delbru</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Mon, 22 Mar 2010 18:01:40 +0000</created>
                <updated>Mon, 13 May 2013 04:04:28 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12848248" author="renaud.delbru" created="Mon, 22 Mar 2010 18:03:49 +0000">&lt;p&gt;A potential patch to the problem that I am using in my codec. The patch also modifies and adapts SimpleIntBlockIndexOutput.&lt;/p&gt;</comment>
                    <comment id="12848274" author="renaud.delbru" created="Mon, 22 Mar 2010 18:35:50 +0000">&lt;p&gt;Fixed SimpleIntBlockIndex* and added unit test. The new implementation of SimpleIntBlockIndex* is even more silly than the previous one, and store a vint at the beginning of each block for recording the length of a block.&lt;/p&gt;</comment>
                    <comment id="12848305" author="mikemccand" created="Mon, 22 Mar 2010 19:55:57 +0000">&lt;blockquote&gt;&lt;p&gt;This can be problematic and causes a big overhead when using large blockSize (e.g., 1024), on small segments or on rare term posting list.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The block is "shared" across postings, so a rare posting list in an otherwise big segment should be fine?&lt;/p&gt;

&lt;p&gt;Small segments will indeed be wasteful, but they'll presumably quickly be merged away.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The new implementation of SimpleIntBlockIndex* is even more silly than the previous one, and store a vint at the beginning of each block for recording the length of a block.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Would other less-silly impls also need to do this?  Ie the thing I want to avoid is foisting onto all block-based codecs the need to track the size of every block...&lt;/p&gt;</comment>
                    <comment id="12848484" author="renaud.delbru" created="Tue, 23 Mar 2010 01:05:56 +0000">&lt;blockquote&gt;
&lt;p&gt;The block is "shared" across postings, so a rare posting list in an otherwise big segment should be fine?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, I didn't got that point until now. Small lists will be inlined with bigger lists over a block.&lt;br/&gt;
However, it means that, most of the time after a seek, you will have to decode a block when only a part of it is of interest (e.g., decode the block, skip the end of the previous posting list, in order to access the start of the posting list you are looking for). But this overhead stays minimal since it will occur only once per query term times the number of posting files. And this overhead is even reduced more if the block compression algorithm is easily "seekable" like for FOR.&lt;br/&gt;
However, I am not sure about the consequences on algorithms like FOR or PFOR which determine the best compression configuration for a block given its list of integers. Certain blocks could be less well compressed since its list of integers could follow more than one distribution. It could be something interesting to try (or are you aware of any kind of experiments or benchmarks that show the benefits/disadvantages of these two approaches ?).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Small segments will indeed be wasteful, but they'll presumably quickly be merged away.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, they will probably disappear after the first merge.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Would other less-silly impls also need to do this? Ie the thing I want to avoid is foisting onto all block-based codecs the need to track the size of every block...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They would have to do something similar, but a clever implementation could reduce well the overhead.&lt;/p&gt;

&lt;p&gt;For example, the core algorithm of Simple9, FOR, PFOR are completely compatible. The only changes I had to made is in *IndexInput#readBlock and *IndexOutput#flushBlock.&lt;br/&gt;
I set the lower bit of numBytes to 0 when the block to write does not have the same size than the previous block. and encode its length (in IndexOutput). Then, in IndexInput, whenever I read a numBytes with the lower bit to 0, I read the length of the block, and pass it to #setUnCompressedData. Since current block algorithms relies on the size of the uncompressed data to terminate their loop, they adapt automatically to the new block size.&lt;/p&gt;

&lt;p&gt;But, indeed, after your comments, the gain seems very minimal, since an incomplete block will only happen once per segment file. The technique I am using was for the case where a block is not "shared" with other posting list.&lt;br/&gt;
But, in the current case, this is not very useful and will add a slight overhead for very large segment file (since I need one bit per block * number of blocks for tracking the end of the block list).&lt;/p&gt;

&lt;p&gt;Thanks for the clarification Michael, and I think you can close the issue, since I am not really sure that my patch is worth keeping given the current way you implemented block-based index.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12439486" name="LUCENE-1458-FixedIntBlockIndexOutput.patch" size="9494" author="renaud.delbru" created="Mon, 22 Mar 2010 18:35:50 +0000"/>
                    <attachment id="12439478" name="LUCENE-1458-FixedIntBlockIndexOutput.patch" size="4336" author="renaud.delbru" created="Mon, 22 Mar 2010 18:03:49 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 22 Mar 2010 19:55:57 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11456</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25379</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2338] Some tests catch Exceptions in separate threads and just print a stack trace - the test does not fail</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2338</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Some tests catch Exceptions in separate threads and just print a stack trace - the test does not fail. The test should fail. Since &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2274" title="Catch exceptions in Threads created by JUnit tasks"&gt;&lt;del&gt;LUCENE-2274&lt;/del&gt;&lt;/a&gt;, the LuceneTestCase(J4) class installs an UncaughtExceptionHandler, so this type of catching and solely printing a Stack trace is a bad idea. Problem is, that the run() method of threads is not allowed to throw checked Exceptions.&lt;/p&gt;

&lt;p&gt;Two possibilities:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Catch checked Exceptions in the run() method and wrap into RuntimeException or call Assert.fail() instead&lt;/li&gt;
	&lt;li&gt;Use Executors&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12459796">LUCENE-2338</key>
            <summary>Some tests catch Exceptions in separate threads and just print a stack trace - the test does not fail</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Sun, 21 Mar 2010 16:12:59 +0000</created>
                <updated>Fri, 10 May 2013 00:05:29 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12849334" author="sanjoy" created="Wed, 24 Mar 2010 18:23:41 +0000">&lt;p&gt;Hi Uwe,&lt;/p&gt;

&lt;p&gt;I can work on this bug.  Do you have any test cases to reproduce this behavior?&lt;/p&gt;

&lt;p&gt;Sanjoy&lt;/p&gt;</comment>
                    <comment id="12849352" author="thetaphi" created="Wed, 24 Mar 2010 18:48:21 +0000">&lt;p&gt;Hi Sanjoy,&lt;/p&gt;

&lt;p&gt;just grep on "printStackTrace(" to find test cases that simply print out stacktraces and ignore exceptions. Possible examples can be seen also in related issues like &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1814" title="Some Lucene tests try and use a Junit Assert in new threads"&gt;&lt;del&gt;LUCENE-1814&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for your interest in fixing this!&lt;/p&gt;</comment>
                    <comment id="13233496" author="mikemccand" created="Tue, 20 Mar 2012 15:38:07 +0000">&lt;p&gt;Our test framework fails tests w/ errant exceptions from threads now...&lt;/p&gt;</comment>
                    <comment id="13233498" author="thetaphi" created="Tue, 20 Mar 2012 15:41:06 +0000">&lt;p&gt;Were all tests already converted to not supress exceptions in threads? This is why the issue is still open...&lt;/p&gt;</comment>
                    <comment id="13233525" author="mikemccand" created="Tue, 20 Mar 2012 16:26:34 +0000">&lt;blockquote&gt;&lt;p&gt;Were all tests already converted to not supress exceptions in threads? This is why the issue is still open...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh, woops: I don't know!&lt;/p&gt;

&lt;p&gt;Reopening...&lt;/p&gt;</comment>
                    <comment id="13237095" author="hossman" created="Fri, 23 Mar 2012 20:40:50 +0000">&lt;p&gt;Issue is marked 3.6 and actively being discussed but has no assignee - assigning to most active committer contributing patches/discussion so far to triage wether this can/should be pushed to 4.0 or not.&lt;/p&gt;</comment>
                    <comment id="13237155" author="thetaphi" created="Fri, 23 Mar 2012 21:29:28 +0000">&lt;p&gt;I remove 3.x branch. I chacked all tests using e.printStackTrace() and all have some (outdated) logic to report the failure. In trunk we should rewrite those tests to simply fail()/rethrow as RuntimeEx on any Exception in threads. This way the failures are reported consistently.&lt;/p&gt;</comment>
                    <comment id="13412299" author="hossman" created="Thu, 12 Jul 2012 00:03:44 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429701" author="rcmuir" created="Tue, 7 Aug 2012 04:41:23 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310040">
                <name>Required</name>
                                <outwardlinks description="requires">
                            <issuelink>
            <issuekey id="12456941">LUCENE-2274</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 24 Mar 2010 18:23:41 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3696</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25381</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2335] optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2335</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff from java-dev thread "Sorting with little memory: A suggestion", started by Toke Eskildsen.&lt;/p&gt;

&lt;p&gt;When sorting by SortField.STRING we currently ask FieldCache for a StringIndex on that field.&lt;/p&gt;

&lt;p&gt;This can consumes tons of RAM, when the values are mostly unique (eg a title field), as it populates both int[] ords as well as String[] values.&lt;/p&gt;

&lt;p&gt;But, if the index is only one segment, and the search sets fillFields=false, we don't need the String[] values, just the int[] ords.  If the app needs to show the fields it can pull them (for the 1 page) from stored fields.&lt;/p&gt;

&lt;p&gt;This can be a potent optimization &amp;#8211; alot of RAM saved &amp;#8211; for optimized indexes.&lt;/p&gt;

&lt;p&gt;When fixing this we must take care to share the int[] ords if some queries do fillFields=true and some =false... ie, FieldCache will be called twice and it should share the int[] ords across those invocations.&lt;/p&gt;</description>
                <environment/>
            <key id="12459721">LUCENE-2335</key>
            <summary>optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Fri, 19 Mar 2010 22:58:46 +0000</created>
                <updated>Fri, 10 May 2013 00:05:30 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12847638" author="mikemccand" created="Fri, 19 Mar 2010 23:00:58 +0000">&lt;p&gt;If anyone wants to take this, feel free!  I won't be able to (for a long time at least!).&lt;/p&gt;</comment>
                    <comment id="12847647" author="yseeley@gmail.com" created="Fri, 19 Mar 2010 23:30:04 +0000">&lt;p&gt;This is also useful for MultiReader (or equivalent if we get rid of it).&lt;br/&gt;
Before we went per-segment searching, I always meant to do an optimization that would skip the String[] part of StringIndex when it wasn't needed.  Of course, now it is needed for per-segment string sorting... but ord-only can still be useful.&lt;/p&gt;</comment>
                    <comment id="12847650" author="toke" created="Fri, 19 Mar 2010 23:34:57 +0000">&lt;p&gt;I obviously have a share in this and would like to give the inner workings a go (I've made a proof of concept at &lt;a href="http://github.com/tokee/lucene" class="external-link"&gt;http://github.com/tokee/lucene&lt;/a&gt; that I'll use as base). However, I am not all that familiar with the finer details of the Lucene code base, so I'll proably need help for integrating the code.&lt;/p&gt;

&lt;p&gt;If I read the Lucene code correctly, the actual Strings need only be resolved for the requested X documents (using FieldComparator.value). If this is true, there is no need for stored fields: We can get the Strings from the indexed fields instead, as we keep track of the ordinals for the Terms.&lt;/p&gt;</comment>
                    <comment id="12847744" author="mikemccand" created="Sat, 20 Mar 2010 08:04:54 +0000">&lt;blockquote&gt;&lt;p&gt;If this is true, there is no need for stored fields: We can get the Strings from the indexed fields instead, as we keep track of the ordinals for the Terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Meaning, after we're done collecting hits for this segment, you'd make a 2nd pass to resolve the ord -&amp;gt; value for all docs that made the cut?  This may be slowish?  Or would you somehow try to do it only at the end, ie for only docs that made the cut across all segments?&lt;/p&gt;

&lt;p&gt;We'd probably want to change the API, somehow, bulk-load the ords, so that we'd make single forward sweep (ie, visit the ords in order).&lt;/p&gt;</comment>
                    <comment id="12847766" author="toke" created="Sat, 20 Mar 2010 11:34:54 +0000">&lt;p&gt;The sort-first-then-resolve-Strings is what I did in the proof of concept. The speed is that of TermsInfoReader, where it delivers a Term from a given position. If this is too slow for multiple segments, the segment-spanning ordered ordinals-approach could be tried.&lt;/p&gt;

&lt;p&gt;As for deprecating stored fields, then I guess there's the issue of spatial locality. Wouldn't moving the bytes into the inverted term index bloat it in a way that makes all searches slower?&lt;/p&gt;

&lt;p&gt;There's an issue of having multiple terms in the same field for a given document, which also ties into facets. It takes some more logic to handle this, but I think it can be done without excessive memory or processing load: Basically we make two passes, where the first pass determines the optimal packed structure and the second pass fills in the ordinals. This would give us a memory overhead of&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
#docs + #references_to_terms + #terms ints
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;for very fast facet structure building with support for collator sorted terms in the facet result. This is basically what we're already doing at Statsbiblioteket - the only real difference is whether the Strings are pulled from the Terms index or from an external structure.&lt;/p&gt;

&lt;p&gt;Saving RAM, this could be be done using PackedInts&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
#docs*log2(#references_to_terms) + #references_to_terms*log2(#terms) + #terms*log2(#terms) bits
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;but I am afraid that access time would suffer. A hybrid&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
#docs*32 + #references_to_terms*32 + #terms*log2(#terms) bits
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;would be just as fast for building as the non-packed version and a wee bit slower for the final fetching of the terms.&lt;/p&gt;

&lt;p&gt;Of course, just as with fillFields=true searches, the calculated Terms must be extracted at the end. For faceting, this can be quite a load.&lt;/p&gt;

&lt;p&gt;The facet-supporting structure is not as simple as the sorting-optimized one. I realize that supporting facets from the start might be quite a large jump. However, if API-breaks are requires, I guess it would be best to do it as few times as possible?&lt;/p&gt;</comment>
                    <comment id="12848380" author="mikemccand" created="Mon, 22 Mar 2010 21:55:23 +0000">&lt;blockquote&gt;&lt;p&gt;The sort-first-then-resolve-Strings is what I did in the proof of concept. The speed is that of TermsInfoReader, where it delivers a Term from a given position. If this is too slow for multiple segments, the segment-spanning ordered ordinals-approach could be tried.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I fear both approaches are too slow.  (This is why we have the&lt;br/&gt;
per-docXfield String values RAM resident now).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As for deprecating stored fields, then I guess there's the issue of spatial locality. Wouldn't moving the bytes into the inverted term index bloat it in a way that makes all searches slower?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We're not planning on deprecating stored fields?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;There's an issue of having multiple terms in the same field for a given document, which also ties into facets.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For sorting it must be a single term right?&lt;/p&gt;

&lt;p&gt;Also, for facets, you have to visit all docs, aggregating, not just&lt;br/&gt;
the final top N in the queue?&lt;/p&gt;

&lt;p&gt;Facets are fun to discuss... but let's keep this issue focused on how&lt;br/&gt;
to optimize the single-segment sorting case with fillFields=false.&lt;/p&gt;</comment>
                    <comment id="12848589" author="toke" created="Tue, 23 Mar 2010 07:10:57 +0000">&lt;p&gt;I can see that I messed up reading your previous answer, regarding stored fields. Let's just forget is as to not confuse the issue further.&lt;/p&gt;

&lt;p&gt;As for facets, they are equivalent to sorting in the aspect that resolving the actual Strings can be delayed until the very end. I'll try and contain myself on the facet subject and focus on sorting though.&lt;/p&gt;

&lt;p&gt;I have used some time tinkering with the problem of spanning multiple segments and it seems to me that the generation of a "global" list of sorted ordinals should be feasible without too much overhead. Basically we want to preserve sequential access as much as possible, so merging sorted ordinals from segments will benefit from a read-ahead cache. By letting the reader deliver ordinals by an iterater, it is free to implement such a cache when necessary. I envision the signature to be something like&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
Iterator&amp;lt;OrdinalTerm&amp;gt; getOrdinalTerms(
      &lt;span class="code-object"&gt;String&lt;/span&gt; persistenceKey, Comparator&amp;lt;&lt;span class="code-object"&gt;Object&lt;/span&gt;&amp;gt; comparator, &lt;span class="code-object"&gt;String&lt;/span&gt; field,
      &lt;span class="code-object"&gt;boolean&lt;/span&gt; collectDocIDs) &lt;span class="code-keyword"&gt;throws&lt;/span&gt; IOException;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where OrdinalTerm contains ordinal, Term and docID.&lt;/p&gt;

&lt;p&gt;The beauty of all this is that the mapping is from docID-&amp;gt;sortedOrdinal index (which it has to be for fast comparison), so keeping the possibility of resolving the Strings after the sort (fillFields=true) is free in terms of storage space and processing time.&lt;/p&gt;

&lt;p&gt;I hope to have a patch out soon for SegmentReader so that it is possible to perform a sorted search "the Lucene way" rather than the hack I use in my proof of concept. However, vacation starts friday...&lt;/p&gt;</comment>
                    <comment id="12848628" author="mikemccand" created="Tue, 23 Mar 2010 09:11:05 +0000">&lt;blockquote&gt;&lt;p&gt;I have used some time tinkering with the problem of spanning multiple segments and it seems to me that the generation of a "global" list of sorted ordinals should be feasible without too much overhead.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you can this it'll be super-awesome &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  The holy grail of "sort by String"...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As for facets, they are equivalent to sorting in the aspect that resolving the actual Strings can be delayed until the very end.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh OK.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I hope to have a patch out soon for SegmentReader so that it is possible to perform a sorted search "the Lucene way" rather than the hack I use in my proof of concept.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I'm looking forward to it!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However, vacation starts friday...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Have a good vacation &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12850055" author="toke" created="Fri, 26 Mar 2010 09:43:33 +0000">&lt;p&gt;Just a little status on development.&lt;/p&gt;

&lt;p&gt;The SegmentReader has been modified to implement the &lt;tt&gt;getOrdinalTerms&lt;/tt&gt; stated above and it works as expected (no surprise, as this is just a refactoring and cleanup of the code from the proof of concept). I'm moving on to &lt;tt&gt;DirectoryReader&lt;/tt&gt;. When that is done, I'll make a preliminary patch. For easy experimentation, I could try and make &lt;tt&gt;the new Sort(new SortField("myfield", new Locale("da")));&lt;/tt&gt; use the new code.&lt;/p&gt;

&lt;p&gt;Sorting 10M terms still takes about 6 minutes on my machine. I've profiled the code and about 40% of the time is spend on requesting terms (with a cache of 20000 terms, locale da, using a conventional harddisk). There's room for optimization by smarter caching of terms and of course using a faster Collator than Java's default. Using simple &lt;tt&gt;String.compareTo&lt;/tt&gt;, which leads to nearly sequential access to terms, takes 44 seconds for the 10M terms. The key to real improvement seems to be a better exploitation of the observation that "Unicode order is fairly aligned to most Locale-based orders". It is already working somewhat as dumb in-memory merge-sort of the 10M Strings takes about 10 minutes. I'll save that for later though.&lt;/p&gt;

&lt;p&gt;Now, for merges there is some decisions to make. The primary one being how to handle terms that are the same for different segments. For the first take, I'll avoid merging such terms and thus allow the iterator to deliver the same term more than once for {{DirectoryReader}}s, but with different ordinals. This should work fine for direct building of a docID-&amp;gt;sortedOrdinalIndex map, usable for sorting, which is the primary use-case.&lt;/p&gt;

&lt;p&gt;If the structure is to be used for faceting, the terms instances needs to be collapsed. Luckily the logic for doing so resides at the code that used the iterator, rather than internally in the readers. Using Exposed for memory-efficient faceting should still be feasible with the general code. If the order of the tags in the facet is not significant, the Unicode-order-comparator can be used, making it very fast (1-2 minutes / 10M unique terms) to build the structure for a given field. ...But I digress and will try and get back on track with the real code.&lt;/p&gt;</comment>
                    <comment id="12850174" author="toke" created="Fri, 26 Mar 2010 15:15:00 +0000">&lt;p&gt;The exposed DirectoryReader is now implemented and the timing does not look horrible. Stepping through ordered terms for a DirectoryReader to build the structure needed for sorting is a bit faster than the sum of sorting the individual segments. That's not quite good enough, but there is still room for a clever read-ahead cache to make iteration more sequential at the SegmentReader-level.&lt;/p&gt;

&lt;p&gt;To give a ball-park figure: Something like 1 minute / 1 million terms for segment-level sorting, which is re-used for non-changed segments on re-open. Then something like half that time for directory-level merging, which must be done fully at re-open.&lt;/p&gt;

&lt;p&gt;There's no easy to use plug-in replacement yet (and it seems hard to do anyway, as the sorter gets the readers one at a time) and the code at github is in shambles, so no patch either. Sorry. I expect to get back to hacking in a week.&lt;/p&gt;</comment>
                    <comment id="12853847" author="toke" created="Tue, 6 Apr 2010 11:41:17 +0100">&lt;p&gt;I've mis-read Michael McCandless intention with this issue and have been working under a broader scope. A new issue has been opened at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2369" class="external-link"&gt;https://issues.apache.org/jira/browse/LUCENE-2369&lt;/a&gt; where I will continue that work.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2335" title="optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache"&gt;LUCENE-2335&lt;/a&gt; could be implemented by extending &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2369" title="Locale-based sort by field with low memory overhead"&gt;LUCENE-2369&lt;/a&gt; to take null as a comparator. For single-segments this should be on par with a dedicated implementation and for multi-segments the merging of the ordinals should be fairly efficient. The big downside is that this would be a hard decision, meaning that multi-segment sorted search without locale would always have the start-up overhead.&lt;/p&gt;

&lt;p&gt;I guess the better choice would be to recommend using &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2335" title="optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache"&gt;LUCENE-2335&lt;/a&gt; for non-locale based sorted searches only if increased startup-time is acceptable.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 19 Mar 2010 23:30:04 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11460</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25384</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2296] Include UML diagrams in javadocs</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2296</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;As Lucene source becomes more and more complex it would be helpful to provide a visual overview of Lucene classes in a form of UML class diagrams. These can be created automatically during the build process, using ApiViz (&lt;a href="http://code.google.com/p/apiviz/" class="external-link"&gt;http://code.google.com/p/apiviz/&lt;/a&gt;).&lt;/p&gt;</description>
                <environment>&lt;p&gt;Any environment supported by AT&amp;amp;T GraphViz, e.g. Linux, MS Windows, etc.&lt;/p&gt;</environment>
            <key id="12458232">LUCENE-2296</key>
            <summary>Include UML diagrams in javadocs</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="ab">Andrzej Bialecki </reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Mar 2010 11:45:08 +0000</created>
                <updated>Fri, 10 May 2013 00:05:30 +0100</updated>
                                    <version>3.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>2</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="12841826" author="ab" created="Fri, 5 Mar 2010 11:55:31 +0000">&lt;p&gt;Example class diagram for org.apache.lucene.search. Graphs are clickable, i.e. they are linked to respective javadoc pages.&lt;/p&gt;</comment>
                    <comment id="12841827" author="ab" created="Fri, 5 Mar 2010 11:59:26 +0000">&lt;p&gt;Patch to use the apiviz doclet. This assumes that the apiviz jar was downloaded manually (apiviz is LGPL). We can add ant magic to download it automatically from its site.&lt;/p&gt;</comment>
                    <comment id="12841830" author="ab" created="Fri, 5 Mar 2010 12:28:02 +0000">&lt;p&gt;Here's a similar graph built from Solr sources, to befriend hardcore Luceners with Solr ... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13219771" author="feristhia" created="Thu, 1 Mar 2012 03:39:43 +0000">&lt;p&gt;Hi Andrzej,&lt;/p&gt;

&lt;p&gt;Do you have a downloadable generated UML for this ? Thx.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12438003" name="apiviz.patch" size="496" author="ab" created="Fri, 5 Mar 2010 11:59:26 +0000"/>
                    <attachment id="12438004" name="package-summary.png" size="118809" author="ab" created="Fri, 5 Mar 2010 12:28:02 +0000"/>
                    <attachment id="12438002" name="package-summary.png" size="393775" author="ab" created="Fri, 5 Mar 2010 11:55:31 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 1 Mar 2012 03:39:43 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2914</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25423</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2271] Function queries producing scores of -inf or NaN (e.g. 1/x) return incorrect results with TopScoreDocCollector</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2271</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is a foolowup to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2270" title="queries with zero boosts don&amp;#39;t work"&gt;&lt;del&gt;LUCENE-2270&lt;/del&gt;&lt;/a&gt;, where a part of this problem was fixed (boost = 0 leading to NaN scores, which is also un-intuitive), but in general, function queries in Solr can create these invalid scores easily. In previous version of Lucene these scores ordered correct (except NaN, which mixes up results), but never invalid document ids are returned (like Integer.MAX_VALUE).&lt;/p&gt;

&lt;p&gt;The problem is: TopScoreDocCollector pre-fills the HitQueue with sentinel ScoreDocs with a score of -inf and a doc id of Integer.MAX_VALUE. For the HQ to work, this sentinel must be smaller than all posible values, which is not the case:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;-inf is equal and the document is not inserted into the HQ, as not competitive, but the HQ is not yet full, so the sentinel values keep in the HQ and result is the Integer.MAX_VALUE docs. This problem is solveable (and only affects the Ordered collector) by chaning the exit condition to:
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (score &amp;lt;= pqTop.score &amp;amp;&amp;amp; pqTop.doc != &lt;span class="code-object"&gt;Integer&lt;/span&gt;.MAX_VALUE) {
    &lt;span class="code-comment"&gt;// Since docs are returned in-order (i.e., increasing doc Id), a document
&lt;/span&gt;    &lt;span class="code-comment"&gt;// with equal score to pqTop.score cannot compete since HitQueue favors
&lt;/span&gt;    &lt;span class="code-comment"&gt;// documents with lower doc Ids. Therefore reject those docs too.
&lt;/span&gt;    &lt;span class="code-keyword"&gt;return&lt;/span&gt;;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;The NaN case can be fixed in the same way, but then has another problem: all comparisons with NaN result in false (none of these is true): x &amp;lt; NaN, x &amp;gt; NaN, NaN == NaN. This leads to the fact that HQ's lessThan always returns false, leading to unexspected ordering in the PQ and sometimes the sentinel values do not stay at the top of the queue. A later hit then overrides the top of the queue but leaves the incorrect sentinels  unchanged -&amp;gt; invalid results. This can be fixed in two ways in HQ:&lt;br/&gt;
Force all sentinels to the top:
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;protected&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;boolean&lt;/span&gt; lessThan(ScoreDoc hitA, ScoreDoc hitB) {
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (hitA.doc == &lt;span class="code-object"&gt;Integer&lt;/span&gt;.MAX_VALUE)
      &lt;span class="code-keyword"&gt;return&lt;/span&gt; &lt;span class="code-keyword"&gt;true&lt;/span&gt;;
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (hitB.doc == &lt;span class="code-object"&gt;Integer&lt;/span&gt;.MAX_VALUE)
      &lt;span class="code-keyword"&gt;return&lt;/span&gt; &lt;span class="code-keyword"&gt;false&lt;/span&gt;;
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (hitA.score == hitB.score)
      &lt;span class="code-keyword"&gt;return&lt;/span&gt; hitA.doc &amp;gt; hitB.doc; 
    &lt;span class="code-keyword"&gt;else&lt;/span&gt;
      &lt;span class="code-keyword"&gt;return&lt;/span&gt; hitA.score &amp;lt; hitB.score;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or alternatively have a defined order for NaN (Float.compare sorts them after +inf):&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;protected&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;boolean&lt;/span&gt; lessThan(ScoreDoc hitA, ScoreDoc hitB) {
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (hitA.score == hitB.score)
      &lt;span class="code-keyword"&gt;return&lt;/span&gt; hitA.doc &amp;gt; hitB.doc; 
    &lt;span class="code-keyword"&gt;else&lt;/span&gt;
      &lt;span class="code-keyword"&gt;return&lt;/span&gt; &lt;span class="code-object"&gt;Float&lt;/span&gt;.compare(hitA.score, hitB.score) &amp;lt; 0;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The problem with both solutions is, that we have now more comparisons per hit and the use of sentinels is questionable. I would like to remove the sentinels and use the old pre 2.9 code for comparing and using PQ.add() when a competitive hit arrives. The order of NaN would be unspecified.&lt;/p&gt;

&lt;p&gt;To fix the order of NaN, it would be better to replace all score comparisons by Float.compare() &lt;span class="error"&gt;&amp;#91;also in FieldComparator&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;I would like to delay 2.9.2 and 3.0.1 until this problem is discussed and solved.&lt;/p&gt;</description>
                <environment/>
            <key id="12456755">LUCENE-2271</key>
            <summary>Function queries producing scores of -inf or NaN (e.g. 1/x) return incorrect results with TopScoreDocCollector</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Feb 2010 08:05:22 +0000</created>
                <updated>Fri, 10 May 2013 00:05:30 +0100</updated>
                                    <version>2.9</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12835678" author="rcmuir" created="Fri, 19 Feb 2010 10:15:54 +0000">&lt;p&gt;its not a bug, as its doc'ed to work this way.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
 * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;NOTE&amp;lt;/b&amp;gt;: The values &lt;span class="code-object"&gt;Float&lt;/span&gt;.Nan,
 * &lt;span class="code-object"&gt;Float&lt;/span&gt;.NEGATIVE_INFINITY and &lt;span class="code-object"&gt;Float&lt;/span&gt;.POSITIVE_INFINITY are
 * not valid scores.  This collector will not properly
 * collect hits with such scores.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12835683" author="rcmuir" created="Fri, 19 Feb 2010 10:23:45 +0000">&lt;p&gt;I don't think we should do anything to fix NaN, such as using more expensive comparisons (Float.compareTo) and stuff. as it is not a number, its an invalid score. &lt;/p&gt;

&lt;p&gt;i think function queries shoudl throw and exception instead of producing NaN, this problem is only limited to function queries.&lt;/p&gt;

&lt;p&gt;I think fixing scores of negative infinity make more sense, as these are unpreventable (again only a problem with function queries!) and at least negative infinity is actually a number.&lt;/p&gt;

&lt;p&gt;i think "fixing" a top-N collector, or "fixing" anything that sorts NaN is wrong. NaN doesnt have a properly defined sort order. NaN has an order hacked into Float.compareTo, but this is different. sorting the primitive type makes no sense, and the documentation should stay that it doesnt work with TSDC.&lt;/p&gt;</comment>
                    <comment id="12835699" author="thetaphi" created="Fri, 19 Feb 2010 11:11:35 +0000">&lt;p&gt;This is patch that supports NaN and -inf.&lt;/p&gt;

&lt;p&gt;The cost of the additional checks in HitQueue.lessThan are neglectible, as they only occur when a competitive hit is really inserted into the queue. The check enforces all sentinels to the top of the queue, regardless what their score is (because always NaN != NaN).&lt;/p&gt;</comment>
                    <comment id="12835700" author="rcmuir" created="Fri, 19 Feb 2010 11:14:23 +0000">&lt;p&gt;attached is a patch, written by Uwe. as far as a "bugfix" i prefer this patch, as the more complicated, performance-intrusive NaN fixes I think should be something we do in 3.1&lt;/p&gt;

&lt;p&gt;e.g., "fixing" NaN to work will likely slow down people getting large numbers of results, and i don't think we should do that in bugfix releases. &lt;/p&gt;

&lt;p&gt;but in 3.1, we could change it, include some large results-oriented collectors for these people, and the whole thing would make sense.&lt;/p&gt;</comment>
                    <comment id="12835701" author="thetaphi" created="Fri, 19 Feb 2010 11:14:25 +0000">&lt;p&gt;Sorry reverted a comment remove.&lt;/p&gt;</comment>
                    <comment id="12835717" author="rcmuir" created="Fri, 19 Feb 2010 12:02:05 +0000">&lt;blockquote&gt;&lt;p&gt;The cost of the additional checks in HitQueue.lessThan are neglectible, as they only occur when a competitive hit is really inserted into the queue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This should be benchmarked for MultiSearcher and ParallelMultiSearcher, too, as they also use HitQueue.&lt;/p&gt;</comment>
                    <comment id="12835743" author="thetaphi" created="Fri, 19 Feb 2010 13:35:12 +0000">&lt;p&gt;Patch with testcases for trunk, but should work on branches, too (after removing @Override). Without the fixes in HitQueue or TSDC the tests fail.&lt;/p&gt;</comment>
                    <comment id="12835750" author="yseeley@gmail.com" created="Fri, 19 Feb 2010 13:56:44 +0000">&lt;blockquote&gt;&lt;p&gt;its not a bug, as its doc'ed to work this way. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, so it was a design bug too.&lt;/p&gt;</comment>
                    <comment id="12835758" author="rcmuir" created="Fri, 19 Feb 2010 14:21:09 +0000">&lt;blockquote&gt;&lt;p&gt;OK, so it was a design bug too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A design bug that function queries score docs with an invalid score (NaN) instead of throwing an exception?&lt;/p&gt;</comment>
                    <comment id="12835762" author="yseeley@gmail.com" created="Fri, 19 Feb 2010 14:40:54 +0000">&lt;blockquote&gt;&lt;p&gt;A design bug that function queries score docs with an invalid score (NaN) instead of throwing an exception?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, a design bug that -Inf scores were disallowed, esp since they were handled just fine in the past.&lt;/p&gt;

&lt;p&gt;NaN is different - it's more of a judgement call depending on the cost to handle it.&lt;/p&gt;</comment>
                    <comment id="12835763" author="thetaphi" created="Fri, 19 Feb 2010 14:49:00 +0000">&lt;p&gt;The cost to handle NaN is the modified lessThan() in HitQueue.&lt;/p&gt;</comment>
                    <comment id="12835780" author="thetaphi" created="Fri, 19 Feb 2010 15:37:03 +0000">&lt;p&gt;Improved test, that also checks for increasing doc ids when score identical&lt;/p&gt;</comment>
                    <comment id="12836178" author="yseeley@gmail.com" created="Sat, 20 Feb 2010 14:38:14 +0000">&lt;p&gt;I'm starting to think that handling NaN is as important as handling the infinities.&lt;br/&gt;
This is because once you allow infinities, it's easy to get a NaN with a simple multiplication by 0.&lt;/p&gt;</comment>
                    <comment id="12836179" author="rcmuir" created="Sat, 20 Feb 2010 14:47:51 +0000">&lt;blockquote&gt;&lt;p&gt;This is because once you allow infinities, it's easy to get a NaN with a simple multiplication by 0. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;maybe we should keep it as is then, and only allow finite results&lt;/p&gt;</comment>
                    <comment id="12836180" author="yseeley@gmail.com" created="Sat, 20 Feb 2010 14:51:43 +0000">&lt;blockquote&gt;&lt;p&gt;maybe we should keep it as is then, and only allow finite results&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But how?  Finite results can combine and overflow into infinite results with things like boolean query, so we can't give a definite range of values that a query can safely produce.&lt;/p&gt;</comment>
                    <comment id="12836181" author="rcmuir" created="Sat, 20 Feb 2010 14:57:00 +0000">&lt;blockquote&gt;&lt;p&gt;But how? Finite results can combine and overflow into infinite results with things like boolean query, so we can't give a definite range of values that a query can safely produce.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;right, but this really only happens with function queries. this is my problem, with normal queries how can this happen?&lt;br/&gt;
its a very very very very special case, and it seems overkill to add all these checks to performance-sensitive top-docs collection just for function queries, to handle NaN and infinites&lt;/p&gt;</comment>
                    <comment id="12836182" author="thetaphi" created="Sat, 20 Feb 2010 14:57:56 +0000">&lt;p&gt;In my opinion we should fix it using the attached patch and in the future 3.1 do some refactoring:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;no sentinels&lt;/li&gt;
	&lt;li&gt;define a order for NaN, as NaN breaks the complete order of results (because PQ cannot handle the case that lessThan(a,b) returns false and also lessThan(b,a) when NaN is involved)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12836183" author="rcmuir" created="Sat, 20 Feb 2010 15:01:25 +0000">&lt;blockquote&gt;&lt;p&gt;In my opinion we should fix it using the attached patch and in the future 3.1 do some refactoring: &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;in general agree, on one condition that it doesnt slow down normal queries, MultiSearcher, or ParallelMultiSearcher significantly (benchmarks)&lt;/p&gt;

&lt;p&gt;in my opinion the thing is documented not to work for these values, so its not a bug at all. if we want to support these values we should redesign the collector (in 3.1)&lt;/p&gt;</comment>
                    <comment id="12836191" author="thetaphi" created="Sat, 20 Feb 2010 15:55:21 +0000">&lt;p&gt;Here a simplier patch with sentinels removed. You can maybe think about a better if-check in the out of order collector&lt;/p&gt;</comment>
                    <comment id="12836194" author="thetaphi" created="Sat, 20 Feb 2010 16:11:57 +0000">&lt;p&gt;Sorry, insertWithOverflow is correct!&lt;/p&gt;</comment>
                    <comment id="12836197" author="creamyg" created="Sat, 20 Feb 2010 16:24:21 +0000">&lt;p&gt;An awful lot of thought went into optimizing those collection algorithms.  I&lt;br/&gt;
disagree with many of the design decisions that were made, but it seems rushed&lt;br/&gt;
to blithely revert those optimizations.&lt;/p&gt;

&lt;p&gt;FWIW, the SortCollector in KS (designed on the Lucy list last spring, would be&lt;br/&gt;
in Lucy but some prereqs haven't gone in yet) doesn't have the problem with&lt;br/&gt;
-Inf sentinels.  It uses an array of "actions" representing sort rules to&lt;br/&gt;
determine whether a hit is "competitive" and should be inserted into the&lt;br/&gt;
queue; the first action is set to AUTO_ACCEPT (meaning try inserting the hit&lt;br/&gt;
into the queue) until the queue fills up, and then again to AUTO_ACCEPT at the&lt;br/&gt;
start of each segment.  It's not necessary to fill up the queue with dummy&lt;br/&gt;
hits beforehand.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-none"&gt;
static INLINE bool_t
SI_competitive(SortCollector *self, i32_t doc_id)
{
    u8_t *const actions = self-&amp;gt;actions;
    u32_t i = 0;

    /* Iterate through our array of actions, returning as quickly as
     * possible. */
    do {
        switch (actions[i] &amp;amp; ACTIONS_MASK) {
            case AUTO_ACCEPT:
                return true;
            case AUTO_REJECT:
                return false;
            case AUTO_TIE:
                break;
            case COMPARE_BY_SCORE: {
                    float score = Matcher_Score(self-&amp;gt;matcher);
                    if  (score &amp;gt; self-&amp;gt;bubble_score) {
                        self-&amp;gt;bumped-&amp;gt;score = score;
                        return true;
                    }
                    else if (score &amp;lt; self-&amp;gt;bubble_score) {
                        return false;
                    }
                }
                break;
            case COMPARE_BY_SCORE_REV: {
                // ...
            case COMPARE_BY_DOC_ID:
                // ...
            case COMPARE_BY_ORD1: {
                // ...
        }
    } while (++i &amp;lt; self-&amp;gt;num_actions);

    /* If we've made it this far and we're still tied, reject the doc so that
     * we prefer items already in the queue.  This has the effect of
     * implicitly breaking ties by doc num, since docs are collected in order.
     */
    return false;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12836203" author="thetaphi" created="Sat, 20 Feb 2010 16:50:50 +0000">&lt;p&gt;Attached is the patch with the testcase from the previous one (with sentinels). All tests pass.&lt;/p&gt;</comment>
                    <comment id="12836214" author="mikemccand" created="Sat, 20 Feb 2010 18:03:44 +0000">&lt;p&gt;I would rather not change the core default collector here, when&lt;br/&gt;
sorting by score.&lt;/p&gt;

&lt;p&gt;All of the patches being considered would add an extra comparison/and&lt;br/&gt;
(or maybe if) per collect call, which while presumably small, is still&lt;br/&gt;
an added cost.&lt;/p&gt;

&lt;p&gt;Likely this added cost is in the noise of a benchmark so we wouldn't&lt;br/&gt;
be able to conclude much by benching... but enough of these added&lt;br/&gt;
costs will eventually add up.  At the end of the day you are adding&lt;br/&gt;
non-zero work for the CPU to do, for every collect.&lt;/p&gt;

&lt;p&gt;This is Lucene's searching hotspot, so we should only be adding extra&lt;br/&gt;
code on this critical path when it's really needed.&lt;/p&gt;

&lt;p&gt;It's ashame to make everyone pay that price when in practice very few&lt;br/&gt;
users need to collect -Inf/NaN scoring docs.  This hasn't been hit in&lt;br/&gt;
a "real" use case yet.&lt;/p&gt;

&lt;p&gt;In fact, before 2.9 (ie up to and including 2.4), we by default&lt;br/&gt;
filtered out all docs scoring &amp;lt;= 0.0, so -Inf and NaN were always&lt;br/&gt;
filtered out, anyway.&lt;/p&gt;

&lt;p&gt;Users who somehow do hit this in a real use case have a good&lt;br/&gt;
workaround: use TopFieldCollector, sorting by Relevance.  This will&lt;br/&gt;
properly handle -Inf, and will at least collect NaN scoring docs (but&lt;br/&gt;
their sort order will be random, as it always has).  Or, use&lt;br/&gt;
PositiveScoresOnlyCollector (to get back to Lucene 2.4 behavior).  Or,&lt;br/&gt;
create a custom collector.&lt;/p&gt;</comment>
                    <comment id="12836217" author="thetaphi" created="Sat, 20 Feb 2010 18:12:48 +0000">&lt;p&gt;Here is a new impl that only has exactly one additional check in the initial collection (when th pq is not yet full). After the PQ is full, the collector is replaced by the short-cutting one.&lt;/p&gt;

&lt;p&gt;This impl should even be faster than before, if the additional method call does not count and is removed by the JVM (which it should, because its clearly predictable)&lt;/p&gt;</comment>
                    <comment id="12836220" author="thetaphi" created="Sat, 20 Feb 2010 18:16:21 +0000">&lt;p&gt;more improved&lt;/p&gt;</comment>
                    <comment id="12836228" author="thetaphi" created="Sat, 20 Feb 2010 18:38:18 +0000">&lt;p&gt;More optimized version with more local variables. This is the version for the benchmark-try.&lt;/p&gt;</comment>
                    <comment id="12836232" author="thetaphi" created="Sat, 20 Feb 2010 19:16:10 +0000">&lt;p&gt;Here a benchmark task made by grant. Run collector.alg and wait long enough.&lt;/p&gt;</comment>
                    <comment id="12836252" author="thetaphi" created="Sat, 20 Feb 2010 20:16:10 +0000">&lt;p&gt;More improved version, now equal to prefilled queue case, as the collector reuses overflowed ScoreDoc instances.&lt;/p&gt;</comment>
                    <comment id="12836265" author="thetaphi" created="Sat, 20 Feb 2010 21:39:33 +0000">&lt;p&gt;I did some benchmarks (Java 1.5.0_22, 64bit, Win7, Core2Duo P8700) will do more tomorrow when i set up a large testing environment with 3 separate checkouts containing the three collector versions):&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;The latest approach (&lt;a href="https://issues.apache.org/jira/secure/attachment/12436458/LUCENE-2271.patch" class="external-link"&gt;https://issues.apache.org/jira/secure/attachment/12436458/LUCENE-2271.patch&lt;/a&gt;) with no sentinels using the delegation and exchanging the inner collector was as fast as the original unpatched version&lt;/li&gt;
	&lt;li&gt;The approach with sentinels but fixed HitQueue ordering and extra checks (&lt;a href="https://issues.apache.org/jira/secure/attachment/12436329/LUCENE-2271.patch" class="external-link"&gt;https://issues.apache.org/jira/secure/attachment/12436329/LUCENE-2271.patch&lt;/a&gt;), showed (as exspected) a little overhead: The ordered collector was as fast as the unpatched unordered collector (because one check more) - so i would not use this patch&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12836273" author="thetaphi" created="Sat, 20 Feb 2010 22:22:28 +0000">&lt;p&gt;Fix an issue when numDocs==0.&lt;/p&gt;</comment>
                    <comment id="12836384" author="mikemccand" created="Sun, 21 Feb 2010 15:36:56 +0000">&lt;p&gt;This is a delightfully clever solution, delegating to a contained&lt;br/&gt;
collector and then swapping in one collector for the startup (when the&lt;br/&gt;
queue is not yet full) and then a new collector once the queue is&lt;br/&gt;
full.&lt;/p&gt;

&lt;p&gt;It's the closest equivalent we in javaland can reach, to using&lt;br/&gt;
function pointers in C.&lt;/p&gt;

&lt;p&gt;But, I don't think we should commit this &amp;#8211; this makes the code more&lt;br/&gt;
complex, and doesn't really gain enough (performance is the same) in&lt;br/&gt;
return.  It also relies more heavily on the JVM being able to optimize away&lt;br/&gt;
the method call.&lt;/p&gt;

&lt;p&gt;Yes it handles -inf/nan, but I don't think Lucene's default&lt;br/&gt;
sort-by-relevance collector needs to (prior to 2.9 we silently&lt;br/&gt;
filtered out such hits, as well as all hits with score &amp;lt;= 0.0).&lt;/p&gt;</comment>
                    <comment id="12836400" author="mikemccand" created="Sun, 21 Feb 2010 17:01:11 +0000">&lt;p&gt;I think we should fix a few doc issues, and add asserts (attached patch for trunk).&lt;/p&gt;</comment>
                    <comment id="12836451" author="thetaphi" created="Sun, 21 Feb 2010 21:41:44 +0000">&lt;p&gt;After applying Mike's patch (with modified asserts to correctly detect NaN), updated my patch of the delegating and -inf/NaN aware TopScoreDocCollector.&lt;/p&gt;

&lt;p&gt;Maybe we should add it as a separate collector for function queries in 3.1. Maybe with correct NaN ordering?&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12456940">SOLR-1785</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12436452" name="LUCENE-2271-bench.patch" size="27424" author="thetaphi" created="Sat, 20 Feb 2010 19:16:10 +0000"/>
                    <attachment id="12436540" name="LUCENE-2271-maybe-as-separate-collector.patch" size="12301" author="thetaphi" created="Sun, 21 Feb 2010 21:41:44 +0000"/>
                    <attachment id="12436503" name="LUCENE-2271.patch" size="2996" author="mikemccand" created="Sun, 21 Feb 2010 17:01:11 +0000"/>
                    <attachment id="12436463" name="LUCENE-2271.patch" size="11369" author="thetaphi" created="Sat, 20 Feb 2010 22:22:28 +0000"/>
                    <attachment id="12436458" name="LUCENE-2271.patch" size="10053" author="thetaphi" created="Sat, 20 Feb 2010 20:18:30 +0000"/>
                    <attachment id="12436443" name="LUCENE-2271.patch" size="6551" author="thetaphi" created="Sat, 20 Feb 2010 16:50:50 +0000"/>
                    <attachment id="12436329" name="LUCENE-2271.patch" size="5855" author="thetaphi" created="Fri, 19 Feb 2010 15:37:03 +0000"/>
                    <attachment id="12436308" name="TSDC.patch" size="1306" author="rcmuir" created="Fri, 19 Feb 2010 11:14:23 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 19 Feb 2010 10:15:54 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11518</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25448</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2264] Add missing tests for PayloadXxxQuery</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2264</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is a followup for  &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1941" title="MinPayloadFunction returns 0 when only one payload is present"&gt;&lt;del&gt;LUCENE-1941&lt;/del&gt;&lt;/a&gt; and the discussion in IRC. The Payload queries have no real working tests, esp they are missing for the Min/Max/Avg functions.&lt;/p&gt;</description>
                <environment/>
            <key id="12456262">LUCENE-2264</key>
            <summary>Add missing tests for PayloadXxxQuery</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Sun, 14 Feb 2010 12:49:25 +0000</created>
                <updated>Fri, 10 May 2013 00:05:30 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                        <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12437147">LUCENE-1941</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11525</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25455</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2262] QueryParser should now allow leading '?' wildcards</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2262</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;QueryParser currently throws an exception if a wildcard term begins with the '?' operator.&lt;/p&gt;

&lt;p&gt;The current documentation describes why this is:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;When set, * or ? are allowed as the first character of a PrefixQuery and WildcardQuery.
Note that this can produce very slow queries on big indexes. 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the flexible indexing branch, wildcard queries with leading '?' operator are no longer slow on big indexes (they do not enumerate terms in linear fashion).&lt;br/&gt;
Thus, it no longer makes sense to throw a ParseException for a leading '?'&lt;/p&gt;

&lt;p&gt;So, users should be able to perform a query of "?foo" and no longer get a ParseException from the QueryParser.&lt;/p&gt;

&lt;p&gt;For the flexible indexing branch, wildcard queries of  'foo?', '?foo', 'f?oo', etc are all the same from a performance perspective.&lt;/p&gt;</description>
                <environment/>
            <key id="12456239">LUCENE-2262</key>
            <summary>QueryParser should now allow leading '?' wildcards</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Sat, 13 Feb 2010 19:24:36 +0000</created>
                <updated>Fri, 10 May 2013 00:05:30 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12833442" author="rcmuir" created="Sat, 13 Feb 2010 19:33:30 +0000">&lt;p&gt;initial patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;doesnt throw the exception for leading ? when matchVersion &amp;gt;= 3.1&lt;/li&gt;
	&lt;li&gt;contrib's queryparser doesnt use Version, so i simply change AllowLeadingWildcardProcessor&lt;/li&gt;
	&lt;li&gt;the backwards tests will fail because &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2248" title="Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 starts"&gt;&lt;del&gt;LUCENE-2248&lt;/del&gt;&lt;/a&gt;: TestQueryParser uses LUCENE_CURRENT when it should really be using some constant that resolves to LUCENE_30: we can fix the test to use LUCENE_30 instead of LUCENE_CURRENT&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12833443" author="rcmuir" created="Sat, 13 Feb 2010 19:34:31 +0000">&lt;p&gt;patch for the backwards tests (not the best I admit, but uwe has fixed this in general for the future)&lt;/p&gt;</comment>
                    <comment id="12833481" author="yseeley@gmail.com" created="Sun, 14 Feb 2010 00:31:27 +0000">&lt;p&gt;It doesn't seem like this type of bug is not one that people would rely on (throwing an exception for ? but not for *), hence we should just fix and not make it conditional on matchVersion?&lt;/p&gt;</comment>
                    <comment id="12833485" author="rcmuir" created="Sun, 14 Feb 2010 01:16:56 +0000">&lt;blockquote&gt;&lt;p&gt;It doesn't seem like this type of bug is not one that people would rely on (throwing an exception for ? but not for *&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yonik, that's not what this issue is about. have you looked at the patch?&lt;/p&gt;</comment>
                    <comment id="12833488" author="rcmuir" created="Sun, 14 Feb 2010 01:52:49 +0000">&lt;p&gt;btw Yonik, in my opinion disallowing these queries with leading wildcards, be it * or ? or whatever, is rather silly, since we allow even slower fuzzyqueries by default.&lt;br/&gt;
so another option would be to do away with this 'setAllowLeadingWildcard' stuff alltogether, and if you want to disallow something, extend QueryParser yourself and throw ParseException yourself. I would actually prefer this.&lt;/p&gt;

&lt;p&gt;i do think we should provide back compat to duplicate the previous behavior, its perfectly reasonable that someone would have special handling for these leading wildcards (e.g. Solr does).&lt;/p&gt;

&lt;p&gt;but this issue is simply about the fact that in flex branch, leading ? is now fast, so there is no excuse to disallow it anymore, hence we should relax the queryparser to only throw a fit on leading *&lt;/p&gt;</comment>
                    <comment id="12833493" author="yseeley@gmail.com" created="Sun, 14 Feb 2010 03:01:06 +0000">&lt;blockquote&gt;&lt;p&gt;yonik, that's not what this issue is about. have you looked at the patch?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nope, I was going by the comments. Looks like I misinterpreted them.  Sorry.&lt;/p&gt;

&lt;p&gt;As an aside: In solr-land I encourage adding API examples, etc, to lower the bar for giving feedback (i.e. they don't have to read the patch to give an opinion on what the syntax of a new request should look like).  That's not a criticism of your description (which was fine) but rather the implication that I should have read the patch (and I may have misread that too).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;in my opinion disallowing these queries with leading wildcards, be it * or ? or whatever, is rather silly, since we allow even slower fuzzyqueries by default.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree.&lt;/p&gt;</comment>
                    <comment id="12833494" author="rcmuir" created="Sun, 14 Feb 2010 03:16:58 +0000">&lt;p&gt;update description to be less terse&lt;/p&gt;</comment>
                    <comment id="12833496" author="rcmuir" created="Sun, 14 Feb 2010 03:22:20 +0000">&lt;blockquote&gt;&lt;p&gt;in my opinion disallowing these queries with leading wildcards, be it * or ? or whatever, is rather silly, since we allow even slower fuzzyqueries by default.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;Agree.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What do you think, should we skip this step then and simply deprecate the entire setAllowLeadingWildcard concept all together, setting it to true for Version &amp;gt;= 3.1?&lt;/p&gt;

&lt;p&gt;The only concern I have is that doing this might mislead someone into thinking leading * is no longer slow, which it still is (its faster than 3.0 but the computational complexity is still the same, unlike ?). Here is a comparison of 3.0 and trunk, on 10M terms, from the &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1606" title="Automaton Query/Filter (scalable regex)"&gt;&lt;del&gt;LUCENE-1606&lt;/del&gt;&lt;/a&gt; benchmark:&lt;/p&gt;

&lt;table class='confluenceTable'&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class='confluenceTh'&gt;Pattern&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Avg query time (ms) Lucene 3.0&lt;/th&gt;
&lt;th class='confluenceTh'&gt;Avg query time (ms) Flex&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;?NNNNNN&lt;/td&gt;
&lt;td class='confluenceTd'&gt;2408.5&lt;/td&gt;
&lt;td class='confluenceTd'&gt;28.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class='confluenceTd'&gt;*NNNNNN&lt;/td&gt;
&lt;td class='confluenceTd'&gt;3258.3&lt;/td&gt;
&lt;td class='confluenceTd'&gt;1048.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12435781" name="LUCENE-2262_backwards.patch" size="893" author="rcmuir" created="Sat, 13 Feb 2010 19:34:31 +0000"/>
                    <attachment id="12435780" name="LUCENE-2262.patch" size="9314" author="rcmuir" created="Sat, 13 Feb 2010 19:33:30 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 14 Feb 2010 00:31:27 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11527</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25457</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2227] separate chararrayset interface from impl</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2227</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;CharArraySet should be abstract&lt;br/&gt;
the hashing implementation currently being used should instead be called CharArrayHashSet&lt;/p&gt;

&lt;p&gt;currently our 'CharArrayHashSet' is hardcoded across Lucene, but others might want their own impl.&lt;br/&gt;
For example, implementing CharArraySet as DFA with org.apache.lucene.util.automaton gives faster contains(char[], int, int) performance, as it can do a 'fast fail' and need not hash the entire string.&lt;/p&gt;

&lt;p&gt;This is useful as it speeds up indexing in StopFilter.&lt;/p&gt;

&lt;p&gt;I did not think this would be faster but i did benchmarks over and over with the reuters corpus, and it is, even with english text's wierd average word length of 5&lt;/p&gt;</description>
                <environment/>
            <key id="12445927">LUCENE-2227</key>
            <summary>separate chararrayset interface from impl</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Tue, 19 Jan 2010 12:59:54 +0000</created>
                <updated>Mon, 13 May 2013 04:04:10 +0100</updated>
                                    <version>3.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11561</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25492</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2187] improve lucene's similarity algorithm defaults</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2187</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;First things first: I am not an IR guy. The goal of this issue is to make 'surgical' tweaks to lucene's formula to bring its performance up to that of more modern algorithms such as BM25.&lt;/p&gt;

&lt;p&gt;In my opinion, the concept of having some 'flexible' scoring with good speed across the board is an interesting goal, but not practical in the short term.&lt;/p&gt;

&lt;p&gt;Instead here I propose incorporating some work similar to lnu.ltc and friends, but slightly different. I noticed this seems to be in line with that paper published before about the trec million queries track... &lt;/p&gt;

&lt;p&gt;Here is what I propose in pseudocode (overriding DefaultSimilarity):&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  @Override
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;float&lt;/span&gt; tf(&lt;span class="code-object"&gt;float&lt;/span&gt; freq) {
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; 1 + (&lt;span class="code-object"&gt;float&lt;/span&gt;) &lt;span class="code-object"&gt;Math&lt;/span&gt;.log(freq);
  }
  
  @Override
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;float&lt;/span&gt; lengthNorm(&lt;span class="code-object"&gt;String&lt;/span&gt; fieldName, &lt;span class="code-object"&gt;int&lt;/span&gt; numTerms) {
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; (&lt;span class="code-object"&gt;float&lt;/span&gt;) (1 / ((1 - slope) * pivot + slope * numTerms));
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where slope is a constant (I used 0.25 for all relevance evaluations: the goal is to have a better default), and pivot is the average field length. Obviously we shouldnt make the user provide this but instead have the system provide it.&lt;/p&gt;

&lt;p&gt;These two pieces do not improve lucene much independently, but together they are competitive with BM25 scoring with the test collections I have run so far. &lt;/p&gt;

&lt;p&gt;The idea here is that this logarithmic tf normalization is independent of the tf / mean TF that you see in some of these algorithms, in fact I implemented lnu.ltc with cosine pivoted length normalization and log(tf)/log(mean TF) stuff and it did not fare as well as this method, and this is simpler, we do not need to calculate this mean TF at all.&lt;/p&gt;

&lt;p&gt;The BM25-like "binary" pivot here works better on the test collections I have run, but of course only with the tf modification.&lt;/p&gt;

&lt;p&gt;I am uploading a document with results from 3 test collections (Persian, Hindi, and Indonesian). I will test at least 3 more languages... yes including English... across more collections and upload those results also, but i need to process these corpora to run the tests with the benchmark package, so this will take some time (maybe weeks)&lt;/p&gt;

&lt;p&gt;so, please rip it apart with scoring theory etc, but keep in mind 2 of these 3 test collections are in the openrelevance svn, so if you think you have a great idea, don't hesitate to test it and upload results, this is what it is for. &lt;/p&gt;

&lt;p&gt;also keep in mind again I am not a scoring or IR guy, the only thing i can really bring to the table here is the willingness to do a lot of relevance testing!&lt;/p&gt;</description>
                <environment/>
            <key id="12444511">LUCENE-2187</key>
            <summary>improve lucene's similarity algorithm defaults</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Sat, 2 Jan 2010 20:15:45 +0000</created>
                <updated>Mon, 13 May 2013 04:03:51 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/query/scoring</component>
                        <due/>
                    <votes>0</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="12795909" author="rcmuir" created="Sat, 2 Jan 2010 20:17:31 +0000">&lt;p&gt;document with some simple results from the 3 collections i tested thus far.&lt;/p&gt;

&lt;p&gt;i chose to display simple graphs with descriptions of the collections and some of their peculiarities. &lt;/p&gt;

&lt;p&gt;if you want submission.txt dumps or verbose output from trec_eval, I can do that too, but I think its less useful to start with.&lt;/p&gt;</comment>
                    <comment id="12796364" author="rcmuir" created="Mon, 4 Jan 2010 21:28:02 +0000">&lt;p&gt;attaching updated document with results for a 4th test collection, on english. for this one BM25 did not fare so well.&lt;/p&gt;

&lt;p&gt;For the lazy, here are the MAP values:&lt;/p&gt;

&lt;p&gt;StandardAnalyzer&lt;br/&gt;
Default Scoring: 0.3837&lt;br/&gt;
BM25 Scoring:  0.3580&lt;br/&gt;
Improved Scoring: 0.3994&lt;/p&gt;

&lt;p&gt;StandardAnalyzer + Porter&lt;br/&gt;
Default Scoring: 0.4333&lt;br/&gt;
BM25 Scoring: 0.4131&lt;br/&gt;
Improved Scoring: 0.4515&lt;/p&gt;

&lt;p&gt;StandardAnalyzer + Porter + MoreLikeThis (top 5 docs)&lt;br/&gt;
Default Scoring: 0.5234&lt;br/&gt;
BM25 Scoring: 0.5087&lt;br/&gt;
Improved Scoring: 0.5474&lt;/p&gt;

&lt;p&gt;Note that 0.5572 was the highest performing MAP on this corpus (Microsoft Research) in FIRE 2008: &lt;a href="http://www.isical.ac.in/~fire/paper/Udupa-mls-fire2008.pdf" class="external-link"&gt;http://www.isical.ac.in/~fire/paper/Udupa-mls-fire2008.pdf&lt;/a&gt;&lt;/p&gt;
</comment>
                    <comment id="12796366" author="rcmuir" created="Mon, 4 Jan 2010 21:37:03 +0000">&lt;p&gt;sorry, correct some transposition of axes labels and some grammatical mistakes &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12796514" author="rcmuir" created="Tue, 5 Jan 2010 03:43:23 +0000">&lt;p&gt;attached is a patch with the Similarity impl. of course you have to manually supply this pivot value (avg doc. length), for now.&lt;/p&gt;</comment>
                    <comment id="13543985" author="tburtonwest" created="Fri, 4 Jan 2013 16:18:00 +0000">&lt;p&gt;Hi Robert,&lt;/p&gt;

&lt;p&gt;Is this implementation made moot by the new GSOC work, or would it still be worth testing this as well as BM25, DFR and INF?    &lt;/p&gt;

&lt;p&gt;I can't seem to find a link to the ORP collections.  Can you point me to it?&lt;br/&gt;
(I plan to test with our long docs, but thought I would try out some of the ORP collections as well)&lt;/p&gt;


&lt;p&gt;Tom&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12444498">LUCENE-2186</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12429413" name="LUCENE-2187.patch" size="5004" author="rcmuir" created="Tue, 5 Jan 2010 03:43:23 +0000"/>
                    <attachment id="12429376" name="scoring.pdf" size="151370" author="rcmuir" created="Mon, 4 Jan 2010 21:37:03 +0000"/>
                    <attachment id="12429373" name="scoring.pdf" size="151326" author="rcmuir" created="Mon, 4 Jan 2010 21:28:02 +0000"/>
                    <attachment id="12429282" name="scoring.pdf" size="127681" author="rcmuir" created="Sat, 2 Jan 2010 20:17:31 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 4 Jan 2013 16:18:00 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11599</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25532</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2171] Over synchronization for read-only index readers in SegmentTermDocs</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2171</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In SegmentTermDocs constructor (from 2.9.1)&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
46	  &lt;span class="code-keyword"&gt;protected&lt;/span&gt; SegmentTermDocs(SegmentReader parent) {
47	    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.parent = parent;
48	    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.freqStream = (IndexInput) parent.core.freqStream.clone();
49	    &lt;span class="code-keyword"&gt;synchronized&lt;/span&gt; (parent) {
50	      &lt;span class="code-keyword"&gt;this&lt;/span&gt;.deletedDocs = parent.deletedDocs;
51	    }
52	    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.skipInterval = parent.core.getTermsReader().getSkipInterval();
53	    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.maxSkipLevels = parent.core.getTermsReader().getMaxSkipLevels();
54	  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The synchronization on "parent" for accessing deletedDocs is unnecessary on readonly indexes.  If that access was moved into the SegmentReader then it could be protected there by default and overridden in ReadonlySegmentReader.&lt;/p&gt;</description>
                <environment/>
            <key id="12443787">LUCENE-2171</key>
            <summary>Over synchronization for read-only index readers in SegmentTermDocs</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jminard">Jayson Minard</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Fri, 18 Dec 2009 18:18:16 +0000</created>
                <updated>Mon, 13 May 2013 04:03:25 +0100</updated>
                                    <version>2.9.1</version>
                <version>3.0</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12792561" author="mikemccand" created="Fri, 18 Dec 2009 18:24:01 +0000">&lt;p&gt;Super &amp;#8211; wanna whip up a patch?&lt;/p&gt;</comment>
                    <comment id="12792637" author="earwin" created="Fri, 18 Dec 2009 20:50:18 +0000">&lt;p&gt;(without looking deep) I have a feeling that for RW Reader &lt;em&gt;synchronized&lt;/em&gt; is also unnecessary - &lt;em&gt;volatile&lt;/em&gt; will suffice.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 18 Dec 2009 18:24:01 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11614</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25548</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2154] Need a clean way for Dir/MultiReader to "merge" the AttributeSources of the sub-readers</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2154</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The flex API allows extensibility at the Fields/Terms/Docs/PositionsEnum levels, for a codec to set custom attrs.&lt;/p&gt;

&lt;p&gt;But, it's currently broken for Dir/MultiReader, which must somehow share attrs across all the sub-readers.  Somehow we must make a single attr source, and tell each sub-reader's enum to use that instead of creating its own.  Hopefully Uwe can work some magic here &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12443139">LUCENE-2154</key>
            <summary>Need a clean way for Dir/MultiReader to "merge" the AttributeSources of the sub-readers</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Fri, 11 Dec 2009 20:30:24 +0000</created>
                <updated>Fri, 10 May 2013 00:05:31 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12789571" author="thetaphi" created="Fri, 11 Dec 2009 22:11:46 +0000">&lt;p&gt;I have an idea, but please don't hurt me, it uses reflection.&lt;br/&gt;
What is needed:&lt;br/&gt;
Like in the TokenStream BW layer, we had a class called TokenWrapper that implemented all interfaces for basic Tokens but was a wrapper around a Token. This Token was exchangeable.&lt;br/&gt;
The idea for this case is:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;DirReader/MultiReader cannot share thesame AttributeSource at all, because e.g. if th DirReader's enum first tries to call next() on on the first enum, the attributes are overriden. If DirrReader then instead also calls next() on the second sub-enum, the data of the first is overriden and cannot be restaured. The idea is now, to have the same Wrapper like TokenWrapper, where the interface impls behind can be exchanged, so the Dir/MultiReader enum just points its own attribute wrapper to the correct attributeset behind.&lt;/li&gt;
	&lt;li&gt;The problem are custom attributes or e.g. BoostAttribute. Nobody knows what attributes may be generated, so it is impossible to create an wrapper for all.&lt;br/&gt;
But Java has a solution: &lt;a href="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/reflect/Proxy.html" class="external-link"&gt;http://java.sun.com/j2se/1.5.0/docs/api/java/lang/reflect/Proxy.html&lt;/a&gt;. This class implements a proxy stub for a set of interfaces (you just pass all interfaces needed) andthe returned object can be added to the local DirReader/MultiReaders enum attsource using addAttribute(). You only have to point to a implementation class that gets the Method reference and calls the corresponding method in the current interface (Like the actual Token in TokenWrapper) of the sub-enum.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Sounds like ugly magic, but I have to test how fast it is and how it exactly works with AttributeSource. But the proxy instances could be cached like the TokenStream BW compatibility.&lt;/p&gt;</comment>
                    <comment id="12789580" author="mikemccand" created="Fri, 11 Dec 2009 22:28:16 +0000">&lt;p&gt;Good grief... pulling out the heavy guns here!&lt;/p&gt;</comment>
                    <comment id="12789875" author="thetaphi" created="Sun, 13 Dec 2009 11:30:22 +0000">&lt;p&gt;I tried to implement that with JDK's reflect.Proxy, which would be really cool, but sucess stopped once I reached the point that the attribute interface implementation must subclass AttributeImpl vs. generated proxies subclass reflect.Proxy.&lt;br/&gt;
A fix would be to use cglib, but that would be an external reference. With cglib it would even be possible to autogenerate a fast proxy without reflection at all (e.g. using the Emitter class on the lowest level)!&lt;/p&gt;

&lt;p&gt;The only current solution I see is to enforce all Attributes not only to implement a *Impl, but also provide a class *Proxy, that respects the above code. But this classes would be so stupid (only contain a modifiable delegate of the same attribute type and forward all methods to it). Cglib code that does this is hard stuff, but simply to implement.&lt;/p&gt;</comment>
                    <comment id="12831483" author="renaud.delbru" created="Tue, 9 Feb 2010 15:16:01 +0000">&lt;p&gt;Sorry in advance, maybe what I am saying is out of scope due to my partial understanding of the problem.&lt;/p&gt;

&lt;p&gt;I have start to look at the problem, in order to be able to use my own attributes from my own DocsAdnPositionsEnum classes.&lt;br/&gt;
would it not be simpler to create a MultiAttributeSource that is instantiated in the MultiDocsAndPositionsEnum. At creation time, all the AttributeSource of the subreaders (which are available) will be passed in its constructor. This MultiAttributeSource will delegate the getAttribute call to the right DocsAndPositionsEnum$AttributeSource. &lt;/p&gt;

&lt;p&gt;There is not a single AttributeSource shared by all the subreader, but each subreader keeps its own AttributeSource. In this way, attributes are not overridden. The MultiAttributeSource is in fact like a Wrapper.&lt;/p&gt;

&lt;p&gt;One problem is when there is custom attributes, e.g. BoostAttribute. If I understand correctly, if the user tries to access the BoostAttribute, but one of the subreader does not know it, the IllegalArgumentException will be thrown. Under the hood, the MultiAttributeSource can check if the attribute exists on the current subreader, and if not it can rely on a default attribute, or a previously stored attribute (coming from a previous subreader).&lt;/p&gt;

&lt;p&gt;I am not sure if what I am saying is making some sense. It looks to me too simple to cover all the cases.  Are there cases I am not aware of ? Could you give me some examples to make me aware of other problems ?&lt;/p&gt;</comment>
                    <comment id="12831489" author="thetaphi" created="Tue, 9 Feb 2010 15:25:38 +0000">&lt;p&gt;The problem is the following:&lt;/p&gt;

&lt;p&gt;Attributes are not to be retrieved on every call to next(), they are get/added after construction. If you have a consumer of your MultiEnum, it calls attributes().getAttribute exactly one time before start to enumerate tokens/positions/whatever. If your proposed MultiAttributeSource would return the attribute of the first sub-enum, the consumer would stay with this attribute instance forever. If the MultiEnum then changes to another sub-enum, the consumer would not see the new attribute.&lt;/p&gt;

&lt;p&gt;Because of that the right way is not to have a MultiAttributeSource. What you need are proxy attributes. The Attributes itsself must be proxies, delegating the call to the current enum's corresponding attribute. The same was done in Lucene 2.9 to emulate the backwards compatibility for TokenStreams. The proxy was TokenWrapper. These ProxyAttributes would look exactly like this TokenWrapper impl class.&lt;/p&gt;</comment>
                    <comment id="12831521" author="renaud.delbru" created="Tue, 9 Feb 2010 16:20:20 +0000">&lt;p&gt;I see. The problem is to return to the consumer a unique attribute reference when attributes().getAttribute is called, and then updates the references when iterating the enums in order to propagate the attribute changes to the consumer.&lt;/p&gt;

&lt;p&gt;I am trying to propose a (possible) alternative solution (if I understood the problem correctly), which can avoid reflection, but could potentially need a modification of the Attribute interface.&lt;/p&gt;

&lt;p&gt;If the MultiAttributeSource will create its own set of unique references for each attribute (the list of different attribute classes can be retrieved by calling the getAttributeClassesIterator() method of the AttributeSource for each subreader, we can then create a list of unique references, one reference for each type of attributes), the goal is then to update these references after each enum iteration or sub-enum change (in order to propagate the changes to the consumer).&lt;/p&gt;

&lt;p&gt;Unfortunately, I don't see any interface on the Attribute interface to 'copy' a given attribute. Each AttributeImpl could implement this 'copy method', which copies the state of a given attribute of the same class.&lt;br/&gt;
Then, in the MultiDocsAndPositionsEnum, after each iteration or each sub-enum change, a call to MultiAttributeSource can be made explicitly to update the unique references of the different attributes. This update method will under the hood (1) check if the sub-enum is aware of the attribute class, (2) get the attribute from the sub-enum, and (3) copy the attribute to the unique attribute reference kept by MultiAttributeSource.&lt;/p&gt;

&lt;p&gt;Could this solution possibly work ?&lt;/p&gt;</comment>
                    <comment id="12831646" author="mikemccand" created="Tue, 9 Feb 2010 20:24:20 +0000">&lt;p&gt;What if we require that all segments are the same codec, if you want to use attributes from a Multi*Enum?  (I think this limitation is fine... and if it's not, one could still operate per-segment with different attr impls per segment).&lt;/p&gt;

&lt;p&gt;This way, every segment would share the same attr impl for a given attr interface?&lt;/p&gt;

&lt;p&gt;And then couldn't we somehow force each segment to use the same attr impl as the last segment(s)?&lt;/p&gt;</comment>
                    <comment id="12831938" author="thetaphi" created="Wed, 10 Feb 2010 10:47:16 +0000">&lt;p&gt;That would work. So the MultiEnum would use its own AttributeSource and passes it downto the sub-enums. For that the ctor of *Enums should allow to pass an AttrubuteSource. I can provide patch.&lt;/p&gt;</comment>
                    <comment id="12832435" author="thetaphi" created="Thu, 11 Feb 2010 10:04:03 +0000">&lt;p&gt;Here is a first patch about cglib-generated proxy attributes.&lt;/p&gt;

&lt;p&gt;In IRC we found out yesterday, that the proposed idea to share the attributes accross all Multi*Enums would result in problems as the call to next() on any sub-enum would overwrite the contents of the attributes of the previous sub-enum which would make TermsEnum not working (because e.g. TermsEnum looks forward by calling next() an all sub-enums and choosing the lowest term to return - after calling each enums next() the attributes of the first enums cannot be restored without captureState &amp;amp; co, as overwritten by the next() call to the last enum).&lt;/p&gt;

&lt;p&gt;This patch needs cglib-nodep-2.2.jar put into the lib-folder of the checkout &lt;a href="http://sourceforge.net/projects/cglib/files/cglib2/2.2/cglib-nodep-2.2.jar/download" class="external-link"&gt;http://sourceforge.net/projects/cglib/files/cglib2/2.2/cglib-nodep-2.2.jar/download&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It contains a test and that shows how the usage is. The central part is cglib's Enhancer that creates a dynamic class extending ProxyAttributeImpl (which defines the general AttributeImpl methods delegating to the delegate) and implementing the requested Attribute interface using a MethodInterceptor.&lt;/p&gt;

&lt;p&gt;Please note: This uses no reflection (only during in-memory class file creation, which is only run one time on "loading" the proxy class). The proxy implements MethodInterceptor and uses the fast MethodProxy class (which is also generated by cglib for each proxied method, too) and can invoke the delegated method directly (without reflection) on the delegate.&lt;/p&gt;

&lt;p&gt;The test verifies everything works and also compares speed by using a TermAttribute natively and proxied. The speed is lower (which is not caused by reflection, but by the MethodInterceptor creating an array of parameters and boxing/unboxing native parameters into the Object[]), but for the testcase I have seen about only  50% more time needed.&lt;/p&gt;

&lt;p&gt;The generated classes are cached and reused (like DEFAULT_ATTRIBUTE_FACTORY does).&lt;/p&gt;

&lt;p&gt;To get maximum speed and no external libraries, the code implemented by Enhancer can be rewritten natively using the Apache Harmony java.lang.reflect.Proxy implementation source code as basis. The hardest part in generating bytecode is the ConstantPool in class files. But as the proxy methods are simply delegating and no magic like boxing/unboxing is needed, the generated bytecode is rather simple.&lt;/p&gt;

&lt;p&gt;One other use-case for these proxies is AppendingTokenStream, which is not possible since 3.0 without captureState (in old TS API it was possible, because you could reuse the same TokenInstance even over the appended streams). In the new TS api, the appending stream must have a "view" on the attributes of the current consuming sub-stream.&lt;/p&gt;</comment>
                    <comment id="12832447" author="thetaphi" created="Thu, 11 Feb 2010 11:24:13 +0000">&lt;p&gt;I had some more fun. Made ProxyAttributeSource non-final and added class name policy to also contain the corresponding interface (to make stack traces on errors nicer).&lt;/p&gt;

&lt;p&gt;Here the example output:&lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;    [junit] DEBUG: Created class org.apache.lucene.util.ProxyAttributeSource$ProxyAttributeImpl$$TermAttribute$$EnhancerByCGLIB$$6100bdf9 for attribute org.apache.lucene.analysis.tokenattributes.TermAttribute
    [junit] DEBUG: Created class org.apache.lucene.util.ProxyAttributeSource$ProxyAttributeImpl$$TypeAttribute$$EnhancerByCGLIB$$6f89c3ff for attribute org.apache.lucene.analysis.tokenattributes.TypeAttribute
    [junit] DEBUG: Created class org.apache.lucene.util.ProxyAttributeSource$ProxyAttributeImpl$$FlagsAttribute$$EnhancerByCGLIB$$4668733c for attribute org.apache.lucene.analysis.tokenattributes.FlagsAttribute
    [junit] Time taken using org.apache.lucene.analysis.tokenattributes.TermAttributeImpl:
    [junit]   1476.090658 ms for 10000000 iterations
    [junit] Time taken using org.apache.lucene.util.ProxyAttributeSource$ProxyAttributeImpl$$TermAttribute$$EnhancerByCGLIB$$6100bdf
9:
    [junit]   1881.295734 ms for 10000000 iterations
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12833095" author="thetaphi" created="Fri, 12 Feb 2010 18:04:01 +0000">&lt;p&gt;Here the last CGLIB patch for reference.&lt;/p&gt;

&lt;p&gt;Now the real cool class created using JAVASSIST &lt;a href="http://www.javassist.org/" class="external-link"&gt;http://www.javassist.org/&lt;/a&gt;:&lt;br/&gt;
You have to place the latest javassist.jar (Mozilla/LGPL licensed) in the lib/ folder and apply the patch. What it does is the fastest proxy we can think of:&lt;br/&gt;
It creates a subclass of ProxyAttributeImpl that implements all methods of the interface natively in bytecode using JAVASSIST's bytecode generation tools (a subset of the Java language spec).&lt;/p&gt;

&lt;p&gt;The micro-benchmark shows, no difference between proxied and native method - as hotspot removes the extra method call.&lt;/p&gt;

&lt;p&gt;With Javassist it would even be possible to create classes that implement our interfaces around simple fields that are set by get/setters. Just like Eclipse's create get/set around a private field. That would be really cool. Or we could create combining attributes on the fly, Michael Busch would be excited. All *Impl classes we currently have would be almost obsolete (except TermAttributeImpl, which is rather complex). We could also create dynamic State classes for capturing state...&lt;/p&gt;

&lt;p&gt;Nice, but a little bit hackish. Maybe we put this first into contrib and supply a ConcenatingTokenStream as demo impl and also other Solr TokenStreams that are no longer easy with the Attributes without proxies (Robert listed some).&lt;/p&gt;</comment>
                    <comment id="12833121" author="thetaphi" created="Fri, 12 Feb 2010 19:28:22 +0000">&lt;p&gt;Better patch without classloader problems.&lt;/p&gt;</comment>
                    <comment id="12833285" author="thetaphi" created="Sat, 13 Feb 2010 01:53:45 +0000">&lt;p&gt;More cool, less casts, more speed.&lt;/p&gt;</comment>
                    <comment id="12839808" author="thetaphi" created="Mon, 1 Mar 2010 19:20:58 +0000">&lt;p&gt;Here the third incarnation of the patch. A little bit more complicated, because directly assemblying bytecode, but much cleaner and without strange parsers. It uses Jakarta BCEL, which is e.g. also used by Apache XALAN XSLTC, and its even included in JDK5 (somehow inofficial in renamed com.sun.internal packages).&lt;/p&gt;

&lt;p&gt;The approach is the same as with JavAssist, but has several advantages:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;The typing is hard, so all bytecode is generated using real Type classes, retrieved from reflection analysis of the Attribute-Interface to proxy. The method signature is copied.&lt;/li&gt;
	&lt;li&gt;It will not likely break with later VDKs if the class format changes again. The Problem with JavAssist is, that it loads the interface and implementation classes from the bytecode and must analyze it. If we get Java 7 and a new classformat, this will likely break (as e.g. classes loaded from rt.jar may not be analyzed). The BCEL approach does not use loading classes in bytecode, it just uses reflection to generate the proxy method signatures and inserts the byte code to delegate to the delegate.&lt;/li&gt;
	&lt;li&gt;The generated class is not loaded by a hack, but instead a delegating ClassLoader is dynamically created to load the class into the JVM. The classloader delegates all other requests to the proxied Attribute's classloader.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;By the way, XALAN is bundling BCEL, in old version, but under the original package names, which may lead to conflics when also bundling in lucene. I would prefer to import the whole source (in parts) like automaton was put into o.a.l.util. But that only, if we really want to do it like this way. But I still think for some TokenStreams this would be a real speed improvement, so we can also make a contrib package out of it.&lt;/p&gt;</comment>
                    <comment id="12855725" author="thetaphi" created="Sun, 11 Apr 2010 14:54:52 +0100">&lt;p&gt;Slightly improved patch to correctly work with CharTermAttribute (as it defines methods also defined by ProxyAttributeImpl as final, so override failure).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12441388" name="ASF.LICENSE.NOT.GRANTED--LUCENE-2154-Jakarta-BCEL.patch" size="21278" author="thetaphi" created="Sun, 11 Apr 2010 14:54:52 +0100"/>
                    <attachment id="12435713" name="LUCENE-2154-cglib.patch" size="14225" author="thetaphi" created="Fri, 12 Feb 2010 18:04:01 +0000"/>
                    <attachment id="12437501" name="LUCENE-2154-Jakarta-BCEL.patch" size="20691" author="thetaphi" created="Mon, 1 Mar 2010 19:20:58 +0000"/>
                    <attachment id="12435757" name="LUCENE-2154-javassist.patch" size="16695" author="thetaphi" created="Sat, 13 Feb 2010 01:53:45 +0000"/>
                    <attachment id="12435721" name="LUCENE-2154-javassist.patch" size="15630" author="thetaphi" created="Fri, 12 Feb 2010 19:28:22 +0000"/>
                    <attachment id="12435568" name="LUCENE-2154.patch" size="13973" author="thetaphi" created="Thu, 11 Feb 2010 11:24:13 +0000"/>
                    <attachment id="12435565" name="LUCENE-2154.patch" size="13463" author="thetaphi" created="Thu, 11 Feb 2010 10:04:03 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>7.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 11 Dec 2009 22:11:46 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11629</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25565</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2125] Ability to store and retrieve attributes in the inverted index</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2125</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Now that we have the cool attribute-based TokenStream API and also the&lt;br/&gt;
great new flexible indexing features, the next logical step is to&lt;br/&gt;
allow storing the attributes inline in the posting lists. Currently&lt;br/&gt;
this is only supported for the PayloadAttribute.&lt;/p&gt;

&lt;p&gt;The flex search APIs already provide an AttributeSource, so there will&lt;br/&gt;
be a very clean and performant symmetry. It should be seamlessly&lt;br/&gt;
possible for the user to define a new attribute, add it to the&lt;br/&gt;
TokenStream, and then retrieve it from the flex search APIs.&lt;/p&gt;

&lt;p&gt;What I'm planning to do is to add additional methods to the token&lt;br/&gt;
attributes (e.g. by adding a new class TokenAttributeImpl, which&lt;br/&gt;
extends AttributeImpl and is the super class of all impls in&lt;br/&gt;
o.a.l.a.tokenattributes):&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;void serialize(DataOutput)&lt;/li&gt;
	&lt;li&gt;void deserialize(DataInput)&lt;/li&gt;
	&lt;li&gt;boolean storeInIndex()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The indexer will only call the serialize method of an&lt;br/&gt;
TokenAttributeImpl in case its storeInIndex() returns true. &lt;/p&gt;

&lt;p&gt;The big advantage here is the ease-of-use: A user can implement in one&lt;br/&gt;
place everything necessary to add the attribute to the index.&lt;/p&gt;

&lt;p&gt;Btw: I'd like to introduce DataOutput and DataInput as super classes&lt;br/&gt;
of IndexOutput and IndexInput. They will contain methods like&lt;br/&gt;
readByte(), readVInt(), etc., but methods such as close(),&lt;br/&gt;
getFilePointer() etc. will stay in the super classes.&lt;/p&gt;

&lt;p&gt;Currently the payload concept is hardcoded in &lt;br/&gt;
TermsHashPerField and FreqProxTermsWriterPerField. These classes take&lt;br/&gt;
care of copying the contents of the PayloadAttribute over into the &lt;br/&gt;
intermediate in-memory postinglist representation and reading it&lt;br/&gt;
again. Ideally these classes should not know about specific&lt;br/&gt;
attributes, but only call serialze() on those attributes that shall&lt;br/&gt;
be stored in the posting list.&lt;/p&gt;

&lt;p&gt;We also need to change the PositionsEnum and PositionsConsumer APIs to&lt;br/&gt;
deal with attributes instead of payloads.&lt;/p&gt;

&lt;p&gt;I think the new codecs should all support storing attributes. Only the&lt;br/&gt;
preflex one should be hardcoded to only take the PayloadAttribute into&lt;br/&gt;
account.&lt;/p&gt;

&lt;p&gt;We'll possibly need another extension point that allows us to influence &lt;br/&gt;
compression across multiple postings. Today we use the&lt;br/&gt;
length-compression trick for the payloads: if the previous payload had&lt;br/&gt;
the same length as the current one, we don't store the length&lt;br/&gt;
explicitly again, but only set a bit in the shifted position VInt. Since&lt;br/&gt;
often all payloads of one posting list have the same length, this&lt;br/&gt;
results in effective compression.&lt;br/&gt;
Now an advanced user might want to implement a similar encoding, where&lt;br/&gt;
it's not enough to just control serialization of a single value, but&lt;br/&gt;
where e.g. the previous position can be taken into account to decide&lt;br/&gt;
how to encode a value. &lt;br/&gt;
I'm not sure yet how this extension point should look like. Maybe the&lt;br/&gt;
flex APIs are actually already sufficient.&lt;/p&gt;

&lt;p&gt;One major goal of this feature is performance: It ought to be more &lt;br/&gt;
efficient to e.g. define an attribute that writes and reads a single &lt;br/&gt;
VInt than storing that VInt as a payload. The payload has the overhead&lt;br/&gt;
of converting the data into a byte array first. An attribute on the other &lt;br/&gt;
hand should be able to call 'int value = dataInput.readVInt();' directly&lt;br/&gt;
without the byte[] indirection.&lt;/p&gt;

&lt;p&gt;After this part is done I'd like to use a very similar approach for&lt;br/&gt;
column-stride fields.&lt;/p&gt;</description>
                <environment/>
            <key id="12442608">LUCENE-2125</key>
            <summary>Ability to store and retrieve attributes in the inverted index</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 Dec 2009 08:47:28 +0000</created>
                <updated>Fri, 10 May 2013 00:05:31 +0100</updated>
                                    <version>4.0-ALPHA</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12786837" author="mikemccand" created="Mon, 7 Dec 2009 09:40:33 +0000">&lt;p&gt;This sounds great &amp;#8211; and is the logical next step for flex.&lt;/p&gt;

&lt;p&gt;So you'd remove the explicit payload methods in PositionsEnum?  Ie,&lt;br/&gt;
users on migrating to flex would have to switch to the payloads&lt;br/&gt;
attribute?&lt;/p&gt;

&lt;p&gt;Note the that preflex codec only has a reader (FieldsProducer), not a&lt;br/&gt;
writer.  Ie you can read the old index format but not write it.&lt;/p&gt;

&lt;p&gt;Ideally the serialize/unserialize could efficiently handle the&lt;br/&gt;
fixed-length case without using up the 1 bit in the index.&lt;/p&gt;

&lt;p&gt;I wonder if we need to allow codecs to store data into&lt;br/&gt;
SegmentInfo/FieldInfo for this (we don't now).&lt;/p&gt;</comment>
                    <comment id="12786848" author="michaelbusch" created="Mon, 7 Dec 2009 10:15:43 +0000">&lt;blockquote&gt;
&lt;p&gt;So you'd remove the explicit payload methods in PositionsEnum? Ie,&lt;br/&gt;
users on migrating to flex would have to switch to the payloads&lt;br/&gt;
attribute?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that would make sense? Payloads don't have to be treated specially anymore,&lt;br/&gt;
if any attribute can be stored in the posting lists.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note the that preflex codec only has a reader (FieldsProducer), not a&lt;br/&gt;
writer. Ie you can read the old index format but not write it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, so the concern is that people &lt;b&gt;have&lt;/b&gt; to make the switch to the flex APIs&lt;br/&gt;
after upgrading to the next Lucene version if they want to create indexes with &lt;br/&gt;
good old payloads?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Ideally the serialize/unserialize could efficiently handle the&lt;br/&gt;
fixed-length case without using up the 1 bit in the index.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wonder if we need to allow codecs to store data into&lt;br/&gt;
SegmentInfo/FieldInfo for this (we don't now).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;IMO we definitely do. E.g. for backwards-compatibility: if users switch the encoding&lt;br/&gt;
of an attribute, then they need a way to determine in which format it is stored in a &lt;br/&gt;
given segment.&lt;/p&gt;

&lt;p&gt;And we need to open up FieldInfo too: it has to store which and in what order the&lt;br/&gt;
attributes are stored. &lt;/p&gt;

&lt;p&gt;I'm sure these are the things you had in mind too?&lt;/p&gt;</comment>
                    <comment id="12786854" author="mikemccand" created="Mon, 7 Dec 2009 10:38:11 +0000">&lt;p&gt;I think it makes sense to not treat payloads specially in flex, ie, make it an attr.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hmm, so the concern is that people have to make the switch to the flex APIs&lt;br/&gt;
after upgrading to the next Lucene version if they want to create indexes with &lt;br/&gt;
good old payloads?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, not really &amp;#8211; if you stick payloads into your tokens during analysis, presumably the standard (= default) codec would recognize the new payload attr, and store it like normal.  Then, any existing queries that do interesting things w/ payloads (PayloadNear/TermQuery), we'd cutover to the new API, and your custom Similarity would still be invoked?&lt;/p&gt;

&lt;p&gt;It's only if you directly access TermPositions's payload API today, that you'd have to migrate to the new API?&lt;/p&gt;

&lt;p&gt;But, even then, flex does back compat emulation, so a new index written with the standard codec could be accessed via the old API.&lt;/p&gt;

&lt;p&gt;BTW probably the attribute should include a "merge" operation, somehow, to be efficient (simply byte[] copying instead of decode/encode) in the merge case.&lt;/p&gt;</comment>
                    <comment id="12786855" author="michaelbusch" created="Mon, 7 Dec 2009 10:42:50 +0000">&lt;blockquote&gt;
&lt;p&gt;BTW probably the attribute should include a "merge" operation, somehow, to be efficient (simply byte[] copying instead of decode/encode) in the merge case.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, and then I can also close &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1585" title="Allow to control how payloads are merged"&gt;&lt;del&gt;LUCENE-1585&lt;/del&gt;&lt;/a&gt;! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12786856" author="mikemccand" created="Mon, 7 Dec 2009 10:44:07 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I wonder if we need to allow codecs to store data into SegmentInfo/FieldInfo for this (we don't now).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;IMO we definitely do. E.g. for backwards-compatibility: if users switch the encoding&lt;br/&gt;
of an attribute, then they need a way to determine in which format it is stored in a &lt;br/&gt;
given segment.&lt;/p&gt;

&lt;p&gt;And we need to open up FieldInfo too: it has to store which and in what order the&lt;br/&gt;
attributes are stored.&lt;/p&gt;

&lt;p&gt;I'm sure these are the things you had in mind too?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well... some stuff should be written into the header of each file, so eg a switch to encoding could be handled by the simple versioning the Codec API gives you (Codec.writeHeader/Codec.checkHeader).&lt;/p&gt;

&lt;p&gt;But, yeah, for other stuff I've been assuming we need to open up Segment/FieldInfo.&lt;/p&gt;

&lt;p&gt;So eg "omitTermFreqAndPositions" is something we could conceivably put under codec control, ie, Lucene core shouldn't need to know this attr even exists.  But, then we'd need extensibility of Field as well.  We've discussed splitting this setting, to separately control whether the freq is written and whether the positions are written, which makes complete sense.  It'd be great if such a change could be cleanly handled by simply creating a new version of the codec.  Likewise, "hasProx", which is derived from the omitTFAPs of all fields within the segment, should be computed/managed entirely within the codec.&lt;/p&gt;</comment>
                    <comment id="12786857" author="mikemccand" created="Mon, 7 Dec 2009 10:46:29 +0000">&lt;blockquote&gt;&lt;p&gt;Yes, and then I can also close &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1585" title="Allow to control how payloads are merged"&gt;&lt;del&gt;LUCENE-1585&lt;/del&gt;&lt;/a&gt;! &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually, flex now gives your codec a start, here &amp;#8211; the merge has been refactored onto the Fields/Terms/Docs/PositionsEnum base classes.  This means you can make a codec that overrides how positions are merged, to change what's done with the payloads.&lt;/p&gt;

&lt;p&gt;But, the solution proposed in this issue takes it further (better) &amp;#8211; you shouldn't have to override all of positions merging just because one attr (payloads, or another) wants control over how it's merged.&lt;/p&gt;</comment>
                    <comment id="12842259" author="thetaphi" created="Sat, 6 Mar 2010 13:40:53 +0000">&lt;p&gt;I would prefer to not extend AttributeImpl but more make the attribute simply extend another interface: SerializableAttribute that provides input/output methods. Docinverter can then just check with instanceof, if the attribute is to be stored in index.&lt;/p&gt;

&lt;p&gt;This would also help with ProxyAttributes (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2154" title="Need a clean way for Dir/MultiReader to &amp;quot;merge&amp;quot; the AttributeSources of the sub-readers"&gt;LUCENE-2154&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="12842264" author="mikemccand" created="Sat, 6 Mar 2010 14:33:19 +0000">&lt;p&gt;We may need to allow for stateful serializers?&lt;/p&gt;

&lt;p&gt;EG (contribed example) imagine an attr that stays the same for most&lt;br/&gt;
docs, so, attr writes 1 byte for "it's the same or not" and then many&lt;br/&gt;
bytes when there is a change.  The serializer will want to remember&lt;br/&gt;
last value it wrote?  (Hmm though I guess attr could also eg keep a&lt;br/&gt;
bit inside noting that it had changed on the last call to .next(), as&lt;br/&gt;
well).  (The payload encoding length only when length changes is a&lt;br/&gt;
similar example, but, this encoding "takes avantage" of being deeply&lt;br/&gt;
tied to the codec since that bit is merged with the position length&lt;br/&gt;
delta.)&lt;/p&gt;

&lt;p&gt;Or imagine writing strings to the index, but the strings have dups,&lt;br/&gt;
yet you don't know the full universe of strings up front.  So you make&lt;br/&gt;
a dict as you go (first time you see a string you assign it the next&lt;br/&gt;
int).  This case goes beyond first one because this dict must be&lt;br/&gt;
saved on .close() (maybe optionally taking a different DataOutput to&lt;br/&gt;
save its state to), and, codec must remember which file that attr had&lt;br/&gt;
been .close()d on so that at read time it can seek there and init the&lt;br/&gt;
stateful deserializer (which should be lazy... ie if you don't request&lt;br/&gt;
the attr it shouldn't load the dict).&lt;/p&gt;

&lt;p&gt;Also: codec would need to know if serialization is fixed width... or&lt;br/&gt;
maybe expose a .skip() method on deserializer?  EG I may be enuming&lt;br/&gt;
only docs/positions but not attrs (say, running a normal PhraseQuery),&lt;br/&gt;
and I want to just skip (like how we skip payload today when its not&lt;br/&gt;
read).&lt;/p&gt;

&lt;p&gt;I wonder if StandardCodec should inline serialized attrs into existing&lt;br/&gt;
postings lists, or, make separate file to hold them...?&lt;/p&gt;</comment>
                    <comment id="12842512" author="mikemccand" created="Mon, 8 Mar 2010 00:31:34 +0000">&lt;p&gt;One question...&lt;/p&gt;

&lt;p&gt;Say I make an attr and it serializes to variable number of bytes, per&lt;br/&gt;
position.&lt;/p&gt;

&lt;p&gt;How can we design serializer API so that this attr can do the same&lt;br/&gt;
encoding trick we do with payload today?&lt;/p&gt;

&lt;p&gt;Ie where we steal 1 bit from the pos-delta to state whether the length&lt;br/&gt;
changed from last time?&lt;/p&gt;

&lt;p&gt;If we can do this then I think we should remove payload from the&lt;br/&gt;
flex postings API and just move directly to attrs?&lt;/p&gt;

&lt;p&gt;Though: how, also, should we encode more than 1 attr per position?&lt;br/&gt;
Can we somehow make this the responsibility of the serializer (if we&lt;br/&gt;
can somehow get one serializer for all attrs that are serializable in&lt;br/&gt;
the source)?&lt;/p&gt;

&lt;p&gt;This way a user could make their own serializer if they know&lt;br/&gt;
interesting things about the attrs they need to serialize.  EG maybe&lt;br/&gt;
either A or B needs to be serialized but never both... or C only is&lt;br/&gt;
serialized if A is not null... etc.&lt;/p&gt;

&lt;p&gt;If we do this then from Lucene's standpoint the serialization will&lt;br/&gt;
"feel" just like payload feels today &amp;#8211; an optional byte[] that may or&lt;br/&gt;
may not be variable length.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 7 Dec 2009 09:40:33 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11655</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25594</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2122] Use JUnit4 capabilites for more thorough Locale testing for classes deriving from LocalizedTestCase</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2122</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Use the @Parameterized capabilities of Junit4 to allow more extensive testing of Locales.&lt;/p&gt;</description>
                <environment/>
            <key id="12442586">LUCENE-2122</key>
            <summary>Use JUnit4 capabilites for more thorough Locale testing for classes deriving from LocalizedTestCase</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="erickoerickson">Erick Erickson</reporter>
                        <labels>
                    </labels>
                <created>Sun, 6 Dec 2009 22:16:33 +0000</created>
                <updated>Fri, 10 May 2013 00:05:32 +0100</updated>
                                    <version>3.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>general/build</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12786723" author="erickoerickson" created="Sun, 6 Dec 2009 22:19:48 +0000">&lt;p&gt;All tests pass. This modifies all test classes (core and contrib) that derive from LocalizedTestCase. LocalizedTestCase now tests all test methods in all derived classes against all available Locales.&lt;/p&gt;

&lt;p&gt;If we want some of the tests to NOT run against all locales, we'd need to refactor them into their own test class....&lt;/p&gt;</comment>
                    <comment id="12786749" author="rcmuir" created="Mon, 7 Dec 2009 00:59:11 +0000">&lt;p&gt;Hi Erick, I am a little nervous about the change to LocalizedTestCase.tearDown() here.&lt;/p&gt;

&lt;p&gt;I think we must restore the users default Locale, since its a JRE-system wide global thing and we are changing it on the fly here.&lt;/p&gt;

&lt;p&gt;this was stashed away here before:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
 /**
   * Before changing the &lt;span class="code-keyword"&gt;default&lt;/span&gt; Locale, save the &lt;span class="code-keyword"&gt;default&lt;/span&gt; Locale here so that it
   * can be restored.
   */
  &lt;span class="code-keyword"&gt;private&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; Locale defaultLocale = Locale.getDefault();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and restored in tearDown()... otherwise strange things could happen, such as your IDE could go bonkers after running the tests! (but maybe I am missing something)&lt;/p&gt;</comment>
                    <comment id="12786752" author="erickoerickson" created="Mon, 7 Dec 2009 01:47:29 +0000">&lt;p&gt;Restoring original default Locale after test class has been run.&lt;/p&gt;

&lt;p&gt;Thanks Robert!&lt;/p&gt;</comment>
                    <comment id="12786757" author="rcmuir" created="Mon, 7 Dec 2009 02:01:30 +0000">&lt;p&gt;Erick do you think LocalizedTestCase should be abstract?&lt;/p&gt;</comment>
                    <comment id="12787223" author="erickoerickson" created="Tue, 8 Dec 2009 00:49:15 +0000">&lt;p&gt;Made LocalizedTestCase abstract...&lt;/p&gt;</comment>
                    <comment id="12788100" author="rcmuir" created="Wed, 9 Dec 2009 14:12:04 +0000">&lt;p&gt;Hi Erick, in the Date tools test I think you can delete the public static Collection&amp;lt;Locale[]&amp;gt; data(), I think you might have accidentally included it?&lt;/p&gt;</comment>
                    <comment id="12788452" author="erickoerickson" created="Thu, 10 Dec 2009 02:26:49 +0000">&lt;p&gt;OK, I plead advanced senility or some other excuse for the last patch.&lt;/p&gt;

&lt;p&gt;Robert:&lt;br/&gt;
Thanks so much for looking this over, I have no clue what I was thinking with the TestDateTools. Or the other classes that derive from LocalizedTestCase.&lt;/p&gt;

&lt;p&gt;The @Parameterized and @RunWith only needed to be in LocalizedTestCase and all the inheriting classes just rely on the base class to collect the different locales.&lt;/p&gt;

&lt;p&gt;Anyway, this one should be much better....&lt;/p&gt;

&lt;p&gt;Erick&lt;/p&gt;</comment>
                    <comment id="12788455" author="rcmuir" created="Thu, 10 Dec 2009 02:43:39 +0000">&lt;p&gt;thanks Erick, i will play around with the patch some, generally just double-check the locale stuff is doing what we want, looks like it will.&lt;/p&gt;

&lt;p&gt;i havent tested yet, but looking at the code i have a few questions (i can try to add these to the patch just curious what you think):&lt;br/&gt;
1. if a test fails under some locale, say th_TH, will junit 4 attempt to print this parameter out in some way so I know that it failed? If not do you know of a hack?&lt;br/&gt;
2. i am thinking about reordering the locale array so that it tests the default one first. if you are trying to do some test-driven dev it might be strange if the test fails under a different locale first. I think this one is obvious, I will play with it to see how it behaves now.&lt;/p&gt;</comment>
                    <comment id="12788884" author="rcmuir" created="Thu, 10 Dec 2009 20:09:51 +0000">&lt;p&gt;Hi Erick, I played with this patch some and (not intentionally trying) I would get random test failures for TestQueryParser under eclipse... its not really something I am able to repeat though.&lt;/p&gt;

&lt;p&gt;maybe some race condition (I do not know how eclipse executes parameterized tests).... ? &lt;/p&gt;

&lt;p&gt;if it is a problem with my IDE that is one thing, just makes me a little nervous right now. trying to think what could cause this....&lt;/p&gt;</comment>
                    <comment id="12789739" author="rcmuir" created="Sat, 12 Dec 2009 13:46:30 +0000">&lt;p&gt;Hi Erick, I spent some time with this patch today and here is what happened:&lt;/p&gt;

&lt;p&gt;1. i tried to simplify localizedtestcase, so that it works just like the old one, with the exception of using junit4 parameterized facility.&lt;br/&gt;
2. i wrote some bad tests and ensured things worked well such as error messages, default locale being run first, etc etc.&lt;br/&gt;
3. i had everything good to go when i got random failures again, this time from 'ant clean test' about 3 times (pass,fail,pass)&lt;br/&gt;
(sorry i should have done something to capture each test log but i did not)&lt;/p&gt;

&lt;p&gt;because the only real change here is use of the parameterized facility (the logic is the same), it makes me think that we should stick with .runBare() for the time being, because there is something strange going on here and I'm not even trying to break it.&lt;/p&gt;

&lt;p&gt;attached is the modified version of your patch.&lt;/p&gt;</comment>
                    <comment id="12789742" author="rcmuir" created="Sat, 12 Dec 2009 13:54:21 +0000">&lt;p&gt;i am unassigning in case someone else can figure this one out, at my wits end here &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;br/&gt;
perhaps its just something wierd about my environment or something&lt;/p&gt;</comment>
                    <comment id="12789837" author="rcmuir" created="Sun, 13 Dec 2009 02:13:26 +0000">&lt;p&gt;btw, I left 'ant clean test' running in a loop and just checked it with this patch, no problems.&lt;br/&gt;
so perhaps its my own incompetence. Erick can you take a look? Do you see some obvious problem?&lt;/p&gt;</comment>
                    <comment id="12794471" author="erickoerickson" created="Thu, 24 Dec 2009 18:13:49 +0000">&lt;p&gt;Robert:&lt;/p&gt;

&lt;p&gt;Where are we on this? Last I knew your test problems had disappeared, but memory can be tricky....&lt;/p&gt;</comment>
                    <comment id="12794629" author="rcmuir" created="Sat, 26 Dec 2009 16:11:08 +0000">&lt;p&gt;Erick the test problems havent gone away (see latest hudson  failure), but as you can see they appear to be unrelated to your patch, but existing problems.&lt;/p&gt;

&lt;p&gt;I am glad Uwe and Hudson are now seeing the failures I saw, but unfortunately I am unable to reproduce this problem. I think it is very wierd that it only recently has started happening and the Junit 4 jar file is looking very  suspicious to me &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;The next step is to force another test failure (with existing trunk code), except with an additional assertion in TestQueryParser.getParser(), that asserts that the created queryparser has the same Locale as LocalizedTestcase.locale&lt;/p&gt;

&lt;p&gt;If this assertion is not triggered, and a failure happens anyway, then there is no problem with LocalizedTestCase and we should commit your patch. This would mean the problem is instead some problem in the query parser, its tests, date calculations, jvm handling of thai dates, something else, but not the locale-switching.&lt;/p&gt;

&lt;p&gt;if the assertion is triggered, then we need to figure how why before changing the locale-swapping mechanism.&lt;/p&gt;

&lt;p&gt;I ran this test 2500 times last night but i couldnt make it fail. i have a very small portable right now and I will need to wait till the new year to really look at this more...&lt;/p&gt;</comment>
                    <comment id="12794635" author="thetaphi" created="Sat, 26 Dec 2009 17:05:58 +0000">&lt;p&gt;Some additional infos about the Hudson failure:&lt;br/&gt;
In the past Hudson used an internal junit.jar to do the testing, which was in version 4 (the exact version can only be found out by one with hudson access, Mike?). So Hudson was always using Junit4 for testing even in Lucene 2.9. After the clover updates, we removed this JUnit lib path from the ANT command line and therefore also hudson uses 4.7 as shipped with the svn checkout.&lt;/p&gt;

&lt;p&gt;If it is really a JUnit problem, it seems to only occur with version 4.7. But I am not sure, I still stink it may be a thread starvation problem (slow test) or some local time problem that occurred two nights ago.&lt;/p&gt;</comment>
                    <comment id="12794639" author="rcmuir" created="Sat, 26 Dec 2009 18:06:25 +0000">&lt;p&gt;I would be curious to know the previous version of the junit jar file that Hudson was using, if anyone can figure this out. &lt;/p&gt;

&lt;p&gt;I do not want to quickly blame junit or jvm, etc, but its very suspicious that this only recently started happening, and only sporadically for thai locale with the query parser.&lt;/p&gt;

&lt;p&gt;this will be a tricky one to figure out for sure.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12427822" name="LUCENE-2122.patch" size="115835" author="rcmuir" created="Sat, 12 Dec 2009 13:46:30 +0000"/>
                    <attachment id="12427112" name="LUCENE-2122.patch" size="113613" author="erickoerickson" created="Sun, 6 Dec 2009 22:19:48 +0000"/>
                    <attachment id="12427119" name="LUCENE-2122-r2.patch" size="113874" author="erickoerickson" created="Mon, 7 Dec 2009 01:47:29 +0000"/>
                    <attachment id="12427261" name="LUCENE-2122-r3.patch" size="113886" author="erickoerickson" created="Tue, 8 Dec 2009 00:49:15 +0000"/>
                    <attachment id="12427555" name="LUCENE-2122-r4.patch" size="113404" author="erickoerickson" created="Thu, 10 Dec 2009 02:26:49 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 7 Dec 2009 00:59:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11658</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25597</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2101] Default Stopwords should use specific Version in CharArraySet construtor</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2101</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2094" title="Prepare CharArraySet for Unicode 4.0"&gt;&lt;del&gt;LUCENE-2094&lt;/del&gt;&lt;/a&gt; added a version to the constructor of CharArraySet. The default sets in *Analyzer uses Version.LUCENE_CURRENT which currently does not do any harm. Yet, in the future changes to CharArraySet depending on the version could potentially cause index corruption if those default version are not changed. To make sure such a corruption can not happen the default sets should use a specific Version (Version.LUCENE_31)&lt;/p&gt;</description>
                <environment/>
            <key id="12442097">LUCENE-2101</key>
            <summary>Default Stopwords should use specific Version in CharArraySet construtor</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Dec 2009 14:39:39 +0000</created>
                <updated>Fri, 10 May 2013 00:05:32 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12784247" author="simonw" created="Tue, 1 Dec 2009 14:40:08 +0000">&lt;p&gt;linked issues&lt;/p&gt;</comment>
                    <comment id="12976696" author="simonw" created="Mon, 3 Jan 2011 11:02:58 +0000">&lt;p&gt;I think we can simple  move that to v4.0 and v3.1 in  the 3x branch, any objections?&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;

Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java	(revision 1052196)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java	(working copy)
@@ -58,7 +58,7 @@
       &lt;span class="code-quote"&gt;"that"&lt;/span&gt;, &lt;span class="code-quote"&gt;"the"&lt;/span&gt;, &lt;span class="code-quote"&gt;"their"&lt;/span&gt;, &lt;span class="code-quote"&gt;"then"&lt;/span&gt;, &lt;span class="code-quote"&gt;"there"&lt;/span&gt;, &lt;span class="code-quote"&gt;"these"&lt;/span&gt;,
       &lt;span class="code-quote"&gt;"they"&lt;/span&gt;, &lt;span class="code-quote"&gt;"&lt;span class="code-keyword"&gt;this&lt;/span&gt;"&lt;/span&gt;, &lt;span class="code-quote"&gt;"to"&lt;/span&gt;, &lt;span class="code-quote"&gt;"was"&lt;/span&gt;, &lt;span class="code-quote"&gt;"will"&lt;/span&gt;, &lt;span class="code-quote"&gt;"with"&lt;/span&gt;
     );
-    &lt;span class="code-keyword"&gt;final&lt;/span&gt; CharArraySet stopSet = &lt;span class="code-keyword"&gt;new&lt;/span&gt; CharArraySet(Version.LUCENE_CURRENT, 
+    &lt;span class="code-keyword"&gt;final&lt;/span&gt; CharArraySet stopSet = &lt;span class="code-keyword"&gt;new&lt;/span&gt; CharArraySet(Version.LUCENE_40, 
         stopWords.size(), &lt;span class="code-keyword"&gt;false&lt;/span&gt;);
     stopSet.addAll(stopWords);  
     ENGLISH_STOP_WORDS_SET = CharArraySet.unmodifiableSet(stopSet); 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12441571">LUCENE-2094</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11678</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25618</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2099] Revise PositionIncrement in StopFilter / QueryParser</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2099</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Based on the discussion  in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2094" class="external-link"&gt;LUCENE-2094&lt;/a&gt; the way PositionIncrement in StopFiter and QueryParser should be revised and at least extensively documented.&lt;br/&gt;
This issue is a follow-up for the discussion in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2094" class="external-link"&gt;LUCENE-2094&lt;/a&gt;.&lt;/p&gt;</description>
                <environment/>
            <key id="12442078">LUCENE-2099</key>
            <summary>Revise PositionIncrement in StopFilter / QueryParser</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonw">Simon Willnauer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Dec 2009 11:45:45 +0000</created>
                <updated>Fri, 10 May 2013 00:05:32 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12784185" author="simonw" created="Tue, 1 Dec 2009 11:47:10 +0000">&lt;p&gt;attached latest patch from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2094" title="Prepare CharArraySet for Unicode 4.0"&gt;&lt;del&gt;LUCENE-2094&lt;/del&gt;&lt;/a&gt; as &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2099" title="Revise PositionIncrement in StopFilter / QueryParser"&gt;LUCENE-2099&lt;/a&gt;.patch&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12441571">LUCENE-2094</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12426530" name="LUCENE-2099.patch" size="3837" author="simonw" created="Tue, 1 Dec 2009 11:47:10 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11680</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25620</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2082] Performance improvement for merging posting lists</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2082</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;A while ago I had an idea about how to improve the merge performance&lt;br/&gt;
for posting lists. This is currently by far the most expensive part of&lt;br/&gt;
segment merging due to all the VInt de-/encoding. Not sure if an idea&lt;br/&gt;
for improving this was already mentioned in the past?&lt;/p&gt;

&lt;p&gt;So the basic idea is it to perform a raw copy of as much posting data&lt;br/&gt;
as possible. The reason why this is difficult is that we have to&lt;br/&gt;
remove deleted documents. But often the fraction of deleted docs in a&lt;br/&gt;
segment is rather low (&amp;lt;10%?), so it's likely that there are quite&lt;br/&gt;
long consecutive sections without any deletions.&lt;/p&gt;

&lt;p&gt;To find these sections we could use the skip lists. Basically at any&lt;br/&gt;
point during the merge we would find the skip entry before the next&lt;br/&gt;
deleted doc. All entries to this point can be copied without&lt;br/&gt;
de-/encoding of the VInts. Then for the section that has deleted docs&lt;br/&gt;
we perform the "normal" way of merging to remove the deletes. Then we&lt;br/&gt;
check again with the skip lists if we can raw copy the next section.&lt;/p&gt;

&lt;p&gt;To make this work there are a few different necessary changes:&lt;/p&gt;

&lt;p&gt;1) Currently the multilevel skiplist reader/writer can only deal with fixed-size&lt;br/&gt;
skips (16 on the lowest level). It would be an easy change to allow&lt;br/&gt;
variable-size skips, but then the MultiLevelSkipListReader can't&lt;br/&gt;
return numSkippedDocs anymore, which SegmentTermDocs needs -&amp;gt; change 2)&lt;/p&gt;

&lt;p&gt;2) Store the last docID in which a term occurred in the term&lt;br/&gt;
dictionary. This would also be beneficial for other use cases. By&lt;br/&gt;
doing that the SegmentTermDocs#next(), #read() and #skipTo() know when&lt;br/&gt;
the end of the postinglist is reached. Currently they have to track&lt;br/&gt;
the df, which is why after a skip it's important to take the&lt;br/&gt;
numSkippedDocs into account.&lt;/p&gt;

&lt;p&gt;3) Change the merging algorithm according to my description above. It's&lt;br/&gt;
important to create a new skiplist entry at the beginning of every&lt;br/&gt;
block that is copied in raw mode, because its next skip entry's values&lt;br/&gt;
are deltas from the beginning of the block. Also the very first posting, and&lt;br/&gt;
that one only, needs to be decoded/encoded to make sure that the&lt;br/&gt;
payload length is explicitly written (i.e. must not depend on the&lt;br/&gt;
previous length). Also such a skip entry has to be created at the&lt;br/&gt;
beginning of each source segment's posting list. With change 2) we don't&lt;br/&gt;
have to worry about the positions of the skip entries. And having a few&lt;br/&gt;
extra skip entries in merged segments won't hurt much.&lt;/p&gt;


&lt;p&gt;If a segment has no deletions at all this will avoid any&lt;br/&gt;
decoding/encoding of VInts (best case). I think it will also work&lt;br/&gt;
great for segments with a rather low amount of deletions. We should&lt;br/&gt;
probably then have a threshold: if the number of deletes exceeds this&lt;br/&gt;
threshold we should fall back to old style merging.&lt;/p&gt;

&lt;p&gt;I haven't implemented any of this, so there might be complications I&lt;br/&gt;
haven't thought about. Please let me know if you can think of reasons&lt;br/&gt;
why this wouldn't work or if you think more changes are necessary.&lt;/p&gt;

&lt;p&gt;I will probably not have time to work on this soon, but I wanted to&lt;br/&gt;
open this issue to not forget about it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. Anyone should feel free to&lt;br/&gt;
take this!&lt;/p&gt;

&lt;p&gt;Btw: I think the flex-indexing branch would be a great place to try this&lt;br/&gt;
out as a new codec. This would also be good to figure out what APIs&lt;br/&gt;
are needed to make merging fully flexible as well.&lt;/p&gt;




</description>
                <environment/>
            <key id="12441106">LUCENE-2082</key>
            <summary>Performance improvement for merging posting lists</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Wed, 18 Nov 2009 21:52:39 +0000</created>
                <updated>Fri, 10 May 2013 00:05:32 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>2</votes>
                        <watches>7</watches>
                                                    <comments>
                    <comment id="12781027" author="mikemccand" created="Sat, 21 Nov 2009 18:58:57 +0000">&lt;p&gt;This sounds like a neat idea!  For building up a new index (no deletions) it ought to be a sizable performance gain.&lt;/p&gt;

&lt;p&gt;I just committed changes on the flex branch to make it possible for codecs to override merging...&lt;/p&gt;</comment>
                    <comment id="13633333" author="whzz" created="Tue, 16 Apr 2013 21:46:37 +0100">&lt;p&gt;Is anyone actively working on this improvement? If not, I was wondering if I could contribute &amp;#8211; I'm a graduate CS student and for my final project I'm investigating ways to optimize merging and index creation time of text indices. During my research I recently came across this ticket and I thought that I'd like to implement this idea.&lt;/p&gt;

&lt;p&gt;I took a first look at the source code &amp;#8211; it seems that the codebase changed significantly since the time this issue was created. From what I found in the documentation (&lt;a href="http://lucene.apache.org/core/4_0_0/MIGRATE.html" class="external-link"&gt;http://lucene.apache.org/core/4_0_0/MIGRATE.html&lt;/a&gt;), many classes mentioned here changed their names and API. Could you please update the description above or comment on what parts of it are no longer valid?&lt;/p&gt;</comment>
                    <comment id="13635788" author="mikemccand" created="Thu, 18 Apr 2013 23:52:57 +0100">&lt;p&gt;Hi Aleksandra,&lt;/p&gt;

&lt;p&gt;I don't think anyone is working on this now ... it'd be quite a bit of work!&lt;/p&gt;

&lt;p&gt;The classes have changed names but the core idea is the same.  Have a look at PostingsFormat: that's the Codec component that handles reading/writing/merging of all postings files (terms dict, docs/freqs/positions/offsets).  It seems like for this issue you'd need to override Fields/Terms/PostingsConsumer.merge methods.&lt;/p&gt;

&lt;p&gt;But some things here will likely require changes outside of Codec, eg today we always remove deletes while merging, but for this issue it looks like you may want to have a threshold below which the deletes are not removed...&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sat, 21 Nov 2009 18:58:57 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11697</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25637</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2078] Remove dependencies on specific field names or prefixes for field names (i.e. tierPrefix)</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2078</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently, the spatial contrib makes a lot of assumptions about what field names are when these are simply not needed.  By doing so, it prevents re-use in other applications that have setup their fields differently.&lt;/p&gt;</description>
                <environment/>
            <key id="12440953">LUCENE-2078</key>
            <summary>Remove dependencies on specific field names or prefixes for field names (i.e. tierPrefix)</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Tue, 17 Nov 2009 16:10:13 +0000</created>
                <updated>Mon, 13 May 2013 03:57:07 +0100</updated>
                                    <version>2.9</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/spatial</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                        <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12404381">SOLR-773</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2903</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25641</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2073] Document issues involved in building your index with one jdk version and then searching/updating with another</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2073</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I think this needs to go in something of a permenant spot - this isn't a one time release type issues - its going to present over multiple release.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If there is nothing we can do here, then we just have to do the best we can -&lt;br/&gt;
such as a prominent notice alerting that if you transition JVM's between building and searching the index and you are using or doing X, things will break.&lt;/p&gt;

&lt;p&gt;We should put this in a spot that is always pretty visible - perhaps even a new readme file titlted something like IndexBackwardCompatibility or something, to which we can add other tips and gotchyas as they come up. Or MaintainingIndicesAcrossVersions, or FancyWhateverGetsYourAttentionAboutUpgradingStuff. Or a permanent entry/sticky entry at the top of Changes.&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment/>
            <key id="12440857">LUCENE-2073</key>
            <summary>Document issues involved in building your index with one jdk version and then searching/updating with another</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Mon, 16 Nov 2009 21:10:55 +0000</created>
                <updated>Fri, 10 May 2013 00:05:33 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12778547" author="rcmuir" created="Mon, 16 Nov 2009 21:16:42 +0000">&lt;p&gt;Mark, I agree, there are two issues I know of:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;using a new java version can change unicode version, which changes character properties, which makes things tokenize differently.&lt;/li&gt;
	&lt;li&gt;using a different default locale can change at least some things in contrib, for example PatternAnalyzer uses a Locale-based toLowerCase() method, which will break if default locale is different between index and search.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;... &amp;lt;other things you can think of&amp;gt;&lt;/p&gt;</comment>
                    <comment id="12778815" author="thetaphi" created="Tue, 17 Nov 2009 10:44:56 +0000">&lt;p&gt;How about adding a general note into some JRE_VERSION_MIGRATION.txt on each release, also add this to the non-version specific site docs?&lt;br/&gt;
The first version that would contain it in the distribution would be 3.0, but it should be noted on the site-docs, that this affects &lt;b&gt;all&lt;/b&gt; versions. Something like "whenever you change JRE major versions 1.4 -&amp;gt; 1.5 -&amp;gt; 1.6 -&amp;gt; 1.7 you may have to reindex, because the underlying VM's Unicode version could have changed in a way, affecting your tokenizers."&lt;/p&gt;</comment>
                    <comment id="12778862" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 13:22:42 +0000">&lt;p&gt;Sounds good to me.&lt;/p&gt;</comment>
                    <comment id="12778875" author="creamyg" created="Tue, 17 Nov 2009 13:43:15 +0000">&lt;p&gt;Which components are affected by this?  I think just Analyzers and query&lt;br/&gt;
parsers, yes?&lt;/p&gt;

&lt;p&gt;If that's true, my inclination would be to add a note to the javadocs for each&lt;br/&gt;
such class. In every case, it's theoretically possible to build alternative&lt;br/&gt;
implementations which are unaffected by upgrading the JVM.  &lt;/p&gt;

&lt;p&gt;This isn't a fundamental problem with the Lucene architecture; it's an&lt;br/&gt;
artifact of the way certain classes are implemented.  Outside of the affected&lt;br/&gt;
components, Lucene doesn't get down and dirty with Unicode properties and&lt;br/&gt;
other fast-moving stuff &amp;#8211; it's just dealing in UTF-8 bytes, Java strings,&lt;br/&gt;
etc.  Those things can change (Modified UTF-8, shudder), but they move on a&lt;br/&gt;
slower timescale.&lt;/p&gt;

&lt;p&gt;Arguably, Analyzer subclasses shouldn't be in core for reasons like this.&lt;br/&gt;
Perhaps there could be an "ICUAnalysis" package which depends on ICU4J, so&lt;br/&gt;
that Unicode-related index incompatibilites occur when you upgrade your&lt;br/&gt;
Unicode library.  Though most people would probably choose to use the&lt;br/&gt;
smaller-footprint, zero-dependency "JVMAnalysis" package, where reindexing&lt;br/&gt;
would be required after a JVM upgrade.&lt;/p&gt;

&lt;p&gt;The software certifiers wouldn't like that, and I'm not seriously advocating&lt;br/&gt;
such a disruptive change (yet), but I just wanted to illustrate that this is a&lt;br/&gt;
contained problem.&lt;/p&gt;
</comment>
                    <comment id="12778880" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 13:49:17 +0000">&lt;blockquote&gt;&lt;p&gt; If that's true, my inclination would be to add a note to the javadocs for each&lt;br/&gt;
such class. In every case, it's theoretically possible to build alternative&lt;br/&gt;
implementations which are unaffected by upgrading the JVM. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think thats a good idea, but I'd still like the top level warning as well - I think it should be specific in explaining the problem, and not claim that it exists for everything - Robert has already laid out a couple specific examples we can mention. The problem with the javadoc approach only is that its very easy to miss - I think its a good place to lay out the details for that particular class - but it would still be great to have a visible top level warning so that users know to be careful of this.&lt;/p&gt;</comment>
                    <comment id="12778882" author="rcmuir" created="Tue, 17 Nov 2009 13:51:35 +0000">&lt;blockquote&gt;&lt;p&gt;Which components are affected by this? I think just Analyzers and query parsers, yes? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think just tokenstreams / analyzers.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This isn't a fundamental problem with the Lucene architecture; it's an artifact of the way certain classes are implemented.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yes, you are exactly right. StandardTokenizer does not have this problem. its behavior is independent of the users JVM, although dependent on the JVM of the developer who last re-ran jflex &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Perhaps there could be an "ICUAnalysis" package which depends on ICU4J&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah, this is what I think most of us who need unicode support do anyway. so i tried to start a good impl of this in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1488" title="multilingual analyzer based on icu"&gt;&lt;del&gt;LUCENE-1488&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
even if you do not care about unicode support, maybe you care that things like .isWhiteSpace() are faster than in the jdk:&lt;br/&gt;
&lt;a href="http://site.icu-project.org/home/why-use-icu4j" class="external-link"&gt;http://site.icu-project.org/home/why-use-icu4j&lt;/a&gt; (see under Performance section).&lt;/p&gt;
</comment>
                    <comment id="12778886" author="rcmuir" created="Tue, 17 Nov 2009 14:05:02 +0000">&lt;p&gt;Hello, whereever we put this text (i do not care), I thought the following would be a start.&lt;/p&gt;

&lt;p&gt;please help me make it easier to digest/understand&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;If possible, use the same major JVM version at both index and search time.
Different JVM versions may implement different versions of the Unicode Standard, which will change the way Lucene treats your text.
For example, under Java 1.4, LetterTokenizer will split around the character U+02C6, but under Java 5 it will not.
This is because Java 1.4 implements Unicode 3, but Java 5 implements Unicode 4.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12778890" author="rcmuir" created="Tue, 17 Nov 2009 14:12:40 +0000">&lt;p&gt;I guess that warning still isnt good enough. if you bump jvm version you should reindex older data too, and not just add new documents to it...&lt;/p&gt;</comment>
                    <comment id="12778948" author="rcmuir" created="Tue, 17 Nov 2009 15:47:50 +0000">&lt;p&gt;i tried to improve the wording a bit.&lt;/p&gt;</comment>
                    <comment id="12778955" author="rcmuir" created="Tue, 17 Nov 2009 16:02:55 +0000">&lt;p&gt;ok, i think this one is better.&lt;br/&gt;
For convenience, I list which JRE versions correspond to which unicode versions.&lt;/p&gt;

&lt;p&gt;Because I think that for example, if someone goes from Java 5 to Java 6, they should not worry about this.&lt;/p&gt;</comment>
                    <comment id="12778998" author="creamyg" created="Tue, 17 Nov 2009 17:25:05 +0000">&lt;p&gt;I like this:&lt;/p&gt;

&lt;p&gt;&amp;gt; some parts of Lucene&lt;/p&gt;

&lt;p&gt;... but I still think the message is a little too aggressive.  There are a lot&lt;br/&gt;
of people just using ye olde StandardAnalyzer, and they don't need to reindex.&lt;br/&gt;
We don't need to spread our own FUD. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Can we change it to say "Analyzers", and then refer people to the docs for&lt;br/&gt;
their specific Analyzer?  Alternatively, should that notification just contain&lt;br/&gt;
a complete list of the affected classes?&lt;/p&gt;</comment>
                    <comment id="12778999" author="rcmuir" created="Tue, 17 Nov 2009 17:29:00 +0000">&lt;p&gt;Hi Marvin, thanks for reviewing it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;... but I still think the message is a little too aggressive. There are a lot&lt;br/&gt;
of people just using ye olde StandardAnalyzer, and they don't need to reindex.&lt;br/&gt;
We don't need to spread our own FUD. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;are you sure? StandardAnalyzer uses LowerCaseFilter, i did not check to see if any casing properties have changed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Can we change it to say "Analyzers", and then refer people to the docs for&lt;br/&gt;
their specific Analyzer? Alternatively, should that notification just contain&lt;br/&gt;
a complete list of the affected classes?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Basically I am trying to be ambiguous, because I'm not sure what all is affected without doing some significant analysis!&lt;br/&gt;
I will do this, if its the right thing though.&lt;/p&gt;</comment>
                    <comment id="12779000" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 17:29:05 +0000">&lt;p&gt;Agreed - we don't want to cause those that don't need to to reindex just to try and be safe.&lt;/p&gt;</comment>
                    <comment id="12779004" author="rcmuir" created="Tue, 17 Nov 2009 17:36:02 +0000">&lt;p&gt;Mark, but here is the crux of the issue:&lt;/p&gt;

&lt;p&gt;While i think casing property is normative and should not change, new characters can be introduced with new casing properties.&lt;br/&gt;
This of course should not affect Simple/StopAnalyzer, but may affect StandardAnalyzer.&lt;/p&gt;

&lt;p&gt;The reason is that StandardTokenizer contains hardcoded (sometimes oversized) ranges that may include some characters that were previously unassigned in Unicode 3.&lt;br/&gt;
If they are assigned in Unicode 4, with a casing property, then this means for lucene, they were indexed in uppercase in 1.4 but lowercase in 1.5... i hope this makes sense.&lt;/p&gt;</comment>
                    <comment id="12779006" author="creamyg" created="Tue, 17 Nov 2009 17:41:26 +0000">&lt;p&gt;&amp;gt; are you sure? StandardAnalyzer uses LowerCaseFilter,&lt;/p&gt;

&lt;p&gt;No, I'm not sure.  &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  I was confusing StandardAnalyzer and StandardTokenizer.&lt;/p&gt;

&lt;p&gt;I still think that there are a lot of people who don't need to reindex,&lt;br/&gt;
because, for example, their entire corpus is limited to Latin-1 code points. &lt;/p&gt;

&lt;p&gt;Conversely, the people most likely to be affected are the people most likely&lt;br/&gt;
to be on the lookout for this kind of think.  I think it's important to&lt;br/&gt;
reach this group, without unduly alarming those who don't really need to&lt;br/&gt;
reindex.  Reindexing is a huge pain for some installations.&lt;/p&gt;</comment>
                    <comment id="12779010" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 17:45:43 +0000">&lt;p&gt;Okay, fair enough - in the end I do think we want to err on the side of caution - losing terms/docs in the index can be disastrous for some. Can we contain the statement at all though? What if you are just working with essentially ascii text? Basic English chars? That type of thing ... perhaps saying something to the effect of, in many/some cases, a user might not be affected, but if you need to be sure, you must reindex?&lt;/p&gt;</comment>
                    <comment id="12779011" author="rcmuir" created="Tue, 17 Nov 2009 17:47:37 +0000">&lt;p&gt;I am pretty sure StandardAnalyzer is ok actually now.&lt;br/&gt;
The only time it uses hardcoded ranges is for the CJK definition:&lt;br/&gt;
and the following UnicodeSet is the empty set (0 codepoints):&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;[[\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]&amp;amp;[:Case_Sensitive=True:]]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;WikipediaTokenizer, probably not ok &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12779012" author="rcmuir" created="Tue, 17 Nov 2009 17:52:05 +0000">&lt;p&gt;Mark good point, if you are using "Basic Latin" as exists on your keyboard, you might be ok.&lt;/p&gt;

&lt;p&gt;Of course, one never knows, I feel like i remember Microsoft Word creating strange hyphens and smart quotes automatically... &lt;/p&gt;

&lt;p&gt;I'd love to contain it, if you have some suggested verbage lets put it in there.&lt;/p&gt;</comment>
                    <comment id="12779015" author="creamyg" created="Tue, 17 Nov 2009 17:54:24 +0000">&lt;p&gt;&amp;gt; I am pretty sure StandardAnalyzer is ok actually now.&lt;/p&gt;

&lt;p&gt;Good news!  Thanks for performing that analysis.&lt;/p&gt;</comment>
                    <comment id="12779018" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 18:01:05 +0000">&lt;blockquote&gt;&lt;p&gt;if you are using "Basic Latin" as exists on your keyboard, you might be ok.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Heh - well if thats the strongest you would say that, sounds like its not very containable. Perhaps we just put that?&lt;/p&gt;


&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Yes, word def uses a couple funky things by default with hyphens and quotes - ran into that before. But I think its somewhat up to you to know that. If you are getting your text from another application, than you don't likely properly know you have latin1 stuff. But some people will know - in the newspaper biz, we use pre processing filters to map anything outside of latin1 to latin1 - else remove it. If you don't know your source text well enough, then yes, all bets are off.&lt;/p&gt;</comment>
                    <comment id="12779020" author="rcmuir" created="Tue, 17 Nov 2009 18:04:27 +0000">&lt;blockquote&gt;
&lt;p&gt;&amp;gt; I am pretty sure StandardAnalyzer is ok actually now.&lt;/p&gt;

&lt;p&gt;Good news! Thanks for performing that analysis.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Doh! this case pair stability policy was not introduced until 5.0, so to validate, i must do further analysis&lt;br/&gt;
ref: "unicode back compat policy": &lt;a href="http://www.unicode.org/policies/stability_policy.html" class="external-link"&gt;http://www.unicode.org/policies/stability_policy.html&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12779023" author="thetaphi" created="Tue, 17 Nov 2009 18:05:45 +0000">&lt;blockquote&gt;&lt;p&gt;Of course, one never knows, I feel like i remember Microsoft Word creating strange hyphens and smart quotes automatically... &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is the same strange soft hyphen we talked about before... Normally you should simply remove it during indexing using a CharFilter (I do this when indexing word documents and so on).&lt;/p&gt;</comment>
                    <comment id="12779030" author="rcmuir" created="Tue, 17 Nov 2009 18:30:02 +0000">&lt;p&gt;Hi, I completely validated. StandardAnalyzer is ok. none of the casing changed for any previous &lt;span class="error"&gt;&amp;#91;:letter:&amp;#93;&lt;/span&gt; characters from Unicode 3 in Unicode 4.&lt;/p&gt;

&lt;p&gt;I'll work on adding more explanation&lt;/p&gt;
</comment>
                    <comment id="12779035" author="rcmuir" created="Tue, 17 Nov 2009 18:41:40 +0000">&lt;p&gt;very happy to add new verbage here mentioning that StandardAnalyzer is ok.&lt;/p&gt;</comment>
                    <comment id="12779038" author="rcmuir" created="Tue, 17 Nov 2009 18:48:10 +0000">&lt;p&gt;the crappy test script i wrote to confirm the lowercasing behavior.&lt;br/&gt;
maybe we need it again if we go to java 1.7 in this decade, or maybe i screwed something up &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12779043" author="thetaphi" created="Tue, 17 Nov 2009 18:55:28 +0000">&lt;p&gt;Text looks good, maybe we should still add the "basic latin as on your keyboard".&lt;/p&gt;</comment>
                    <comment id="12779049" author="rcmuir" created="Tue, 17 Nov 2009 19:03:53 +0000">&lt;p&gt;Uwe, ok, I am currently trying to come up with a way to word this.&lt;/p&gt;

&lt;p&gt;Unknowns like the soft hyphen thing and other things word processors, etc might do make me really cautious to say something like this...&lt;br/&gt;
I don't want someone to unnecessarily reindex, but on the other hand, I don't want to see bug report or whatever either.&lt;/p&gt;</comment>
                    <comment id="12779056" author="rcmuir" created="Tue, 17 Nov 2009 19:14:37 +0000">&lt;p&gt;i added the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In general, whether or not you need to re-index largely depends upon the data that&lt;br/&gt;
you are searching, and what was changed in any given Unicode version. For example,&lt;br/&gt;
if you are completely sure that your content is limited to the "Basic Latin" range&lt;br/&gt;
of Unicode, you can safely ignore this.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                    <comment id="12779078" author="rcmuir" created="Tue, 17 Nov 2009 19:38:48 +0000">&lt;p&gt;guys I would like to commit this file soon, let me know if you see any issues with it.&lt;/p&gt;

&lt;p&gt;I think this additional verbage i added about "basic latin" is safe, the only properties that have changed even up to unicode 5.1 are minor changes to bidirectional properties.&lt;br/&gt;
This only affects rendering/layout.&lt;/p&gt;</comment>
                    <comment id="12779083" author="thetaphi" created="Tue, 17 Nov 2009 19:52:45 +0000">&lt;p&gt;+1, ok&lt;/p&gt;</comment>
                    <comment id="12779086" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 19:56:40 +0000">&lt;p&gt;Looks great! +1&lt;/p&gt;</comment>
                    <comment id="12779094" author="rcmuir" created="Tue, 17 Nov 2009 20:09:09 +0000">&lt;p&gt;Committed revision 881466.&lt;/p&gt;

&lt;p&gt;should we keep the issue open Mark, or close it?&lt;br/&gt;
I think we will potentially have some specialized problems in 3.1 also... (due to fixing unicode 4/suppl characters)&lt;/p&gt;</comment>
                    <comment id="12779096" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 20:13:24 +0000">&lt;p&gt;Okay - popped it to 3.1 to ensure we look at this again.&lt;/p&gt;</comment>
                    <comment id="12779098" author="rcmuir" created="Tue, 17 Nov 2009 20:17:57 +0000">&lt;p&gt;Cool, we can do something similar in 3.1 &lt;/p&gt;

&lt;p&gt;While it seems crazy, like these issues affect no one, someone told me today about relying upon broken unicode 4 behavior in lucene for chinese tokenization....!&lt;/p&gt;</comment>
                    <comment id="13292101" author="hossman" created="Sat, 9 Jun 2012 00:49:30 +0100">&lt;p&gt;rmuir already wrote a doc on this back in 2009, not sure why it was the jira was left open.&lt;/p&gt;

&lt;p&gt;rmuir: can you resolve if you think we're good here, or update with what you think is still needed otherwise?&lt;/p&gt;</comment>
                    <comment id="13412294" author="hossman" created="Thu, 12 Jul 2012 00:03:44 +0100">&lt;p&gt;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment&lt;/p&gt;</comment>
                    <comment id="13429693" author="rcmuir" created="Tue, 7 Aug 2012 04:41:18 +0100">&lt;p&gt;rmuir20120906-bulk-40-change&lt;/p&gt;</comment>
                    <comment id="13554263" author="steve_rowe" created="Tue, 15 Jan 2013 20:11:46 +0000">&lt;p&gt;AFAICT, &lt;tt&gt;lucene/JRE_VERION_MIGRATION.txt&lt;/tt&gt; doesn't need any modifications for the 4.1 release.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12425249" name="LUCENE-2073.patch" size="2017" author="rcmuir" created="Tue, 17 Nov 2009 19:14:37 +0000"/>
                    <attachment id="12425245" name="LUCENE-2073.patch" size="1721" author="rcmuir" created="Tue, 17 Nov 2009 18:41:40 +0000"/>
                    <attachment id="12425231" name="LUCENE-2073.patch" size="1020" author="rcmuir" created="Tue, 17 Nov 2009 16:02:55 +0000"/>
                    <attachment id="12425228" name="LUCENE-2073.patch" size="856" author="rcmuir" created="Tue, 17 Nov 2009 15:47:50 +0000"/>
                    <attachment id="12425246" name="stdAnalyzerTest.java" size="1722" author="rcmuir" created="Tue, 17 Nov 2009 18:48:10 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 16 Nov 2009 21:16:42 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2915</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25646</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2047] IndexWriter should immediately resolve deleted docs to docID in near-real-time mode</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2047</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Spinoff from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1526" title="For near real-time search, use paged copy-on-write BitVector impl"&gt;&lt;del&gt;LUCENE-1526&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When deleteDocuments(Term) is called, we currently always buffer the&lt;br/&gt;
Term and only later, when it's time to flush deletes, resolve to&lt;br/&gt;
docIDs.  This is necessary because we don't in general hold&lt;br/&gt;
SegmentReaders open.&lt;/p&gt;

&lt;p&gt;But, when IndexWriter is in NRT mode, we pool the readers, and so&lt;br/&gt;
deleting in the foreground is possible.&lt;/p&gt;

&lt;p&gt;It's also beneficial, in that in can reduce the turnaround time when&lt;br/&gt;
reopening a new NRT reader by taking this resolution off the reopen&lt;br/&gt;
path.  And if multiple threads are used to do the deletion, then we&lt;br/&gt;
gain concurrency, vs reopen which is not concurrent when flushing the&lt;br/&gt;
deletes.&lt;/p&gt;</description>
                <environment/>
            <key id="12440208">LUCENE-2047</key>
            <summary>IndexWriter should immediately resolve deleted docs to docID in near-real-time mode</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 9 Nov 2009 20:25:16 +0000</created>
                <updated>Fri, 10 May 2013 00:05:33 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12775115" author="jasonrutherglen" created="Mon, 9 Nov 2009 20:32:15 +0000">&lt;p&gt;Is this going to be an option or default to true only when NRT is on?&lt;/p&gt;

&lt;p&gt;Also, I can create a patch for this.&lt;/p&gt;</comment>
                    <comment id="12775121" author="mikemccand" created="Mon, 9 Nov 2009 20:42:16 +0000">&lt;p&gt;I think simply default to true.&lt;/p&gt;

&lt;p&gt;Yes, please cons up a patch!&lt;/p&gt;</comment>
                    <comment id="12776198" author="jasonrutherglen" created="Wed, 11 Nov 2009 00:50:25 +0000">&lt;p&gt;Deletes occur immediately if poolReader is true&lt;/p&gt;

&lt;p&gt;I'm not sure updateDocument needs to delete immediately, as it's also writing a document, the deletes later would be lost in the noise.&lt;/p&gt;</comment>
                    <comment id="12776203" author="mikemccand" created="Wed, 11 Nov 2009 00:55:44 +0000">&lt;p&gt;Thanks Jason!&lt;/p&gt;

&lt;p&gt;I would think we do want to delete live for updateDocument as well?  It's not clear the deletion will be in the noise (if it hits a disk seek, it won't).&lt;/p&gt;</comment>
                    <comment id="12776204" author="mikemccand" created="Wed, 11 Nov 2009 00:56:03 +0000">&lt;p&gt;Pushing to 3.1...&lt;/p&gt;</comment>
                    <comment id="12776674" author="jasonrutherglen" created="Wed, 11 Nov 2009 21:39:31 +0000">&lt;blockquote&gt;&lt;p&gt;I would think we do want to delete live for&lt;br/&gt;
updateDocument as well? It's not clear the deletion will be in&lt;br/&gt;
the noise (if it hits a disk seek, it won't).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The delete shouldn't hit a disk seek because with poolReaders,&lt;br/&gt;
the deletes are applied and carried over in RAM. I'd assume the&lt;br/&gt;
overhead of flushing the segment would be the main consumption&lt;br/&gt;
of CPU and perhaps IO? Which with &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1313" title="Near Realtime Search (using a built in RAMDirectory)"&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt;, is less of an&lt;br/&gt;
issue. &lt;/p&gt;

&lt;p&gt;For consistency with deleteDocument, I'll add live deleting to&lt;br/&gt;
updateDocument. &lt;/p&gt;</comment>
                    <comment id="12776691" author="mikemccand" created="Wed, 11 Nov 2009 22:23:58 +0000">&lt;p&gt;We could very likely hit seeks resolving the term -&amp;gt; docID.  EG to consult the terms dict index (eg page fault), terms dict itself, and then to load the posting.  (Once we have pulsing, we can save that seek).&lt;/p&gt;</comment>
                    <comment id="12776804" author="jasonrutherglen" created="Thu, 12 Nov 2009 03:01:38 +0000">&lt;blockquote&gt;&lt;p&gt;likely hit seeks resolving the term -&amp;gt; docID&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, duh!  Hopefully we won't get into the delete by doc id discussion again.  &lt;/p&gt;</comment>
                    <comment id="12776863" author="jasonrutherglen" created="Thu, 12 Nov 2009 06:05:20 +0000">&lt;p&gt;I added deleting live for updateDocument.&lt;/p&gt;

&lt;p&gt;TestNRTReaderWithThreads and TestIndexWriterReader passes.&lt;/p&gt;
</comment>
                    <comment id="12776865" author="jasonrutherglen" created="Thu, 12 Nov 2009 06:12:33 +0000">&lt;p&gt;I'd like to remove the syncing in the deleteLive methods, however this causes other tangential exceptions.  &lt;/p&gt;</comment>
                    <comment id="12776868" author="jasonrutherglen" created="Thu, 12 Nov 2009 06:24:56 +0000">&lt;p&gt;Without syncing on the writer, we run into the SR's files being deleted out from under it while it's being obtained for deletion (I think).&lt;/p&gt;</comment>
                    <comment id="12776877" author="jasonrutherglen" created="Thu, 12 Nov 2009 07:00:30 +0000">&lt;p&gt;After thumbing a bit through the code, somehow, and this is a&lt;br/&gt;
bit too complex for me to venture a guess on, we'd need to&lt;br/&gt;
prevent the deletion of the SR's files we're deleting from, even&lt;br/&gt;
if that SR is no longer live. Which means possibly interacting&lt;br/&gt;
with the deletion policy, or some other method. It's a bit hairy&lt;br/&gt;
in the segment info transaction shuffling that goes on in IW. &lt;/p&gt;

&lt;p&gt;I believe asyncing the live deletes is a good thing, as that way&lt;br/&gt;
we're taking advantage of concurrency. The possible expense is in&lt;br/&gt;
deleting from segments that are no longer live while the&lt;br/&gt;
deleting is occurring.&lt;/p&gt;</comment>
                    <comment id="12777325" author="jasonrutherglen" created="Fri, 13 Nov 2009 01:46:39 +0000">&lt;p&gt;I think there's a fundamental design issue here. What happens to&lt;br/&gt;
documents that need to be deleted but are still in the RAM&lt;br/&gt;
buffer? Because we're not buffering the deletes those wouldn't&lt;br/&gt;
be applied. Or would we still buffer the deletes, then only&lt;br/&gt;
apply them only to the new SR created from the RAM buffer when&lt;br/&gt;
flush or getReader is called?&lt;/p&gt;
</comment>
                    <comment id="12777436" author="mikemccand" created="Fri, 13 Nov 2009 10:30:45 +0000">&lt;blockquote&gt;
&lt;p&gt;we'd need to&lt;br/&gt;
prevent the deletion of the SR's files we're deleting from, even&lt;br/&gt;
if that SR is no longer live. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It's strange that anything here is needed, because, when you check a&lt;br/&gt;
reader out from the pool, it's incRef'd, which should mean the files&lt;br/&gt;
need no protection.  Something strange is up... could it be that when&lt;br/&gt;
you checkout that reader to do deletions, it wasn't already open, and&lt;br/&gt;
then on trying to open it, its files were already deleted?  (In which&lt;br/&gt;
case, that segment has been merged away, and, the merge has committed,&lt;br/&gt;
ie already carried over all deletes, and so you should instead be&lt;br/&gt;
deleting against that merged segment).&lt;/p&gt;

&lt;p&gt;So I think the sync(IW) is in fact necessary?  Note that the current&lt;br/&gt;
approach (deferring resolving term -&amp;gt; docIDs until flush time) aiso&lt;br/&gt;
sync(IW)'d, so we're not really changing that, here.  Though I agree&lt;br/&gt;
it would be nice to not have to sync(IW).  Really what we need to sync&lt;br/&gt;
on is "any merge that is merging this segment away and now wants to&lt;br/&gt;
commit".  That's actually a very narrow event so someday (separate&lt;br/&gt;
issue) if we could refine the sync'ing to that, it should be a good&lt;br/&gt;
net throughput improvement for updateDocument.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What happens to&lt;br/&gt;
documents that need to be deleted but are still in the RAM&lt;br/&gt;
buffer?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh, yes.  We must still buffer for this case, and resolve these&lt;br/&gt;
deletes against the newly flushed segment.  I think we need a separate&lt;br/&gt;
buffer that tracks pending delete terms only against the RAM buffer?&lt;/p&gt;

&lt;p&gt;Also, instead of actually setting the bits in SR's deletedDocs, I&lt;br/&gt;
think you should buffer the deleted docIDs into DW's&lt;br/&gt;
deletesInRAM.docIDs?  Ie, we do the resolution of Term/Query -&amp;gt; docID,&lt;br/&gt;
but buffer the docIDs we resolved to.  This is necessary for&lt;br/&gt;
correctness in exceptional situations, eg if you do a bunch of&lt;br/&gt;
updateDocuments, then DW hits an aborting exception (meaning its RAM&lt;br/&gt;
buffer may be corrupt) then DW currently discards the RAM buffer, but,&lt;br/&gt;
leaves previously flushed segments intact, so that if you then commit,&lt;br/&gt;
you have a consistent index.  Ie, in that situation, we don't want the&lt;br/&gt;
docs deleted by updateDocument calls to be committed to the index, so&lt;br/&gt;
we need to buffer them.&lt;/p&gt;</comment>
                    <comment id="12777638" author="jasonrutherglen" created="Fri, 13 Nov 2009 20:20:30 +0000">&lt;blockquote&gt;&lt;p&gt;It's strange that anything here is needed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was obtaining the segment infos synced, had a small block of&lt;br/&gt;
unsynced code, then synced obtaining the sometimes defunct&lt;br/&gt;
readers. Fixed that part, then the errors went away!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the sync(IW) is in fact necessary? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm hoping we can do the deletes unsynced, which will make this&lt;br/&gt;
patch a net performance gain because we're allowing multiple&lt;br/&gt;
threads to delete concurrently (whereas today we're performing&lt;br/&gt;
them synced at flush time, i.e. the current patch is merely&lt;br/&gt;
shifting the term/query lookup cost from flush to&lt;br/&gt;
deleteDocument).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;buffer the deleted docIDs into DW's deletesInRAM.docIDs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'll need to step through this, as it's a little strange to me&lt;br/&gt;
how DW knows the doc id to cache for a particular SR, i.e. how&lt;br/&gt;
are they mapped to an SR? Oh there's the DW.remapDeletes method?&lt;br/&gt;
Hmm...&lt;/p&gt;

&lt;p&gt;Couldn't we save off a per SR BV for the update doc rollback&lt;br/&gt;
case, merging the special updated doc BV into the SR's deletes&lt;br/&gt;
on successful flush, throwing them away on failure?  Memory is&lt;br/&gt;
less of a concern with the paged BV from the pending &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1526" title="For near real-time search, use paged copy-on-write BitVector impl"&gt;&lt;del&gt;LUCENE-1526&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
patch.  On a delete by query with many hits, I'm concerned about&lt;br/&gt;
storing too many doc id Integers in BufferedDeletes. &lt;/p&gt;

&lt;p&gt;Without syncing, new deletes could arrive, and we'd need to&lt;br/&gt;
queue them, and apply them to new segments, or newly merged&lt;br/&gt;
segments because we're not locking the segments. Otherwise some&lt;br/&gt;
deletes could be lost. &lt;/p&gt;

&lt;p&gt;A possible solution is, deleteDocument would synchronously add&lt;br/&gt;
the delete query/term to a queue per SR and return.&lt;br/&gt;
Asynchronously (i.e. in background threads) the deletes could be&lt;br/&gt;
applied. Merging would aggregate the incoming SR's queued&lt;br/&gt;
deletes (as they haven't been applied yet) into the merged&lt;br/&gt;
reader's delete queue. On flush we'd wait for these queued&lt;br/&gt;
deletes to be applied.  After flush, the queues would be clear&lt;br/&gt;
and we'd start over.  And because the delete queue is per reader,&lt;br/&gt;
it would be thrown away with the closed reader. &lt;/p&gt;</comment>
                    <comment id="12777773" author="jasonrutherglen" created="Sat, 14 Nov 2009 00:41:24 +0000">&lt;p&gt;Also, if the per SR delete queue were implemented, we could expose&lt;br/&gt;
the callback, and allow users to delete by doc id, edit norms&lt;br/&gt;
(and in the future, update field caches) for a particular&lt;br/&gt;
IndexReader.  We'd pass the reader via a callback that resembles&lt;br/&gt;
IndexReaderWarmer, then deletes, norms updates, etc, could be&lt;br/&gt;
performed like they can be today with a non-readonly IR.&lt;/p&gt;</comment>
                    <comment id="12777912" author="mikemccand" created="Sat, 14 Nov 2009 13:16:42 +0000">&lt;blockquote&gt;
&lt;p&gt;I'm hoping we can do the deletes unsynced, which will make this&lt;br/&gt;
patch a net performance gain because we're allowing multiple&lt;br/&gt;
threads to delete concurrently (whereas today we're performing&lt;br/&gt;
them synced at flush time, i.e. the current patch is merely&lt;br/&gt;
shifting the term/query lookup cost from flush to&lt;br/&gt;
deleteDocument).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Merely shifting the cost off the critical reopen path is already a&lt;br/&gt;
good step forward &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;But I agree, if we can also allow deletes to happen concurrently, that&lt;br/&gt;
would be fabulous.&lt;/p&gt;

&lt;p&gt;Currently the buffered deleted docIDs in DW are only due to exceptions&lt;br/&gt;
hit while adding a document.  EG say for a given doc there was an&lt;br/&gt;
exception during analysis, after yielding 100 tokens.  At this point&lt;br/&gt;
DW's RAM buffer (postings) has already been changed to hold this&lt;br/&gt;
docID, and we can't easily undo that.  So we instead mark the docID as&lt;br/&gt;
immediately deleted.  These deleted docIDs can then carry in RAM for&lt;br/&gt;
some time, and get remapped when merges commit.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On a delete by query with many hits, I'm concerned about storing too many doc id Integers in BufferedDeletes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, that's a risk if we are buffering an Integer for each deleted&lt;br/&gt;
docID, so we should avoid that.&lt;/p&gt;

&lt;p&gt;We really just need an efficient IntSet.  Or, it's even possible a&lt;br/&gt;
data structure that does no deduping (eg a simple growable int[])&lt;br/&gt;
would be fine, since apps would tend not to delete the same docID over&lt;br/&gt;
and over.  Since we flush when deletes use too much RAM, we'd be&lt;br/&gt;
protected from silly apps...&lt;/p&gt;

&lt;p&gt;Or maybe start with a growable int[], but cutover to BV once that's&lt;br/&gt;
too large?  This would protect the worst case of deleting by a single&lt;br/&gt;
query matching many docs in the index.&lt;/p&gt;

&lt;p&gt;Whatever data structure it is, it should live under BufferedDeletes,&lt;br/&gt;
so the exception logic that already discards these deletes on hitting&lt;br/&gt;
an aborting exception will just continue to work.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Memory is less of a concern with the paged BV from the pending &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1526" title="For near real-time search, use paged copy-on-write BitVector impl"&gt;&lt;del&gt;LUCENE-1526&lt;/del&gt;&lt;/a&gt; patch&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm not sure we even want to switch to a paged BV for NRT in general&lt;br/&gt;
(just added comment to that effect), though I guess for just this case&lt;br/&gt;
(buffering deletes in IW in NRT mode) it could make sense?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Couldn't we save off a per SR BV for the update doc rollback&lt;br/&gt;
case, merging the special updated doc BV into the SR's deletes&lt;br/&gt;
on successful flush, throwing them away on failure? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like that approach.  Because it means the normal case (no exception&lt;br/&gt;
is hit) is very fast (no merging of the two BufferedDeletes, on flush,&lt;br/&gt;
like we do today; and no docID remapping required), and only on&lt;br/&gt;
exception must we restore the previously saved deletes.&lt;/p&gt;

&lt;p&gt;Another alternative would be to redefine the semantics of IW on&lt;br/&gt;
hitting an error.... right now, if you hit an aborting exception (one&lt;br/&gt;
that may have corrupted DW's internal RAM postings), you only lose&lt;br/&gt;
those docs buffered in DW at the time.  Any already flushed segments &amp;amp;&lt;br/&gt;
new deletions within this IW session are preserved.  So in theory if&lt;br/&gt;
you closed your IW, those changes would persist.&lt;/p&gt;

&lt;p&gt;We could consider relaxing this, such that the entire session of IW is&lt;br/&gt;
rolled back, to the last commit/when-IW-was-opened, just like we now&lt;br/&gt;
do with OOME.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A possible solution is, deleteDocument would synchronously add&lt;br/&gt;
the delete query/term to a queue per SR and return.&lt;br/&gt;
Asynchronously (i.e. in background threads) the deletes could be&lt;br/&gt;
applied. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I'd prefer not to add further BG threads to IW, ie, take the app's&lt;br/&gt;
thread to perform the deletion.  If the app wants concurrency for&lt;br/&gt;
deletes, it can use multiple threads.&lt;/p&gt;

&lt;p&gt;I believe we only have to sync on the merge committing its deletes,&lt;br/&gt;
right?  So we should make a separate lock for that?&lt;/p&gt;

&lt;p&gt;And, on commit() we must also wait for all in-flight deletes to&lt;br/&gt;
finish.&lt;/p&gt;

&lt;p&gt;Finally, when a new segment is flushed, we should resolve the buffered&lt;br/&gt;
Term/Query deletions against it, during the flush?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, if the per SR delete queue were implemented, we could expose&lt;br/&gt;
the callback, and allow users to delete by doc id, edit norms&lt;br/&gt;
(and in the future, update field caches) for a particular&lt;br/&gt;
IndexReader. We'd pass the reader via a callback that resembles&lt;br/&gt;
IndexReaderWarmer, then deletes, norms updates, etc, could be&lt;br/&gt;
performed like they can be today with a non-readonly IR.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'd like to strongly decouple this discussion of extensibility, from&lt;br/&gt;
what we're doing in this issue.  I think it's a good idea, but let's&lt;br/&gt;
handle it separately &amp;#8211; maybe under &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2026" title="Refactoring of IndexWriter"&gt;LUCENE-2026&lt;/a&gt; (refactoring IW,&lt;br/&gt;
which'd pull out the reader pool).  This issue should all be "under&lt;br/&gt;
the hood" improvements.&lt;/p&gt;
</comment>
                    <comment id="12778335" author="mikemccand" created="Mon, 16 Nov 2009 13:22:29 +0000">&lt;p&gt;Thinking more on this... I'm actually no longer convinced that this&lt;br/&gt;
change is worthwhile.&lt;/p&gt;

&lt;p&gt;Net/net this will not improve the dps/qps throughput on a given fixed&lt;br/&gt;
hardware, because this is a zero sum game: the deletes must be&lt;br/&gt;
resolved one way or another.&lt;/p&gt;

&lt;p&gt;Whether we do it in batch (as today), or incrementally/concurrently,&lt;br/&gt;
one at a time as they arrive, the same work must be done.  In fact,&lt;br/&gt;
batch should be less costly in practice since it clearly has temporal&lt;br/&gt;
locality in resolving terms -&amp;gt; postings, so on a machine whose IO&lt;br/&gt;
cache can't hold the entire index in RAM, bulk flushing should be&lt;br/&gt;
a win.&lt;/p&gt;

&lt;p&gt;It's true that net latency of reopen will be reduced by being&lt;br/&gt;
incremental, but Lucene shouldn't aim to be able to reopen 100s of&lt;br/&gt;
times per second: I think that's a mis-feature (most apps don't need&lt;br/&gt;
it), and those that really do can and should use an approach like&lt;br/&gt;
Zoie.&lt;/p&gt;

&lt;p&gt;Finally, one can always set the max buffered delete terms/docs to&lt;br/&gt;
something low, to achieve this same tradeoff.  It's true that won't&lt;br/&gt;
get you concurrent resolving of deleted Terms -&amp;gt; docIDs, but I bet in&lt;br/&gt;
practice that concurrency isn't necessary (ie the performance of a&lt;br/&gt;
single thread resolving all buffered deletes is plenty fast). &lt;/p&gt;

&lt;p&gt;If the reopen time today is plenty fast, especially if you configure&lt;br/&gt;
your writer to flush often, then I don't think we need incremental&lt;br/&gt;
resolving of the deletions?&lt;/p&gt;</comment>
                    <comment id="12778452" author="jasonrutherglen" created="Mon, 16 Nov 2009 18:57:33 +0000">&lt;p&gt;I want to replay how DW handle the updateDoc call to see if my&lt;br/&gt;
understanding is correct. &lt;/p&gt;

&lt;p&gt;1: Analyzing hits an exception for a doc, it's doc id has&lt;br/&gt;
already been allocated so we mark it for deletion later (on&lt;br/&gt;
flush?) in BufferedDeletes.&lt;/p&gt;

&lt;p&gt;2: RAM Buffer writing hits an exception, we've had updates which&lt;br/&gt;
marked deletes in current segments, however they haven't been&lt;br/&gt;
applied yet because they're stored in BufferedDeletes docids.&lt;br/&gt;
They're applied on successful flush. &lt;/p&gt;

&lt;p&gt;Are these the two scenarios correct or am I completely off&lt;br/&gt;
target? If correct, isn't update doc already deleting in the&lt;br/&gt;
foreground?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;prefer not to add further BG threads&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe we can use 1.5's ReentrantReadWriteLock to effectively&lt;br/&gt;
allow multiple del/update doc calls to concurrently acquire the&lt;br/&gt;
read lock, and perform the deletes in the foreground. The write&lt;br/&gt;
lock could be acquired during commitDeletes, commit(), and after&lt;br/&gt;
a segment is flushed? I'm not sure it would be necessary to&lt;br/&gt;
acquire this write lock anytime segment infos is changed?&lt;/p&gt;

&lt;p&gt;I think it's important to remove unnecessary global locks on&lt;br/&gt;
unitary operations (like deletes). We've had great results&lt;br/&gt;
removing these locks for isDeleted, (NIO)FSDirectory where we&lt;br/&gt;
didn't think there'd be an improvement, and there was. I think&lt;br/&gt;
this patch (or a follow on one that implements the shared lock&lt;br/&gt;
solution) could effectively increase throughput (for deleting&lt;br/&gt;
and updating), measurably.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Lucene shouldn't aim to be able to reopen 100s of times&lt;br/&gt;
per second&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Reopening after every doc could be a valid case that I suspect&lt;br/&gt;
will come up again in the future. I don't think it's too hard to&lt;br/&gt;
support. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; It's true that net latency of reopen will be reduced by&lt;br/&gt;
being incremental, but Lucene shouldn't aim to be able to reopen&lt;br/&gt;
100s of times per second: &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perhaps update/del throughput will increase because of the&lt;br/&gt;
shared lock which would makes the patch(s) worth implementing.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; but I bet in practice that concurrency isn't necessary&lt;br/&gt;
(ie the performance of a single thread resolving all buffered&lt;br/&gt;
deletes is plenty fast). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We thought the same thing about the sync in FSDirectory, and it&lt;br/&gt;
turned out that in practice, NIOFSDir is an order of magnitude&lt;br/&gt;
faster on *nix machines. For NRT, every little bit of&lt;br/&gt;
concurrency will probably increase throughput. (i.e. most users&lt;br/&gt;
will have their indexes in IO cache and/or a ram dir, which&lt;br/&gt;
means we wouldn't be penalizing concurrency as we are today with&lt;br/&gt;
the global lock IW for del/up docs). &lt;/p&gt;

&lt;p&gt;I'm going to go ahead and wrap up this patch, which will shift&lt;br/&gt;
deletion cost to the del/up methods (still synchronously). Then&lt;br/&gt;
create a separate patch that implements the shared lock&lt;br/&gt;
solution. &lt;/p&gt;

&lt;p&gt;Exposing SRs for updates by the user can be done today, I'll&lt;br/&gt;
open a patch for this.&lt;/p&gt;</comment>
                    <comment id="12778620" author="mikemccand" created="Mon, 16 Nov 2009 22:52:18 +0000">&lt;blockquote&gt;&lt;p&gt;Reopening after every doc could be a valid case that I suspect will come up again in the future.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suspect the vast majority of apps would be fine with 10 reopens per&lt;br/&gt;
second.&lt;/p&gt;

&lt;p&gt;Those that really must reopen 100s of times per second can use Zoie,&lt;br/&gt;
or an approach like it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don't think it's too hard to support.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Whoa!  Merely thinking about and discussing even how to run proper&lt;br/&gt;
tests for NRT, let alone the possible improvements to Lucene on the&lt;br/&gt;
table, is sucking up all my time &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think it's important to remove unnecessary global locks on&lt;br/&gt;
unitary operations (like deletes).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I agree we should in general always improve our concurrency.  In&lt;br/&gt;
this case, resolving deletes syncs the entire IW + DW, so that blocks&lt;br/&gt;
indexing new docs, launching/committing merges, flushing, etc. which&lt;br/&gt;
we should fix.  I just don't think NRT is really a driver for this...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1: Analyzing hits an exception for a doc, it's doc id has&lt;br/&gt;
already been allocated so we mark it for deletion later (on&lt;br/&gt;
flush?) in BufferedDeletes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Analyzing or any other "non-aborting" exception, right.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2: RAM Buffer writing hits an exception, we've had updates which&lt;br/&gt;
marked deletes in current segments, however they haven't been&lt;br/&gt;
applied yet because they're stored in BufferedDeletes docids.&lt;br/&gt;
They're applied on successful flush.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No &amp;#8211; the deletes are buffered as Term, Query or docids, in the&lt;br/&gt;
BufferedDeletes.  The only case that buffers docids now is your #1&lt;br/&gt;
above.  On successful flush, these buffered things are moved to the&lt;br/&gt;
deletesFlush (but not resolved).  They are only resolved when we&lt;br/&gt;
decide it's time to apply them (just before a new merge starts, or,&lt;br/&gt;
when we've triggered the time-to-flush-deletes limits).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe we can use 1.5's ReentrantReadWriteLock to effectively&lt;br/&gt;
allow multiple del/update doc calls to concurrently acquire the&lt;br/&gt;
read lock, and perform the deletes in the foreground.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that should work well?&lt;/p&gt;</comment>
                    <comment id="12779037" author="jasonrutherglen" created="Tue, 17 Nov 2009 18:44:38 +0000">&lt;blockquote&gt;&lt;p&gt;resolving deletes syncs the entire IW + DW, so that&lt;br/&gt;
blocks indexing new docs, launching/committing merges, flushing,&lt;br/&gt;
etc... I just don't think NRT is really a driver for&lt;br/&gt;
this...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I think it's a general improvement that's come to light&lt;br/&gt;
during NRT development and testing. I think NRT is great in this&lt;br/&gt;
regard because it stresses Lucene in a completely new way, which&lt;br/&gt;
improves it for the general batch use case (i.e. users can&lt;br/&gt;
simply start utilizing NRT features when they need to without&lt;br/&gt;
worry). &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; 1: Analyzing hits an exception for a doc, it's doc id&lt;br/&gt;
has already been allocated so we mark it for deletion later (on&lt;br/&gt;
flush?) in BufferedDeletes. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So there's only one use case right now, which is only when&lt;br/&gt;
analyzing an individual doc fails. The update doc adds the term&lt;br/&gt;
to the BufferedDeletes for later application. Makes sense. I&lt;br/&gt;
think we can resolve the update doc term in the foreground.  I'm&lt;br/&gt;
wondering if we need a different doc id queue for these? I get&lt;br/&gt;
the hunch yes, because the other doc ids need to be applied even&lt;br/&gt;
on IO exception, whereas update doc id will not be applied?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; 2: RAM Buffer writing hits an exception, we've had&lt;br/&gt;
updates which marked deletes in current segments, however they&lt;br/&gt;
haven't been applied yet because they're stored in&lt;br/&gt;
BufferedDeletes docids. They're applied on successful flush.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In essence we need to implement number 2?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Analyzing or any other "non-aborting" exception,&lt;br/&gt;
right.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is an example of a non-aborting exception?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; use 1.5's ReentrantReadWriteLock &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'll incorporate RRWL into the follow on concurrent updating&lt;br/&gt;
patch. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; Whoa! Merely thinking about and discussing even how to&lt;br/&gt;
run proper tests for NRT, let alone the possible improvements to&lt;br/&gt;
Lucene on the table, is sucking up all my time &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yow, I didn't know. Thanks!&lt;/p&gt;</comment>
                    <comment id="12779075" author="jasonrutherglen" created="Tue, 17 Nov 2009 19:33:12 +0000">&lt;p&gt;I'm browsing through the applyDeletes call path, I'm tempted to&lt;br/&gt;
rework how we're doing this. For my own thinking I'd still like&lt;br/&gt;
to have a queue of deletes per SR and for the ram doc buffer. I&lt;br/&gt;
think this gives future flexibility and makes it really clear&lt;br/&gt;
when debugging what's happening underneath. I find the remapping&lt;br/&gt;
doc ids to be confusing, meaning stepping through the code would&lt;br/&gt;
seem to be difficult. If we're storing doc ids alongside the SR&lt;br/&gt;
the docs correspond to, there's a lot less to worry about and&lt;br/&gt;
just seems clearer. This may make integrating &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1313" title="Near Realtime Search (using a built in RAMDirectory)"&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
directly into IW more feasible as then we're working directly at&lt;br/&gt;
the SR level, and can tie the synchronization process together.&lt;br/&gt;
Also this could make exposing SRs externally easier and aid in&lt;br/&gt;
making IW more modular in the future?&lt;/p&gt;

&lt;p&gt;I can't find the code that handles aborts. &lt;/p&gt;</comment>
                    <comment id="12779093" author="mikemccand" created="Tue, 17 Nov 2009 20:09:05 +0000">
&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;1: Analyzing hits an exception for a doc, it's doc id has already been allocated so we mark it for deletion later (on flush?) in BufferedDeletes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So there's only one use case right now, which is only when&lt;br/&gt;
analyzing an individual doc fails. The update doc adds the term&lt;br/&gt;
to the BufferedDeletes for later application.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, it adds the docID for later application.  This is the one case&lt;br/&gt;
(entirely internal to IW) where we delete by docID in the writer.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think we can resolve the update doc term in the foreground. I'm&lt;br/&gt;
wondering if we need a different doc id queue for these? I get&lt;br/&gt;
the hunch yes, because the other doc ids need to be applied even&lt;br/&gt;
on IO exception, whereas update doc id will not be applied?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we can use the same queue &amp;#8211; whether they are applied or not&lt;br/&gt;
follows exactly the same logic (ie, successful flush moves all&lt;br/&gt;
deletesInRAM over to deletesFlushed), ie, an aborting exception&lt;br/&gt;
cleares the deletesInRAM.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2: RAM Buffer writing hits an exception, we've had&lt;br/&gt;
updates which marked deletes in current segments, however they&lt;br/&gt;
haven't been applied yet because they're stored in&lt;br/&gt;
BufferedDeletes docids. They're applied on successful flush.&lt;/p&gt;

&lt;p&gt;In essence we need to implement number 2?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm confused &amp;#8211; #2 is already what's implemented in IW, today.&lt;/p&gt;

&lt;p&gt;The changes on the table now are to:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Immediately resolve deleted (updated) terms/queries -&amp;gt; docIDs&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Change how we enqueue docIDs to be per-SR, instead.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;But: you still must buffer Term/Query for the current RAM buffer,&lt;br/&gt;
    and on flushing it to a real segment, resolve them to docIDs.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Otherwise we don't need to change what's done today (ie, keep the&lt;br/&gt;
deletesInRAM vs deletesFlushed)?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What is an example of a non-aborting exception?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Anything hit during analysis (ie, TokenStream.incrementToken()), or&lt;br/&gt;
anywhere except DocInverterPerField in the indexing chain (eg if we&lt;br/&gt;
hit something when writing the stored fields, I don't think it'll&lt;br/&gt;
abort).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I'm browsing through the applyDeletes call path, I'm tempted to&lt;br/&gt;
rework how we're doing this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this would be a good improvement &amp;#8211; it would mean we don't&lt;br/&gt;
need to ever remapDeletes, right?&lt;/p&gt;

&lt;p&gt;The thing is... this has to work in non-NRT mode, too.  Somehow, you&lt;br/&gt;
have to buffer such a deleted docID against the next segment to be&lt;br/&gt;
flushed (ie the current RAM buffer).  And once it's flushed, we move&lt;br/&gt;
the docID from deletesInRAM (stored per-SR) to the SR's actual deleted&lt;br/&gt;
docs BV.&lt;/p&gt;

&lt;p&gt;We would still keep the deletes partitioned, into those done during&lt;br/&gt;
the current RAM segment vs those successfully flushed, right?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I can't find the code that handles aborts.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It's DW's abort() method, and eg in DocInverterPerField we call&lt;br/&gt;
DW.setAborting on exception to note that this exception is an aborting&lt;br/&gt;
one.&lt;/p&gt;</comment>
                    <comment id="12779274" author="jasonrutherglen" created="Wed, 18 Nov 2009 03:17:10 +0000">&lt;ul&gt;
	&lt;li&gt;There's pending deletes (aka updateDoc generated deletes) per&lt;br/&gt;
SR. They're stored in a pending deletes BV in SR. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;commitMergedDeletes maps the pending deletes into the&lt;br/&gt;
mergeReader. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DW.abort clears the pending deletes from all SRs.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;On successful flush, the SR pending deletes are merged into the&lt;br/&gt;
primary del docs BV. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Deletes are still buffered, however they're only applied to the&lt;br/&gt;
newly flushed segment (rather than all readers). If the applying&lt;br/&gt;
fails, I think we need to keep some of the rollback from the&lt;br/&gt;
original applyDeletes?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The foreground deleting seems to break a couple of tests in&lt;br/&gt;
TestIndexWriter.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Mike, you mentioned testing getReader missing deletes etc (in&lt;br/&gt;
response to potential file handle leakage), which test or&lt;br/&gt;
benchmark did you use for this?&lt;/p&gt;</comment>
                    <comment id="12781288" author="jasonrutherglen" created="Mon, 23 Nov 2009 02:52:14 +0000">&lt;p&gt;In the updateDocument and deleteDocument methods, deletes are&lt;br/&gt;
buffered per segment reader synchronized on writer. Immediately&lt;br/&gt;
after, outside the sync block, they're deleted from the existing&lt;br/&gt;
SRs. If a new SR is added, it's because of a flush (which has&lt;br/&gt;
it's own buffered deletes), or from a merge which is&lt;br/&gt;
synchronized on writer. In other words, we won't lose deletes,&lt;br/&gt;
they'll always be applied on a flush, and the resolution of&lt;br/&gt;
deletes to doc ids happens un-synchronized on writer.&lt;/p&gt;

&lt;p&gt;Update document adds the term to the queued deletes, then&lt;br/&gt;
resolves and adds the doc ids to an Integer list (for now). This&lt;br/&gt;
is where we may want to use an growable int[] or int set.&lt;/p&gt;

&lt;p&gt;Flush applies queued update doc deleted doc ids to the SRs.&lt;/p&gt;

&lt;p&gt;commitMerge merges queued deletes from the incoming SRs. Doc ids&lt;br/&gt;
are mapped to the merged reader.&lt;/p&gt;</comment>
                    <comment id="12781290" author="jasonrutherglen" created="Mon, 23 Nov 2009 03:03:31 +0000">&lt;p&gt;When docWriter aborts on the RAM buffer, we clear out the queued updateDoc deletes.&lt;/p&gt;</comment>
                    <comment id="12781309" author="jasonrutherglen" created="Mon, 23 Nov 2009 05:09:55 +0000">&lt;p&gt;DocWriter abort created a deadlock noticed in TestIndexWriter.testIOExceptionDuringAbortWithThreads.  This is fixed by clearing via the reader pool.  Other tests fail in TIW.&lt;/p&gt;</comment>
                    <comment id="12781747" author="jasonrutherglen" created="Tue, 24 Nov 2009 02:25:22 +0000">&lt;p&gt;TestIndexWriter passes, mostly due to removing assertions in&lt;br/&gt;
reader pool release which presumed deletions would be&lt;br/&gt;
synchronized with closing a reader. They're not anymore.&lt;br/&gt;
Deletions can come in at almost anytime, so a reader may be&lt;br/&gt;
closed by the pool while still carrying deletes. The releasing&lt;br/&gt;
thread may not be synchronized on writer because we're allowing&lt;br/&gt;
deletions to occur un-synchronized.&lt;/p&gt;

&lt;p&gt;I suppose we need more tests to insure the assertions are in&lt;br/&gt;
fact not needed.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12425926" name="LUCENE-2047.patch" size="18620" author="jasonrutherglen" created="Tue, 24 Nov 2009 02:25:22 +0000"/>
                    <attachment id="12425802" name="LUCENE-2047.patch" size="17652" author="jasonrutherglen" created="Mon, 23 Nov 2009 05:09:55 +0000"/>
                    <attachment id="12425793" name="LUCENE-2047.patch" size="16773" author="jasonrutherglen" created="Mon, 23 Nov 2009 03:03:31 +0000"/>
                    <attachment id="12425792" name="LUCENE-2047.patch" size="15646" author="jasonrutherglen" created="Mon, 23 Nov 2009 02:52:14 +0000"/>
                    <attachment id="12425299" name="LUCENE-2047.patch" size="13565" author="jasonrutherglen" created="Wed, 18 Nov 2009 03:17:10 +0000"/>
                    <attachment id="12424704" name="LUCENE-2047.patch" size="10207" author="jasonrutherglen" created="Thu, 12 Nov 2009 06:05:20 +0000"/>
                    <attachment id="12424542" name="LUCENE-2047.patch" size="5721" author="jasonrutherglen" created="Wed, 11 Nov 2009 00:50:25 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>7.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 9 Nov 2009 20:32:15 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11730</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25672</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2026] Refactoring of IndexWriter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2026</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I've been thinking for a while about refactoring the IndexWriter into&lt;br/&gt;
two main components.&lt;/p&gt;

&lt;p&gt;One could be called a SegmentWriter and as the&lt;br/&gt;
name says its job would be to write one particular index segment. The&lt;br/&gt;
default one just as today will provide methods to add documents and&lt;br/&gt;
flushes when its buffer is full.&lt;br/&gt;
Other SegmentWriter implementations would do things like e.g. appending or&lt;br/&gt;
copying external segments &lt;span class="error"&gt;&amp;#91;what addIndexes*() currently does&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The second component's job would it be to manage writing the segments&lt;br/&gt;
file and merging/deleting segments. It would know about&lt;br/&gt;
DeletionPolicy, MergePolicy and MergeScheduler. Ideally it would&lt;br/&gt;
provide hooks that allow users to manage external data structures and&lt;br/&gt;
keep them in sync with Lucene's data during segment merges.&lt;/p&gt;

&lt;p&gt;API wise there are things we have to figure out, such as where the&lt;br/&gt;
updateDocument() method would fit in, because its deletion part&lt;br/&gt;
affects all segments, whereas the new document is only being added to&lt;br/&gt;
the new segment.&lt;/p&gt;

&lt;p&gt;Of course these should be lower level APIs for things like parallel&lt;br/&gt;
indexing and related use cases. That's why we should still provide&lt;br/&gt;
easy to use APIs like today for people who don't need to care about&lt;br/&gt;
per-segment ops during indexing. So the current IndexWriter could&lt;br/&gt;
probably keeps most of its APIs and delegate to the new classes.&lt;/p&gt;</description>
                <environment/>
            <key id="12439802">LUCENE-2026</key>
            <summary>Refactoring of IndexWriter</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 Nov 2009 23:15:21 +0000</created>
                <updated>Fri, 10 May 2013 00:05:33 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>1</votes>
                        <watches>5</watches>
                                                    <comments>
                    <comment id="12773329" author="john.wang@gmail.com" created="Wed, 4 Nov 2009 01:27:16 +0000">&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12773429" author="mikemccand" created="Wed, 4 Nov 2009 09:11:15 +0000">&lt;p&gt;+1!  IndexWriter has become immense.&lt;/p&gt;

&lt;p&gt;I think we should also pull out ReaderPool?&lt;/p&gt;</comment>
                    <comment id="12773432" author="michaelbusch" created="Wed, 4 Nov 2009 09:38:43 +0000">&lt;blockquote&gt;
&lt;p&gt;I think we should also pull out ReaderPool?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1!&lt;/p&gt;</comment>
                    <comment id="12788838" author="earwin" created="Thu, 10 Dec 2009 18:52:19 +0000">&lt;p&gt;We need an ability to see segment write (and probably deleted doc list write) as a discernible atomic operation. Right now it looks like several file writes, and we can't, say - redirect all files belonging to a certain segment to another Directory (well, in a simple manner). 'Something' should sit between a Directory (or several Directories) and IndexWriter.&lt;/p&gt;

&lt;p&gt;If we could do this, the current NRT search implementation will be largely obsoleted, innit? Just override the default impl of 'something' and send smaller segments to ram, bigger to disk, copy ram segments to disk asynchronously if we want to. Then we can use your granma's IndexReader and IndexWriter, totally decoupled from each other, and have blazing fast addDocument-commit-reopen turnaround.&lt;/p&gt;</comment>
                    <comment id="12788840" author="earwin" created="Thu, 10 Dec 2009 18:53:14 +0000">&lt;p&gt;Oh, forgive me if I just said something stupid &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12788856" author="mikemccand" created="Thu, 10 Dec 2009 19:31:12 +0000">&lt;p&gt;I think what you're describing is in fact the approach that &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1313" title="Near Realtime Search (using a built in RAMDirectory)"&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt; is taking; it's doing the switching internally between the main Dir &amp;amp; a private RAM Dir.&lt;/p&gt;

&lt;p&gt;But in my testing so far (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2061" title="Create benchmark &amp;amp; approach for testing Lucene&amp;#39;s near real-time performance"&gt;LUCENE-2061&lt;/a&gt;), it doesn't seem like it'll help performance much.  Ie, the OS generally seems to do a fine job putting those segments in RAM, itself.  Ie, by maintaining a write cache.  The weirdness is: that only holds true if you flush the segments when they are tiny (once per second, every 100 docs, in my test) &amp;#8211; not yet sure why that's the case.  I'm going to re-run perf tests on a more mainstream OS (my tests are all OpenSolaris) and see if that strangeness still happens.&lt;/p&gt;

&lt;p&gt;But I think you still need to not do commit() during the reopen.&lt;/p&gt;

&lt;p&gt;I do think refactoring IW so that there is a separate component that keeps track of segments in the index, may simplify NRT, in that you can go to that source for your current "segments file" even if that segments file is uncommitted.  In such a world you could do something like IndexReader.open(SegmentState) and it would be able to open (and, reopen) the real-time reader.  It's just that it's seeing changes to the SegmentState done by the writer, even if they're not yet committed.&lt;/p&gt;</comment>
                    <comment id="12789473" author="earwin" created="Fri, 11 Dec 2009 19:18:14 +0000">&lt;p&gt;If I understand everything right, with current uberfast reopens (thanks per-segment search), the only thing that makes index/commit/reopen cycle slow is the 'sync' call. That sync call on memory-based Directory is noop.&lt;/p&gt;

&lt;p&gt;And no, you really should commit() to be able to see stuff on reopen() &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; My god, seeing changes that aren't yet commited - that violates the meaning of 'commit'.&lt;/p&gt;

&lt;p&gt;The original purporse of current NRT code was.. well.. let me remember.. NRT search! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; With per-segment caches and sync lag defeated you get the delay between doc being indexed and becoming searchable under tens of milliseconds. Is that not NRT enough to introduce tight coupling between classes that have absolutely no other reason to be coupled??&lt;br/&gt;
Lucene 4.0. Simplicity is our candidate! Vote for Simplicity!&lt;/p&gt;

&lt;p&gt;*: Okay, there remains an issue of merges that piggyback on commits, so writing and commiting one smallish segment suddenly becomes a time-consuming operation. But that's a completely separate issue. Go, fix your mergepolicies and have a thread that merges asynchronously.&lt;/p&gt;</comment>
                    <comment id="12789555" author="mikemccand" created="Fri, 11 Dec 2009 21:47:21 +0000">&lt;blockquote&gt;&lt;p&gt;If I understand everything right, with current uberfast reopens (thanks per-segment search), the only thing that makes index/commit/reopen cycle slow is the 'sync' call.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, per-segment searching was the most important step towards&lt;br/&gt;
NRT.  It's a great step forward...&lt;/p&gt;

&lt;p&gt;But the fsync call is a killer, so avoiding it in the NRT path is&lt;br/&gt;
necessary.  It's also very OS/FS dependent.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That sync call on memory-based Directory is noop.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Until you need to spillover to disk because your RAM buffer is full?&lt;/p&gt;

&lt;p&gt;Also, if IW.commit() is called, I would expect any changes in RAM&lt;br/&gt;
should be committed to the real dir (stable storage)?&lt;/p&gt;

&lt;p&gt;And, going through RAM first will necessarily be a hit on indexing&lt;br/&gt;
throughput (Jake estimates 10% hit in Zoie's case).  Really, our&lt;br/&gt;
current approach goes through RAM as well, in that OS's write cache&lt;br/&gt;
(if the machine has spare RAM) will quickly accept the small index&lt;br/&gt;
files &amp;amp; write them in the BG.  It's not clear we can do better than&lt;br/&gt;
the OS here...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And no, you really should commit() to be able to see stuff on reopen()  My god, seeing changes that aren't yet commited - that violates the meaning of 'commit'.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Uh, this is an API that clearly states that its purpose is to search&lt;br/&gt;
the uncommitted changes.  If you really want to be "pure"&lt;br/&gt;
transactional, don't use this API &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The original purporse of current NRT code was.. well.. let me remember.. NRT search!  With per-segment caches and sync lag defeated you get the delay between doc being indexed and becoming searchable under tens of milliseconds. Is that not NRT enough to introduce tight coupling between classes that have absolutely no other reason to be coupled?? Lucene 4.0. Simplicity is our candidate! Vote for Simplicity!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In fact I favor our current approach because of its simplicity.&lt;/p&gt;

&lt;p&gt;Have a look at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1313" title="Near Realtime Search (using a built in RAMDirectory)"&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt; (adds RAMDir as you're discussing), or,&lt;br/&gt;
Zoie, which also adds the RAMDir and backgrounds resolving deleted&lt;br/&gt;
docs &amp;#8211; they add complexity to Lucene that I don't think is warranted.&lt;/p&gt;

&lt;p&gt;My general feeling at this point is with per-segment searching, and&lt;br/&gt;
fsync avoided, NRT performance is excellent.&lt;/p&gt;

&lt;p&gt;We've explored a number of possible tweaks to improve it &amp;#8211;&lt;br/&gt;
writing first to RAMDir (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1313" title="Near Realtime Search (using a built in RAMDirectory)"&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt;), resolving deletes in the&lt;br/&gt;
foreground (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2047" title="IndexWriter should immediately resolve deleted docs to docID in near-real-time mode"&gt;LUCENE-2047&lt;/a&gt;), using paged BitVector for deletions&lt;br/&gt;
(&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1526" title="For near real-time search, use paged copy-on-write BitVector impl"&gt;&lt;del&gt;LUCENE-1526&lt;/del&gt;&lt;/a&gt;), Zoie (buffering segments in RAM &amp;amp; backgrounds resolving&lt;br/&gt;
deletes), etc., but, based on testing so far, I don't see the&lt;br/&gt;
justification for the added complexity.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;*: Okay, there remains an issue of merges that piggyback on commits, so writing and commiting one smallish segment suddenly becomes a time-consuming operation. But that's a completely separate issue. Go, fix your mergepolicies and have a thread that merges asynchronously.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This already runs in the BG by default.  But warming the reader on the&lt;br/&gt;
merged segment (before lighting it) is important (IW does this today).&lt;/p&gt;</comment>
                    <comment id="12789604" author="earwin" created="Fri, 11 Dec 2009 23:16:56 +0000">&lt;blockquote&gt;&lt;p&gt;Until you need to spillover to disk because your RAM buffer is full?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No, buffer is there only to decouple indexing from writing. Can be spilt over asynchronously without waiting for it to be filled up.&lt;/p&gt;

&lt;p&gt;Okay, we agree on a zillion of things, except simpicity of the current NRT, and approach to commit().&lt;/p&gt;

&lt;p&gt;Good commit() behaviour consists of two parts:&lt;br/&gt;
1. Everything commit()ed is guaranteed to be on disk.&lt;br/&gt;
2. Until commit() is called, reading threads don't see new/updated records.&lt;/p&gt;

&lt;p&gt;Now we want more speed, and are ready to sacrifice something if needed.&lt;br/&gt;
You decide to sacrifice new record (in)visibility. No choice, but to hack into IW to allow readers see its hot, fresh innards.&lt;/p&gt;

&lt;p&gt;I say it's better to sacrifice write guarantee. In the rare case the process/machine crashes, you can reindex last few minutes' worth of docs. Now you don't have to hack into IW and write specialized readers. Hence, simpicity. You have only one straightforward writer, you have only one straightforward reader (which is nicely immutable and doesn't need any synchronization code).&lt;/p&gt;

&lt;p&gt;In fact you don't even need to sacrifice write guarantee. What was the reason for it? The only one I can come up with is - the thread that does writes and sync() is different from the thread that calls commit(). But, commit() can return a Future. &lt;br/&gt;
So the process goes as:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;You index docs, nobody sees them, nor deletions.&lt;/li&gt;
	&lt;li&gt;You call commit(), the docs/deletes are written down to memory (NRT case)/disk (non-NRT case). Right after calling commit() every newly reopened Reader is guaranteed to see your docs/deletes.&lt;/li&gt;
	&lt;li&gt;Background thread does write-to-disk+sync(NRT case)/just sync (non-NRT case), and fires up the Future returned from commit(). At this point all data is guaranteed to be written and braced for a crash, ram cache or not, OS/raid controller cache or not.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For back-compat purporses we can use another name for that Future-returning-commit(), and current commit() will just call this new method and wait on future returned.&lt;/p&gt;

&lt;p&gt;Okay, with that I'm probably shutting up on the topic until I can back myself up with code. Sadly, my current employer is happy with update lag in tens of seconds &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12789614" author="creamyg" created="Fri, 11 Dec 2009 23:57:53 +0000">&lt;p&gt;&amp;gt; I say it's better to sacrifice write guarantee. &lt;/p&gt;

&lt;p&gt;I don't grok why sync is the default, especially given how sketchy hardware &lt;br/&gt;
drivers are about obeying fsync:    &lt;/p&gt;

&lt;div class="panel" style="border-width: 1px;"&gt;&lt;div class="panelContent"&gt;
&lt;p&gt;  But, beware: some hardware devices may in fact cache writes even during &lt;br/&gt;
  fsync,  and return before the bits are actually on stable storage, to give the     &lt;br/&gt;
  appearance of faster performance.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;IMO, it should have been an option which defaults to false, to be enabled only by &lt;br/&gt;
users who have the expertise to ensure that fsync() is actually doing what &lt;br/&gt;
it advertises. But what's done is done (and Lucy will probably just do something &lt;br/&gt;
different.)&lt;/p&gt;

&lt;p&gt;With regard to Lucene NRT, though, turning sync() off would really help.  If and &lt;br/&gt;
when some sort of settings class comes about, an enableSync(boolean enabled) &lt;br/&gt;
method seems like it would come in handy.&lt;/p&gt;</comment>
                    <comment id="12789618" author="jake.mannix" created="Sat, 12 Dec 2009 00:03:27 +0000">&lt;blockquote&gt;&lt;p&gt;Now we want more speed, and are ready to sacrifice something if needed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;You decide to sacrifice new record (in)visibility. No choice, but to hack into IW to allow readers see its hot, fresh innards.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Chiming in here that of course, you don't &lt;b&gt;need&lt;/b&gt; (ie there is a choice) to hack into the IW to do this.  Zoie is a completely user-land solution which modifies no IW/IR internals and yet achieves millisecond index-to-query-visibility turnaround while keeping speedy indexing and query performance.  It just keeps the RAMDir outside encapsulated in an object (an IndexingSystem) which has IndexReaders built off of both the RAMDir and the FSDir, and hides the implementation details (in fact the IW itself) from the user.  &lt;/p&gt;

&lt;p&gt;The API for this kind of thing doesn't &lt;b&gt;have&lt;/b&gt; to be tightly coupled, and I would agree with you that it shouldn't be.&lt;/p&gt;</comment>
                    <comment id="12789708" author="mikemccand" created="Sat, 12 Dec 2009 10:55:53 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Until you need to spillover to disk because your RAM buffer is full?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, buffer is there only to decouple indexing from writing. Can be spilt over asynchronously without waiting for it to be filled up.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But this is where things start to get complex... the devil is in the&lt;br/&gt;
details here.  How do you carry over your deletes?  This spillover&lt;br/&gt;
will take time &amp;#8211; do you block all indexing while that's happening&lt;br/&gt;
(not great)?  Do you do it gradually (start spillover when half full,&lt;br/&gt;
but still accept indexing)?  Do you throttle things if index rate&lt;br/&gt;
exceeds flush rate?  How do you recover on exception?&lt;/p&gt;

&lt;p&gt;NRT today let's the OS's write cache decide how to use RAM to speed up&lt;br/&gt;
writing of these small files, which keeps things alot simpler for us.&lt;br/&gt;
I don't see why we should add complexity to Lucene to replicate what&lt;br/&gt;
the OS is doing for us (NOTE: I don't really trust the OS in the&lt;br/&gt;
reverse case... I do think Lucene should read into RAM the data&lt;br/&gt;
structures that are important).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You decide to sacrifice new record (in)visibility. No choice, but to hack into IW to allow readers see its hot, fresh innards.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;Now you don't have to hack into IW and write specialized readers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Probably we'll just have to disagree here... NRT isn't a hack &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;IW is already hanging onto completely normal segments.  Ie, the index&lt;br/&gt;
has been updated with these segments, just not yet published so&lt;br/&gt;
outside readers can see it.  All NRT does is let a reader see this&lt;br/&gt;
private view.&lt;/p&gt;

&lt;p&gt;The readers that an NRT reader expoes are normal SegmentReaders &amp;#8211;&lt;br/&gt;
it's just that rather than consult a segments_N on disk to get the&lt;br/&gt;
segment metadata, they pulled from IW's uncommitted in memory&lt;br/&gt;
SegmentInfos instance.&lt;/p&gt;

&lt;p&gt;Yes we've talked about the "hot innards" solution &amp;#8211; an IndexReader&lt;br/&gt;
impl that can directly search DW's ram buffer &amp;#8211; but that doesn't look&lt;br/&gt;
necessary today, because performance of NRT is good with the simple&lt;br/&gt;
solution we have now.&lt;/p&gt;

&lt;p&gt;NRT reader also gains performance by carrying over deletes in RAM.  We&lt;br/&gt;
should eventually do the same thing with norms &amp;amp; field cache.  No&lt;br/&gt;
reason to write to disk, then right away read again.&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;You index docs, nobody sees them, nor deletions.&lt;/li&gt;
	&lt;li&gt;You call commit(), the docs/deletes are written down to memory (NRT case)/disk (non-NRT case). Right after calling commit() every newly reopened Reader is guaranteed to see your docs/deletes.&lt;/li&gt;
	&lt;li&gt;Background thread does write-to-disk+sync(NRT case)/just sync (non-NRT case), and fires up the Future returned from commit(). At this point all data is guaranteed to be written and braced for a crash, ram cache or not, OS/raid controller cache or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;But this is not a commit, if docs/deletes are written down into RAM?&lt;br/&gt;
Ie, commit could return, then the machine could crash, and you've lost&lt;br/&gt;
changes?  Commit should go through to stable storage before returning?&lt;br/&gt;
Maybe I'm just missing the big picture of what you're proposing&lt;br/&gt;
here...&lt;/p&gt;

&lt;p&gt;Also, you can build all this out on top of Lucene today?  Zoie is a&lt;br/&gt;
proof point of this.  (Actually: how does your proposal differ from&lt;br/&gt;
Zoie?  Maybe that'd help shed light...).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I say it's better to sacrifice write guarantee. In the rare case the process/machine crashes, you can reindex last few minutes' worth of docs. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is not that simple &amp;#8211; if you skip the fsync, and OS crashes/you&lt;br/&gt;
lose power, your index can easily become corrupt.  The resulting&lt;br/&gt;
CheckIndex -fix can easily need to remove large segments.&lt;/p&gt;

&lt;p&gt;The OS's write cache makes no gurantees on the order in which the&lt;br/&gt;
files you've written find their way to disk.&lt;/p&gt;

&lt;p&gt;Another option (we've discussed this) would be journal file approach&lt;br/&gt;
(ie transaction log, like most DBs use).  You only have one file to&lt;br/&gt;
fsync, and you replay to recover.  But that'd be a big change for&lt;br/&gt;
Lucene, would add complexity, and can be accomplished outside of&lt;br/&gt;
Lucene if an app really wants to...&lt;/p&gt;

&lt;p&gt;Let me try turning this around: in your componentization of&lt;br/&gt;
SegmentReader, why does it matter who's tracking which components are&lt;br/&gt;
needed to make up a given SR?  In the IndexReader.open case, it's a&lt;br/&gt;
SegmntInfos instance (obtained by loading segments_N file from disk).&lt;br/&gt;
In the NRT case, it's also a SegmentInfos instace (the one IW is&lt;br/&gt;
privately keeping track of and only publishing on commit).  At the&lt;br/&gt;
component level, creating the SegmentReader should be no different?&lt;/p&gt;</comment>
                    <comment id="12789714" author="mikemccand" created="Sat, 12 Dec 2009 11:22:09 +0000">&lt;blockquote&gt;
&lt;p&gt;&amp;gt; I say it's better to sacrifice write guarantee.&lt;/p&gt;

&lt;p&gt;I don't grok why sync is the default, especially given how sketchy hardware &lt;br/&gt;
drivers are about obeying fsync:&lt;/p&gt;

&lt;div class="panel" style="border-width: 1px;"&gt;&lt;div class="panelContent"&gt;
&lt;p&gt;But, beware: some hardware devices may in fact cache writes even during &lt;br/&gt;
fsync, and return before the bits are actually on stable storage, to give the &lt;br/&gt;
appearance of faster performance.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;

&lt;p&gt;It's unclear how often this scare-warning is true in practice (scare&lt;br/&gt;
warnings tend to spread very easily without concrete data); it's in&lt;br/&gt;
the javadocs for completeness sake.  I expect (though have no data to&lt;br/&gt;
back this up...) that most OS/IO systems "out there" do properly&lt;br/&gt;
implement fsync.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;IMO, it should have been an option which defaults to false, to be enabled only by &lt;br/&gt;
users who have the expertise to ensure that fsync() is actually doing what &lt;br/&gt;
it advertises. But what's done is done (and Lucy will probably just do something &lt;br/&gt;
different.)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that's a poor default (trades safety for performance), unless&lt;br/&gt;
Lucy eg uses a transaction log so you can concretely bound what's lost&lt;br/&gt;
on crash/power loss.  Or, if you go back to autocommitting I guess...&lt;/p&gt;

&lt;p&gt;If we did this in Lucene, you can have unbounded corruption.  It's not&lt;br/&gt;
just the last few minutes of updates...&lt;/p&gt;

&lt;p&gt;So, I don't think we should even offer the option to turn it off.  You&lt;br/&gt;
can easily subclass your FSDir impl and make sync() a no-op if your&lt;br/&gt;
really want to...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;With regard to Lucene NRT, though, turning sync() off would really help. If and &lt;br/&gt;
when some sort of settings class comes about, an enableSync(boolean enabled) &lt;br/&gt;
method seems like it would come in handy.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You don't need to turn off sync for NRT &amp;#8211; that's the whole point.  It&lt;br/&gt;
gives you a reader without syncing the files.  Really, this is your&lt;br/&gt;
safety tradeoff &amp;#8211; it means you can commit less frequently, since the&lt;br/&gt;
NRT reader can search the latest updates.  But, your app has&lt;br/&gt;
complete control over how it wants to to trade safety for performance.&lt;/p&gt;</comment>
                    <comment id="12789716" author="mikemccand" created="Sat, 12 Dec 2009 11:29:56 +0000">&lt;blockquote&gt;&lt;p&gt;Zoie is a completely user-land solution which modifies no IW/IR internals and yet achieves millisecond index-to-query-visibility turnaround while keeping speedy indexing and query performance. It just keeps the RAMDir outside encapsulated in an object (an IndexingSystem) which has IndexReaders built off of both the RAMDir and the FSDir, and hides the implementation details (in fact the IW itself) from the user.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, one can always not use NRT and build their own layers on top.&lt;/p&gt;

&lt;p&gt;But, Zoie has &lt;b&gt;alot&lt;/b&gt; of code to accomplish this &amp;#8211; the devil really is&lt;br/&gt;
in the details to "simply write first to a RAMDir".  This is why I'd&lt;br/&gt;
like Earwin to look @ Zoie and clarify his proposed approach, in&lt;br/&gt;
contrast...&lt;/p&gt;

&lt;p&gt;Actually, here's a question: how quickly can Zoie turn around a&lt;br/&gt;
commit()?  Seems like it must take more time than Lucene, since it does&lt;br/&gt;
extra stuff (flush RAM buffers to disk, materialize deletes) before&lt;br/&gt;
even calling IW.commit.&lt;/p&gt;

&lt;p&gt;At the end of the day, any NRT system has to trade safety for&lt;br/&gt;
performance (bypass the sync call in the NRT reader)....&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The API for this kind of thing doesn't have to be tightly coupled, and I would agree with you that it shouldn't be.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't consider NRT today to be a tight coupling (eg, the pending&lt;br/&gt;
refactoring of IW would nicely separate it out).  If we implement the&lt;br/&gt;
IR that searches DW's RAM buffer, then I'd agree &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12789905" author="creamyg" created="Sun, 13 Dec 2009 15:38:28 +0000">&lt;p&gt;&amp;gt; I think that's a poor default (trades safety for performance), unless&lt;br/&gt;
&amp;gt; Lucy eg uses a transaction log so you can concretely bound what's lost&lt;br/&gt;
&amp;gt; on crash/power loss. Or, if you go back to autocommitting I guess...&lt;/p&gt;

&lt;p&gt;Search indexes should not be used for canonical data storage &amp;#8211; they should be&lt;br/&gt;
built &lt;b&gt;on top of&lt;/b&gt; canonical data storage.  Guarding against power failure&lt;br/&gt;
induced corruption in a database is an imperative.  Guarding against power&lt;br/&gt;
failure induced corruption in a search index is a feature, not an imperative.&lt;/p&gt;

&lt;p&gt;Users have many options for dealing with the potential for such corruption.&lt;br/&gt;
You can go back to your canonical data store and rebuild your index from&lt;br/&gt;
scratch when it happens.  In a search cluster environment, you can rsync a&lt;br/&gt;
known-good copy from another node.  Potentially, you might enable&lt;br/&gt;
fsync-before-commit and keep your own transaction log.  However, if the time&lt;br/&gt;
it takes to rebuild or recover an index from scratch would have caused you&lt;br/&gt;
unacceptable downtime, you can't possibly be operating in a&lt;br/&gt;
single-point-of-failure environment where a power failure could take you down&lt;br/&gt;
anyway &amp;#8211; so other recovery options are available to you.&lt;/p&gt;

&lt;p&gt;Turning on fsync is only one step towards ensuring index integrity; others&lt;br/&gt;
steps involve making decisions about hard drives, RAID arrays, failover&lt;br/&gt;
strategies, network and off-site backups, etc, and are outside of our domain&lt;br/&gt;
as library authors.  We cannot meet the needs of users who need guaranteed&lt;br/&gt;
index integrity on our own.&lt;/p&gt;

&lt;p&gt;For everybody else, what turning on fsync by default achieves is to make an&lt;br/&gt;
exceedingly rare event rarer.  That's valuable, but not essential.  My&lt;br/&gt;
argument is that since the search indexes should not be used for canonical&lt;br/&gt;
storage, and since fsync is not testably reliable and not sufficient on its&lt;br/&gt;
own, it's a good engineering compromise to prioritize performance.  &lt;/p&gt;

&lt;p&gt;&amp;gt; If we did this in Lucene, you can have unbounded corruption. It's not&lt;br/&gt;
&amp;gt; just the last few minutes of updates...&lt;/p&gt;

&lt;p&gt;Wasn't that a possibility under autocommit as well?   All it takes is for the&lt;br/&gt;
OS to finish flushing the new snapshot file to persistent storage before it&lt;br/&gt;
finishes flushing a segment data file needed by that snapshot, and for the&lt;br/&gt;
power failure to squeeze in between. &lt;/p&gt;

&lt;p&gt;In practice, locality of reference is going to make the window very very&lt;br/&gt;
small, since those two pieces of data will usually get written very close to&lt;br/&gt;
each other on the persistent media.&lt;/p&gt;

&lt;p&gt;I've seen a lot more messages to our user lists over the years about data&lt;br/&gt;
corruption caused by bugs and misconfigurations than by power failures.&lt;/p&gt;

&lt;p&gt;But really, that's as it should be.  Ensuring data integrity to the degree&lt;br/&gt;
required by a database is costly &amp;#8211; it requires far more rigorous testing, and&lt;br/&gt;
far more conservative development practices.  If we accept that our indexes&lt;br/&gt;
must &lt;b&gt;never&lt;/b&gt; go corrupt, it will retard innovation.&lt;/p&gt;

&lt;p&gt;Of course we should work very hard to prevent index corruption.  However, I'm&lt;br/&gt;
much more concerned about stuff like silent omission of search results due to&lt;br/&gt;
overzealous, overly complex optimizations than I am about problems arising&lt;br/&gt;
from power failures.  When a power failure occurs, you know it &amp;#8211; so you get&lt;br/&gt;
the opportunity to fsck the disk, run checkIndex(), perform data integrity&lt;br/&gt;
reconciliation tests against canonical storage, and if anything fails, take&lt;br/&gt;
whatever recovery actions you deem necessary.&lt;/p&gt;

&lt;p&gt;&amp;gt; You don't need to turn off sync for NRT - that's the whole point. It&lt;br/&gt;
&amp;gt; gives you a reader without syncing the files. &lt;/p&gt;

&lt;p&gt;I suppose this is where Lucy and Lucene differ.  Thanks to mmap and the&lt;br/&gt;
near-instantaneous reader opens it has enabled, we don't need to keep a&lt;br/&gt;
special reader alive.  Since there's no special reader, the only way to get&lt;br/&gt;
data to a search process is to go through a commit.  But if we fsync on every&lt;br/&gt;
commit, we'll drag down indexing responsiveness.  Fishishing the commit and&lt;br/&gt;
returning control to client code as quickly as possible is a high priority for&lt;br/&gt;
us.&lt;/p&gt;

&lt;p&gt;Furthermore, I don't want us to have to write the code to support a&lt;br/&gt;
near-real-time reader hanging off of IndexWriter a la Lucene.  The&lt;br/&gt;
architectural discussions have made for very interesting reading, but the&lt;br/&gt;
design seems to be tricky to pull off, and implementation simplicity in core&lt;br/&gt;
search code is a high priority for Lucy.  It's better for Lucy to kill two&lt;br/&gt;
birds with one stone and concentrate on making &lt;b&gt;all&lt;/b&gt; index opens fast. &lt;/p&gt;

&lt;p&gt;&amp;gt; Really, this is your safety tradeoff - it means you can commit less&lt;br/&gt;
&amp;gt; frequently, since the NRT reader can search the latest updates. But, your&lt;br/&gt;
&amp;gt; app has complete control over how it wants to to trade safety for&lt;br/&gt;
&amp;gt; performance.&lt;/p&gt;

&lt;p&gt;So long as fsync is an option, the app always has complete control, regardless&lt;br/&gt;
of whether the default setting is fsync or no fsync.&lt;/p&gt;

&lt;p&gt;If a Lucene app wanted to increase NRT responsiveness and throughput, and if&lt;br/&gt;
absolute index integrity wasn't a concern because it had been addressed&lt;br/&gt;
through other means (e.g. multi-node search cluster), would turning off fsync&lt;br/&gt;
speed things up under any of the proposed designs?&lt;/p&gt;</comment>
                    <comment id="12789971" author="jasonrutherglen" created="Mon, 14 Dec 2009 02:52:11 +0000">&lt;p&gt;I think large scale NRT installations may eventually require a&lt;br/&gt;
distributed transaction log. The implementation details have yet&lt;br/&gt;
to be determined however it could potentially solve the issue of&lt;br/&gt;
data loss being discussed. One candidate is a combo of Zookeeper&lt;br/&gt;
+ Bookeeper. I would venture to guess this could be implemented&lt;br/&gt;
as a part of Solr, however we've got a lot of work to do for&lt;br/&gt;
Solr to be reasonably NRT efficient (see the tracking issue&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/SOLR-1606" title="Integrate Near Realtime "&gt;&lt;del&gt;SOLR-1606&lt;/del&gt;&lt;/a&gt;), and we're just starting on the Zookeeper&lt;br/&gt;
implementation &lt;a href="https://issues.apache.org/jira/browse/SOLR-1277" title="Implement a Solr specific naming service (using Zookeeper)"&gt;&lt;del&gt;SOLR-1277&lt;/del&gt;&lt;/a&gt;... &lt;/p&gt;</comment>
                    <comment id="12790988" author="mikemccand" created="Tue, 15 Dec 2009 22:20:27 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think that's a poor default (trades safety for performance), unless Lucy eg uses a transaction log so you can concretely bound what's lost on crash/power loss. Or, if you go back to autocommitting I guess...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Search indexes should not be used for canonical data storage - they should be&lt;br/&gt;
built on top of canonical data storage.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree with that, in theory, but I think in practice it's too&lt;br/&gt;
idealistic to force/expect apps to meet that ideal.&lt;/p&gt;

&lt;p&gt;I expect for many apps it's a major cost to unexpectedly lose the&lt;br/&gt;
search index on power loss / OS crash.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Users have many options for dealing with the potential for such corruption.&lt;br/&gt;
You can go back to your canonical data store and rebuild your index from&lt;br/&gt;
scratch when it happens. In a search cluster environment, you can rsync a&lt;br/&gt;
known-good copy from another node. Potentially, you might enable&lt;br/&gt;
fsync-before-commit and keep your own transaction log. However, if the time&lt;br/&gt;
it takes to rebuild or recover an index from scratch would have caused you&lt;br/&gt;
unacceptable downtime, you can't possibly be operating in a&lt;br/&gt;
single-point-of-failure environment where a power failure could take you down&lt;br/&gt;
anyway - so other recovery options are available to you.&lt;/p&gt;

&lt;p&gt;Turning on fsync is only one step towards ensuring index integrity; others&lt;br/&gt;
steps involve making decisions about hard drives, RAID arrays, failover&lt;br/&gt;
strategies, network and off-site backups, etc, and are outside of our domain&lt;br/&gt;
as library authors. We cannot meet the needs of users who need guaranteed&lt;br/&gt;
index integrity on our own.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, high availability apps will already take their measures to&lt;br/&gt;
protect the search index / recovery process, going beyond fsync.&lt;br/&gt;
EG, making a hot backup of Lucene index is now straightforwarded.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For everybody else, what turning on fsync by default achieves is to make an&lt;br/&gt;
exceedingly rare event rarer. That's valuable, but not essential. My&lt;br/&gt;
argument is that since the search indexes should not be used for canonical&lt;br/&gt;
storage, and since fsync is not testably reliable and not sufficient on its&lt;br/&gt;
own, it's a good engineering compromise to prioritize performance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Losing power to the machine, or OS crash, or the user doing a hard&lt;br/&gt;
power down because OS isn't responding, I think are not actually&lt;br/&gt;
&lt;b&gt;that&lt;/b&gt; uncommon in an end user setting.  Think of a desktop app&lt;br/&gt;
embedding Lucene/Lucy...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;If we did this in Lucene, you can have unbounded corruption. It's not just the last few minutes of updates...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wasn't that a possibility under autocommit as well? All it takes is for the&lt;br/&gt;
OS to finish flushing the new snapshot file to persistent storage before it&lt;br/&gt;
finishes flushing a segment data file needed by that snapshot, and for the&lt;br/&gt;
power failure to squeeze in between.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not after &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1044" title="Behavior on hard power shutdown"&gt;&lt;del&gt;LUCENE-1044&lt;/del&gt;&lt;/a&gt;... autoCommit simply called commit() at certain&lt;br/&gt;
opportune times (after finish big merges), which does the right thing&lt;br/&gt;
(I hope!).  The segments file is not written until all files it&lt;br/&gt;
references are sync'd.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In practice, locality of reference is going to make the window very very&lt;br/&gt;
small, since those two pieces of data will usually get written very close to&lt;br/&gt;
each other on the persistent media.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure about that &amp;#8211; it depends on how effectively the OS's write cache&lt;br/&gt;
"preserves" that locality.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I've seen a lot more messages to our user lists over the years about data&lt;br/&gt;
corruption caused by bugs and misconfigurations than by power failures.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would agree, though, I think it may be a sampling problem... ie&lt;br/&gt;
people whose machines crashed and they lost the search index would&lt;br/&gt;
often not raise it on the list (vs say a persistent config issue that keeps&lt;br/&gt;
leading to corruption).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But really, that's as it should be. Ensuring data integrity to the degree&lt;br/&gt;
required by a database is costly - it requires far more rigorous testing, and&lt;br/&gt;
far more conservative development practices. If we accept that our indexes&lt;br/&gt;
must never go corrupt, it will retard innovation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It's not really that costly, with NRT &amp;#8211; you can get a searcher on the&lt;br/&gt;
index without paying the commit cost.  And now you can call commit&lt;br/&gt;
however frequently you need to.  Quickly turning around a new&lt;br/&gt;
searcher, and how frequently you commit, are now independent.&lt;/p&gt;

&lt;p&gt;Also, having the app explicitly decouple these two notions keeps the&lt;br/&gt;
door open for future improvements.  If we force absolutely all sharing&lt;br/&gt;
to go through the filesystem then that limits the improvements we can&lt;br/&gt;
make to NRT.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Of course we should work very hard to prevent index corruption. However, I'm&lt;br/&gt;
much more concerned about stuff like silent omission of search results due to&lt;br/&gt;
overzealous, overly complex optimizations than I am about problems arising&lt;br/&gt;
from power failures. When a power failure occurs, you know it - so you get&lt;br/&gt;
the opportunity to fsck the disk, run checkIndex(), perform data integrity&lt;br/&gt;
reconciliation tests against canonical storage, and if anything fails, take&lt;br/&gt;
whatever recovery actions you deem necessary.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well... I think search performance is important, and we should pursue it&lt;br/&gt;
even if we risk bugs.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;You don't need to turn off sync for NRT - that's the whole point. It gives you a reader without syncing the files.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suppose this is where Lucy and Lucene differ. Thanks to mmap and the&lt;br/&gt;
near-instantaneous reader opens it has enabled, we don't need to keep a&lt;br/&gt;
special reader alive. Since there's no special reader, the only way to get&lt;br/&gt;
data to a search process is to go through a commit. But if we fsync on every&lt;br/&gt;
commit, we'll drag down indexing responsiveness. Fishishing the commit and&lt;br/&gt;
returning control to client code as quickly as possible is a high priority for&lt;br/&gt;
us.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;NRT reader isn't that special &amp;#8211; the only things different is 1) it&lt;br/&gt;
loaded the segments_N "file" from IW instead of the filesystem, and 2)&lt;br/&gt;
it uses a reader pool to "share" the underlying SegmentReaders with&lt;br/&gt;
other places that have loaded them.  I guess, if Lucy won't allow&lt;br/&gt;
this, then, yes, forcing a commit in order to reopen is very costly,&lt;br/&gt;
and so sacrificing safety is a tradeoff you have to make.&lt;/p&gt;

&lt;p&gt;Alternatively, you could keep the notion "flush" (an unsafe commit)&lt;br/&gt;
alive?  You write the segments file, but make no effort to ensure it's&lt;br/&gt;
durability (and also preserve the last "true" commit).  Then a normal&lt;br/&gt;
IR.reopen suffices...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Furthermore, I don't want us to have to write the code to support a&lt;br/&gt;
near-real-time reader hanging off of IndexWriter a la Lucene. The&lt;br/&gt;
architectural discussions have made for very interesting reading, but the&lt;br/&gt;
design seems to be tricky to pull off, and implementation simplicity in core&lt;br/&gt;
search code is a high priority for Lucy. It's better for Lucy to kill two&lt;br/&gt;
birds with one stone and concentrate on making all index opens fast.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But shouldn't you at least give an option for index durability?  Even&lt;br/&gt;
if we disagree about the default?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Really, this is your safety tradeoff - it means you can commit less frequently, since the NRT reader can search the latest updates. But, your app has complete control over how it wants to to trade safety for performance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So long as fsync is an option, the app always has complete control,&lt;br/&gt;
regardless of whether the default setting is fsync or no fsync.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well it is an "option" in Lucene &amp;#8211; "it's just software"  &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  I don't&lt;br/&gt;
want to make it easy to be unsafe.  Lucene shouldn't sacrifice safety&lt;br/&gt;
of the index... and with NRT there's no need to make that tradeoff.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If a Lucene app wanted to increase NRT responsiveness and throughput, and if&lt;br/&gt;
absolute index integrity wasn't a concern because it had been addressed&lt;br/&gt;
through other means (e.g. multi-node search cluster), would turning off fsync&lt;br/&gt;
speed things up under any of the proposed designs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, turning off fsync would speed things up &amp;#8211; you could fall back to&lt;br/&gt;
simple reopen and get good performance (NRT should still be faster&lt;br/&gt;
since the readers are pooled).  The "use RAMDir on top of Lucene"&lt;br/&gt;
designs would be helped less since fsync is a noop in RAMDir.&lt;/p&gt;</comment>
                    <comment id="12791549" author="creamyg" created="Wed, 16 Dec 2009 19:46:27 +0000">&lt;p&gt;&amp;gt;&amp;gt; Wasn't that a possibility under autocommit as well? All it takes is for the&lt;br/&gt;
&amp;gt;&amp;gt; OS to finish flushing the new snapshot file to persistent storage before it&lt;br/&gt;
&amp;gt;&amp;gt; finishes flushing a segment data file needed by that snapshot, and for the&lt;br/&gt;
&amp;gt;&amp;gt; power failure to squeeze in between.&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Not after &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1044" title="Behavior on hard power shutdown"&gt;&lt;del&gt;LUCENE-1044&lt;/del&gt;&lt;/a&gt;... autoCommit simply called commit() at certain&lt;br/&gt;
&amp;gt; opportune times (after finish big merges), which does the right thing (I&lt;br/&gt;
&amp;gt; hope!). The segments file is not written until all files it references are&lt;br/&gt;
&amp;gt; sync'd.&lt;/p&gt;

&lt;p&gt;FWIW, autoCommit doesn't really have a place in Lucy's&lt;br/&gt;
one-segment-per-indexing-session model.&lt;/p&gt;

&lt;p&gt;Revisiting the &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1044" title="Behavior on hard power shutdown"&gt;&lt;del&gt;LUCENE-1044&lt;/del&gt;&lt;/a&gt; threads, one passage stood out:&lt;/p&gt;

&lt;div class="panel" style="border-width: 1px;"&gt;&lt;div class="panelContent"&gt;
&lt;p&gt;    &lt;a href="http://www.gossamer-threads.com/lists/lucene/java-dev/54321#54321" class="external-link"&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/54321#54321&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    This is why in a db system, the only file that is sync'd is the log&lt;br/&gt;
    file - all other files can be made "in sync" from the log file - and&lt;br/&gt;
    this file is normally striped for optimum write performance. Some&lt;br/&gt;
    systems have special "log file drives" (some even solid state, or&lt;br/&gt;
    battery backed ram) to aid the performance. &lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The fact that we have to sync all files instead of just one seems sub-optimal.&lt;/p&gt;

&lt;p&gt;Yet Lucene is not well set up to maintain a transaction log.  The very act of&lt;br/&gt;
adding a document to Lucene is inherently lossy even if all fields are stored,&lt;br/&gt;
because doc boost is not preserved.&lt;/p&gt;

&lt;p&gt;&amp;gt; Also, having the app explicitly decouple these two notions keeps the&lt;br/&gt;
&amp;gt; door open for future improvements. If we force absolutely all sharing&lt;br/&gt;
&amp;gt; to go through the filesystem then that limits the improvements we can&lt;br/&gt;
&amp;gt; make to NRT.&lt;/p&gt;

&lt;p&gt;However, Lucy has much more to gain going through the file system than Lucene&lt;br/&gt;
does, because we don't necessarily incur JVM startup costs when launching a&lt;br/&gt;
new process.  The Lucene approach to NRT &amp;#8211; specialized reader hanging off of&lt;br/&gt;
writer &amp;#8211; is constrained to a single process.  The Lucy approach &amp;#8211; fast index&lt;br/&gt;
opens enabled by mmap-friendly index formats &amp;#8211; is not.&lt;/p&gt;

&lt;p&gt;The two approaches aren't mutually exclusive.  It will be possible to augment&lt;br/&gt;
Lucy with a specialized index reader within a single process.  However, A)&lt;br/&gt;
there seems to be a lot of disagreement about just how to integrate that&lt;br/&gt;
reader, and B) there seem to be ways to bolt that functionality on top of the&lt;br/&gt;
existing classes.  Under those circumstances, I think it makes more sense to&lt;br/&gt;
keep that feature external for now.&lt;/p&gt;

&lt;p&gt;&amp;gt; Alternatively, you could keep the notion "flush" (an unsafe commit)&lt;br/&gt;
&amp;gt; alive? You write the segments file, but make no effort to ensure it's&lt;br/&gt;
&amp;gt; durability (and also preserve the last "true" commit). Then a normal&lt;br/&gt;
&amp;gt; IR.reopen suffices...&lt;/p&gt;

&lt;p&gt;That sounds promising.  The semantics would differ from those of Lucene's&lt;br/&gt;
flush(), which doesn't make changes visible.&lt;/p&gt;

&lt;p&gt;We could implement this by somehow marking a "committed" snapshot and a&lt;br/&gt;
"flushed" snapshot differently, either by adding an "fsync" property to the&lt;br/&gt;
snapshot file that would be false after a flush() but true after a commit(),&lt;br/&gt;
or by encoding the property within the snapshot filename.  The file purger&lt;br/&gt;
would have to ensure that all index files referenced by either the last&lt;br/&gt;
committed snapshot or the last flushed snapshot were off limits.  A rollback()&lt;br/&gt;
would zap all changes since the last commit().  &lt;/p&gt;

&lt;p&gt;Such a scheme allows the the top level app to avoid the costs of fsync while&lt;br/&gt;
maintaining its own transaction log &amp;#8211; perhaps with the optimizations&lt;br/&gt;
suggested above (separate disk, SSD, etc). &lt;/p&gt;</comment>
                    <comment id="12791936" author="mikemccand" created="Thu, 17 Dec 2009 14:06:50 +0000">&lt;blockquote&gt;
&lt;p&gt;FWIW, autoCommit doesn't really have a place in Lucy's&lt;br/&gt;
one-segment-per-indexing-session model.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, autoCommit just means "periodically call commit".  So, if you&lt;br/&gt;
decide to offer a commit() operation, then autoCommit would just wrap&lt;br/&gt;
that?  But, I don't think autoCommit should be offered... app should&lt;br/&gt;
decide.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Revisiting the &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1044" title="Behavior on hard power shutdown"&gt;&lt;del&gt;LUCENE-1044&lt;/del&gt;&lt;/a&gt; threads, one passage stood out:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.gossamer-threads.com/lists/lucene/java-dev/54321#54321" class="external-link"&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/54321#54321&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is why in a db system, the only file that is sync'd is the log&lt;br/&gt;
file - all other files can be made "in sync" from the log file - and&lt;br/&gt;
this file is normally striped for optimum write performance. Some&lt;br/&gt;
systems have special "log file drives" (some even solid state, or&lt;br/&gt;
battery backed ram) to aid the performance.&lt;/p&gt;

&lt;p&gt;The fact that we have to sync all files instead of just one seems sub-optimal.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, but, that cost is not on the reopen path, so it's much less&lt;br/&gt;
important.  Ie, the app can freely choose how frequently it wants to&lt;br/&gt;
commit, completely independent from how often it needs to reopen.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Yet Lucene is not well set up to maintain a transaction log. The very act of&lt;br/&gt;
adding a document to Lucene is inherently lossy even if all fields are stored,&lt;br/&gt;
because doc boost is not preserved.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't see that those two statements are related.&lt;/p&gt;

&lt;p&gt;One can "easily" (meaning, it's easily decoupled from core) make a&lt;br/&gt;
transaction log on top of lucene &amp;#8211; just serialize your docs/analzyer&lt;br/&gt;
selection/etc to the log &amp;amp; sync it periodically.&lt;/p&gt;

&lt;p&gt;But, that's orthogonal to what Lucene does &amp;amp; doesn't preserve in its&lt;br/&gt;
index (and, yes, Lucene doesn't precisely preserve boosts).&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Also, having the app explicitly decouple these two notions keeps the door open for future improvements. If we force absolutely all sharing to go through the filesystem then that limits the improvements we can make to NRT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;However, Lucy has much more to gain going through the file system than Lucene&lt;br/&gt;
does, because we don't necessarily incur JVM startup costs when launching a&lt;br/&gt;
new process. The Lucene approach to NRT - specialized reader hanging off of&lt;br/&gt;
writer - is constrained to a single process. The Lucy approach - fast index&lt;br/&gt;
opens enabled by mmap-friendly index formats - is not.&lt;/p&gt;

&lt;p&gt;The two approaches aren't mutually exclusive. It will be possible to augment&lt;br/&gt;
Lucy with a specialized index reader within a single process. However, A)&lt;br/&gt;
there seems to be a lot of disagreement about just how to integrate that&lt;br/&gt;
reader, and B) there seem to be ways to bolt that functionality on top of the&lt;br/&gt;
existing classes. Under those circumstances, I think it makes more sense to&lt;br/&gt;
keep that feature external for now.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Again: NRT is not a "specialized reader".  It's a normal read-only&lt;br/&gt;
DirectoryReader, just like you'd get from IndexReader.open, with the&lt;br/&gt;
only difference being that it consulted IW to find which segments to&lt;br/&gt;
open.  Plus, it's pooled, so that if IW already has a given segment&lt;br/&gt;
reader open (say because deletes were applied or merges are running),&lt;br/&gt;
it's reused.&lt;/p&gt;

&lt;p&gt;We've discussed making it specialized (eg directly asearching DW's ram&lt;br/&gt;
buffer, caching recently flushed segments in RAM, special&lt;br/&gt;
incremental-copy-on-write data structures for deleted docs, etc.) but&lt;br/&gt;
so far these changes don't seem worthwhile.&lt;/p&gt;

&lt;p&gt;The current approach to NRT is simple... I haven't yet seen&lt;br/&gt;
performance gains strong enough to justify moving to "specialized&lt;br/&gt;
readers".&lt;/p&gt;

&lt;p&gt;Yes, Lucene's approach must be in the same JVM.  But we get important&lt;br/&gt;
gains from this &amp;#8211; reusing a single reader (the pool), carrying over&lt;br/&gt;
merged deletions directly in RAM (and eventually field cache &amp;amp; norms&lt;br/&gt;
too &amp;#8211; &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1785" title="Simple FieldCache merging"&gt;LUCENE-1785&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Instead, Lucy (by design) must do all sharing &amp;amp; access all index data&lt;br/&gt;
through the filesystem (a decision, I think, could be dangerous),&lt;br/&gt;
which will necessarily increase your reopen time.  Maybe in practice&lt;br/&gt;
that cost is small though... the OS write cache should keep everything&lt;br/&gt;
fresh... but you still must serialize.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Alternatively, you could keep the notion "flush" (an unsafe commit) alive? You write the segments file, but make no effort to ensure it's durability (and also preserve the last "true" commit). Then a normal IR.reopen suffices...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That sounds promising. The semantics would differ from those of Lucene's&lt;br/&gt;
flush(), which doesn't make changes visible.&lt;/p&gt;

&lt;p&gt;We could implement this by somehow marking a "committed" snapshot and a&lt;br/&gt;
"flushed" snapshot differently, either by adding an "fsync" property to the&lt;br/&gt;
snapshot file that would be false after a flush() but true after a commit(),&lt;br/&gt;
or by encoding the property within the snapshot filename. The file purger&lt;br/&gt;
would have to ensure that all index files referenced by either the last&lt;br/&gt;
committed snapshot or the last flushed snapshot were off limits. A rollback()&lt;br/&gt;
would zap all changes since the last commit().&lt;/p&gt;

&lt;p&gt;Such a scheme allows the the top level app to avoid the costs of fsync while&lt;br/&gt;
maintaining its own transaction log - perhaps with the optimizations&lt;br/&gt;
suggested above (separate disk, SSD, etc).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In fact, this would make Lucy's approach to NRT nearly identical to&lt;br/&gt;
Lucene NRT.&lt;/p&gt;

&lt;p&gt;The only difference is, instead of getting the current uncommitted&lt;br/&gt;
segments_N via RAM, Lucy uses the filesystem.  And, of course&lt;br/&gt;
Lucy doesn't pool readers.  So this is really a Lucy-ification of&lt;br/&gt;
Lucene's approach to NRT.&lt;/p&gt;

&lt;p&gt;So it has the same benefits as Lucene's NRT, ie, lets Lucy apps&lt;br/&gt;
decouple decisions about safety (commit) and freshness (reopen&lt;br/&gt;
turnaround time).&lt;/p&gt;</comment>
                    <comment id="12792625" author="creamyg" created="Fri, 18 Dec 2009 20:19:54 +0000">&lt;p&gt;&amp;gt; Well, autoCommit just means "periodically call commit". So, if you&lt;br/&gt;
&amp;gt; decide to offer a commit() operation, then autoCommit would just wrap&lt;br/&gt;
&amp;gt; that? But, I don't think autoCommit should be offered... app should&lt;br/&gt;
&amp;gt; decide.&lt;/p&gt;

&lt;p&gt;Agreed, autoCommit had benefits under legacy Lucene, but wouldn't be important&lt;br/&gt;
now.  If we did add some sort of "automatic commit" feature, it would mean&lt;br/&gt;
something else: commit every change instantly.  But that's easy to implement&lt;br/&gt;
via a wrapper, so there's no point cluttering the the primary index writer&lt;br/&gt;
class to support such a feature.&lt;/p&gt;

&lt;p&gt;&amp;gt; Again: NRT is not a "specialized reader". It's a normal read-only&lt;br/&gt;
&amp;gt; DirectoryReader, just like you'd get from IndexReader.open, with the&lt;br/&gt;
&amp;gt; only difference being that it consulted IW to find which segments to&lt;br/&gt;
&amp;gt; open. Plus, it's pooled, so that if IW already has a given segment&lt;br/&gt;
&amp;gt; reader open (say because deletes were applied or merges are running),&lt;br/&gt;
&amp;gt; it's reused.&lt;/p&gt;

&lt;p&gt;Well, it seems to me that those two features make it special &amp;#8211; particularly&lt;br/&gt;
the pooling of SegmentReaders.  You can't take advantage of that outside the&lt;br/&gt;
context of IndexWriter:&lt;/p&gt;

&lt;p&gt;&amp;gt; Yes, Lucene's approach must be in the same JVM. But we get important&lt;br/&gt;
&amp;gt; gains from this - reusing a single reader (the pool), carrying over&lt;br/&gt;
&amp;gt; merged deletions directly in RAM (and eventually field cache &amp;amp; norms&lt;br/&gt;
&amp;gt; too - &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1785" title="Simple FieldCache merging"&gt;LUCENE-1785&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Exactly.  In my view, that's what makes that reader "special": unlike ordinary&lt;br/&gt;
Lucene IndexReaders, this one springs into being with its caches already&lt;br/&gt;
primed rather than in need of lazy loading.&lt;/p&gt;

&lt;p&gt;But to achieve those benefits, you have to mod the index writing process.&lt;br/&gt;
Those modifications are not necessary under the Lucy model, because the mere&lt;br/&gt;
act of writing the index stores our data in the system IO cache.&lt;/p&gt;

&lt;p&gt;&amp;gt; Instead, Lucy (by design) must do all sharing &amp;amp; access all index data&lt;br/&gt;
&amp;gt; through the filesystem (a decision, I think, could be dangerous),&lt;br/&gt;
&amp;gt; which will necessarily increase your reopen time. &lt;/p&gt;

&lt;p&gt;Dangerous in what sense?&lt;/p&gt;

&lt;p&gt;Going through the file system is a tradeoff, sure &amp;#8211; but it's pretty nice to&lt;br/&gt;
design your low-latency search app free from any concern about whether&lt;br/&gt;
indexing and search need to be coordinated within a single process.&lt;br/&gt;
Furthermore, if separate processes are your primary concurrency model, going&lt;br/&gt;
through the file system is actually mandatory to achieve best performance on a&lt;br/&gt;
multi-core box.  Lucy won't always be used with multi-threaded hosts.&lt;/p&gt;

&lt;p&gt;I actually think going through the file system is dangerous in a different&lt;br/&gt;
sense: it puts pressure on the file format spec.  The easy way to achieve IPC&lt;br/&gt;
between writers and readers will be to dump stuff into one of the JSON files&lt;br/&gt;
to support the killer-feature-du-jour &amp;#8211; such as what I'm proposing with this&lt;br/&gt;
"fsync" key in the snapshot file.  But then we wind up with a bunch of crap&lt;br/&gt;
cluttering up our index metadata files.  I'm determined that Lucy will have a&lt;br/&gt;
more coherent file format than Lucene, but with this IPC requirement we're&lt;br/&gt;
setting our community up to push us in the wrong direction.  If we're not&lt;br/&gt;
careful, we could end up with a file format that's an unmaintainable jumble.&lt;/p&gt;

&lt;p&gt;But you're talking performance, not complexity costs, right?&lt;/p&gt;

&lt;p&gt;&amp;gt; Maybe in practice that cost is small though... the OS write cache should&lt;br/&gt;
&amp;gt; keep everything fresh... but you still must serialize.&lt;/p&gt;

&lt;p&gt;Anecdotally, at Eventful one of our indexes is 5 GB with 16 million records&lt;br/&gt;
and 900 MB worth of sort cache data; opening a fresh searcher and loading all&lt;br/&gt;
sort caches takes circa 21 ms.&lt;/p&gt;

&lt;p&gt;There's room to improve that further &amp;#8211; we haven't yet implemented&lt;br/&gt;
IndexReader.reopen() &amp;#8211; but that was fast enough to achieve what we wanted to&lt;br/&gt;
achieve.&lt;/p&gt;</comment>
                    <comment id="12792629" author="jasonrutherglen" created="Fri, 18 Dec 2009 20:29:16 +0000">&lt;blockquote&gt;&lt;p&gt;Anecdotally, at Eventful one of our indexes is 5 GB with 16 million records&lt;br/&gt;
and 900 MB worth of sort cache data; opening a fresh searcher and loading all&lt;br/&gt;
sort caches takes circa 21 ms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Marvin, very cool!  Are you using the mmap module you mentioned at ApacheCon?&lt;/p&gt;</comment>
                    <comment id="12792638" author="creamyg" created="Fri, 18 Dec 2009 20:51:03 +0000">&lt;p&gt;Yes, this is using the sort cache model worked out this spring on lucy-dev.&lt;br/&gt;
The memory mapping happens within FSFileHandle (&lt;a href="https://issues.apache.org/jira/browse/LUCY-83" title="FSFileHandle"&gt;&lt;del&gt;LUCY-83&lt;/del&gt;&lt;/a&gt;). SortWriter &lt;br/&gt;
and SortReader haven't made it into the Lucy repository yet.&lt;/p&gt;</comment>
                    <comment id="12792713" author="mikemccand" created="Sat, 19 Dec 2009 00:10:54 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Again: NRT is not a "specialized reader". It's a normal read-only DirectoryReader, just like you'd get from IndexReader.open, with the only difference being that it consulted IW to find which segments to open. Plus, it's pooled, so that if IW already has a given segment reader open (say because deletes were applied or merges are running), it's reused.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, it seems to me that those two features make it special - particularly&lt;br/&gt;
the pooling of SegmentReaders. You can't take advantage of that outside the&lt;br/&gt;
context of IndexWriter:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK so mabye a little special &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; But, really that pooling should be&lt;br/&gt;
factored out of IW.  It's not writer specific.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Yes, Lucene's approach must be in the same JVM. But we get important gains from this - reusing a single reader (the pool), carrying over merged deletions directly in RAM (and eventually field cache &amp;amp; norms too - &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1785" title="Simple FieldCache merging"&gt;LUCENE-1785&lt;/a&gt;).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Exactly. In my view, that's what makes that reader "special": unlike ordinary&lt;br/&gt;
Lucene IndexReaders, this one springs into being with its caches already&lt;br/&gt;
primed rather than in need of lazy loading.&lt;/p&gt;

&lt;p&gt;But to achieve those benefits, you have to mod the index writing process.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mod the index writing, and the reader reopen, to use the shared pool.&lt;br/&gt;
The pool in itself isn't writer specific.&lt;/p&gt;

&lt;p&gt;Really the pool is just like what you tap into when you call reopen &amp;#8211;&lt;br/&gt;
that method looks at the current "pool" of already opened segments,&lt;br/&gt;
sharing what it can.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Those modifications are not necessary under the Lucy model, because the mere act of writing the index stores our data in the system IO cache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But, that's where Lucy presumably takes a perf hit.  Lucene can share&lt;br/&gt;
these in RAM, not usign the filesystem as the intermediary (eg we do&lt;br/&gt;
that today with deletions; norms/field cache/eventual CSF can do the&lt;br/&gt;
same.)  Lucy must go through the filesystem to share.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Instead, Lucy (by design) must do all sharing &amp;amp; access all index data through the filesystem (a decision, I think, could be dangerous), which will necessarily increase your reopen time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Dangerous in what sense?&lt;/p&gt;

&lt;p&gt;Going through the file system is a tradeoff, sure - but it's pretty nice to&lt;br/&gt;
design your low-latency search app free from any concern about whether&lt;br/&gt;
indexing and search need to be coordinated within a single process.&lt;br/&gt;
Furthermore, if separate processes are your primary concurrency model, going&lt;br/&gt;
through the file system is actually mandatory to achieve best performance on a&lt;br/&gt;
multi-core box. Lucy won't always be used with multi-threaded hosts.&lt;/p&gt;

&lt;p&gt;I actually think going through the file system is dangerous in a different&lt;br/&gt;
sense: it puts pressure on the file format spec. The easy way to achieve IPC&lt;br/&gt;
between writers and readers will be to dump stuff into one of the JSON files&lt;br/&gt;
to support the killer-feature-du-jour - such as what I'm proposing with this&lt;br/&gt;
"fsync" key in the snapshot file. But then we wind up with a bunch of crap&lt;br/&gt;
cluttering up our index metadata files. I'm determined that Lucy will have a&lt;br/&gt;
more coherent file format than Lucene, but with this IPC requirement we're&lt;br/&gt;
setting our community up to push us in the wrong direction. If we're not&lt;br/&gt;
careful, we could end up with a file format that's an unmaintainable jumble.&lt;/p&gt;

&lt;p&gt;But you're talking performance, not complexity costs, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mostly I was thinking performance, ie, trusting the OS to make good&lt;br/&gt;
decisions about what should be RAM resident, when it has limited&lt;br/&gt;
information...&lt;/p&gt;

&lt;p&gt;But, also risky is that all important data structures must be&lt;br/&gt;
"file-flat", though in practice that doesn't seem like an issue so&lt;br/&gt;
far?  The RAM resident things Lucene has &amp;#8211; norms, deleted docs, terms&lt;br/&gt;
index, field cache &amp;#8211; seem to "cast" just fine to file-flat.  If we&lt;br/&gt;
switched to an FST for the terms index I guess that could get&lt;br/&gt;
tricky...&lt;/p&gt;

&lt;p&gt;Wouldn't shared memory be possible for process-only concurrent models?&lt;br/&gt;
Also, what popular systems/environments have this requirement (only&lt;br/&gt;
process level concurrency) today?&lt;/p&gt;

&lt;p&gt;It's wonderful that Lucy can startup really fast, but, for most apps&lt;br/&gt;
that's not nearly as important as searching/indexing performance,&lt;br/&gt;
right?  I mean, you start only once, and then you handle many, many&lt;br/&gt;
searches / index many documents, with that process, usually?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Maybe in practice that cost is small though... the OS write cache should keep everything fresh... but you still must serialize.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Anecdotally, at Eventful one of our indexes is 5 GB with 16 million records&lt;br/&gt;
and 900 MB worth of sort cache data; opening a fresh searcher and loading all&lt;br/&gt;
sort caches takes circa 21 ms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's fabulously fast!&lt;/p&gt;

&lt;p&gt;But you really need to also test search/indexing throughput, reopen time&lt;br/&gt;
(I think) once that's online for Lucy...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There's room to improve that further - we haven't yet implemented&lt;br/&gt;
IndexReader.reopen() - but that was fast enough to achieve what we wanted to&lt;br/&gt;
achieve.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is reopen even necessary in Lucy?&lt;/p&gt;</comment>
                    <comment id="12792939" author="creamyg" created="Sun, 20 Dec 2009 02:09:27 +0000">&lt;p&gt;&amp;gt; But, that's where Lucy presumably takes a perf hit. Lucene can share&lt;br/&gt;
&amp;gt; these in RAM, not usign the filesystem as the intermediary (eg we do&lt;br/&gt;
&amp;gt; that today with deletions; norms/field cache/eventual CSF can do the&lt;br/&gt;
&amp;gt; same.) Lucy must go through the filesystem to share.&lt;/p&gt;

&lt;p&gt;For a flush(), I don't think there's a significant penalty.  The only extra&lt;br/&gt;
costs Lucy will pay are the bookkeeping costs to update the file system state&lt;br/&gt;
and to create the objects that read the index data.  Those are real, but since&lt;br/&gt;
we're skipping the fsync(), they're small.  As far as the actual data, I don't&lt;br/&gt;
see that there's a difference.  Reading from memory mapped RAM isn't any&lt;br/&gt;
slower than reading from malloc'd RAM.&lt;/p&gt;

&lt;p&gt;If we have to fsync(), there'll be a cost, but in Lucene you have to pay that&lt;br/&gt;
same cost, too.  Lucene expects to get around it with IndexWriter.getReader().&lt;br/&gt;
In Lucy, we'll get around it by having you call flush() and then reopen a&lt;br/&gt;
reader somewhere, often in another proecess.  &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;In both cases, the availability of fresh data is decoupled from the fsync.&lt;/li&gt;
	&lt;li&gt;In both cases, the indexing process has to be careful about dropping data&lt;br/&gt;
    on the floor before a commit() succeeds.&lt;/li&gt;
	&lt;li&gt;In both cases, it's possible to protect against unbounded corruption by&lt;br/&gt;
    rolling back to the last commit.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&amp;gt; Mostly I was thinking performance, ie, trusting the OS to make good&lt;br/&gt;
&amp;gt; decisions about what should be RAM resident, when it has limited&lt;br/&gt;
&amp;gt; information...&lt;/p&gt;

&lt;p&gt;Right, for instance because we generally can't force the OS to pin term&lt;br/&gt;
dictionaries in RAM, as discussed a while back.  It's not an ideal situation,&lt;br/&gt;
but Lucene's approach isn't bulletproof either, since Lucene's term&lt;br/&gt;
dictionaries can get paged out too.  &lt;/p&gt;

&lt;p&gt;We're sure not going to throw away all the advantages of mmap and go back to&lt;br/&gt;
reading data structures into process RAM just because of that.&lt;/p&gt;

&lt;p&gt;&amp;gt; But, also risky is that all important data structures must be "file-flat",&lt;br/&gt;
&amp;gt; though in practice that doesn't seem like an issue so far? &lt;/p&gt;

&lt;p&gt;It's a constraint.  For instance, to support mmap, string sort caches&lt;br/&gt;
currently require three "files" each: ords, offsets, and UTF-8 character data.  &lt;/p&gt;

&lt;p&gt;The compound file system makes the file proliferation bearable, though.  And&lt;br/&gt;
it's actually nice in a way to have data structures as named files, strongly&lt;br/&gt;
separated from each other and persistent.&lt;/p&gt;

&lt;p&gt;If we were willing to ditch portability, we could cast to arrays of structs in&lt;br/&gt;
Lucy &amp;#8211; but so far we've just used primitives.  I'd like to keep it that way,&lt;br/&gt;
since it would be nice if the core Lucy file format was at least theoretically&lt;br/&gt;
compatible with a pure Java implementation.  But Lucy plugins could break that&lt;br/&gt;
rule and cast to structs if desired.  &lt;/p&gt;

&lt;p&gt;&amp;gt; The RAM resident things Lucene has - norms, deleted docs, terms index, field&lt;br/&gt;
&amp;gt; cache - seem to "cast" just fine to file-flat. &lt;/p&gt;

&lt;p&gt;There are often benefits to keeping stuff "file-flat", particularly when the&lt;br/&gt;
file-flat form is compressed.  If we were to expand those sort caches to&lt;br/&gt;
string objects, they'd take up more RAM than they do now.&lt;/p&gt;

&lt;p&gt;I think the only significant drawback is security: we can't trust memory&lt;br/&gt;
mapped data the way we can data which has been read into process RAM and&lt;br/&gt;
checked on the way in.  For instance, we need to perform UTF-8 sanity checking&lt;br/&gt;
each time a string sort cache value escapes the controlled environment of the&lt;br/&gt;
cache reader.  If the sort cache value was instead derived from an existing&lt;br/&gt;
string in process RAM, we wouldn't need to check it.&lt;/p&gt;

&lt;p&gt;&amp;gt; If we switched to an FST for the terms index I guess that could get&lt;br/&gt;
&amp;gt; tricky...&lt;/p&gt;

&lt;p&gt;Hmm, I haven't been following that.  Too much work to keep up with those&lt;br/&gt;
giganto patches for flex indexing, even though it's a subject I'm intimately&lt;br/&gt;
acquainted with and deeply interested in.  I plan to look it over when you're&lt;br/&gt;
done and see if we can simplify it.  &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;&amp;gt; Wouldn't shared memory be possible for process-only concurrent models?&lt;/p&gt;

&lt;p&gt;IPC is a platform-compatibility nightmare.  By restricting ourselves to&lt;br/&gt;
communicating via the file system, we save ourselves oodles of engineering&lt;br/&gt;
time.  And on really boring, frustrating work, to boot.&lt;/p&gt;

&lt;p&gt;&amp;gt; Also, what popular systems/environments have this requirement (only process&lt;br/&gt;
&amp;gt; level concurrency) today?&lt;/p&gt;

&lt;p&gt;Perl's threads suck.  Actually all threads suck.  Perl's are just worse than&lt;br/&gt;
average &amp;#8211; and so many Perl binaries are compiled without them.  Java threads&lt;br/&gt;
suck less, but they still suck &amp;#8211; look how much engineering time you folks&lt;br/&gt;
blow on managing that stuff.  Threads are a terrible programming model.&lt;/p&gt;

&lt;p&gt;I'm not into the idea of forcing Lucy users to use threads.  They should be&lt;br/&gt;
able to use processes as their primary concurrency model if they want.&lt;/p&gt;

&lt;p&gt;&amp;gt; It's wonderful that Lucy can startup really fast, but, for most apps that's&lt;br/&gt;
&amp;gt; not nearly as important as searching/indexing performance, right? &lt;/p&gt;

&lt;p&gt;Depends.  &lt;/p&gt;

&lt;p&gt;Total indexing throughput in both Lucene and KinoSearch has been pretty decent&lt;br/&gt;
for a long time.  However, there's been a large gap between average index&lt;br/&gt;
update performance and worst case index update performance, especially when&lt;br/&gt;
you factor in sort cache loading.  There are plenty of applications that may&lt;br/&gt;
not have very high throughput requirements but where it may not be acceptable&lt;br/&gt;
for an index update to take several seconds or several minutes every once in a&lt;br/&gt;
while, even if it usually completes faster.&lt;/p&gt;

&lt;p&gt;&amp;gt; I mean, you start only once, and then you handle many, many&lt;br/&gt;
&amp;gt; searches / index many documents, with that process, usually?&lt;/p&gt;

&lt;p&gt;Sometimes the person who just performed the action that updated the index is&lt;br/&gt;
the only one you care about.  For instance, to use a feature request that came&lt;br/&gt;
in from Slashdot a while back, if someone leaves a comment on your website,&lt;br/&gt;
it's nice to have it available in the search index right away.&lt;/p&gt;

&lt;p&gt;Consistently fast index update responsiveness makes personalization of the&lt;br/&gt;
customer experience easier.&lt;/p&gt;

&lt;p&gt;&amp;gt; But you really need to also test search/indexing throughput, reopen time&lt;br/&gt;
&amp;gt; (I think) once that's online for Lucy...&lt;/p&gt;

&lt;p&gt;Naturally.&lt;/p&gt;

&lt;p&gt;&amp;gt; Is reopen even necessary in Lucy?&lt;/p&gt;

&lt;p&gt;Probably.  If you have a boatload of segments and a boatload of fields, you&lt;br/&gt;
might start to see file opening and metadata parsing costs come into play.  If&lt;br/&gt;
it turns out that for some indexes reopen() can knock down the time from say,&lt;br/&gt;
100 ms to 10 ms or less, I'd consider that sufficient justification.&lt;/p&gt;</comment>
                    <comment id="12792996" author="mikemccand" created="Sun, 20 Dec 2009 15:41:25 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;But, that's where Lucy presumably takes a perf hit. Lucene can share these in RAM, not usign the filesystem as the intermediary (eg we do that today with deletions; norms/field cache/eventual CSF can do the same.) Lucy must go through the filesystem to share.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For a flush(), I don't think there's a significant penalty. The only extra&lt;br/&gt;
costs Lucy will pay are the bookkeeping costs to update the file system state&lt;br/&gt;
and to create the objects that read the index data. Those are real, but since&lt;br/&gt;
we're skipping the fsync(), they're small. As far as the actual data, I don't&lt;br/&gt;
see that there's a difference.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But everything must go through the filesystem with Lucy...&lt;/p&gt;

&lt;p&gt;Eg, with Lucene, deletions are not written to disk until you commit.&lt;br/&gt;
Flush doesn't write the del file, merging doesn't, etc.  The deletes&lt;br/&gt;
are carried in RAM.  We could (but haven't yet &amp;#8211; NRT turnaround time&lt;br/&gt;
is already plenty fast) do the same with norms, field cache, terms&lt;br/&gt;
dict index, etc.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Reading from memory mapped RAM isn't any slower than reading from malloc'd RAM.&lt;/p&gt;

&lt;p&gt;Right, for instance because we generally can't force the OS to pin term&lt;br/&gt;
dictionaries in RAM, as discussed a while back. It's not an ideal situation,&lt;br/&gt;
but Lucene's approach isn't bulletproof either, since Lucene's term&lt;br/&gt;
dictionaries can get paged out too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As long as the page is hot... (in both cases!).&lt;/p&gt;

&lt;p&gt;But by using file-backed RAM (not malloc'd RAM), you're telling the OS&lt;br/&gt;
it's OK if it chooses to swap it out. Sure, malloc'd RAM can be&lt;br/&gt;
swapped out too... but that should be less frequent (and, we can&lt;br/&gt;
control this behavior, somewhat, eg swappiness).&lt;/p&gt;

&lt;p&gt;It's similar to using a weak v strong reference in java.  By using&lt;br/&gt;
file-backed RAM you tell the OS it's fair game for swapping.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If we have to fsync(), there'll be a cost, but in Lucene you have to pay that&lt;br/&gt;
same cost, too. Lucene expects to get around it with IndexWriter.getReader().&lt;br/&gt;
In Lucy, we'll get around it by having you call flush() and then reopen a&lt;br/&gt;
reader somewhere, often in another proecess.&lt;/p&gt;

&lt;p&gt;In both cases, the availability of fresh data is decoupled from the fsync.&lt;br/&gt;
In both cases, the indexing process has to be careful about dropping data&lt;br/&gt;
on the floor before a commit() succeeds.&lt;br/&gt;
In both cases, it's possible to protect against unbounded corruption by&lt;br/&gt;
rolling back to the last commit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The two approaches are basically the same, so, we get the same&lt;br/&gt;
features &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;It's just that Lucy uses the filesystem for sharing, and Lucene shares&lt;br/&gt;
through RAM.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We're sure not going to throw away all the advantages of mmap and go back to reading data structures into process RAM just because of that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess my confusion is what are all the other benefits of using&lt;br/&gt;
file-backed RAM?  You can efficiently use process only concurrency&lt;br/&gt;
(though shared memory is technically an option for this too), and you&lt;br/&gt;
have wicked fast open times (but, you still must warm, just like&lt;br/&gt;
Lucene).  What else?  Oh maybe the ability to inform OS &lt;b&gt;not&lt;/b&gt; to cache&lt;br/&gt;
eg the reads done when merging segments.  That's one I sure wish&lt;br/&gt;
Lucene could use...&lt;/p&gt;

&lt;p&gt;In exchange you risk the OS making poor choices about what gets&lt;br/&gt;
swapped out (LRU policy is too simplistic... not all pages are created&lt;br/&gt;
equal), must down cast all data structures to file-flat, must share&lt;br/&gt;
everything through the filesystem, (perf hit to NRT).&lt;/p&gt;

&lt;p&gt;I do love how pure the file-backed RAM approach is, but I worry that&lt;br/&gt;
down the road it'll result in erratic search performance in certain&lt;br/&gt;
app profiles.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;But, also risky is that all important data structures must be "file-flat", though in practice that doesn't seem like an issue so far?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It's a constraint. For instance, to support mmap, string sort caches&lt;br/&gt;
currently require three "files" each: ords, offsets, and UTF-8 character data.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, that you need 3 files for the string sort cache is a little&lt;br/&gt;
spooky... that's 3X the chance of a page fault.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The compound file system makes the file proliferation bearable, though. And&lt;br/&gt;
it's actually nice in a way to have data structures as named files, strongly&lt;br/&gt;
separated from each other and persistent.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But the CFS construction must also go through the filesystem (like&lt;br/&gt;
Lucene) right?  So you still incur IO load of creating the small&lt;br/&gt;
files, then 2nd pass to consolidate.&lt;/p&gt;

&lt;p&gt;I agree there's a certain design purity to having the files clearly&lt;br/&gt;
separate out the elements of the data structures, but if it means&lt;br/&gt;
erratic search performance... function over form?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If we were willing to ditch portability, we could cast to arrays of structs in&lt;br/&gt;
Lucy - but so far we've just used primitives. I'd like to keep it that way,&lt;br/&gt;
since it would be nice if the core Lucy file format was at least theoretically&lt;br/&gt;
compatible with a pure Java implementation. But Lucy plugins could break that&lt;br/&gt;
rule and cast to structs if desired.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Someday we could make a Lucene codec that interacts with a Lucy&lt;br/&gt;
index... would be a good exercise to go though to see if the flex API&lt;br/&gt;
really is "flex" enough...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;The RAM resident things Lucene has - norms, deleted docs, terms index, field cache - seem to "cast" just fine to file-flat.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There are often benefits to keeping stuff "file-flat", particularly when the&lt;br/&gt;
file-flat form is compressed. If we were to expand those sort caches to&lt;br/&gt;
string objects, they'd take up more RAM than they do now.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We've leaving them as UTF8 by default for Lucene (with the flex&lt;br/&gt;
changes).  Still, the terms index once loaded does have silly RAM&lt;br/&gt;
overhead... we can cut that back a fair amount though.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think the only significant drawback is security: we can't trust memory&lt;br/&gt;
mapped data the way we can data which has been read into process RAM and&lt;br/&gt;
checked on the way in. For instance, we need to perform UTF-8 sanity checking&lt;br/&gt;
each time a string sort cache value escapes the controlled environment of the&lt;br/&gt;
cache reader. If the sort cache value was instead derived from an existing&lt;br/&gt;
string in process RAM, we wouldn't need to check it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sigh, that's a curious downside... so term decode intensive uses&lt;br/&gt;
(merging, range queries, I guess maybe term dict lookup) take the&lt;br/&gt;
brunt of that hit?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;If we switched to an FST for the terms index I guess that could get tricky...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, I haven't been following that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There's not much to follow &amp;#8211; it's all just talk at this point.  I&lt;br/&gt;
don't think anyone's built a prototype yet &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Too much work to keep up with those giganto patches for flex indexing,&lt;br/&gt;
even though it's a subject I'm intimately acquainted with and deeply&lt;br/&gt;
interested in. I plan to look it over when you're done and see if we&lt;br/&gt;
can simplify it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And then we'll borrow back your simplifications &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Lather, rinse,&lt;br/&gt;
repeat.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Wouldn't shared memory be possible for process-only concurrent models?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;IPC is a platform-compatibility nightmare. By restricting ourselves to&lt;br/&gt;
communicating via the file system, we save ourselves oodles of&lt;br/&gt;
engineering time. And on really boring, frustrating work, to boot.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I had assumed so too, but I was surprised that Python's&lt;br/&gt;
multiprocessing module exposes a simple API for sharing objects from&lt;br/&gt;
parent to forked child.  It's at least a counter example (though, in&lt;br/&gt;
all fairness, I haven't looked at the impl &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; ), ie, there seems to be&lt;br/&gt;
some hope of containing shared memory under a consistent API.&lt;/p&gt;

&lt;p&gt;I'm just pointing out that "going through the filesystem" isn't the&lt;br/&gt;
only way to have efficient process-only concurrency.  Shared memory&lt;br/&gt;
is another option, but, yes it has tradeoffs.&lt;/p&gt;

&lt;blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;Also, what popular systems/environments have this requirement (only process level concurrency) today?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perl's threads suck. Actually all threads suck. Perl's are just worse than&lt;br/&gt;
average - and so many Perl binaries are compiled without them. Java threads&lt;br/&gt;
suck less, but they still suck - look how much engineering time you folks&lt;br/&gt;
blow on managing that stuff. Threads are a terrible programming model.&lt;/p&gt;

&lt;p&gt;I'm not into the idea of forcing Lucy users to use threads. They should be&lt;br/&gt;
able to use processes as their primary concurrency model if they want.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, working with threads is a nightmare (eg have a look at Java's&lt;br/&gt;
memory model).  I think the jury is still out (for our species) just&lt;br/&gt;
how, long term, we'll make use of concurrency with the machines.  I&lt;br/&gt;
think we may need to largely take "time" out of our programming&lt;br/&gt;
languages, eg switch to much more declarative code, or&lt;br/&gt;
something... wanna port Lucy to Erlang?&lt;/p&gt;

&lt;p&gt;But I'm not sure process only concurrency, sharing only via&lt;br/&gt;
file-backed memory, is the answer either &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;It's wonderful that Lucy can startup really fast, but, for most apps that's not nearly as important as searching/indexing performance, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Depends.&lt;/p&gt;

&lt;p&gt;Total indexing throughput in both Lucene and KinoSearch has been pretty decent&lt;br/&gt;
for a long time. However, there's been a large gap between average index&lt;br/&gt;
update performance and worst case index update performance, especially when&lt;br/&gt;
you factor in sort cache loading. There are plenty of applications that may&lt;br/&gt;
not have very high throughput requirements but where it may not be acceptable&lt;br/&gt;
for an index update to take several seconds or several minutes every once in a&lt;br/&gt;
while, even if it usually completes faster.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I mean, you start only once, and then you handle many, many searches / index many documents, with that process, usually?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sometimes the person who just performed the action that updated the index is&lt;br/&gt;
the only one you care about. For instance, to use a feature request that came&lt;br/&gt;
in from Slashdot a while back, if someone leaves a comment on your website,&lt;br/&gt;
it's nice to have it available in the search index right away.&lt;/p&gt;

&lt;p&gt;Consistently fast index update responsiveness makes personalization of the&lt;br/&gt;
customer experience easier.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Turnaround time for Lucene NRT is already very fast, as is.  After an&lt;br/&gt;
immense merge, it'll be the worst, but if you warm the reader first,&lt;br/&gt;
that won't be an issue.&lt;/p&gt;

&lt;p&gt;Using Zoie you can make reopen time insanely fast (much faster than I&lt;br/&gt;
think necessary for most apps), but at the expense of some expected&lt;br/&gt;
hit to searching/indexing throughput.  I don't think that's the right&lt;br/&gt;
tradeoff for Lucene.&lt;/p&gt;

&lt;p&gt;I suspect Lucy is making a similar tradeoff, ie, that search&lt;br/&gt;
performance will be erratic due to page faults, at a smallish gain in&lt;br/&gt;
reopen time.&lt;/p&gt;

&lt;p&gt;Do you have any hard numbers on how much time it takes Lucene to load&lt;br/&gt;
from a hot IO cache, populating its RAM resident data structures?  I&lt;br/&gt;
wonder in practice what extra cost we are really talking about... it's&lt;br/&gt;
RAM to RAM "translation" of data structures (if the files are hot).&lt;br/&gt;
FieldCache we just have to fix to stop doing uninversion... (ie we&lt;br/&gt;
need CSF).&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Is reopen even necessary in Lucy?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Probably. If you have a boatload of segments and a boatload of fields, you&lt;br/&gt;
might start to see file opening and metadata parsing costs come into play. If&lt;br/&gt;
it turns out that for some indexes reopen() can knock down the time from say,&lt;br/&gt;
100 ms to 10 ms or less, I'd consider that sufficient justification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;OK.  Then, you are basically pooling your readers &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Ie, you do allow&lt;br/&gt;
in-process sharing, but only among readers.&lt;/p&gt;</comment>
                    <comment id="12793431" author="creamyg" created="Tue, 22 Dec 2009 00:07:23 +0000">&lt;p&gt;&amp;gt; I guess my confusion is what are all the other benefits of using&lt;br/&gt;
&amp;gt; file-backed RAM? You can efficiently use process only concurrency&lt;br/&gt;
&amp;gt; (though shared memory is technically an option for this too), and you&lt;br/&gt;
&amp;gt; have wicked fast open times (but, you still must warm, just like&lt;br/&gt;
&amp;gt; Lucene). &lt;/p&gt;

&lt;p&gt;Processes are Lucy's primary concurrency model.  ("The OS is our JVM.")&lt;br/&gt;
Making process-only concurrency efficient isn't optional &amp;#8211; it's a &lt;b&gt;core&lt;/b&gt;&lt;br/&gt;
&lt;b&gt;concern&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&amp;gt; What else? Oh maybe the ability to inform OS not to cache&lt;br/&gt;
&amp;gt; eg the reads done when merging segments. That's one I sure wish&lt;br/&gt;
&amp;gt; Lucene could use...&lt;/p&gt;

&lt;p&gt;Lightweight searchers mean architectural freedom.  &lt;/p&gt;

&lt;p&gt;Create 2, 10, 100, 1000 Searchers without a second thought &amp;#8211; as many as you&lt;br/&gt;
need for whatever app architecture you just dreamed up &amp;#8211; then destroy them&lt;br/&gt;
just as effortlessly.  Add another worker thread to your search server without&lt;br/&gt;
having to consider the RAM requirements of a heavy searcher object.  Create a&lt;br/&gt;
command-line app to search a documentation index without worrying about&lt;br/&gt;
daemonizing it.  Etc.&lt;/p&gt;

&lt;p&gt;If your normal development pattern is a single monolithic Java process, then&lt;br/&gt;
that freedom might not mean much to you.  But with their low per-object RAM&lt;br/&gt;
requirements and fast opens, lightweight searchers are easy to use within a&lt;br/&gt;
lot of other development patterns. For example: lightweight searchers work &lt;br/&gt;
well for maxing out multiple CPU cores under process-only concurrency.&lt;/p&gt;

&lt;p&gt;&amp;gt; In exchange you risk the OS making poor choices about what gets&lt;br/&gt;
&amp;gt; swapped out (LRU policy is too simplistic... not all pages are created&lt;br/&gt;
&amp;gt; equal), &lt;/p&gt;

&lt;p&gt;The Linux virtual memory system, at least, is not a pure LRU.  It utilizes a&lt;br/&gt;
page aging algo which prioritizes pages that have historically been accessed&lt;br/&gt;
frequently even when they have not been accessed recently:&lt;/p&gt;

&lt;div class="panel" style="border-width: 1px;"&gt;&lt;div class="panelContent"&gt;
&lt;p&gt;    &lt;a href="http://sunsite.nus.edu.sg/LDP/LDP/tlk/node40.html" class="external-link"&gt;http://sunsite.nus.edu.sg/LDP/LDP/tlk/node40.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The default action when a page is first allocated, is to give it an&lt;br/&gt;
    initial age of 3. Each time it is touched (by the memory management&lt;br/&gt;
    subsystem) it's age is increased by 3 to a maximum of 20. Each time the&lt;br/&gt;
    Kernel swap daemon runs it ages pages, decrementing their age by 1.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And while that system may not be ideal from our standpoint, it's still pretty&lt;br/&gt;
good.  In general, the operating system's virtual memory scheme is going to&lt;br/&gt;
work fine as designed, for us and everyone else, and minimize memory&lt;br/&gt;
availability wait times.&lt;/p&gt;

&lt;p&gt;When will swapping out the term dictionary be a problem?  &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;For indexes where queries are made frequently, no problem.&lt;/li&gt;
	&lt;li&gt;Foir systems with plenty of RAM, no problem.&lt;/li&gt;
	&lt;li&gt;For systems that aren't very busy, no problem.&lt;/li&gt;
	&lt;li&gt;&lt;del&gt;For small indexes, no problem.&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The only situation we're talking about is infrequent queries against &lt;del&gt;large&lt;/del&gt;&lt;br/&gt;
indexes on busy boxes where RAM isn't abundant.  Under those circumstances, it&lt;br/&gt;
&lt;b&gt;might&lt;/b&gt; be noticable that Lucy's term dictionary gets paged out somewhat&lt;br/&gt;
sooner than Lucene's.&lt;/p&gt;

&lt;p&gt;But in general, if the term dictionary gets paged out, so what?  Nobody was&lt;br/&gt;
using it.  Maybe nobody will make another query against that index until next&lt;br/&gt;
week.  Maybe the OS made the right decision.&lt;/p&gt;

&lt;p&gt;OK, so there's a vulnerable bubble where the the query rate against &lt;br/&gt;
&lt;del&gt;a large index&lt;/del&gt; an index is neither too fast nor too slow, on busy machines &lt;br/&gt;
where RAM isn't abundant.  I don't think that bubble ought to drive major &lt;br/&gt;
architectural decisions.&lt;/p&gt;

&lt;p&gt;Let me turn your question on its head.  What does Lucene gain in return for&lt;br/&gt;
the slow index opens and large process memory footprint of its heavy&lt;br/&gt;
searchers?&lt;/p&gt;

&lt;p&gt;&amp;gt; I do love how pure the file-backed RAM approach is, but I worry that&lt;br/&gt;
&amp;gt; down the road it'll result in erratic search performance in certain&lt;br/&gt;
&amp;gt; app profiles.&lt;/p&gt;

&lt;p&gt;If necessary, there's a straightforward remedy: slurp the relevant files into&lt;br/&gt;
RAM at object construction rather than mmap them.  The rest of the code won't &lt;br/&gt;
know the difference between malloc'd RAM and mmap'd RAM.  The slurped files &lt;br/&gt;
won't take up any more space than the analogous Lucene data structures; more &lt;br/&gt;
likely, they'll take up less.&lt;/p&gt;

&lt;p&gt;That's the kind of setting we'd hide away in the IndexManager class rather&lt;br/&gt;
than expose as prominent API, and it would be a hint to index components&lt;br/&gt;
rather than an edict.&lt;/p&gt;

&lt;p&gt;&amp;gt; Yeah, that you need 3 files for the string sort cache is a little&lt;br/&gt;
&amp;gt; spooky... that's 3X the chance of a page fault.&lt;/p&gt;

&lt;p&gt;Not when using the compound format.&lt;/p&gt;

&lt;p&gt;&amp;gt; But the CFS construction must also go through the filesystem (like&lt;br/&gt;
&amp;gt; Lucene) right? So you still incur IO load of creating the small&lt;br/&gt;
&amp;gt; files, then 2nd pass to consolidate.&lt;/p&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;p&gt;&amp;gt; I think we may need to largely take "time" out of our programming&lt;br/&gt;
&amp;gt; languages, eg switch to much more declarative code, or&lt;br/&gt;
&amp;gt; something... wanna port Lucy to Erlang?&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; But I'm not sure process only concurrency, sharing only via&lt;br/&gt;
&amp;gt; file-backed memory, is the answer either&lt;/p&gt;

&lt;p&gt;I think relying heavily on file-backed memory is particularly appropriate for&lt;br/&gt;
Lucy because the write-once file format works well with MAP_SHARED memory&lt;br/&gt;
segments.  If files were being modified and had to be protected with&lt;br/&gt;
semaphores, it wouldn't be as sweet a match.&lt;/p&gt;

&lt;p&gt;Focusing on process-only concurrency also works well for Lucy because host&lt;br/&gt;
threading models differ substantially and so will only be accessible via a&lt;br/&gt;
generalized interface from the Lucy C core.  It will be difficult to tune&lt;br/&gt;
threading performance through that layer of indirection &amp;#8211; I'm guessing beyond&lt;br/&gt;
the ability of most developers since few will be experts in multiple host&lt;br/&gt;
threading models.  In contrast, expertise in process level concurrency will be&lt;br/&gt;
easier to come by and to nourish.&lt;/p&gt;

&lt;p&gt;&amp;gt; Using Zoie you can make reopen time insanely fast (much faster than I&lt;br/&gt;
&amp;gt; think necessary for most apps), but at the expense of some expected&lt;br/&gt;
&amp;gt; hit to searching/indexing throughput. I don't think that's the right&lt;br/&gt;
&amp;gt; tradeoff for Lucene.&lt;/p&gt;

&lt;p&gt;But as Jake pointed out early in the thread, Zoie achieves those insanely fast&lt;br/&gt;
reopens without tight coupling to IndexWriter and its components.  The&lt;br/&gt;
auxiliary RAM index approach is well proven.&lt;/p&gt;

&lt;p&gt;&amp;gt; Do you have any hard numbers on how much time it takes Lucene to load&lt;br/&gt;
&amp;gt; from a hot IO cache, populating its RAM resident data structures?&lt;/p&gt;

&lt;p&gt;Hmm, I don't spend a lot of time working with Lucene directly, so I might not&lt;br/&gt;
be the person most likely to have data like that at my fingertips.  Maybe that&lt;br/&gt;
McCandless dude can help you out, he runs a lot of benchmarks.  &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; &lt;/p&gt;

&lt;p&gt;Or maybe ask the Solr folks?  I see them on solr-user all the time talking &lt;br/&gt;
about "MaxWarmingSearchers". &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;&amp;gt; OK. Then, you are basically pooling your readers  Ie, you do allow&lt;br/&gt;
&amp;gt; in-process sharing, but only among readers.&lt;/p&gt;

&lt;p&gt;Not sure about that. Lucy's IndexReader.reopen() would open new SegReaders for&lt;br/&gt;
each new segment, but they would be private to each parent PolyReader.  So if&lt;br/&gt;
you reopened two IndexReaders at the same time after e.g.  segment "seg_12"&lt;br/&gt;
had been added, each would create a new, private SegReader for "seg_12".&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Edit&lt;/b&gt;: updated to correct assertions about virtual memory performance with&lt;br/&gt;
small indexes.&lt;/p&gt;</comment>
                    <comment id="12793737" author="mikemccand" created="Tue, 22 Dec 2009 19:08:41 +0000">
&lt;blockquote&gt;
&lt;p&gt;Processes are Lucy's primary concurrency model. ("The OS is our JVM.")&lt;br/&gt;
Making process-only concurrency efficient isn't optional - it's a core&lt;br/&gt;
concern.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lightweight searchers mean architectural freedom.&lt;/p&gt;

&lt;p&gt;Create 2, 10, 100, 1000 Searchers without a second thought - as many as you&lt;br/&gt;
need for whatever app architecture you just dreamed up - then destroy them&lt;br/&gt;
just as effortlessly. Add another worker thread to your search server without&lt;br/&gt;
having to consider the RAM requirements of a heavy searcher object. Create a&lt;br/&gt;
command-line app to search a documentation index without worrying about&lt;br/&gt;
daemonizing it. Etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is definitely neat.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Linux virtual memory system, at least, is not a pure LRU. It utilizes a&lt;br/&gt;
page aging algo which prioritizes pages that have historically been accessed&lt;br/&gt;
frequently even when they have not been accessed recently:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://sunsite.nus.edu.sg/LDP/LDP/tlk/node40.html" class="external-link"&gt;http://sunsite.nus.edu.sg/LDP/LDP/tlk/node40.html&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Very interesting &amp;#8211; thanks.  So it also factors in how much the page&lt;br/&gt;
was used in the past, not just how long it's been since the page was&lt;br/&gt;
last used.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;When will swapping out the term dictionary be a problem?&lt;/p&gt;

&lt;p&gt;For indexes where queries are made frequently, no problem.&lt;br/&gt;
Foir systems with plenty of RAM, no problem.&lt;br/&gt;
For systems that aren't very busy, no problem.&lt;br/&gt;
For small indexes, no problem.&lt;br/&gt;
The only situation we're talking about is infrequent queries against large&lt;br/&gt;
indexes on busy boxes where RAM isn't abundant. Under those circumstances, it&lt;br/&gt;
might be noticable that Lucy's term dictionary gets paged out somewhat&lt;br/&gt;
sooner than Lucene's.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Even smallish indexes can see the pages swapped out?  I'd think at&lt;br/&gt;
low-to-moderate search traffic, any index could be at risk, depdending&lt;br/&gt;
on whether other stuff in the machine wanting RAM or IO cache is&lt;br/&gt;
running.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But in general, if the term dictionary gets paged out, so what? Nobody was&lt;br/&gt;
using it. Maybe nobody will make another query against that index until next&lt;br/&gt;
week. Maybe the OS made the right decision.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You can't afford many page faults until the latency becomes very&lt;br/&gt;
apparent (until we're all on SSDs... at which point this may all be&lt;br/&gt;
moot).&lt;/p&gt;

&lt;p&gt;Right &amp;#8211; the metric that the swapper optimizes is overall efficient&lt;br/&gt;
use of the machine's resources.&lt;/p&gt;

&lt;p&gt;But I think that's often a poor metric for search apps... I think&lt;br/&gt;
consistency on the search latency is more important, though I agree it&lt;br/&gt;
depends very much on the app.&lt;/p&gt;

&lt;p&gt;I don't like the same behavior in my desktop &amp;#8211; when I switch to my&lt;br/&gt;
mail client, I don't want to wait 10 seconds for it to swap the pages&lt;br/&gt;
back in.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Let me turn your question on its head. What does Lucene gain in return for&lt;br/&gt;
the slow index opens and large process memory footprint of its heavy&lt;br/&gt;
searchers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Consistency in the search time.  Assuming the OS doesn't swap our&lt;br/&gt;
pages out...&lt;/p&gt;

&lt;p&gt;And of course Java pretty much forces threads-as-concurrency (JVM&lt;br/&gt;
startup time, hotspot compilation, are costly).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If necessary, there's a straightforward remedy: slurp the relevant files into&lt;br/&gt;
RAM at object construction rather than mmap them. The rest of the code won't &lt;br/&gt;
know the difference between malloc'd RAM and mmap'd RAM. The slurped files &lt;br/&gt;
won't take up any more space than the analogous Lucene data structures; more &lt;br/&gt;
likely, they'll take up less.&lt;/p&gt;

&lt;p&gt;That's the kind of setting we'd hide away in the IndexManager class rather&lt;br/&gt;
than expose as prominent API, and it would be a hint to index components&lt;br/&gt;
rather than an edict.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, this is how Lucy would force warming.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Yeah, that you need 3 files for the string sort cache is a little spooky... that's 3X the chance of a page fault.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not when using the compound format.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But, even within that CFS file, these three sub-files will not be&lt;br/&gt;
local?  Ie you'll still have to hit three pages per "lookup" right?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think relying heavily on file-backed memory is particularly appropriate for&lt;br/&gt;
Lucy because the write-once file format works well with MAP_SHARED memory&lt;br/&gt;
segments. If files were being modified and had to be protected with&lt;br/&gt;
semaphores, it wouldn't be as sweet a match.&lt;/p&gt;&lt;/blockquote&gt;  

&lt;p&gt;Write-once is good for Lucene too.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Focusing on process-only concurrency also works well for Lucy because host&lt;br/&gt;
threading models differ substantially and so will only be accessible via a&lt;br/&gt;
generalized interface from the Lucy C core. It will be difficult to tune&lt;br/&gt;
threading performance through that layer of indirection - I'm guessing beyond&lt;br/&gt;
the ability of most developers since few will be experts in multiple host&lt;br/&gt;
threading models. In contrast, expertise in process level concurrency will be&lt;br/&gt;
easier to come by and to nourish.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm confused by this &amp;#8211; eg Python does a great job presenting a simple&lt;br/&gt;
threads interface and implementing it on major OSs.  And it seems like&lt;br/&gt;
Lucy would not need anything crazy-os-specific wrt threads?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Do you have any hard numbers on how much time it takes Lucene to load from a hot IO cache, populating its RAM resident data structures?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, I don't spend a lot of time working with Lucene directly, so I might not&lt;br/&gt;
be the person most likely to have data like that at my fingertips. Maybe that&lt;br/&gt;
McCandless dude can help you out, he runs a lot of benchmarks.  &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I'd guess that field cache is slowish; deleted docs &amp;amp; norms are&lt;br/&gt;
very fast; terms index is somewhere in between.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Or maybe ask the Solr folks? I see them on solr-user all the time talking about "MaxWarmingSearchers". &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm &amp;#8211; not sure what's up with that.  Looks like maybe it's the&lt;br/&gt;
auto-warming that might happen after a commit.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;OK. Then, you are basically pooling your readers Ie, you do allow in-process sharing, but only among readers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure about that. Lucy's IndexReader.reopen() would open new SegReaders for&lt;br/&gt;
each new segment, but they would be private to each parent PolyReader. So if&lt;br/&gt;
you reopened two IndexReaders at the same time after e.g. segment "seg_12"&lt;br/&gt;
had been added, each would create a new, private SegReader for "seg_12".&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You're right, you'd get two readers for seg_12 in that case.  By&lt;br/&gt;
"pool" I meant you're tapping into all the sub-readers that the&lt;br/&gt;
existing reader have opened &amp;#8211; the reader is your pool of sub-readers.&lt;/p&gt;</comment>
                    <comment id="12793918" author="creamyg" created="Wed, 23 Dec 2009 03:59:23 +0000">&lt;p&gt;&amp;gt; Very interesting - thanks. So it also factors in how much the page&lt;br/&gt;
&amp;gt; was used in the past, not just how long it's been since the page was&lt;br/&gt;
&amp;gt; last used.&lt;/p&gt;

&lt;p&gt;In theory, I think that means the term dictionary will tend to be favored over&lt;br/&gt;
the posting lists.  In practice... hard to say, it would be difficult to test.&lt;br/&gt;
&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;&amp;gt; Even smallish indexes can see the pages swapped out? &lt;/p&gt;

&lt;p&gt;Yes, you're right &amp;#8211; the wait time to get at a small term dictionary isn't&lt;br/&gt;
necessarily small.  I've amended my previous post, thanks.&lt;/p&gt;

&lt;p&gt;&amp;gt; And of course Java pretty much forces threads-as-concurrency (JVM&lt;br/&gt;
&amp;gt; startup time, hotspot compilation, are costly).&lt;/p&gt;

&lt;p&gt;Yes.  Java does a lot of stuff that most operating systems can also do, but of&lt;br/&gt;
course provides a coherent platform-independent interface.  In Lucy we're&lt;br/&gt;
going to try to go back to the OS for some of the stuff that Java likes to&lt;br/&gt;
take over &amp;#8211; provided that we can develop a sane genericized interface using&lt;br/&gt;
configuration probing and #ifdefs.  &lt;/p&gt;

&lt;p&gt;It's nice that as long as the box is up our OS-as-JVM is always running, so we&lt;br/&gt;
don't have to worry about its (quite lengthy) startup time. &lt;/p&gt;

&lt;p&gt;&amp;gt; Right, this is how Lucy would force warming.&lt;/p&gt;

&lt;p&gt;I think slurp-instead-of-mmap is orthogonal to warming, because we can warm&lt;br/&gt;
file-backed RAM structures by forcing them into the IO cache, using either the&lt;br/&gt;
cat-to-dev-null trick or something more sophisticated.  The&lt;br/&gt;
slurp-instead-of-mmap setting would cause warming as a side effect, but the&lt;br/&gt;
main point would be to attempt to persuade the virtual memory system that&lt;br/&gt;
certain data structures should have a higher status and not be paged out as&lt;br/&gt;
quickly.&lt;/p&gt;

&lt;p&gt;&amp;gt; But, even within that CFS file, these three sub-files will not be&lt;br/&gt;
&amp;gt; local? Ie you'll still have to hit three pages per "lookup" right?&lt;/p&gt;

&lt;p&gt;They'll be next to each other in the compound file because CompoundFileWriter&lt;br/&gt;
orders them alphabetically.  For big segments, though, you're right that they&lt;br/&gt;
won't be right next to each other, and you could possibly incur as many as&lt;br/&gt;
three page faults when retrieving a sort cache value.&lt;/p&gt;

&lt;p&gt;But what are the alternatives for variable width data like strings?  You need&lt;br/&gt;
the ords array anyway for efficient comparisons, so what's left are the&lt;br/&gt;
offsets array and the character data.&lt;/p&gt;

&lt;p&gt;An array of String objects isn't going to have better locality than one solid&lt;br/&gt;
block of memory dedicated to offsets and another solid block of memory&lt;br/&gt;
dedicated to file data, and it's no fewer derefs even if the string object&lt;br/&gt;
stores its character data inline &amp;#8211; more if it points to a separate allocation&lt;br/&gt;
(like Lucy's CharBuf does, since it's mutable). &lt;/p&gt;

&lt;p&gt;For each sort cache value lookup, you're going to need to access two blocks of&lt;br/&gt;
memory.  &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;With the array of String objects, the first is the memory block dedicated&lt;br/&gt;
    to the array, and the second is the memory block dedicated to the String&lt;br/&gt;
    object itself, which contains the character data.&lt;/li&gt;
	&lt;li&gt;With the file-backed block sort cache, the first memory block is the&lt;br/&gt;
    offsets array, and the second is the character data array.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think the locality costs should be approximately the same... have I missed &lt;br/&gt;
anything?&lt;/p&gt;

&lt;p&gt;&amp;gt; Write-once is good for Lucene too.&lt;/p&gt;

&lt;p&gt;Hellyeah.&lt;/p&gt;

&lt;p&gt;&amp;gt; And it seems like Lucy would not need anything crazy-os-specific wrt&lt;br/&gt;
&amp;gt; threads?&lt;/p&gt;

&lt;p&gt;It depends on how many classes we want to make thread-safe, and it's not just&lt;br/&gt;
the OS, it's the host.&lt;/p&gt;

&lt;p&gt;The bare minimum is simply to make Lucy thread-safe as a library.  That's&lt;br/&gt;
pretty close, because Lucy studiously avoided global variables whenever&lt;br/&gt;
possible.  The only problems that have to be addressed are the VTable_registry&lt;br/&gt;
Hash, race conditions when creating new subclasses via dynamic VTable&lt;br/&gt;
singletons, and refcounts on the VTable objects themselves.&lt;/p&gt;

&lt;p&gt;Once those issues are taken care of, you'll be able to use Lucy objects in&lt;br/&gt;
separate threads with no problem, e.g. one Searcher per thread.&lt;/p&gt;

&lt;p&gt;However, if you want to &lt;b&gt;share&lt;/b&gt; Lucy objects (other than VTables) across&lt;br/&gt;
threads, all of a sudden we have to start thinking about "synchronized",&lt;br/&gt;
"volatile", etc.  Such constructs may not be efficient or even possible under&lt;br/&gt;
some threading models.&lt;/p&gt;

&lt;p&gt;&amp;gt; Hmm I'd guess that field cache is slowish; deleted docs &amp;amp; norms are&lt;br/&gt;
&amp;gt; very fast; terms index is somewhere in between.&lt;/p&gt;

&lt;p&gt;That jibes with my own experience.  So maybe consider file-backed sort caches&lt;br/&gt;
in Lucene, while keeping the status quo for everything else?&lt;/p&gt;

&lt;p&gt;&amp;gt; You're right, you'd get two readers for seg_12 in that case. By&lt;br/&gt;
&amp;gt; "pool" I meant you're tapping into all the sub-readers that the&lt;br/&gt;
&amp;gt; existing reader have opened - the reader is your pool of sub-readers.&lt;/p&gt;

&lt;p&gt;Each unique SegReader will also have dedicated "sub-reader" objects: two&lt;br/&gt;
"seg_12" SegReaders means two "seg_12" DocReaders, two "seg_12"&lt;br/&gt;
PostingsReaders, etc.  However, all those sub-readers will share the same&lt;br/&gt;
file-backed RAM data, so in that sense they're pooled.&lt;/p&gt;</comment>
                    <comment id="12794095" author="mikemccand" created="Wed, 23 Dec 2009 16:26:58 +0000">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Very interesting - thanks. So it also factors in how much the page was used in the past, not just how long it's been since the page was  last used.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In theory, I think that means the term dictionary will tend to be&lt;br/&gt;
favored over the posting lists. In practice... hard to say, it would&lt;br/&gt;
be difficult to test.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right... though, I think the top "trunks" frequently used by the&lt;br/&gt;
binary search, will stay hot.  But as you get deeper into the terms&lt;br/&gt;
index, it's not as clear.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;And of course Java pretty much forces threads-as-concurrency (JVM startup time, hotspot compilation, are costly).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. Java does a lot of stuff that most operating systems can also do, but of&lt;br/&gt;
course provides a coherent platform-independent interface. In Lucy we're&lt;br/&gt;
going to try to go back to the OS for some of the stuff that Java likes to&lt;br/&gt;
take over - provided that we can develop a sane genericized interface using&lt;br/&gt;
configuration probing and #ifdefs.&lt;/p&gt;

&lt;p&gt;It's nice that as long as the box is up our OS-as-JVM is always running, so we&lt;br/&gt;
don't have to worry about its (quite lengthy) startup time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OS as JVM is a nice analogy.  Java of course gets in the way, too,&lt;br/&gt;
like we cannot properly set IO priorities, we can't give hints to the&lt;br/&gt;
OS to tell it not to cache certain reads/writes (ie segment merging),&lt;br/&gt;
can't pin pages &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;, etc.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Right, this is how Lucy would force warming.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think slurp-instead-of-mmap is orthogonal to warming, because we can warm&lt;br/&gt;
file-backed RAM structures by forcing them into the IO cache, using either the&lt;br/&gt;
cat-to-dev-null trick or something more sophisticated. The&lt;br/&gt;
slurp-instead-of-mmap setting would cause warming as a side effect, but the&lt;br/&gt;
main point would be to attempt to persuade the virtual memory system that&lt;br/&gt;
certain data structures should have a higher status and not be paged out as&lt;br/&gt;
quickly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Woops, sorry, I misread &amp;#8211; now I understand.  You can easily make&lt;br/&gt;
certain files ram resident, and then be like Lucene (except the data&lt;br/&gt;
structures are more compact).  Nice.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;But, even within that CFS file, these three sub-files will not be local? Ie you'll still have to hit three pages per "lookup" right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They'll be next to each other in the compound file because CompoundFileWriter&lt;br/&gt;
orders them alphabetically. For big segments, though, you're right that they&lt;br/&gt;
won't be right next to each other, and you could possibly incur as many as&lt;br/&gt;
three page faults when retrieving a sort cache value.&lt;/p&gt;

&lt;p&gt;But what are the alternatives for variable width data like strings? You need&lt;br/&gt;
the ords array anyway for efficient comparisons, so what's left are the&lt;br/&gt;
offsets array and the character data.&lt;/p&gt;

&lt;p&gt;An array of String objects isn't going to have better locality than one solid&lt;br/&gt;
block of memory dedicated to offsets and another solid block of memory&lt;br/&gt;
dedicated to file data, and it's no fewer derefs even if the string object&lt;br/&gt;
stores its character data inline - more if it points to a separate allocation&lt;br/&gt;
(like Lucy's CharBuf does, since it's mutable).&lt;/p&gt;

&lt;p&gt;For each sort cache value lookup, you're going to need to access two blocks of&lt;br/&gt;
memory.&lt;/p&gt;

&lt;p&gt;With the array of String objects, the first is the memory block dedicated&lt;br/&gt;
to the array, and the second is the memory block dedicated to the String&lt;br/&gt;
object itself, which contains the character data.&lt;br/&gt;
With the file-backed block sort cache, the first memory block is the&lt;br/&gt;
offsets array, and the second is the character data array.&lt;br/&gt;
I think the locality costs should be approximately the same... have I missed &lt;br/&gt;
anything?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You're right, Lucene risks 3 (ord array, String array, String object)&lt;br/&gt;
page faults on each lookup as well.&lt;/p&gt;

&lt;p&gt;Actually why can't ord &amp;amp; offset be one, for the string sort cache?&lt;br/&gt;
Ie, if you write your string data in sort order, then the offsets are&lt;br/&gt;
also in sort order?  (I think we may have discussed this already?)&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;And it seems like Lucy would not need anything crazy-os-specific wrt threads?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It depends on how many classes we want to make thread-safe, and it's not just&lt;br/&gt;
the OS, it's the host.&lt;/p&gt;

&lt;p&gt;The bare minimum is simply to make Lucy thread-safe as a library. That's&lt;br/&gt;
pretty close, because Lucy studiously avoided global variables whenever&lt;br/&gt;
possible. The only problems that have to be addressed are the VTable_registry&lt;br/&gt;
Hash, race conditions when creating new subclasses via dynamic VTable&lt;br/&gt;
singletons, and refcounts on the VTable objects themselves.&lt;/p&gt;

&lt;p&gt;Once those issues are taken care of, you'll be able to use Lucy objects in&lt;br/&gt;
separate threads with no problem, e.g. one Searcher per thread.&lt;/p&gt;

&lt;p&gt;However, if you want to share Lucy objects (other than VTables) across&lt;br/&gt;
threads, all of a sudden we have to start thinking about "synchronized",&lt;br/&gt;
"volatile", etc. Such constructs may not be efficient or even possible under&lt;br/&gt;
some threading models.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK it is indeed hairy.  You don't want to have to create Lucy's&lt;br/&gt;
equivalent of the JMM...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Hmm I'd guess that field cache is slowish; deleted docs &amp;amp; norms are very fast; terms index is somewhere in between.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That jibes with my own experience. So maybe consider file-backed sort caches&lt;br/&gt;
in Lucene, while keeping the status quo for everything else?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perhaps, but it'd still make me nervous &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  When we get&lt;br/&gt;
CSF (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1231" title="Column-stride fields (aka per-document Payloads)"&gt;&lt;del&gt;LUCENE-1231&lt;/del&gt;&lt;/a&gt;) online we should make it&lt;br/&gt;
pluggable enough so that one could create an mmap impl.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;You're right, you'd get two readers for seg_12 in that case. By "pool" I meant you're tapping into all the sub-readers that the existing reader have opened - the reader is your pool of sub-readers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Each unique SegReader will also have dedicated "sub-reader" objects: two&lt;br/&gt;
"seg_12" SegReaders means two "seg_12" DocReaders, two "seg_12"&lt;br/&gt;
PostingsReaders, etc. However, all those sub-readers will share the same&lt;br/&gt;
file-backed RAM data, so in that sense they're pooled.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK&lt;/p&gt;</comment>
                    <comment id="12794137" author="creamyg" created="Wed, 23 Dec 2009 18:26:18 +0000">&lt;p&gt;&amp;gt; we can't give hints to the OS to tell it not to cache certain reads/writes&lt;br/&gt;
&amp;gt; (ie segment merging), &lt;/p&gt;

&lt;p&gt;For what it's worth, we haven't really solved that problem in Lucy either.&lt;br/&gt;
The sliding window abstraction we wrapped around mmap/MapViewOfFile largely&lt;br/&gt;
solved the problem of running out of address space on 32-bit operating&lt;br/&gt;
systems.  However, there's currently no way to invoke madvise through Lucy's&lt;br/&gt;
IO abstraction layer &amp;#8211; it's a little tricky with compound files.  &lt;/p&gt;

&lt;p&gt;Linux, at least, requires that the buffer supplied to madvise be page-aligned.&lt;br/&gt;
So, say we're starting off on a posting list, and we want to communicate to&lt;br/&gt;
the OS that it should treat the region we're about to read as MADV_SEQUENTIAL.&lt;br/&gt;
If the start of the postings file is in the middle of a 4k page and the file&lt;br/&gt;
right before it is a term dictionary, we don't want to indicate that that&lt;br/&gt;
region should be treated as sequential.&lt;/p&gt;

&lt;p&gt;I'm not sure how to solve that problem without violating the encapsulation of&lt;br/&gt;
the compound file model.  Hmm, maybe we could store metadata about the virtual&lt;br/&gt;
files indicating usage patterns (sequential, random, etc.)?  Since files are&lt;br/&gt;
generally part of dedicated data structures whose usage patterns are known at &lt;br/&gt;
index time.&lt;/p&gt;

&lt;p&gt;Or maybe we just punt on that use case and worry only about segment merging.  &lt;br/&gt;
Hmm, wouldn't the act of deleting a file (and releasing all file descriptors) tell&lt;br/&gt;
the OS that it's free to recycle any memory pages associated with it?&lt;/p&gt;

&lt;p&gt;&amp;gt; Actually why can't ord &amp;amp; offset be one, for the string sort cache?&lt;br/&gt;
&amp;gt; Ie, if you write your string data in sort order, then the offsets are&lt;br/&gt;
&amp;gt; also in sort order? (I think we may have discussed this already?)&lt;/p&gt;

&lt;p&gt;Right, we discussed this on lucy-dev last spring:&lt;/p&gt;

&lt;p&gt;    &lt;a href="http://markmail.org/message/epc56okapbgit5lw" class="external-link"&gt;http://markmail.org/message/epc56okapbgit5lw&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Incidentally, some of this thread replays our exchange at the top of&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1458" title="Further steps towards flexible indexing"&gt;&lt;del&gt;LUCENE-1458&lt;/del&gt;&lt;/a&gt; from a year ago.  It was fun to go back and reread that: in the&lt;br/&gt;
interrim, we've implemented segment-centric search and memory mapped field&lt;br/&gt;
caches and term dictionaries, both of which were first discussed back then.&lt;br/&gt;
&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Ords are great for low cardinality fields of all kinds, but become less&lt;br/&gt;
efficient for high cardinality primitive numeric fields.  For simplicity's&lt;br/&gt;
sake, the prototype implementation of mmap'd field caches in KS always uses&lt;br/&gt;
ords.&lt;/p&gt;

&lt;p&gt;&amp;gt; You don't want to have to create Lucy's equivalent of the JMM...&lt;/p&gt;

&lt;p&gt;The more I think about making Lucy classes thread safe, the harder it seems.&lt;br/&gt;
&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;  I'd like to make it possible to share a Schema across threads, for&lt;br/&gt;
instance, but that means all its Analyzers, etc have to be thread-safe as&lt;br/&gt;
well, which isn't practical when you start getting into contributed&lt;br/&gt;
subclasses.  &lt;/p&gt;

&lt;p&gt;Even if we succeed in getting Folders and FileHandles thread safe, it will be&lt;br/&gt;
hard for the user to keep track of what they can and can't do across threads.&lt;br/&gt;
"Don't share anything" is a lot easier to understand.&lt;/p&gt;

&lt;p&gt;We reap a big benefit by making Lucy's metaclass infrastructure thread-safe.&lt;br/&gt;
Beyond that, seems like there's a lot of pain for little gain.&lt;/p&gt;</comment>
                    <comment id="12794161" author="mikemccand" created="Wed, 23 Dec 2009 19:08:20 +0000">&lt;blockquote&gt;
&lt;p&gt;For what it's worth, we haven't really solved that problem in Lucy either.&lt;br/&gt;
The sliding window abstraction we wrapped around mmap/MapViewOfFile largely&lt;br/&gt;
solved the problem of running out of address space on 32-bit operating&lt;br/&gt;
systems. However, there's currently no way to invoke madvise through Lucy's&lt;br/&gt;
IO abstraction layer - it's a little tricky with compound files.&lt;/p&gt;

&lt;p&gt;Linux, at least, requires that the buffer supplied to madvise be page-aligned.&lt;br/&gt;
So, say we're starting off on a posting list, and we want to communicate to&lt;br/&gt;
the OS that it should treat the region we're about to read as MADV_SEQUENTIAL.&lt;br/&gt;
If the start of the postings file is in the middle of a 4k page and the file&lt;br/&gt;
right before it is a term dictionary, we don't want to indicate that that&lt;br/&gt;
region should be treated as sequential.&lt;/p&gt;

&lt;p&gt;I'm not sure how to solve that problem without violating the encapsulation of&lt;br/&gt;
the compound file model. Hmm, maybe we could store metadata about the virtual&lt;br/&gt;
files indicating usage patterns (sequential, random, etc.)? Since files are&lt;br/&gt;
generally part of dedicated data structures whose usage patterns are known at &lt;br/&gt;
index time.&lt;/p&gt;

&lt;p&gt;Or maybe we just punt on that use case and worry only about segment merging. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Storing metadata seems OK.  It'd be optional for codecs to declare that...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hmm, wouldn't the act of deleting a file (and releasing all file descriptors) tell&lt;br/&gt;
the OS that it's free to recycle any memory pages associated with it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It better!&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Actually why can't ord &amp;amp; offset be one, for the string sort cache? Ie, if you write your string data in sort order, then the offsets are also in sort order? (I think we may have discussed this already?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, we discussed this on lucy-dev last spring:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://markmail.org/message/epc56okapbgit5lw" class="external-link"&gt;http://markmail.org/message/epc56okapbgit5lw&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I'll go try to catch up... but I'm about to drop &lt;span class="error"&gt;&amp;#91;sort of&amp;#93;&lt;/span&gt;&lt;br/&gt;
offline for a week and a half!  There's alot of reading there!  Should&lt;br/&gt;
be a prereq that we first go back and re-read what we said "the last&lt;br/&gt;
time"... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Incidentally, some of this thread replays our exchange at the top of&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1458" title="Further steps towards flexible indexing"&gt;&lt;del&gt;LUCENE-1458&lt;/del&gt;&lt;/a&gt; from a year ago. It was fun to go back and reread that: in the&lt;br/&gt;
interrim, we've implemented segment-centric search and memory mapped field&lt;br/&gt;
caches and term dictionaries, both of which were first discussed back then.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice!  &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Ords are great for low cardinality fields of all kinds, but become less&lt;br/&gt;
efficient for high cardinality primitive numeric fields. For simplicity's&lt;br/&gt;
sake, the prototype implementation of mmap'd field caches in KS always uses&lt;br/&gt;
ords.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;You don't want to have to create Lucy's equivalent of the JMM...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The more I think about making Lucy classes thread safe, the harder it seems.&lt;br/&gt;
 I'd like to make it possible to share a Schema across threads, for&lt;br/&gt;
instance, but that means all its Analyzers, etc have to be thread-safe as&lt;br/&gt;
well, which isn't practical when you start getting into contributed&lt;br/&gt;
subclasses.&lt;/p&gt;

&lt;p&gt;Even if we succeed in getting Folders and FileHandles thread safe, it will be&lt;br/&gt;
hard for the user to keep track of what they can and can't do across threads.&lt;br/&gt;
"Don't share anything" is a lot easier to understand.&lt;/p&gt;

&lt;p&gt;We reap a big benefit by making Lucy's metaclass infrastructure thread-safe.&lt;br/&gt;
Beyond that, seems like there's a lot of pain for little gain.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah.  Threads are not easy &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="13244701" author="budili" created="Mon, 2 Apr 2012 22:53:14 +0100">&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am a Computer Science student from Germany. I would like to contribute to this project under GSoC 2012. I have very good experience in Java. I have some questions to this project, can someone help me? IRC or instant messanger? &lt;/p&gt;

&lt;p&gt;Thank You&lt;br/&gt;
Tim&lt;/p&gt;</comment>
                    <comment id="13245125" author="mikemccand" created="Tue, 3 Apr 2012 11:57:22 +0100">&lt;p&gt;Is there anyone who can volunteer to be a mentor for this issue...?&lt;/p&gt;</comment>
                    <comment id="13245257" author="simonw" created="Tue, 3 Apr 2012 13:19:52 +0100">&lt;p&gt;I would but I am so overloaded with other work right now. I can be the primary mentor if you could help when I am totally blocked. &lt;/p&gt;

&lt;p&gt;Hi Tim, as we are in the Apache Foundation and a open source project we make everything public. So if you have questions please go and start a thread on the dev@l.a.o mailing list and I am happy to help you. For GSoC internal or private issues while GSoC is running we can do private communication.&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13245395" author="budili" created="Tue, 3 Apr 2012 16:21:48 +0100">&lt;p&gt;Hello Michael, hello Simon,&lt;/p&gt;

&lt;p&gt;thanks for the fast response. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So if you have questions please go and start a thread on the dev@l.a.o &lt;span class="error"&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Okay, I do this and start a thread. I have some special questions to the task (Refactoring IndexWriter).&lt;/p&gt;

&lt;p&gt;For example: &lt;br/&gt;
1. Exist unit tests for the code (IndexWriter.java)? &lt;br/&gt;
2. Where i can find the code/software btw. component? (svn, git etc.)&lt;br/&gt;
3. Which IDE I can use for this project? Your Suggestion (Eclipse)? &lt;br/&gt;
4. What's about coding style guides?&lt;br/&gt;
5. &lt;span class="error"&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                    <comment id="13541460" author="markrmiller@gmail.com" created="Mon, 31 Dec 2012 19:04:07 +0000">&lt;p&gt;Been a long time since this has seen action - pushing out of 4.1.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                                <inwardlinks description="is part of">
                            <issuelink>
            <issuekey id="12434404">LUCENE-1879</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 4 Nov 2009 01:27:16 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3950</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25693</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2025] Ability to turn off the store for an index</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2025</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It would be really good in combination with parallel indexing if the&lt;br/&gt;
Lucene store could be turned off entirely for an index. &lt;/p&gt;

&lt;p&gt;The reason is that part of the store is the FieldIndex (.fdx file),&lt;br/&gt;
which contains an 8 bytes pointer for each document in a segment, even&lt;br/&gt;
if a document does not contain any stored fields.&lt;/p&gt;

&lt;p&gt;With parallel indexing we will want to rewrite certain parallel&lt;br/&gt;
indexes to update them, and if such an update affects only a small&lt;br/&gt;
number of documents it will be a waste if you have to write the .fdx&lt;br/&gt;
file every time.&lt;/p&gt;

&lt;p&gt;So in the case where you only want to update a data structure in the&lt;br/&gt;
inverted index it makes sense to separate your index into multiple&lt;br/&gt;
parallel indexes, where the ones you want to update don't contain any&lt;br/&gt;
stored fields.&lt;/p&gt;

&lt;p&gt;It'd be also great to not only allow turning off the store but to make&lt;br/&gt;
it customizable, similarly to what flexible indexing wants to achieve&lt;br/&gt;
regarding the inverted index.&lt;/p&gt;

&lt;p&gt;As a start I'd be happy with the ability to simply turn off the store and to&lt;br/&gt;
add more flexibility later.&lt;/p&gt;</description>
                <environment/>
            <key id="12439796">LUCENE-2025</key>
            <summary>Ability to turn off the store for an index</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                        <label>gsoc2013</label>
                    </labels>
                <created>Tue, 3 Nov 2009 22:45:04 +0000</created>
                <updated>Fri, 10 May 2013 00:05:33 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="13006506" author="mikemccand" created="Mon, 14 Mar 2011 16:47:58 +0000">&lt;p&gt;Simon, watch out for &lt;a href="https://issues.apache.org/jira/browse/INFRA-3517" title="Can we strip trailing comma from Jira&amp;#39;s labels (eg, &amp;#39;gsoc2011,&amp;#39; --&amp;gt; &amp;#39;gsoc2011&amp;#39;)"&gt;&lt;del&gt;INFRA-3517&lt;/del&gt;&lt;/a&gt; &amp;#8211; we have to be careful, when labeling, to not use the label with a trailing comma stuck on!&lt;/p&gt;

&lt;p&gt;Ie this issue now has two such labels: 'gosc2011,' and 'mentor,'&lt;/p&gt;
</comment>
                    <comment id="13006522" author="simonw" created="Mon, 14 Mar 2011 17:13:47 +0000">&lt;blockquote&gt;&lt;p&gt;Ie this issue now has two such labels: 'gosc2011,' and 'mentor,'&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;thanks mike I changed them back to have no commas&lt;/p&gt;</comment>
                    <comment id="13281846" author="simonw" created="Wed, 23 May 2012 20:55:10 +0100">&lt;p&gt;moving this over to 4.1 this won't happen in 4.0 anymore&lt;/p&gt;</comment>
                    <comment id="13446003" author="rcmuir" created="Fri, 31 Aug 2012 15:40:53 +0100">&lt;p&gt;One simple way to do this today is to just use a codec that has a NoStoredFieldsImpl,&lt;br/&gt;
Throws exception in its writer impl if you ask it to actually write any stored fields &lt;br/&gt;
(e.g. startDocument&amp;#40;n&amp;#41; is called where n &amp;gt; 0), and does nothing in its reader impl.&lt;/p&gt;

&lt;p&gt;I think for the typical case its fairly uncommon, i looked into seeing if we could&lt;br/&gt;
optimize this case for Lucene40's impl, but it introduces a lot of scary situations&lt;br/&gt;
for things like bulk merge.&lt;/p&gt;

&lt;p&gt;So for now I really think this is a simple safe way at the moment, if someone wants to &lt;br/&gt;
turn it off they just set this as their codec on indexwriter.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                                <inwardlinks description="is part of">
                            <issuelink>
            <issuekey id="12434404">LUCENE-1879</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12408724">LUCENE-1458</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 14 Mar 2011 16:47:58 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3951</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25694</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2023] Improve performance of SmartChineseAnalyzer</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2023</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I've noticed SmartChineseAnalyzer is a bit slow, compared to say CJKAnalyzer on chinese text.&lt;/p&gt;

&lt;p&gt;This patch improves the internal hhmm implementation. &lt;br/&gt;
Time to index my chinese corpus is 75% of the previous time.&lt;/p&gt;</description>
                <environment/>
            <key id="12439551">LUCENE-2023</key>
            <summary>Improve performance of SmartChineseAnalyzer</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Fri, 30 Oct 2009 17:00:47 +0000</created>
                <updated>Fri, 10 May 2013 00:05:33 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>modules/analysis</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12772003" author="dmsmith" created="Fri, 30 Oct 2009 17:36:11 +0000">&lt;p&gt;If we have a 2.9.2 release, can this be there too?&lt;/p&gt;</comment>
                    <comment id="12772013" author="rcmuir" created="Fri, 30 Oct 2009 17:59:52 +0000">&lt;p&gt;DM, personally I don't see the slowness as a bug... though I am very aware word segmentation speed is really important to chinese users.&lt;/p&gt;

&lt;p&gt;but if all the other devs jumped up and said yeah its really critical to be in a bugfix release, i'd backport it to java 1.4&lt;/p&gt;

&lt;p&gt;in my opinion its just an improvement.&lt;/p&gt;</comment>
                    <comment id="12772022" author="dmsmith" created="Fri, 30 Oct 2009 18:20:03 +0000">&lt;p&gt;I fully understand that at some point, "just say no."&lt;/p&gt;

&lt;p&gt;I don't think it warrants a new bug fix release, but if there is one then it would be a "nice to have" iff the backport is a trivial effort.&lt;/p&gt;

&lt;p&gt;That said, a 25% improvement is substantial.&lt;/p&gt;</comment>
                    <comment id="12772023" author="markrmiller@gmail.com" created="Fri, 30 Oct 2009 18:20:35 +0000">&lt;p&gt;How long are you stuck on 1.4 DM &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; You guys are gonna upgrade those Macs someday aren't you? Or is it a matter of supporting that platform for your users?&lt;/p&gt;

&lt;p&gt;IMO, it wouldn't be horrible to stretch easily backported optimizations considering this is the last 1.4 release - as long as someone is willing to do it - and we have another release.&lt;/p&gt;</comment>
                    <comment id="12772027" author="rcmuir" created="Fri, 30 Oct 2009 18:26:48 +0000">&lt;p&gt;well, i'm more than willing to do the backport, if will be used.&lt;/p&gt;

&lt;p&gt;for now, maybe someone can take a look and double-check the patch for trunk, smartcn is always dangerous territory. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; &lt;/p&gt;

&lt;p&gt;(for the record the big bottleneck was the hashmap usage in these graphs, its rather silly since its indexed by int, you know the number of values up front, etc)&lt;/p&gt;</comment>
                    <comment id="12772034" author="dmsmith" created="Fri, 30 Oct 2009 18:37:02 +0000">&lt;p&gt;Thanks, Mark.&lt;/p&gt;

&lt;p&gt;I'm stuck with 1.4 for the near future. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I wish people would upgrade their MacOS! I'll have a release soon and then another in 4-6 months. The release after that will be Java 5 and will probably follow shortly after.&lt;/p&gt;

&lt;p&gt;I'm willing to do the work, especially if it is trivial and it is something I use&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Perhaps, it would be best to create a separate issue targeting 2.9.2 (or whatever the next release might be), linking it to the original(s), mark it as an "Improvement" and attach appropriate patch(es). Maybe one issue for all improvements, or one per "parent" issue? Then if there is a bugfix that necessitates such a release, the patches would be applied for that release.&lt;/p&gt;</comment>
                    <comment id="12772050" author="dmsmith" created="Fri, 30 Oct 2009 19:29:19 +0000">&lt;p&gt;Robert,&lt;br/&gt;
You have in BigramDictionary:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;boolean&lt;/span&gt; isToExist(&lt;span class="code-object"&gt;int&lt;/span&gt; to) {
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; to &amp;lt; tokenPairListTable.length &amp;amp;&amp;amp; tokenPairListTable[to] != &lt;span class="code-keyword"&gt;null&lt;/span&gt;;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And you call it in:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; void addSegTokenPair(SegTokenPair tokenPair) {
    &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt; to = tokenPair.to;
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (!isToExist(to)) {
      ArrayList&amp;lt;SegTokenPair&amp;gt; newlist = &lt;span class="code-keyword"&gt;new&lt;/span&gt; ArrayList&amp;lt;SegTokenPair&amp;gt;();
      newlist.add(tokenPair);
      tokenPairListTable[to] = newlist;
      tableSize++;
    } &lt;span class="code-keyword"&gt;else&lt;/span&gt; {
      List&amp;lt;SegTokenPair&amp;gt; tokenPairList = tokenPairListTable[to];
      tokenPairList.add(tokenPair);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The check in addSegTokenPair assumes the isToExist(to) returns false when "to" is in bounds because "tokenPairListTable&lt;span class="error"&gt;&amp;#91;to&amp;#93;&lt;/span&gt;" will throw an array bounds exception otherwise. Is it an invariant that tokenPair.to will always be in bounds?&lt;/p&gt;

&lt;p&gt;In the same way the array in SegGraph, does the same thing.&lt;/p&gt;

&lt;p&gt;With the former implementation, it did not have an issue.&lt;/p&gt;

&lt;p&gt;Other than that, it looks good.&lt;/p&gt;</comment>
                    <comment id="12772054" author="rcmuir" created="Fri, 30 Oct 2009 19:32:53 +0000">&lt;p&gt;hi DM,&lt;/p&gt;

&lt;p&gt;i think the bounds checks are redundant actually, &lt;br/&gt;
With both situations, the bounds are calculated up front in the constructor.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is it an invariant that tokenPair.to will always be in bounds?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, in this case.&lt;/p&gt;

&lt;p&gt;The reason I did this is for isToExist, etc is because those methods are public... but this stuff is pkg private anyway so maybe i should delete the bounds checks altogether???&lt;/p&gt;</comment>
                    <comment id="12772058" author="rcmuir" created="Fri, 30 Oct 2009 19:44:14 +0000">&lt;p&gt;change these redundant bounds checks to assertions as DM observed.&lt;/p&gt;</comment>
                    <comment id="12772325" author="rcmuir" created="Sun, 1 Nov 2009 14:50:28 +0000">&lt;p&gt;updated patch, shaves off another 5%&lt;/p&gt;

&lt;p&gt;my avg indexing throughput:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;cjkanalyzer: 3447k/s&lt;/li&gt;
	&lt;li&gt;orig smartcn: 1357k/s&lt;/li&gt;
	&lt;li&gt;patched smartcn: 1965k/s&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;there are serious memory consumption problems in the n^2 part of the algorithm (BiSegGraph), will see about improving it more.&lt;/p&gt;</comment>
                    <comment id="12772341" author="rcmuir" created="Sun, 1 Nov 2009 16:45:18 +0000">&lt;p&gt;create a generic graph that is reusable, used by both SegGraph and BiSegGraph.&lt;br/&gt;
This cleans up the code a lot and prevents billions of arraylists from being created in n^2 style.&lt;/p&gt;</comment>
                    <comment id="12772346" author="rcmuir" created="Sun, 1 Nov 2009 18:39:18 +0000">&lt;p&gt;in BiSegGraph, char[] was being created in n^2 fashion for each edge (SegTokenPair), even though its only used for weight calculation.&lt;br/&gt;
instead, add methods to BigramDictionary to get the frequency of a bigram: getFrequency(char left[], char right[]) without this silliness.&lt;/p&gt;

&lt;p&gt;new figures are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;cjkanalyzer: 3447k/s&lt;/li&gt;
	&lt;li&gt;orig smartcn: 1357k/s&lt;/li&gt;
	&lt;li&gt;patched smartcn: 2125k/s&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12772348" author="rcmuir" created="Sun, 1 Nov 2009 18:47:09 +0000">&lt;p&gt;Question, the smartcn internals are pkg private (and marked experimental to boot),&lt;br/&gt;
I'd like to keep this clean and theres some unused stuff that could now be deprecated or removed.&lt;/p&gt;

&lt;p&gt;should this be 3.0 or 3.1? should i deprecate or clean house (since its experimental and pkg private)?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                    <comment id="12772350" author="dmsmith" created="Sun, 1 Nov 2009 18:56:40 +0000">&lt;p&gt;Internals are internals. Anyone digging into smartcn's internals should be hog-tied and whipped. They can change at any moment and without warning. (IMHO)&lt;/p&gt;</comment>
                    <comment id="12772352" author="rcmuir" created="Sun, 1 Nov 2009 19:00:21 +0000">&lt;blockquote&gt;&lt;p&gt;Anyone digging into smartcn's internals should be hog-tied and whipped&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;oh no, I think I am in trouble then! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12772356" author="rcmuir" created="Sun, 1 Nov 2009 19:14:07 +0000">&lt;p&gt;i think this BiSegGraph might be a lot better as a 2D array of weights instead of complex object graph.&lt;br/&gt;
i'll try it and see what happens.&lt;/p&gt;</comment>
                    <comment id="12772522" author="rcmuir" created="Mon, 2 Nov 2009 14:37:00 +0000">&lt;p&gt;latest iteration, gets rid of SegTokenPair/PathNode.&lt;br/&gt;
BiSegGraph still isn't as simple or efficient as it should be,&lt;br/&gt;
but my indexing speed is up to 2400k/s &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12772569" author="rcmuir" created="Mon, 2 Nov 2009 17:43:53 +0000">&lt;p&gt;refactor a lot of this analyzer:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;move hhmm specific stuff (like WordType, CharType, Utility) into hhmm package&lt;/li&gt;
	&lt;li&gt;move/remove tokenfilter specific stuff (like lowercasing, full-width conversion) out of hhmm package (uses LowerCaseFilter, adds FullWidthFilter)&lt;/li&gt;
	&lt;li&gt;remove the stopwords list, it was full of various punctuation, all of which got converted by "SegTokenFilter" into a comma anyway. instead just don't emit punctuation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;to me, this refactoring makes the analyzer easier to debug. it also happens to improve performance (up to 2500k/s now)&lt;/p&gt;</comment>
                    <comment id="12772638" author="rcmuir" created="Mon, 2 Nov 2009 20:11:37 +0000">&lt;p&gt;fix WordTokenFilter to use Version, because if its not going to output delimiters thru stopFilter and then remove them, then it needs to adjust posInc (depending on version)&lt;/p&gt;</comment>
                    <comment id="12774750" author="rcmuir" created="Sun, 8 Nov 2009 09:48:43 +0000">&lt;p&gt;if no one objects I'd rather work this into 3.1, along with some refactoring of this code.&lt;/p&gt;

&lt;p&gt;DM Smith, if this causes you a problem I would rather just upload a java 1.4 patch for you to improve your performance than slip it into 3.0.  there was already a bug in 2.9 in this analyzer so I don't want to introduce a new one without having a lot of time to play with this code.&lt;/p&gt;</comment>
                    <comment id="12793044" author="mikemccand" created="Sun, 20 Dec 2009 21:21:46 +0000">&lt;p&gt;I think given that this package is brand new, and in contrib (no back compat promise unless explicitly stated), and Robert has some solid improvements, it should be fine to make non-back-compatible changes here.&lt;/p&gt;

&lt;p&gt;Also I think we should only do this for 3.1?  This is a big change... and it breaks back compat.  It shouldn't be backported to past releases?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12423848" name="LUCENE-2023.patch" size="83564" author="rcmuir" created="Mon, 2 Nov 2009 20:11:37 +0000"/>
                    <attachment id="12423833" name="LUCENE-2023.patch" size="77985" author="rcmuir" created="Mon, 2 Nov 2009 17:43:53 +0000"/>
                    <attachment id="12423822" name="LUCENE-2023.patch" size="39343" author="rcmuir" created="Mon, 2 Nov 2009 14:37:00 +0000"/>
                    <attachment id="12423781" name="LUCENE-2023.patch" size="23601" author="rcmuir" created="Sun, 1 Nov 2009 18:39:18 +0000"/>
                    <attachment id="12423778" name="LUCENE-2023.patch" size="18558" author="rcmuir" created="Sun, 1 Nov 2009 16:45:18 +0000"/>
                    <attachment id="12423776" name="LUCENE-2023.patch" size="14657" author="rcmuir" created="Sun, 1 Nov 2009 14:50:28 +0000"/>
                    <attachment id="12423713" name="LUCENE-2023.patch" size="10446" author="rcmuir" created="Fri, 30 Oct 2009 19:44:14 +0000"/>
                    <attachment id="12423694" name="LUCENE-2023.patch" size="10414" author="rcmuir" created="Fri, 30 Oct 2009 17:01:30 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 30 Oct 2009 17:36:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11751</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25696</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-2018] Reconsider boolean max clause exception</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2018</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Now that we have smarter multi-term queries, I think its time to reconsider the boolean max clause setting. It made more sense before, because you could hit it more unaware when the multi-term queries got huge - now its more likely that if it happens its because a user built the boolean themselves. And no duh thousands more boolean clauses means slower perf and more resources needed. We don't throw an exception when you try to use a ton of resources in a thousand other ways.&lt;/p&gt;

&lt;p&gt;The current setting also suffers from the static hell argument - especially when you consider something like Solr's multicore feature - you can have different settings for this in different cores, and the last one is going to win. Its ugly. Yes, that could be addressed better in Solr as well - but I still think it should be less ugly in Lucene as well.&lt;/p&gt;

&lt;p&gt;I'd like to consider either doing away with it, or raising it by quite a bit at the least. Or an alternative better solution. Right now, it aint so great.&lt;/p&gt;</description>
                <environment/>
            <key id="12439437">LUCENE-2018</key>
            <summary>Reconsider boolean max clause exception</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Thu, 29 Oct 2009 18:14:50 +0000</created>
                <updated>Fri, 10 May 2013 00:05:34 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12771547" author="mikemccand" created="Thu, 29 Oct 2009 18:44:00 +0000">&lt;p&gt;I think removing it makes sense?&lt;/p&gt;</comment>
                    <comment id="12771554" author="yseeley@gmail.com" created="Thu, 29 Oct 2009 18:48:04 +0000">&lt;p&gt;Fuzzy query uses it to limit terms.  Any others?&lt;/p&gt;</comment>
                    <comment id="12771558" author="markrmiller@gmail.com" created="Thu, 29 Oct 2009 18:55:32 +0000">&lt;p&gt;Ah, right - not the first time I've forgotten Fuzzy didnt get the new rewrite treatment.&lt;/p&gt;</comment>
                    <comment id="12787658" author="markrmiller@gmail.com" created="Tue, 8 Dec 2009 19:00:35 +0000">&lt;p&gt;I still think this should be removed - or moved to the MTQ query itself - then a setting on the queryparser could set it, or a user could set it. It shouldn't be a sys property, and I don't necessarily think it should be on by default either.&lt;/p&gt;</comment>
                    <comment id="12787670" author="mikemccand" created="Tue, 8 Dec 2009 19:12:57 +0000">&lt;blockquote&gt;&lt;p&gt;I still think this should be removed -&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12787674" author="thetaphi" created="Tue, 8 Dec 2009 19:16:10 +0000">&lt;p&gt;+1.&lt;/p&gt;

&lt;p&gt;And Fuzzy Query's PQ then would use the MTQ value as max size.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 29 Oct 2009 18:44:00 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2916</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25701</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1909] Make IndexReader.DEFAULT_TERMS_INDEX_DIVISOR public</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1909</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description/>
                <environment/>
            <key id="12435506">LUCENE-1909</key>
            <summary>Make IndexReader.DEFAULT_TERMS_INDEX_DIVISOR public</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="5" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.png">Trivial</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                    </labels>
                <created>Sat, 12 Sep 2009 13:13:22 +0100</created>
                <updated>Fri, 10 May 2013 00:05:34 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12776354" author="simonw" created="Wed, 11 Nov 2009 08:04:24 +0000">&lt;p&gt;Grant, are you going to change this for 3.0? If not we should move it out to 3.1. If we make it public the constant should be final though. I attached a patch.&lt;/p&gt;

&lt;p&gt;thanks, simon&lt;/p&gt;</comment>
                    <comment id="12776356" author="simonw" created="Wed, 11 Nov 2009 08:09:00 +0000">&lt;p&gt;patch with Javadoc&lt;/p&gt;</comment>
                    <comment id="12776359" author="thetaphi" created="Wed, 11 Nov 2009 08:10:34 +0000">&lt;p&gt;I think, we should have this in 3.0, it's not a big deal.&lt;/p&gt;</comment>
                    <comment id="12776394" author="mikemccand" created="Wed, 11 Nov 2009 09:49:12 +0000">&lt;p&gt;+1 for 3.0&lt;/p&gt;</comment>
                    <comment id="12776397" author="simonw" created="Wed, 11 Nov 2009 10:01:39 +0000">&lt;p&gt;@Grant: can somebody else take this?&lt;/p&gt;</comment>
                    <comment id="12776489" author="thetaphi" created="Wed, 11 Nov 2009 16:04:32 +0000">&lt;p&gt;Simon: Should I take &amp;amp; commit it?&lt;/p&gt;</comment>
                    <comment id="12776490" author="markrmiller@gmail.com" created="Wed, 11 Nov 2009 16:10:47 +0000">&lt;p&gt;umm ... call me crazy, but why are we making this public?&lt;/p&gt;</comment>
                    <comment id="12776492" author="gsingers" created="Wed, 11 Nov 2009 16:14:19 +0000">&lt;p&gt;Because it's nice to know what the default is and be able to refer to that w/o having to hardcode a magic number somewhere else in your app when it is already hardcoded here.&lt;/p&gt;</comment>
                    <comment id="12776494" author="thetaphi" created="Wed, 11 Nov 2009 16:18:49 +0000">&lt;p&gt;+1, I have the same opinion&lt;/p&gt;</comment>
                    <comment id="12776496" author="markrmiller@gmail.com" created="Wed, 11 Nov 2009 16:22:01 +0000">&lt;p&gt;Whats the use case?&lt;/p&gt;

&lt;p&gt;If if the default was different than 1, I'd still think this was weird - but ...&lt;/p&gt;

&lt;p&gt;But the default is 1 - load them all. Seems weird to me to broadcast this beyond documentation. It locks us into that field and I can't see the benefit myself. You have a specific use case that this makes sense for?&lt;/p&gt;</comment>
                    <comment id="12776497" author="markrmiller@gmail.com" created="Wed, 11 Nov 2009 16:23:09 +0000">&lt;p&gt;Okay - 2v1 I guess - but I think you guys have been eating too many of the crazy pills &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; This is a weird default to expose.&lt;/p&gt;</comment>
                    <comment id="12776974" author="thetaphi" created="Thu, 12 Nov 2009 12:38:32 +0000">&lt;p&gt;Hm, I only bought one 500 pills Aspirin package for 3$ in Oakland...&lt;/p&gt;</comment>
                    <comment id="12776981" author="mikemccand" created="Thu, 12 Nov 2009 12:57:24 +0000">&lt;p&gt;I sort of agree with Mark, now... the default is "don't subsample the terms index", so it is rather odd to make this public.&lt;/p&gt;

&lt;p&gt;I think it does make sense to have a private named constant for this (instead of "1" all throughout the code) for code readability.&lt;/p&gt;

&lt;p&gt;Yet, Lucene does do this "static final public default" constant in a number of places.&lt;/p&gt;

&lt;p&gt;My guess is this is a Solr need, right?  Ie, Solr is exposing control over terms index divisor, and would like to set a default for that param, and so the natural thing to do is wire it to Lucene's default.  Outside Solr, it's hard to imagine why apps would need to (should) refer to this constant.&lt;/p&gt;</comment>
                    <comment id="12776997" author="gsingers" created="Thu, 12 Nov 2009 13:49:09 +0000">&lt;p&gt;Boy, I had an awesome use case when I wrote this up, but am forgetting it now.  I think it was something like:&lt;/p&gt;

&lt;p&gt;If you are examining the index ahead of time, you could have heuristics about how much memory you want the system to use when loading a reader.  Thus, I could have a clause in my IndexReader code that did something like:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (index is big or memory is tight){
reader = IndexReader.open(..., largerTermsIndexDiv);
} &lt;span class="code-keyword"&gt;else&lt;/span&gt;{
reader = IndexReader.opent(..., DEFAULT_TERMS_INDEX_DIVISOR);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hard coding a 1 in that else clause seems like a bad thing to do given there is already a "default" value spec. by Lucene.&lt;/p&gt;

&lt;p&gt;Also, since it is package private, if I extend IndexReader, I won't have access to it.&lt;/p&gt;

&lt;p&gt;At the end of the day, it's not a big deal.  &lt;/p&gt;</comment>
                    <comment id="12777006" author="markrmiller@gmail.com" created="Thu, 12 Nov 2009 14:30:53 +0000">&lt;p&gt;I know its not a big deal - but I have an abrasive reputation to maintain &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;The default terms info divisor essentially means that feature is turned off. The 1 just means, don't sub sample the terms index. Its off.&lt;/p&gt;

&lt;p&gt;For your use case, I would do:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (index is big or memory is tight){
  reader = IndexReader.open(..., largerTermsIndexDiv);
} &lt;span class="code-keyword"&gt;else&lt;/span&gt;{
  reader = IndexReader.open(...);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you don't use that argument, you get the default. Whenever I see an issue to open up something in Lucene without a reason that makes sense to me, I think its a good idea to push back. If it really does make sense, it will make it through - but I don't think anything should get a free pass. This one is still not making sense to me.&lt;/p&gt;

&lt;p&gt;Your right - its not a big deal - but I still don't see a reason to open it up.&lt;/p&gt;</comment>
                    <comment id="12777008" author="tsmith" created="Thu, 12 Nov 2009 14:39:05 +0000">&lt;p&gt;I have the following use case:&lt;/p&gt;

&lt;p&gt;i have a configuration bean, this bean can be customized via xml at config time&lt;br/&gt;
in this bean, i expose the setting for the terms index divisor&lt;br/&gt;
so, my bean has to have a default value for this,&lt;/p&gt;

&lt;p&gt;right now, i just use 1 for the default value.&lt;br/&gt;
would be nice if i could just use the lucene constant instead of using 1, as the lucene constant could change in the future (not really likely, but its one less constant i have to maintain)&lt;/p&gt;

&lt;p&gt;if the default is not made public i have 2 options:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;use a hard coded constant in my code for the default value (doing this right now)&lt;/li&gt;
	&lt;li&gt;use an Integer object, and have null be the default&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;the nasty part about the second option is that i now have to do conditional opening of the reader depending on if null is the value (unset), when it would be much simpler (and easier for me to maintain), if i just always pass in that value&lt;/p&gt;</comment>
                    <comment id="12777010" author="markrmiller@gmail.com" created="Thu, 12 Nov 2009 14:51:41 +0000">&lt;p&gt;Okay - fair enough - there is a use case I won't try and argue.&lt;/p&gt;

&lt;p&gt;I will say that I don't see the default &lt;b&gt;ever&lt;/b&gt; changing - it would change on the IndexWriter side - I see this as an extra feature and 1 just means its off - but far be it from me to make an injectors life harder.&lt;/p&gt;</comment>
                    <comment id="12777057" author="markrmiller@gmail.com" created="Thu, 12 Nov 2009 17:03:17 +0000">&lt;p&gt;Eh - still bugging me. If you are allowing users to set this in XML, do you really want to hide what the setting is unless they dig into Lucene internal code? &lt;/p&gt;

&lt;p&gt;As a user, I'm thinking - oh, nice - I can change this - but what am I changing it from? Oh, I have to go dig into Lucene source to see. So you likely want to put in a comment or something saying what the default is - but then you should just put the value, else your still maintaining. It doesnt make sense to have a user go - uh - I'll change this from the unknown default to ... 7!&lt;/p&gt;

&lt;p&gt;This use case hasn't sold me yet after all either. Whatever though - I wouldn't -1 it. "Its not a big deal" is something I've already agreed with.&lt;/p&gt;</comment>
                    <comment id="12777064" author="tsmith" created="Thu, 12 Nov 2009 17:13:56 +0000">&lt;p&gt;users can see the live setting via things like JMX/admin ui&lt;br/&gt;
also, if i intend users to actually change the value regularly, i can provide user facing documentation that would go into detail without the user needing to dig further into lucene internals (memory tuning guide or something)&lt;br/&gt;
currently just exposing this setting myself as a SUPER ADVANCED setting (just in case it will need to be tuned for custom use cases in the future) (can't tune it if its not exposed in config)&lt;/p&gt;
</comment>
                    <comment id="12777065" author="yseeley@gmail.com" created="Thu, 12 Nov 2009 17:16:19 +0000">&lt;blockquote&gt;&lt;p&gt;My guess is this is a Solr need, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think so... IMO, Solr should do it like a normal user should do it - pass it to open().&lt;br/&gt;
IMO, exposing global config isn't needed or desirable (i.e. I agree with Mark)&lt;/p&gt;</comment>
                    <comment id="12777085" author="markrmiller@gmail.com" created="Thu, 12 Nov 2009 17:48:17 +0000">&lt;p&gt;A good admin UI should have this option off by default - ie no terms divisor set on the indexreader side. There is no need to display a number to the user - the option is not in effect by default. Thats always going to make sense. Unless you make your own default and then this is all moot anyway. If Lucene ever decided to change this type of thing, it would happen on the &lt;b&gt;index&lt;/b&gt; side. The admin UI should let you enable a terminfodivisor and set a value greater than 1, or turn of the terms index entirely. If a user hasnt enabled it, use the constructors/getters that don't have that param - if they enable it, use the constructors/getters that have that param.&lt;/p&gt;

&lt;p&gt;Not sold. Not sold for a need with JMX either.&lt;/p&gt;</comment>
                    <comment id="12777109" author="tsmith" created="Thu, 12 Nov 2009 18:39:16 +0000">&lt;p&gt;what you describe requires effectively 2 settings:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;custom term infos divisor enabled/disabled&lt;/li&gt;
	&lt;li&gt;configured value if enabled&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;this then results in more complexity in opening the index reader (conditional opening where a non-conditional open with the configured divisor would do the trick)&lt;br/&gt;
any admin ui would also require more conditional handling of displaying this setting (as you described) (i'm not displaying it other than in JMX now anyway, so it doesn't really matter for me, and JMX just has a readonly attribute that shows the configured value (1 by default))&lt;/p&gt;

&lt;p&gt;personally, i don't care too much if this constant is made public or not (would make it so i use that constant instead of defining my own with the same value), so it only saves me 1 line (and its not like the default will ever change from 1 in the lucene code anyway)&lt;/p&gt;
</comment>
                    <comment id="12777112" author="markrmiller@gmail.com" created="Thu, 12 Nov 2009 18:43:21 +0000">&lt;p&gt;heh - well, if you want to be lazy with the admin code, you can just plug in 1. Do you really need a Lucene constant to tell you that a divisor of 1 is no divisor?&lt;/p&gt;

&lt;p&gt;eg - Lucene is not using that default in a way that you guys are arguing users would want to - its using it to set the thing as off. Perhaps it shouldnt have default in its name - being internal, it kind of doesn't matter though. Really its just saying, the divisor is off. You don't really need to worry about what  the number is - its just avoiding a magic number in Lucene code.&lt;/p&gt;

&lt;p&gt;eg - there is no default number as far as the user should be concerned. The default is that the IndexReader divisor feature is off.&lt;/p&gt;</comment>
                    <comment id="12777116" author="thetaphi" created="Thu, 12 Nov 2009 18:53:26 +0000">&lt;p&gt;By the way, if you use a final constant, without recompiling it would never change, because even Java puts such constants directly as plain value into class files... &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; - so a change in the JAR file of a new Lucene Version would have no effect. See the problem with Constants.LUCENE_MAIN_VERSION (which is solved from 2.9.1 on)&lt;/p&gt;</comment>
                    <comment id="12777118" author="tsmith" created="Thu, 12 Nov 2009 18:56:46 +0000">&lt;p&gt;Only thing i would want the constant for is to known what the default divisor is. The default just happens to be 1 (no divisor/off).&lt;/p&gt;

&lt;p&gt;However (while unlikely) a new version of lucene could default to using a real divisor (maybe once everyone is on solid state disks, a higher divisor will result in the same speed of access, with less memory use), at which point, if i upgrade to a new version of lucene, i want to inherit that changed setting (as the default was selected by people that probably know better than me what will better server the general use of lucene in terms of memory and performance)&lt;/p&gt;

&lt;p&gt;right now, if i want to inherit the default i would have to do a conditional IndexReader.open() and store my setting as a pair (enabled/disabled, divisor), which could be encoded in an Integer object (null = disabled/use lucene default)&lt;/p&gt;

&lt;p&gt;if the constant is made public, its easier for me to inherit that default setting.&lt;br/&gt;
of course at the end of the day, either approach will only be about 5 lines of code difference, so again, i don't really care too much about the outcome of this&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;By the way, if you use a final constant, without recompiling it would never change,...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I never drop a new lucene in without recompiling (so that doesn't cause any difference for me)&lt;/p&gt;</comment>
                    <comment id="12777127" author="markrmiller@gmail.com" created="Thu, 12 Nov 2009 19:07:18 +0000">&lt;p&gt;If you want to inherit the setting, use the correct constructor &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Lucene doesn't advertise a default setting now, because the default is off - you only know we are using a 1 divisor because you are peeking. We could switch to not using that constant at all right now. Nice to have flexibility.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However (while unlikely) a new version of lucene could default to using a real divisor (maybe once everyone is on solid state disks, a higher divisor will result in the same speed of access, with less memory use)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We wouldn't change it there by default. Thats a waste on the index size side. We would change the IndexWriter divisor. We wouldn't waste index size on disk for most people just so some people could lower it back to 1 to get there speed back. I'd -1 the heck out of that. There is no default for you to inherit in any case - thats an impl detail. By the default the feature is off. You can't inherit anything about it.&lt;/p&gt;
</comment>
                    <comment id="12777136" author="tsmith" created="Thu, 12 Nov 2009 19:20:13 +0000">&lt;blockquote&gt;&lt;p&gt;If you want to inherit the setting, use the correct constructor &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;agreed, just a tiny bit of more complexity on my side for that (but its so insignificant that it doesn't really matter, and is really not even worth arguing over)&lt;/p&gt;

&lt;p&gt;if the constant was public, i'd use it, if not, no worries (for me at least)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;By the default the feature is off. You can't inherit anything about it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ideally, i want to inherit that the feature is off by default, then allow config to turn it on (by providing a value greater than one for this setting, or just 1 to allow config to explicitly disable)&lt;br/&gt;
using the constructor with no divisor does this (i just need to call the constructor conditionally depending on if the setting was explicitly configured), thats easy and is no problem to do at all (just a couple of extra lines of code in my app layer)&lt;/p&gt;
</comment>
                    <comment id="12777239" author="thetaphi" created="Thu, 12 Nov 2009 22:33:08 +0000">&lt;p&gt;I think I move this to 3.1 and then we have enough time to close it or not.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12424577" name="LUCENE_1909.patch" size="1136" author="simonw" created="Wed, 11 Nov 2009 08:09:00 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 11 Nov 2009 08:04:24 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11858</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25810</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1890] auto-warming from Apache Solr causes NULL Pointer</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1890</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Sep 6, 2009 12:48:07 PM org.apache.solr.common.SolrException log&lt;br/&gt;
SEVERE: Error during auto-warming of key:org.apache.solr.search.QueryResultKey@b00371eb:java.lang.NullPointerException&lt;br/&gt;
        at org.apache.lucene.spatial.tier.DistanceFieldComparatorSource$DistanceScoreDocLookupComparator.copy(DistanceFieldComparatorSource.java:101)&lt;br/&gt;
        at org.apache.lucene.search.TopFieldCollector$MultiComparatorScoringMaxScoreCollector.collect(TopFieldCollector.java:554)&lt;br/&gt;
        at org.apache.solr.search.DocSetDelegateCollector.collect(DocSetHitCollector.java:98)&lt;br/&gt;
        at org.apache.lucene.search.IndexSearcher.doSearch(IndexSearcher.java:281)&lt;br/&gt;
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:253)&lt;br/&gt;
        at org.apache.lucene.search.Searcher.search(Searcher.java:171)&lt;br/&gt;
        at org.apache.solr.search.SolrIndexSearcher.getDocListAndSetNC(SolrIndexSearcher.java:1088)&lt;br/&gt;
        at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:876)&lt;br/&gt;
        at org.apache.solr.search.SolrIndexSearcher.access$000(SolrIndexSearcher.java:53)&lt;br/&gt;
        at org.apache.solr.search.SolrIndexSearcher$3.regenerateItem(SolrIndexSearcher.java:328)&lt;br/&gt;
        at org.apache.solr.search.LRUCache.warm(LRUCache.java:194)&lt;br/&gt;
        at org.apache.solr.search.SolrIndexSearcher.warm(SolrIndexSearcher.java:1468)&lt;br/&gt;
        at org.apache.solr.core.SolrCore$3.call(SolrCore.java:1142)&lt;br/&gt;
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;</description>
                <environment>&lt;p&gt;Linux&lt;/p&gt;</environment>
            <key id="12434954">LUCENE-1890</key>
            <summary>auto-warming from Apache Solr causes NULL Pointer</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="5">Cannot Reproduce</resolution>
                                <assignee username="gbowyer@fastmail.co.uk">Greg Bowyer</assignee>
                                <reporter username="billnbell">Bill Bell</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Sun, 6 Sep 2009 20:39:25 +0100</created>
                <updated>Mon, 13 May 2013 04:32:42 +0100</updated>
                    <resolved>Mon, 13 May 2013 04:32:42 +0100</resolved>
                            <version>2.4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/spatial</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12751988" author="markrmiller@gmail.com" created="Mon, 7 Sep 2009 01:50:28 +0100">&lt;p&gt;it looks like the distanceFilter is getting set to null - clear could do that, but I don't see anywhere it would be called ... not sure what Solr code you using though - just checked one of the later patches in JIRA. From the Lucene end, I don't see how you could get a null there unless you inited your DistanceFieldComparatorSource with a null filter.&lt;/p&gt;</comment>
                    <comment id="12752961" author="billnbell" created="Wed, 9 Sep 2009 08:29:18 +0100">&lt;p&gt;I get several of these errors. I am using SOLR with LOCAL LUCENE. It is a version from a few months back. I am including solr.war and the local solr in the hope that it might help debug the issue. I tried to run the latest SOLR and LOCAL SOLR, but I am unable to get it to work without several errors.&lt;/p&gt;

&lt;p&gt;Here is the output from /system:&lt;br/&gt;
&amp;lt;response&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="responseHeader"&amp;gt;&lt;br/&gt;
&amp;lt;int name="status"&amp;gt;0&amp;lt;/int&amp;gt;&lt;br/&gt;
&amp;lt;int name="QTime"&amp;gt;117&amp;lt;/int&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="core"&amp;gt;&lt;br/&gt;
&amp;lt;str name="schema"&amp;gt;kaango-0.9.0&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="host"&amp;gt;domU-12-31-39-03-78-41.compute-1.internal&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;date name="now"&amp;gt;2009-09-09T07:26:18.959Z&amp;lt;/date&amp;gt;&lt;br/&gt;
&amp;lt;date name="start"&amp;gt;2009-09-07T07:22:13.146Z&amp;lt;/date&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="directory"&amp;gt;&lt;br/&gt;
&amp;lt;str name="instance"&amp;gt;/vol/solr/core0&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="data"&amp;gt;/vol/solr/core0/data&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="index"&amp;gt;/mnt/solr/core0/data/index&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="lucene"&amp;gt;&lt;br/&gt;
&amp;lt;str name="solr-spec-version"&amp;gt;1.3.0.2009.06.01.20.24.48&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="solr-impl-version"&amp;gt;1.4-dev exported - tomcat - 2009-06-01 20:24:48&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="lucene-spec-version"&amp;gt;2.9-dev&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="lucene-impl-version"&amp;gt;2.9-dev 779312 - 2009-05-27 17:19:55&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="jvm"&amp;gt;&lt;br/&gt;
&amp;lt;str name="version"&amp;gt;10.0-b19&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="name"&amp;gt;Java HotSpot(TM) 64-Bit Server VM&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;int name="processors"&amp;gt;2&amp;lt;/int&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="memory"&amp;gt;&lt;br/&gt;
&amp;lt;str name="free"&amp;gt;4 GB&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="total"&amp;gt;6 GB&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="max"&amp;gt;6 GB&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="used"&amp;gt;1 GB (%23)&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="jmx"&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;str name="bootclasspath"&amp;gt;&lt;br/&gt;
/usr/java/jdk1.6.0_04/jre/lib/resources.jar:/usr/java/jdk1.6.0_04/jre/lib/rt.jar:/usr/java/jdk1.6.0_04/jre/lib/sunrsasign.jar:/usr/java/jdk1.6.0_04/jre/lib/jsse.jar:/usr/java/jdk1.6.0_04/jre/lib/jce.jar:/usr/java/jdk1.6.0_04/jre/lib/charsets.jar:/usr/java/jdk1.6.0_04/jre/classes&lt;br/&gt;
&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="classpath"&amp;gt;:/vol/tomcat/bin/bootstrap.jar&amp;lt;/str&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;arr name="commandLineArgs"&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Xmx6500m&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Xms6500m&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Djava.awt.headless=true&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Dsolr.solr.home=/vol/solr/&amp;lt;/str&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;str&amp;gt;&lt;br/&gt;
-Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager&lt;br/&gt;
&amp;lt;/str&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;str&amp;gt;&lt;br/&gt;
-Djava.util.logging.config.file=/vol/tomcat/conf/logging.properties&lt;br/&gt;
&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Djava.endorsed.dirs=/vol/tomcat/endorsed&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Dcatalina.base=/vol/tomcat&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Dcatalina.home=/vol/tomcat&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str&amp;gt;-Djava.io.tmpdir=/vol/tomcat/temp&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;/arr&amp;gt;&lt;br/&gt;
&amp;lt;date name="startTime"&amp;gt;2009-09-07T07:22:10.29Z&amp;lt;/date&amp;gt;&lt;br/&gt;
&amp;lt;long name="upTimeMS"&amp;gt;173048669&amp;lt;/long&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
−&lt;br/&gt;
&amp;lt;lst name="system"&amp;gt;&lt;br/&gt;
&amp;lt;str name="name"&amp;gt;Linux&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="version"&amp;gt;2.6.21.7-2.fc8xen&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="arch"&amp;gt;amd64&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;double name="systemLoadAverage"&amp;gt;0.3&amp;lt;/double&amp;gt;&lt;br/&gt;
&amp;lt;str name="uname"&amp;gt;(error executing: uname -a)&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="ulimit"&amp;gt;(error executing: ulimit -n)&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;str name="uptime"&amp;gt;(error executing: uptime)&amp;lt;/str&amp;gt;&lt;br/&gt;
&amp;lt;/lst&amp;gt;&lt;br/&gt;
&amp;lt;/response&amp;gt;&lt;/p&gt;</comment>
                    <comment id="12752962" author="billnbell" created="Wed, 9 Sep 2009 08:32:59 +0100">&lt;p&gt;SOLR WAR&lt;/p&gt;</comment>
                    <comment id="12752964" author="billnbell" created="Wed, 9 Sep 2009 08:35:41 +0100">&lt;p&gt;Url for the solr.war: &lt;a href="http://ams.kaango.com.s3.amazonaws.com/solr.war" class="external-link"&gt;http://ams.kaango.com.s3.amazonaws.com/solr.war&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;SEVERE: Error during auto-warming of key:org.apache.solr.search.QueryResultKey@b9e3312a:java.lang.NullPointerException&lt;/p&gt;

&lt;p&gt;Sep 9, 2009 12:00:08 AM org.apache.solr.common.SolrException log&lt;br/&gt;
SEVERE: Error during auto-warming of key:org.apache.solr.search.QueryResultKey@9d1b3222:java.lang.NullPointerException&lt;/p&gt;

&lt;p&gt;Sep 9, 2009 12:00:08 AM org.apache.solr.common.SolrException log&lt;br/&gt;
SEVERE: Error during auto-warming of key:org.apache.solr.search.QueryResultKey@58d1adf1:java.lang.NullPointerException&lt;/p&gt;

&lt;p&gt;Sep 9, 2009 12:00:09 AM org.apache.solr.common.SolrException log&lt;br/&gt;
SEVERE: Error during auto-warming of key:org.apache.solr.search.QueryResultKey@26a73b9b:java.lang.NullPointerException&lt;/p&gt;

&lt;p&gt;Sep 9, 2009 12:00:09 AM org.apache.solr.common.SolrException log&lt;br/&gt;
SEVERE: Error during auto-warming of key:org.apache.solr.search.QueryResultKey@e2d600b1:java.lang.NullPointerException&lt;/p&gt;

</comment>
                    <comment id="13655735" author="gbowyer@fastmail.co.uk" created="Mon, 13 May 2013 04:32:42 +0100">&lt;p&gt;I am going to be bold and make the assumption that, since spatial has been re-worked and Lucene has gone from 2.x -&amp;gt; 4.x this issue is no longer present.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12419025" name="localsolr.jar" size="28116" author="billnbell" created="Wed, 9 Sep 2009 08:32:59 +0100"/>
                    <attachment id="12419026" name="lucene-spatial-2.9-dev.jar" size="59102" author="billnbell" created="Wed, 9 Sep 2009 08:32:59 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 7 Sep 2009 00:50:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2893</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25831</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-1879] Parallel incremental indexing</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1879</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;A new feature that allows building parallel indexes and keeping them in sync on a docID level, independent of the choice of the MergePolicy/MergeScheduler.&lt;/p&gt;

&lt;p&gt;Find details on the wiki page for this feature:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://wiki.apache.org/lucene-java/ParallelIncrementalIndexing" class="external-link"&gt;http://wiki.apache.org/lucene-java/ParallelIncrementalIndexing&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Discussion on java-dev:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://markmail.org/thread/ql3oxzkob7aqf3jd" class="external-link"&gt;http://markmail.org/thread/ql3oxzkob7aqf3jd&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12434404">LUCENE-1879</key>
            <summary>Parallel incremental indexing</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Mon, 31 Aug 2009 09:00:49 +0100</created>
                <updated>Fri, 10 May 2013 00:05:34 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                        <due/>
                    <votes>4</votes>
                        <watches>9</watches>
                                                    <comments>
                    <comment id="12749419" author="michaelbusch" created="Mon, 31 Aug 2009 09:14:18 +0100">&lt;p&gt;I have a prototype version which I implemented in IBM; it contains a version that works on Lucene 2.4.1. I'm not planning on committing as is, because it is implemented on top of Lucene's APIs without any core change and therefore not as efficiently as it could be. The software grant I have lists these files. Shall I attach the tar + md5 here and send the signed software grant to you, Grant? &lt;/p&gt;</comment>
                    <comment id="12751077" author="gsingers" created="Thu, 3 Sep 2009 17:27:02 +0100">&lt;p&gt;Yes on the soft. grant.&lt;/p&gt;</comment>
                    <comment id="12758670" author="michaelbusch" created="Wed, 23 Sep 2009 12:47:18 +0100">&lt;p&gt;MD5 (parallel_incremental_indexing.tar) = b9a92850ad83c4de2dd2f64db2dcceab&lt;br/&gt;
md5 computed on Mac OS 10.5.7&lt;/p&gt;

&lt;p&gt;This tar file contains all files listed in the software grant. It is a prototype that works with Lucene 2.4.x only, not with current trunk.&lt;br/&gt;
It also has some limitations mentioned before, which are not limitations of the design, but rather because it runs on top of Lucene's APIs (I wanted the code to run with an unmodified Lucene jar).&lt;/p&gt;

&lt;p&gt;Next I'll work on a patch that runs with current trunk.&lt;/p&gt;</comment>
                    <comment id="12773466" author="mikemccand" created="Wed, 4 Nov 2009 12:35:07 +0000">&lt;p&gt;I wonder if we could change Lucene's index format to make this feature&lt;br/&gt;
simpler to implement...&lt;/p&gt;

&lt;p&gt;Ie, you're having to go to great lengths (since this is built&lt;br/&gt;
"outside" of Lucene's core) to force multiple separate indexes to&lt;br/&gt;
share everything but the postings files (merge choices, flush,&lt;br/&gt;
deletions files, segments files, turning off the stores, etc.).&lt;/p&gt;

&lt;p&gt;What if we could invert this approach, so that we use only single&lt;br/&gt;
index/IndexWriter, but we allow "partitioned postings", where sets of&lt;br/&gt;
fields are mapped to different postings files in the segment?&lt;/p&gt;

&lt;p&gt;Whenever a doc is indexed, postings from the fields are then written&lt;br/&gt;
according to this partition.  Eg if I map "body" to partition 1, and&lt;br/&gt;
"title" to partition 2, then I'd have two sets of postings files for&lt;br/&gt;
each segment.&lt;/p&gt;

&lt;p&gt;Could something like this work?&lt;/p&gt;</comment>
                    <comment id="12773663" author="michaelbusch" created="Wed, 4 Nov 2009 22:13:30 +0000">&lt;p&gt;I realize the current implementation that's attached here is quite&lt;br/&gt;
complicated, because it works on top of Lucene's APIs.&lt;/p&gt;

&lt;p&gt;However, I really like its flexibility. You can right now easily&lt;br/&gt;
rewrite certain parallel indexes without touching others. I use it in&lt;br/&gt;
quite different ways. E.g you can easily load one parallel index into a&lt;br/&gt;
RAMDirectory or SSD and leave the other ones on the conventional disk.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2025" title="Ability to turn off the store for an index"&gt;LUCENE-2025&lt;/a&gt; only optimizes a certain use case of the parallel indexing,&lt;br/&gt;
where you want to (re)write a parallel index containing &lt;b&gt;only&lt;/b&gt; posting&lt;br/&gt;
lists and this will especially improve scenarios like Yonik pointed&lt;br/&gt;
out a while ago on java-dev where you want to update only a few&lt;br/&gt;
documents, not e.g. a certain field for all documents.&lt;/p&gt;

&lt;p&gt;In other use cases it is certainly desirable to have a parallel index&lt;br/&gt;
that contains a store. It really depends on what data you want to&lt;br/&gt;
update individually.&lt;/p&gt;

&lt;p&gt;The version of parallel indexing that goes into Lucene's core I&lt;br/&gt;
envision quite differently from the current patch here. That's why I'd&lt;br/&gt;
like to refactor the IndexWriter (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2026" title="Refactoring of IndexWriter"&gt;LUCENE-2026&lt;/a&gt;) into SegmentWriter and&lt;br/&gt;
let's call it IndexManager (the component that controls flushing,&lt;br/&gt;
merging, etc.). You can then have a ParallelSegmentWriter, which&lt;br/&gt;
partitions the data into parallel segments, and the IndexManager can&lt;br/&gt;
behave the same way as before.&lt;/p&gt;

&lt;p&gt;You can keep thinking about the whole index as a collection of segments,&lt;br/&gt;
just now it will be a matrix of segments instead of a one-dimensional&lt;br/&gt;
list.&lt;/p&gt;

&lt;p&gt;E.g. the norms could in the future be a parallel segment with a single&lt;br/&gt;
column-stride field that you can update by writing a new generation of&lt;br/&gt;
the parallel segment.&lt;/p&gt;

&lt;p&gt;Things like two-dimensional merge policies will nicely fit into this&lt;br/&gt;
model.&lt;/p&gt;

&lt;p&gt;Different SegmentWriter implementations will allow you to write single&lt;br/&gt;
segments in different ways, e.g. doc-at-a-time (the default one with&lt;br/&gt;
addDocument()) or term-at-a-time (like addIndexes*() works).&lt;/p&gt;

&lt;p&gt;So I agree we can achieve updating posting lists the way you describe,&lt;br/&gt;
but it will be limited to posting lists then. If we allow (re)writing&lt;br/&gt;
&lt;b&gt;segments&lt;/b&gt; in both dimensions I think we will create a more flexible&lt;br/&gt;
approach which is independent on what data structures we add to Lucene&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;as long as they are not global to the index but per-segment as most&lt;br/&gt;
of Lucene's structures are today.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What do you think? Of course I don't want to over-complicate all this,&lt;br/&gt;
but if we can get &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2026" title="Refactoring of IndexWriter"&gt;LUCENE-2026&lt;/a&gt; right, I think we can implement parallel&lt;br/&gt;
indexing in this segment-oriented way nicely.&lt;/p&gt;</comment>
                    <comment id="12774265" author="mikemccand" created="Fri, 6 Nov 2009 10:16:02 +0000">&lt;p&gt;This sounds great!  In fact your proposal for a ParallelSegmentWriter&lt;br/&gt;
is just like what I'm picturing &amp;#8211; making the switching "down low"&lt;br/&gt;
instead of "up high" (above Lucene).  This'd be more generic than just&lt;br/&gt;
the postings files, since all index files can be separately written.&lt;/p&gt;

&lt;p&gt;It'd then a low-level question of whether ParallelSegmentWriter stores&lt;br/&gt;
its files in different Directories, or, a single directory with&lt;br/&gt;
different file names (or maybe sub-directories within a directory, or,&lt;br/&gt;
something else).  It could even use FileSwitchDirectory, eg to direct&lt;br/&gt;
certain segment files to an SSD (another way to achieve your example).&lt;/p&gt;

&lt;p&gt;This should also fit well into &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1458" title="Further steps towards flexible indexing"&gt;&lt;del&gt;LUCENE-1458&lt;/del&gt;&lt;/a&gt; (flexible indexing) &amp;#8211; one&lt;br/&gt;
of the added test cases there creates a per-field codec wrapper that&lt;br/&gt;
lets you use a different codec per field.  Right now, this means&lt;br/&gt;
separate file names in the same Directory for that segment, but we&lt;br/&gt;
could allow the codecs to use different Directories (or, FSD as well)&lt;br/&gt;
if they wanted to.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Different SegmentWriter implementations will allow you to write single&lt;br/&gt;
segments in different ways, e.g. doc-at-a-time (the default one with&lt;br/&gt;
addDocument()) or term-at-a-time (like addIndexes*() works).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you elaborate on this?  How is addIndexes* term-at-a-time?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If we allow (re)writing segments in both dimensions I think we will&lt;br/&gt;
create a more flexible approach which is independent on what data&lt;br/&gt;
structures we add to Lucene&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Dimension 1 is the docs, and dimension 2 is the assignment of fields&lt;br/&gt;
into separate partitions?&lt;/p&gt;</comment>
                    <comment id="12774329" author="michaelbusch" created="Fri, 6 Nov 2009 17:42:56 +0000">&lt;blockquote&gt;
&lt;p&gt;This sounds great! In fact your proposal for a ParallelSegmentWriter&lt;br/&gt;
is just like what I'm picturing - making the switching "down low"&lt;br/&gt;
instead of "up high" (above Lucene). This'd be more generic than just&lt;br/&gt;
the postings files, since all index files can be separately written. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.  The goal should it be to be able to use this for updating Lucene internal things (like norms, column-stride fields), but also giving advanced users APIs, so that they can partition their data into parallel indexes according to their update requirements (which the current "above Lucene" approach allows).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;t'd then a low-level question of whether ParallelSegmentWriter stores&lt;br/&gt;
its files in different Directories, or, a single directory with&lt;br/&gt;
different file names (or maybe sub-directories within a directory, or,&lt;br/&gt;
something else). It could even use FileSwitchDirectory, eg to direct&lt;br/&gt;
certain segment files to an SSD (another way to achieve your example).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Exactly! We should also keep the distributed indexing use case in mind here. It could make sense for systems like Katta to not only shard in the document direction.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This should also fit well into &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1458" title="Further steps towards flexible indexing"&gt;&lt;del&gt;LUCENE-1458&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds great!&lt;/p&gt;</comment>
                    <comment id="12774338" author="michaelbusch" created="Fri, 6 Nov 2009 17:56:35 +0000">&lt;blockquote&gt;
&lt;p&gt;Can you elaborate on this? How is addIndexes* term-at-a-time?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let's say we have an index 1 with two fields a and b and you want to create a new parallel index 2 in which you copy all posting lists of field b. You can achieve this by using addDocument(), if you iterate on all posting lists in 1b in parallel and create for each document in 1 a corresponding document in 2 that contains the terms of the postings lists from 1b that have a posting for the current document. This I called the "document-at-a-time approach".&lt;/p&gt;

&lt;p&gt;However, this is terribly slow (I tried it out), because of all the posting lists you perform I/O on in parallel. It's far more efficient to copy an entire posting list over from 1b to 2, because then you only perform sequential I/O. And if you use 2.addIndexes(IndexReader(1b)), then exactly this happens, because addIndexes(IndexReader) uses the SegmentMerger to add the index. The SegmentMerger iterates the dictionary and consumes the posting lists sequentially. That's why I called this "term-at-a-time approach". In my experience this is for a similar use case as the one I described here orders of magnitudes more efficient. My doc-at-a-time algorithm ran ~20 hours, the term-at-a-time one 8 minutes! The resulting indexes were identical.&lt;/p&gt;</comment>
                    <comment id="12774340" author="michaelbusch" created="Fri, 6 Nov 2009 17:58:18 +0000">&lt;blockquote&gt;
&lt;p&gt;Dimension 1 is the docs, and dimension 2 is the assignment of fields&lt;br/&gt;
into separate partitions?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, dimension 1 is unambiguously the docs. Dimension 2 can be the fields into separate parallel indexes, or also what we call today generations for e.g. the norms files.  &lt;/p&gt;</comment>
                    <comment id="12841078" author="shaie" created="Thu, 4 Mar 2010 06:18:26 +0000">&lt;p&gt;(Warning, this post is long, and is easier to read in JIRA)&lt;/p&gt;

&lt;p&gt;I've investigated the attached code a lot and I'd like to propose a different design and approach to this whole Parallel Index solution. I'll start by describing the limitations of the current design (whether its the approach or the code is debatable):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Lucene is not built/designed properly to a Master/Slave architecture, where different indexes share important files with others (such as segments_N, segments.gen and .del).
	&lt;ul&gt;
		&lt;li&gt;I've realized this when I found that if tests (in this patch) are run with "-ea", there are many assert exceptions that are printed from IndexWriter.startCommit. The reason is the Master just updated one of the segments .del generation (and deleted the previous one), but the Slave is not aware of that yet and looks for the wrong .del file. While this does not run on production (e.g. "-ea" is usually not activated), it does affect the tests because the assertion stops operations abruptly.&lt;/li&gt;
		&lt;li&gt;Though someone can claim we can fix that, I think it points at a problem in the design, and makes the whole solution fragile.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;I think it'd be really neat to introduce a ParallelWriter, equivalent to ParallelReader. The latter does not have a Master/Slave notion and so I don't think PW should have.&lt;/li&gt;
	&lt;li&gt;When I inspected the code carefully, I realized there are lots of hoola hoops done in order to make the Master and Slave in sync. Such hoola hoops may be resolved if Lucene's IW API would be more extensible, but still:
	&lt;ul&gt;
		&lt;li&gt;The MergePolicy used is one that for the Slaves never checks the segments for which merges should actually be done. Rather, it relies on the Master policy to set the proper merges. Which is a must in this design because only the master needs to decide when to merge.&lt;/li&gt;
		&lt;li&gt;However, and here I think it's because of lack of API on IW, the way the merge is done is that the master first calls mergeInit(merge), then on all slaves .maybeMerge() and then it merges that merge. maybeMerge() on the slaves consume all the merges that were decided to be run by the master, while when that finished, the master didn't finish even one merge ...&lt;/li&gt;
		&lt;li&gt;That works though because the MergeScheduler used is a Serial one (not SMS but still Serial) and blocking. However that leads to inconsistencies - slaves' segments view is different at one point in time from the master's.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;The current approach does not support multi-threaded indexing, but I think that's a limitation that could be solved by exposing some API on IW or DW.&lt;/li&gt;
	&lt;li&gt;Only SMS is supported on the slaves.&lt;/li&gt;
	&lt;li&gt;Optimize, expungeDeletes are unsupported. Though the could and perhaps just not implemented.&lt;/li&gt;
	&lt;li&gt;The current approach prevents having an architecture on which some of the parallels reside on different machines, because they share the .del and segments file with the master. It's not the worse limitation in the world, but still a limitation (of having any chance to do it efficiently) I'd like to avoid.&lt;/li&gt;
	&lt;li&gt;And I'm sure there are more disadvantages that I don't remember now.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I'd like to point out that even if the above limitations can be worked around, I still think the Master and Slave notion is not the best approach. At least, I'd like to propose a different approach:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Introduce a ParallelWriter which serves as a manager/orchestrator on top of other IWs. It is not a pure decorator because it drives everything that happens on the IWs, but it does not contain any actual indexing logic (e.g. add/delete/update documents).
	&lt;ul&gt;
		&lt;li&gt;The IWs PW will manage will be named hereinafter Slices.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;IW will expose enough API to perform two-phase operations, like the two-phase commit one can achieve today. Example operations (and I don't cover all for the interest of space):
	&lt;ul&gt;
		&lt;li&gt;addDocument - first obtain a doc ID, then proceed w/ addDocument on all Slices&lt;/li&gt;
		&lt;li&gt;optimize - already exists&lt;/li&gt;
		&lt;li&gt;merge - do the merge on all Slices and stamp it after all finished.&lt;/li&gt;
		&lt;li&gt;deleteDocuments - here we would need to expose some API on IW for DW to get an IndexReader so that IW can still return its readerPool.getReader but PW will return a ParallelSegmentReader or something, to perform the deletes across all Slices.&lt;/li&gt;
		&lt;li&gt;The idea is that we should do enough on the Slices so that if one fails we can still rollback, and the final 'stamp' process will be very fast and less likely to fail.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;For correctness and protectiveness, PW will only accept a Directory and not IW. Reason is:
	&lt;ul&gt;
		&lt;li&gt;We want all sorts of settings like MP, MS, RAM buffer usage to be controlled by PW and not on the Slices. If we allow to pass an IW instance, one could override whatever we set, which is wrong.&lt;/li&gt;
		&lt;li&gt;Even though one could claim that someone 'can shoot himself in the leg freely', I think that we should be resilient enough to protect stupid users from themselves.&lt;/li&gt;
		&lt;li&gt;We will need to allow to pass in an IW Configuration object, so that we can still account for settings such as Analyzer, MaxFieldLength etc., but discard other settings which PW will control directly
		&lt;ul&gt;
			&lt;li&gt;Such Configuration was proposed in the past already and will eliminate lots of methods on IW and ctors.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;On a side note, ParallelReader accepts IR today, which can lead the problems such as one passes two IRs, one read-only and one not, and then deletes documents by the writable IR, with PR not knowing about it. But it's a different issue, and I'll open a separate one for that.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;A special MergeScheduler and MergePolicy will be introduced to allow PW to drive merges across the Slices. The idea is to support whatever MS/MP the application wants (SMS, CMS, custom), and ensuring that when MP decides a merge should be performed, that merge is executed by MS across all Slices. Few things:
	&lt;ul&gt;
		&lt;li&gt;I think that a special MP is not needed, only MS. But need to validate that. If that's true, then apps could use their own custom MPs freely.&lt;/li&gt;
		&lt;li&gt;I think custom MS may be supported ... all that's required is for the MS to run on PW and whenever it calls its merge(), let PW run the merges across all Slices? But I still need to validate that code.&lt;/li&gt;
		&lt;li&gt;CMS can introduce two-level concurrency. One like today which executes different merges decided by MP concurrently. The other would control the concurrency level those merges are executed on the Slices.
		&lt;ul&gt;
			&lt;li&gt;Hmm ... even SMS can benefit from that ...&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I realize that accepting only Directory on PW might limit applications who want to pass in their own IW extension, for whatever reason. But other than saying "if you pass in IW and configure it afterwards, it's on your head", I don't think there is any other option ... Well maybe except if we expose a package-private API for PW to turn off configuration on an IW after it set it, so successive calls to the underlying IW's setters will throw an exception ... hmm might be doable. I'll look into that. If that will work, we might want to do the same for the ParallelReader as well.&lt;/p&gt;

&lt;p&gt;Michael mentioned a scenario above where one would want to rebuild an index Slice. That's still achievable by this design - one should build the IW on the outside and then replace the Directory instance on PW. We'll need to expose such API as well.&lt;/p&gt;

&lt;p&gt;BTW, some of the things I've mentioned can be taken care of in different issues, as follow on improvements, such as two-level concurrency, supporting custom MS etc. I've detailed them here just so we all see the bigger picture that's going on in my head.&lt;/p&gt;

&lt;p&gt;I think I wrote all (or most) of the high-level details. I'd like to start implementing this soon. In my head it's all chewed and digested, so I feel I can start implementing today. If possible, I'd like to get this out in 3.1. I'll try to break this issue down to as many issues as I can, to make the contributions containable. We should just keep in mind for each such issue the larger picture it solves.&lt;/p&gt;

&lt;p&gt;I'd appreciate your comments.&lt;/p&gt;</comment>
                    <comment id="12844168" author="mikemccand" created="Thu, 11 Mar 2010 18:56:10 +0000">&lt;p&gt;I like the ParallelWriter (index slices) approach!&lt;/p&gt;

&lt;p&gt;It sounds quite feasible and more "direct" in how the PW controls each&lt;br/&gt;
sub writer.  It should be as simple as setting null merge&lt;br/&gt;
policy/scheduler on the subs would mean they do no merging themselves,&lt;br/&gt;
but then the PW invokes their .merge methods to explicitly merge at&lt;br/&gt;
the right times.  Vs the current approach that makes "faker" merge&lt;br/&gt;
policy/scheduler (I think?).&lt;/p&gt;

&lt;p&gt;Some of this will require IW to open up some APIs &amp;#8211; eg making docID&lt;br/&gt;
assignment a separate method call.  Likely many of these will just be&lt;br/&gt;
protected APIs w/in IW.&lt;/p&gt;</comment>
                    <comment id="12850268" author="michaelbusch" created="Fri, 26 Mar 2010 18:09:44 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-2324" title="Per thread DocumentsWriters that write their own private segments"&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt; will be helpful to support multi-threaded parallel-indexing.  If we have single-threaded DocumentsWriters, then it should be easy to have a ParallelDocumentsWriter? &lt;/p&gt;</comment>
                    <comment id="12850313" author="shaie" created="Fri, 26 Mar 2010 19:21:03 +0000">&lt;p&gt;The way I planned to support multi-threaded indexing is to do a two-phase addDocument. First, allocate a doc ID from DocumentsWriter (synchronized) and then add the Document to each Slice with that doc ID. DocumentsWriter was not suppose to know it is a parallel index ... something like the following.&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-object"&gt;int&lt;/span&gt; docId = obtainDocId();
&lt;span class="code-keyword"&gt;for&lt;/span&gt; (IndexWriter slice : slices) {
  slice.addDocument(docId, Document);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That allows ParallelWriter to be really an orchestrator/manager of all slices, while each slice can be an IW on its own.&lt;/p&gt;

&lt;p&gt;Now, when you say ParallelDocumentsWriter, I assume you mean that that DocWriter will be aware of the slices? That I think is an interesting idea, which is unrelated to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2324" title="Per thread DocumentsWriters that write their own private segments"&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt;. I.e., ParallelWriter will invoke its addDocument code which will get down to ParallelDocumentWriter, which will allocate the doc ID itself and call each slice's DocWriter.addDocument? And then &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2324" title="Per thread DocumentsWriters that write their own private segments"&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt; will just improve the performance of that process?&lt;/p&gt;

&lt;p&gt;This might require a bigger change to IW then I had anticipated, but perhaps it's worth it.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</comment>
                    <comment id="12850322" author="gsingers" created="Fri, 26 Mar 2010 19:42:12 +0000">&lt;p&gt;First off, I haven't looked at the code here or the comments beyond skimming, but this is something I've had in my head for a long time, but don't have any code.  When I think about the whole update problem, I keep coming back to the notion of Photoshop Layers that essentially mask the underlying part of the photo, w/o damaging it.  The analogy isn't quite the same here, but nevertheless...&lt;/p&gt;

&lt;p&gt;This leads me to wonder if the solution isn't best achieved at the index level and not at the Reader/Writer level.  &lt;/p&gt;

&lt;p&gt;So, thinking out loud here and I'm not sure on the best wording of this:  &lt;br/&gt;
when a document first comes in, it is all in one place, just as it is now.  Then, when an update comes in on a particular field, we somehow mark in the index that the document in question is modified and then we add the new change onto the end of the index (just like we currently do when adding new docs, but this time it's just a doc w/ a single field).    Then, when searching, we would, when scoring the affected documents, go to a secondary process that knew where to look up the incremental changes.  As background merging takes place, these "disjoint" documents would be merged back together.  We'd maybe even consider a "high update" merge scheduler that could more frequently handle these incremental merges.  In a sense, the old field for that document is masked by the new field.  I think, given proper index structure, that we &lt;em&gt;maybe&lt;/em&gt; could make that marking of the old field fast (maybe it's a pointer to the new field, maybe it's just a bit indicating to go look in the "update" segment)&lt;/p&gt;

&lt;p&gt;On the search side, I think performance would still be maintained b/c even in high update envs. you aren't usually talking about more than a few thousand changes in a minute or two and the background merger would be responsible for keeping the total number of disjoint documents low.&lt;/p&gt;</comment>
                    <comment id="12850336" author="shaie" created="Fri, 26 Mar 2010 20:23:55 +0000">&lt;p&gt;Hi Grant - I believe what you describe is related to solving the incremental field updates problem, where someone might want to change the value of a specific document's field. But PI is not about that. Rather, PI is about updating a whole slice at once, ie, changing a field's value across all docs, or adding a field to all docs (I believe such question was asked on the user list few days ago). I've listed above several scenarios where PI is useful for, but unfortunately it is unrelated to incremental field updates.&lt;/p&gt;

&lt;p&gt;If I misunderstood you, then please clarify.&lt;/p&gt;

&lt;p&gt;Re incremental field updates, I think your direction is interesting, and deserves discussion, but in a separate issue/thread?&lt;/p&gt;</comment>
                    <comment id="12850505" author="gsingers" created="Sat, 27 Mar 2010 11:16:10 +0000">&lt;p&gt;Thanks, Shai, I had indeed misread the intent, and was likely further confused due to the fact that Michael B and I discussed it over tasty Belgian Beer in Oakland.  I'll open a discussion on list for incremental field updates.&lt;/p&gt;</comment>
                    <comment id="12855377" author="michaelbusch" created="Fri, 9 Apr 2010 13:59:49 +0100">&lt;blockquote&gt;
&lt;p&gt;I'll start by describing the limitations of the current design (whether its the approach or the code is debatable):&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;FWIW:  The attached code and approach was never meant to be committed.  I attached it for legal reasons, as it contains the IP that IBM donated to Apache via the software grant.  Apache requires to attach the code that is covered by such a grant.&lt;/p&gt;

&lt;p&gt;I wouldn't want the master/slave approach in Lucene core.  You can implement it much nicer &lt;b&gt;inside&lt;/b&gt; of Lucene.  The attached code however was developed with the requirement of having to run on top of an unmodified Lucene version.  &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I've realized this when I found that if tests (in this patch) are run with "-ea", there are many assert exceptions that are printed from IndexWriter.startCommit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The code runs without exceptions with Lucene 2.4.  It doesn't work with 2.9/3.0, but you'll find an upgraded version that works with 3.0 within IBM, Shai.&lt;/p&gt;</comment>
                    <comment id="12855379" author="shaie" created="Fri, 9 Apr 2010 14:06:10 +0100">&lt;p&gt;I have found such version ... and it fails too &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. At least the one I received.&lt;/p&gt;

&lt;p&gt;But never mind that ... as long as we both agree the implementation should change. I didn't mean to say anything bad about what you did .. I know the limitations you had to work with.&lt;/p&gt;</comment>
                    <comment id="13058072" author="hyan" created="Thu, 30 Jun 2011 22:00:24 +0100">&lt;p&gt;Hi, Michael&lt;/p&gt;

&lt;p&gt;Is there any lastest progress on this topic? I am very interested in this!&lt;/p&gt;</comment>
                    <comment id="13073462" author="eksdev" created="Mon, 1 Aug 2011 09:45:44 +0100">&lt;p&gt;The user mentioned above in comment was me, I guess. Commenting here just to add interesting use case that would be perfectly solved by this issue.  &lt;/p&gt;

&lt;p&gt;Imagine solr Master - Slave setup, full document contains CONTENT and ID fields, e.g. 200Mio+ collection. On master, we need field ID indexed in order to process delete/update commands. On slave, we do not need lookup on ID and would like to keep our TermsDictionary small, without exploding TermsDictionary with 200Mio+ unique ID terms (ouch, this is a lot compared to 5Mio unique terms in CONTENT, with or without pulsing). &lt;/p&gt;

&lt;p&gt;With this issue,  this could be nativly achieved by modifying solr UpdateHandler not to transfer "ID-Index" to slaves at all.&lt;/p&gt;

&lt;p&gt;There are other ways to fix it, but this would be the best.(I am currently investigating an option to transfer full index on update, but to filter-out TermsDictionary on IndexReader level (it remains on disk, but this part never gets accessed on slaves). I do not know yet if this is possible at all in general , e.g. FST based term dictionary is already built (prefix compressed TermDict would be doable)&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                <outwardlinks description="incorporates">
                            <issuelink>
            <issuekey id="12439796">LUCENE-2025</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12439802">LUCENE-2026</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12361944">SOLR-139</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12420368" name="parallel_incremental_indexing.tar" size="71680" author="michaelbusch" created="Wed, 23 Sep 2009 12:47:18 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 3 Sep 2009 16:27:02 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2976</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25842</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1823] QueryParser with new features for Lucene 3</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1823</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I'd like to have a new QueryParser implementation in Lucene 3.1, ideally based on the new QP framework in contrib. It should share as much code as possible with the current StandardQueryParser implementation for easy maintainability.&lt;/p&gt;

&lt;p&gt;Wish list (feel free to extend):&lt;/p&gt;

&lt;p&gt;1. &lt;b&gt;Operator precedence&lt;/b&gt;: Support operator precedence for boolean operators&lt;br/&gt;
2. &lt;b&gt;Opaque terms&lt;/b&gt;: Ability to plugin an external parser for certain syntax extensions, e.g. XML query terms&lt;br/&gt;
3. &lt;b&gt;Improved RangeQuery syntax&lt;/b&gt;: Use more intuitive &amp;lt;=, =, &amp;gt;= instead of [] and {}&lt;br/&gt;
4. &lt;b&gt;Support for trierange queries&lt;/b&gt;: See &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1768" title="NumericRange support for new query parser"&gt;&lt;del&gt;LUCENE-1768&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
5. &lt;b&gt;Complex phrases&lt;/b&gt;: See &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1486" title="Wildcards, ORs etc inside Phrase queries"&gt;LUCENE-1486&lt;/a&gt;&lt;br/&gt;
6. &lt;b&gt;ANY operator&lt;/b&gt;: E.g. (a b c d) ANY 3 should match if 3 of the 4 terms occur in the same document&lt;br/&gt;
7. &lt;b&gt;New syntax for Span queries&lt;/b&gt;: I think the surround parser supports this?&lt;br/&gt;
8. &lt;b&gt;Escaped wildcards&lt;/b&gt;: See &lt;a href="https://issues.apache.org/jira/browse/LUCENE-588" title="Escaped wildcard character in wildcard term not handled correctly"&gt;&lt;del&gt;LUCENE-588&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12433389">LUCENE-1823</key>
            <summary>QueryParser with new features for Lucene 3</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="lafa">Luis Alves</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Wed, 19 Aug 2009 00:09:55 +0100</created>
                <updated>Fri, 10 May 2013 00:05:34 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>5</votes>
                        <watches>12</watches>
                                                    <comments>
                    <comment id="12744790" author="lafa" created="Wed, 19 Aug 2009 01:01:36 +0100">&lt;blockquote&gt;
&lt;p&gt;2 Opaque terms&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I propose the following examples for the syntax&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
syntax1:
+a -b ::complexPhrase('other syntax') ::xml('/bookstore/book[price&amp;gt;35.00]') ::googlesyntax('2..20 doughnuts')

syntax2:
+a -b complexPhrase::'other syntax' xml::'/bookstore/book[price&amp;gt;35.00]' googlesyntax::'2..20 doughnuts'

syntax3:
+a -b complePhrase:'other syntax' xml:'/bookstore/book[price&amp;gt;35.00]' googlesyntax:'2..20 doughnuts'
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also have a default SyntaxExtension to make the syntax easier, for example if complexPhrase was the default Syntax Extension,&lt;br/&gt;
the queries above could be written like this:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
syntax1:
+a -b ::('other syntax') ::xml('/bookstore/book[price&amp;gt;35.00]') ::googlesyntax('2..20 doughnuts')
syntax2:
+a -b ::'other syntax' xml::'/bookstore/book[price&amp;gt;35.00]' googlesyntax::'2..20 doughnuts'
syntax3:
+a -b 'other syntax' xml:'/bookstore/book[price&amp;gt;35.00]' googlesyntax:'2..20 doughnuts'
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I would like to call it Query Parser Syntax extensions instead of Opaque Terms.&lt;/p&gt;

&lt;p&gt;+ 1 for syntax 1&lt;/p&gt;
</comment>
                    <comment id="12744792" author="lafa" created="Wed, 19 Aug 2009 01:10:27 +0100">&lt;blockquote&gt;
&lt;p&gt;1. Operator precedence&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The new queryparser already supports this internally by disabling the GroupQueryNodeProcessor. But we don't have any tescases and we need to add a simpler interface for the users by creating a new Lucene3QueryParser class (with a diff name).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2. Opaque terms&lt;br/&gt;
5. Complex phrases&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We should also implement number 5(Complex phrases) using number 2 (Opaque terms)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;8 Escaped wildcards&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1820" title="WildcardQueryNode to expose the positions of the wildcard characters, for easier use in processors and builders"&gt;LUCENE-1820&lt;/a&gt; is also related to this.&lt;/p&gt;</comment>
                    <comment id="12744795" author="lafa" created="Wed, 19 Aug 2009 01:15:42 +0100">&lt;blockquote&gt;
&lt;p&gt;8 Escaped wildcards&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The new queryparser already supports this in the StandardSyntaxParser and most processor, we just need to make it visible to the underlying lucene classes, on the builders.&lt;/p&gt;</comment>
                    <comment id="12744797" author="michaelbusch" created="Wed, 19 Aug 2009 01:17:33 +0100">&lt;p&gt;Hmm, Syntax 2 looks more intuitive to me... looks a bit strange in syntax one to have the :: in front of the syntax name?&lt;/p&gt;</comment>
                    <comment id="12744829" author="lafa" created="Wed, 19 Aug 2009 02:33:15 +0100">&lt;p&gt;syntax1 is more similar to a function call.  In the future we might extend it to support more parameters:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
::xml('syntax', param2)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                    <comment id="12744834" author="lafa" created="Wed, 19 Aug 2009 02:50:18 +0100">&lt;blockquote&gt;
&lt;p&gt;looks a bit strange in syntax one to have the :: in front of the syntax name&lt;/p&gt;&lt;/blockquote&gt;


&lt;p&gt;The current list of escaped chars for the current lucene syntax is:&lt;br/&gt;
"+", "-", "!", "(", ")", ":", "^", "&lt;span class="error"&gt;&amp;#91;&amp;quot;, &amp;quot;&amp;#93;&lt;/span&gt;", "\"", "&lt;/p&gt;
{", "}
&lt;p&gt;", "~", "*", "?", "&lt;br class="atl-forced-newline" /&gt;", """&lt;/p&gt;

&lt;p&gt;I was trying to avoid adding an extra ones, so I reused the ':'&lt;br/&gt;
but we can select another char combination that makes more sense.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;syntax1, the main idea on syntax1 is to make it look like a function call.&lt;/li&gt;
	&lt;li&gt;syntax2 , is very similar to lucene field query syntax but uses the :: operator to avoid overloading the field name syntax.&lt;/li&gt;
	&lt;li&gt;syntax3, will overload field name syntax, and will look very similar to current syntax, but will use single quotes to identify it is calling a syntax extension.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I hope this helps.&lt;/p&gt;</comment>
                    <comment id="12744845" author="gsingers" created="Wed, 19 Aug 2009 03:07:40 +0100">&lt;p&gt;Boosting*Query's support?  &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12745146" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 18:44:46 +0100">&lt;p&gt;Option to not die on bad syntax - keep what works and the rest become terms  - or something along those lines.&lt;/p&gt;</comment>
                    <comment id="12745251" author="michaelbusch" created="Thu, 20 Aug 2009 01:25:20 +0100">&lt;p&gt;I think Solr has a feature similar to what I called 'Opaque terms": Nested Queries.&lt;/p&gt;</comment>
                    <comment id="12747299" author="lafa" created="Tue, 25 Aug 2009 09:15:53 +0100">&lt;p&gt;We can also implement: &lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;foo~(&amp;gt;=1) should really just map to foo.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;details in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-950" title="IllegalArgumentException parsing &amp;quot;foo~1&amp;quot;"&gt;&lt;del&gt;LUCENE-950&lt;/del&gt;&lt;/a&gt; issue.&lt;/p&gt;
</comment>
                    <comment id="12747304" author="lafa" created="Tue, 25 Aug 2009 09:31:35 +0100">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/browse/LUCENE-167" title="[PATCH] QueryParser not handling queries containing AND and OR"&gt;&lt;del&gt;LUCENE-167&lt;/del&gt;&lt;/a&gt; will be implemented by item 1.&lt;/p&gt;

&lt;p&gt;Item 1 will support  NOT - +  AND OR operators with precedence.&lt;/p&gt;</comment>
                    <comment id="12747305" author="lafa" created="Tue, 25 Aug 2009 09:34:37 +0100">&lt;p&gt;Item 3&lt;/p&gt;

&lt;p&gt;will address &lt;a href="https://issues.apache.org/jira/browse/LUCENE-995" title="Add open ended range query syntax to QueryParser"&gt;&lt;del&gt;LUCENE-995&lt;/del&gt;&lt;/a&gt; using a new syntax with &amp;gt;= &amp;lt;= =&lt;/p&gt;</comment>
                    <comment id="12747308" author="lafa" created="Tue, 25 Aug 2009 09:44:03 +0100">&lt;p&gt;I'll also want to fix &lt;a href="https://issues.apache.org/jira/browse/LUCENE-375" title="fish*~ parses to PrefixQuery - should be a parse exception"&gt;LUCENE-375&lt;/a&gt; as part of this issue&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;fish*~ parses to PrefixQuery - should be a parse exception&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12748090" author="adriano_crestani" created="Wed, 26 Aug 2009 20:12:01 +0100">&lt;blockquote&gt;
&lt;p&gt;We can also implement:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;foo~(&amp;gt;=1) should really just map to foo.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;details in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-950" title="IllegalArgumentException parsing &amp;quot;foo~1&amp;quot;"&gt;&lt;del&gt;LUCENE-950&lt;/del&gt;&lt;/a&gt; issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This patch fixes on the contrib/queryparser this bug as discussed on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-950" title="IllegalArgumentException parsing &amp;quot;foo~1&amp;quot;"&gt;&lt;del&gt;LUCENE-950&lt;/del&gt;&lt;/a&gt;. It does not throw IllegalArgumentException anymore for fuzzy values greater or equals 1, it just ignores the fuzzy and create a simple field query. JUnits are also included.&lt;/p&gt;

&lt;p&gt;I used 'ant javacc-contrib-queryparser' to regenerate the StandardSyntaxParser with javacc 4.2.&lt;/p&gt;</comment>
                    <comment id="12779092" author="alioral" created="Tue, 17 Nov 2009 20:07:25 +0000">&lt;p&gt;Proximity  query support could be very nice.  This definitely requires span queries.&lt;/p&gt;

&lt;p&gt;(john OR james OR mar*) NEAR/5 ( smith OR mil*)&lt;/p&gt;
</comment>
                    <comment id="12779672" author="lafa" created="Wed, 18 Nov 2009 21:17:26 +0000">&lt;p&gt;Hi Ali,&lt;/p&gt;

&lt;p&gt;Here another suggestion for the proximity syntax:&lt;br/&gt;
( (john OR james OR mar*) ( smith OR mil*) ) WITHIN 5&lt;/p&gt;

&lt;p&gt;I'll see if I have time to put that on the new parser.&lt;/p&gt;</comment>
                    <comment id="12779692" author="lafa" created="Wed, 18 Nov 2009 22:19:07 +0000">&lt;p&gt;This patch is the first patch to implement the features described on lucene-1823.&lt;br/&gt;
contains:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Operator precedence&lt;/li&gt;
	&lt;li&gt;Opaque terms&lt;/li&gt;
	&lt;li&gt;ANY operator&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The new parser is name standard2, I'm open to change this name please post suggestions &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Also included is a implementation for regex using the syntax discussed in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2039" title="Regex support and beyond in JavaCC QueryParser"&gt;&lt;del&gt;LUCENE-2039&lt;/del&gt;&lt;/a&gt;. I wrote a simple junit and and RegexQueryParser in the test folder. This implementation use the Opaque terms implementation.&lt;/p&gt;</comment>
                    <comment id="12779745" author="lafa" created="Thu, 19 Nov 2009 00:00:42 +0000">&lt;p&gt;Operator precedence order is &lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
ANY, ~, ^, +, -, NOT, AND, OR
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
a OR b AND c 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;will now be executed as &lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
(a OR (b AND c))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The syntax for the ANY operator is: &lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
( a b c d ) ANY 2 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Opaque syntax is:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
extensioName:field:term
extensioName:field:&lt;span class="code-quote"&gt;"phrase"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Default field:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
extensioName::term
extensioName::&lt;span class="code-quote"&gt;"phrase"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the test folder standard2 there is a Opaque implementation for regex (contrib component),&lt;br/&gt;
and the syntax to use this test RegexQueryParser is, all the lunece syntax and the above, plus:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
regex:field:&lt;span class="code-quote"&gt;"regular expression"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
regex::&lt;span class="code-quote"&gt;"^.[aeiou]c.*$"&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12779767" author="lafa" created="Thu, 19 Nov 2009 00:49:02 +0000">&lt;p&gt;I forgot to say that the patch, includes &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1937" title="Add more methods to manipulate QueryNodeProcessorPipeline elements"&gt;&lt;del&gt;LUCENE-1937&lt;/del&gt;&lt;/a&gt; and &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1938" title="Precedence query parser using the contrib/queryparser framework"&gt;&lt;del&gt;LUCENE-1938&lt;/del&gt;&lt;/a&gt; from Adriano Crestani to enable the precedence code.&lt;/p&gt;</comment>
                    <comment id="12784696" author="michaelbusch" created="Wed, 2 Dec 2009 07:57:43 +0000">&lt;p&gt;Luis and Adriano,&lt;/p&gt;

&lt;p&gt;the QP config looks quite overwhelming with all the Attributes. I'm not sure if the AttributeSource/Attribute stuff is a good fit for this type of configuration.&lt;/p&gt;

&lt;p&gt;Couldn't we achieve the same with a Properties (Hashtable) approach and constants or something similar. This would be a good place to start to reduce the complexity of the new QP.&lt;/p&gt;</comment>
                    <comment id="12784709" author="shaie" created="Wed, 2 Dec 2009 08:46:15 +0000">&lt;p&gt;I prefer syntax 2 for the opaque terms. If the idea is to plug in another QP for that opaque term, then it would be best IMO if that QP received the entire string and did what it knows with it. That way, I could pass my::'field1:value OR field2:value2 AND (something else)', and 'my' QP would parse the entire string.&lt;br/&gt;
I don't see how this can be achieved w/ &amp;lt;syntax&amp;gt;:&amp;lt;field&amp;gt;:query, meaning, how can I pass a clause which contains two fields ORed or ANDed? IMO, the simpler the better and it's easy to explain that whatever comes after the '::' (double colons), is passed onto as-is to the assigned QP.&lt;/p&gt;</comment>
                    <comment id="12903849" author="simonw" created="Sat, 28 Aug 2010 18:52:31 +0100">&lt;p&gt;Linked issues for reference and heads up. @Luis, are you still working on that stuff and would you be willing to further maintain the QueryParser in Contrib?&lt;/p&gt;</comment>
                    <comment id="12904892" author="adriano_crestani" created="Wed, 1 Sep 2010 04:51:44 +0100">&lt;p&gt;I agree with Michael, AttributeSource was designed for another purpose, and does not really fit for configuration purposes.&lt;/p&gt;

&lt;p&gt;The map idea is really good and fits well as configuration for the QP, but I would like to restrict the key type, so the user doesn't use a String object as key. String keys may lead to runtime errors, mainly when they are inserted inline. I would prefer to use enums as keys, it would enforce the user to always pass the same object as key when referencing the same configuration. It also avoids duplicated configuration keys, once each enum type has only one instance per JVM.&lt;/p&gt;

&lt;p&gt;If nobody complains about using a Map&amp;lt;Enum&amp;lt;?&amp;gt;, Object&amp;gt; as configuration for QP framework, I will start working on a new patch including these changes soon.&lt;/p&gt;</comment>
                    <comment id="12904901" author="markh" created="Wed, 1 Sep 2010 05:47:28 +0100">&lt;p&gt;Another one for the wishlist - support for the nested documents offered in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2454" title="Nested Document query support"&gt;&lt;del&gt;LUCENE-2454&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An example query of a resume with a parent person doc and multiple child "employment" documents might be as follows:&lt;/p&gt;

&lt;p&gt;   +(name:frederick OR name:fred)  +CHILD(type:contract AND skill:java AND date&amp;gt;=2005 )&lt;/p&gt;

&lt;p&gt;The new feature here is the CHILD(...) construct that shifts query context to a nested document.&lt;br/&gt;
I imagine there is some more formal syntax we could consider lifting from XPath but I thought I'd throw this in while you are contemplating new features.&lt;/p&gt;</comment>
                    <comment id="12923015" author="adriano_crestani" created="Wed, 20 Oct 2010 16:35:41 +0100">&lt;p&gt;Just a reminder that Luis's patch was blocked by &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1938" title="Precedence query parser using the contrib/queryparser framework"&gt;&lt;del&gt;LUCENE-1938&lt;/del&gt;&lt;/a&gt;, which is now resolved, the patch can finally be reviewed/committed.&lt;/p&gt;</comment>
                    <comment id="12923078" author="rcmuir" created="Wed, 20 Oct 2010 19:27:22 +0100">&lt;p&gt;Adriano, I will take a look at the patch.&lt;/p&gt;

&lt;p&gt;A few things have changed:&lt;/p&gt;

&lt;p&gt;the &lt;a href="https://issues.apache.org/jira/browse/LUCENE-950" title="IllegalArgumentException parsing &amp;quot;foo~1&amp;quot;"&gt;&lt;del&gt;LUCENE-950&lt;/del&gt;&lt;/a&gt; issue, I changed the &lt;br/&gt;
FuzzyQuery syntax to allow for foo~1 foo~2 to support exact edit distances... &lt;br/&gt;
so I don't think we need to change anything there.&lt;/p&gt;

&lt;p&gt;Additionally we also added proper regular expression support (via Lucene core's RegexpQuery).&lt;/p&gt;

&lt;p&gt;But i'll play with the patch, and see if i can bring it up to trunk.&lt;/p&gt;

&lt;p&gt;As far as using a Map instead of Attributes for configuration, I think this would be a really good step!&lt;br/&gt;
Are you still interested in working up a patch for this one. At the moment I think all the attributes&lt;br/&gt;
scare people away from the contrib/queryparser.&lt;/p&gt;</comment>
                    <comment id="12923867" author="rcmuir" created="Fri, 22 Oct 2010 15:22:43 +0100">&lt;p&gt;Looking at the patch, its a bit difficult to review since the patch creates a whole new queryparser (Standard2).&lt;/p&gt;

&lt;p&gt;This is just my opinion here:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;I think it would be good to just modify "Standard" with the improvements presented here. I think for contrib/queryparser to succeed, we should worry less about providing exact imitations of the core queryparser, and instead focus on trying to provide a framework and concrete implementation that solves the problems people are facing. In other words, fix what we don't like and provide a parser that works the way we want, and forget about exact compatibility with the core queryparser... if someone wants its exact behavior, they can just use it.&lt;/li&gt;
	&lt;li&gt;It would be much easier if improvements could be on separate patches rather than bundled: For example, &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1938" title="Precedence query parser using the contrib/queryparser framework"&gt;&lt;del&gt;LUCENE-1938&lt;/del&gt;&lt;/a&gt; was easy for me to commit because it was well-contained and covered one single improvement/feature.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                    <comment id="13007737" author="adriano_crestani" created="Wed, 16 Mar 2011 23:25:38 +0000">&lt;p&gt;Hi Robert,&lt;/p&gt;

&lt;p&gt;I completely agree with your statement, the config API scares me also. I would love to submit a patch for it, but I am working for IBM now, and, as a committer, I need to go through some bureaucratic paperwork before doing any new feature for Lucene and it might still take some time &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I had a better idea, I will propose it to be a GSOC project for this year. This way we can also get one more contributor to contrib QP. &lt;/p&gt;</comment>
                    <comment id="13071091" author="ofavre" created="Tue, 26 Jul 2011 14:27:53 +0100">&lt;p&gt;Relates to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3343" title="Comparison operators &amp;gt;,&amp;gt;=,&amp;lt;,&amp;lt;= and = support as RangeQuery syntax in QueryParser"&gt;&lt;del&gt;LUCENE-3343&lt;/del&gt;&lt;/a&gt;: Open range comparison operator &amp;gt;,&amp;gt;=,&amp;lt;,&amp;lt;= and =.&lt;/p&gt;</comment>
                    <comment id="13176390" author="janhoy" created="Wed, 28 Dec 2011 00:59:18 +0000">&lt;p&gt;@Luis, still working on this?&lt;/p&gt;</comment>
                    <comment id="13251875" author="janhoy" created="Wed, 11 Apr 2012 21:00:15 +0100">&lt;p&gt;Luis, are you there? Can you give a status on this. Think this issue needs a general update both the title, description and a strategy for how to proceed. I agree with Robert that incremental progress is better than trying to solve everything in one go.&lt;/p&gt;

&lt;p&gt;Starting to adopt the new Flex QP framework would accelerate QP development also in Solr camp.&lt;/p&gt;</comment>
                    <comment id="13281847" author="simonw" created="Wed, 23 May 2012 20:56:13 +0100">&lt;p&gt;moving this over to 4.1 it seems dead to me though&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12437084">LUCENE-1938</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                <outwardlinks description="incorporates">
                            <issuelink>
            <issuekey id="12314525">LUCENE-375</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12314317">LUCENE-167</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12377970">LUCENE-995</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12378170">LUCENE-996</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12343885">LUCENE-588</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12373077">LUCENE-950</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12417241">LUCENE-1567</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12431778">LUCENE-1768</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12471769">LUCENE-2604</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12515390">LUCENE-3343</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12433316">LUCENE-1820</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12410389">LUCENE-1486</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12440050">LUCENE-2039</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                                <inwardlinks description="is depended upon by">
                            <issuelink>
            <issuekey id="12507943">LUCENE-3130</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12425407" name="lucene_1823_any_opaque_precedence_fuzzybug_v2.patch" size="244947" author="lafa" created="Wed, 18 Nov 2009 23:45:20 +0000"/>
                    <attachment id="12417772" name="lucene_1823_foo_bug_08_26_2009.patch" size="23250" author="adriano_crestani" created="Wed, 26 Aug 2009 20:12:01 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 19 Aug 2009 00:01:36 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11940</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25898</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1820] WildcardQueryNode to expose the positions of the wildcard characters, for easier use in processors and builders</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1820</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Change the  WildcardQueryNode to expose the positions of the wildcard characters.&lt;br/&gt;
This would allow the AllowLeadingWildcardProcessor  not to have to knowledge about the wildcard chars * and ? and avoid double check again.&lt;/p&gt;</description>
                <environment/>
            <key id="12433316">LUCENE-1820</key>
            <summary>WildcardQueryNode to expose the positions of the wildcard characters, for easier use in processors and builders</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="lafa">Luis Alves</reporter>
                        <labels>
                    </labels>
                <created>Tue, 18 Aug 2009 10:01:15 +0100</created>
                <updated>Fri, 10 May 2013 00:05:34 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                        <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12433389">LUCENE-1823</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12417241">LUCENE-1567</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11943</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25900</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1785] Simple FieldCache merging</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1785</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We'll merge the field caches in RAM as the SegmentReader's are&lt;br/&gt;
merged in IndexWriter (the first cut will work in conjunction&lt;br/&gt;
with IW.getReader). There will be an optional callback to&lt;br/&gt;
determine which fields to merge. &lt;/p&gt;</description>
                <environment/>
            <key id="12432340">LUCENE-1785</key>
            <summary>Simple FieldCache merging</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jasonrutherglen">Jason Rutherglen</reporter>
                        <labels>
                    </labels>
                <created>Thu, 6 Aug 2009 02:11:42 +0100</created>
                <updated>Fri, 10 May 2013 00:05:34 +0100</updated>
                                    <version>2.4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                          <timeoriginalestimate seconds="172800">48h</timeoriginalestimate>
                    <timeestimate seconds="172800">48h</timeestimate>
                                  <comments>
                    <comment id="12740033" author="markrmiller@gmail.com" created="Thu, 6 Aug 2009 13:46:06 +0100">&lt;p&gt;I think this might have to be 3.1 ...&lt;/p&gt;</comment>
                    <comment id="12783197" author="jasonrutherglen" created="Sat, 28 Nov 2009 01:56:07 +0000">&lt;p&gt;This mostly works, not committable. I've noticed we're creating&lt;br/&gt;
multiple cache keys (i.e. Entry objects), one with the default&lt;br/&gt;
parser, one with a null parser, that point to the same&lt;br/&gt;
underlying value. &lt;/p&gt;

&lt;p&gt;The field cache merging then tries to merge both of these&lt;br/&gt;
entries into separate objects, causing the field cache sanity&lt;br/&gt;
check to fail. I'm guessing I need to find values that are the&lt;br/&gt;
same for an entry and choose one (the one with a parser?). &lt;/p&gt;

&lt;p&gt;Note: This only works when using IW.getReader&lt;/p&gt;</comment>
                    <comment id="12783291" author="jasonrutherglen" created="Sat, 28 Nov 2009 21:38:12 +0000">&lt;p&gt;The sanity check is fixed by skipping the entry with a null&lt;br/&gt;
value. Removed some of the debugging. I think this patch&lt;br/&gt;
requires a way to handle merging field caches from segments that&lt;br/&gt;
have not yet created their field caches. We can generate the new&lt;br/&gt;
cache if we already have at least 75% of the required caches. &lt;/p&gt;

&lt;p&gt;Also we need to handle deletes.&lt;/p&gt;</comment>
                    <comment id="12783328" author="jasonrutherglen" created="Sat, 28 Nov 2009 23:47:12 +0000">&lt;p&gt;Deletes in the source readers should be handled correctly.  &lt;/p&gt;

&lt;p&gt;We probably need a unit test that verifies the merged caches are exactly what they should be.&lt;/p&gt;</comment>
                    <comment id="12783353" author="jasonrutherglen" created="Sun, 29 Nov 2009 03:25:47 +0000">&lt;p&gt;Cleaned up some more, moved mergeSuccess to mergeMiddle, as otherwise the merge cloned readers had already been released by the time mergeSuccess was reached.&lt;/p&gt;</comment>
                    <comment id="12789985" author="jasonrutherglen" created="Mon, 14 Dec 2009 04:48:58 +0000">&lt;p&gt;We probably need to figure out a way to merge string indexes before committing this?  Is there an efficient way to do this?&lt;/p&gt;</comment>
                    <comment id="12790057" author="mikemccand" created="Mon, 14 Dec 2009 09:24:09 +0000">&lt;p&gt;Can't we just merge-sort into the merged StringIndex?&lt;/p&gt;</comment>
                    <comment id="12790284" author="jasonrutherglen" created="Mon, 14 Dec 2009 18:49:39 +0000">&lt;blockquote&gt;&lt;p&gt;Can't we just merge-sort into the merged StringIndex? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right however what's going to be the fastest way to remove terms that are no longer in the index?  &lt;/p&gt;</comment>
                    <comment id="12790366" author="mikemccand" created="Mon, 14 Dec 2009 21:01:23 +0000">&lt;p&gt;Hmm good question.  Actually, couldn't we pull a Term(s)Enum from the newly merged segment for that field, and use it to eliminate the non-dead terms?&lt;/p&gt;</comment>
                    <comment id="12790369" author="mikemccand" created="Mon, 14 Dec 2009 21:06:38 +0000">&lt;p&gt;Make that... eliminate the &lt;b&gt;dead&lt;/b&gt; terms.&lt;/p&gt;</comment>
                    <comment id="12790391" author="jasonrutherglen" created="Mon, 14 Dec 2009 21:45:30 +0000">&lt;blockquote&gt;&lt;p&gt;couldn't we pull a Term(s)Enum from the newly merged segment for that field&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I think this makes the most sense, and won't adversely affect performance because term dictionary file access is sequential.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12442372">SOLR-1618</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12426354" name="LUCENE-1785.patch" size="22997" author="jasonrutherglen" created="Sun, 29 Nov 2009 03:25:47 +0000"/>
                    <attachment id="12426345" name="LUCENE-1785.patch" size="22148" author="jasonrutherglen" created="Sat, 28 Nov 2009 23:47:12 +0000"/>
                    <attachment id="12426337" name="LUCENE-1785.patch" size="18353" author="jasonrutherglen" created="Sat, 28 Nov 2009 21:38:11 +0000"/>
                    <attachment id="12426310" name="LUCENE-1785.patch" size="18485" author="jasonrutherglen" created="Sat, 28 Nov 2009 01:56:07 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 6 Aug 2009 12:46:06 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11977</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25935</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1743] MMapDirectory should only mmap large files, small files should be opened using SimpleFS/NIOFS</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1743</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is a followup to &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1741" title="Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller parts"&gt;&lt;del&gt;LUCENE-1741&lt;/del&gt;&lt;/a&gt;:&lt;br/&gt;
Javadocs state (in FileChannel#map): "For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods. From the standpoint of performance it is generally only worth mapping relatively large files into memory."&lt;br/&gt;
MMapDirectory should get a user-configureable size parameter that is a lower limit for mmapping files. All files with a size&amp;lt;limit should be opened using a conventional IndexInput from SimpleFS or NIO (another configuration option for the fallback?).&lt;/p&gt;</description>
                <environment/>
            <key id="12430347">LUCENE-1743</key>
            <summary>MMapDirectory should only mmap large files, small files should be opened using SimpleFS/NIOFS</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Tue, 14 Jul 2009 10:38:16 +0100</created>
                <updated>Mon, 13 May 2013 03:53:09 +0100</updated>
                                    <version>2.9</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/store</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12731085" author="eksdev" created="Tue, 14 Jul 2009 20:55:10 +0100">&lt;p&gt;indeed! obvious idea, &lt;/p&gt;

&lt;p&gt;the only thing I do not like with it is making these hidden, deceptive decisions "I said I want MMapDirectory and someone else decided something else for me"... it does not matter if we have conses here now, it may change tomorrow &lt;/p&gt;

&lt;p&gt;probably better way would be to turbo charge FileSwitchDirectory with sexy parametrization options, &lt;br/&gt;
MMapDirectory &amp;lt;- F(fileExtension, minSize, maxSize) // If &amp;lt;fileExtension&amp;gt; and file size less than &amp;lt;maxSize&amp;gt; and greater than &amp;lt;minSize&amp;gt; than open file with MMapDirectory... than go on on next rule... (can be designed upside down as well... changes nothing in idea)&lt;/p&gt;

&lt;p&gt;the same for RAMDir, NIO, FS... &lt;/p&gt;

&lt;p&gt;With this, we can make UwesBestOfMMapDirectoryFor32BitOSs (your proposal here) or &lt;br/&gt;
HighlyConcurentForWindows64WithTermDictionaryInRamAndStoredFieldsOnDiskDirectory just for me &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; &lt;/p&gt;

&lt;p&gt;So the most of the end users take some smart defaults we provide in core, and freaks (Expert users in official lingo &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; have their job easy, just to configure TurboChargedFileSwitchDirectory&lt;/p&gt;

&lt;p&gt;Should be easy to come up with clean design for these "Concrete Directory selection rules" by keeping concrete Directories "pure"&lt;/p&gt;

&lt;p&gt;Cheers, Eks &lt;/p&gt;

</comment>
                    <comment id="12731091" author="mikemccand" created="Tue, 14 Jul 2009 21:04:01 +0100">&lt;p&gt;At a minimum we should make FileSwitchDirectory friendly for subclassing, eg so you can override the currently private getDirectory method to implement your own custom logic.  Hmm... we should somehow pass the "context" (at least 'read' vs 'write') to getDirectory() as well...&lt;/p&gt;</comment>
                    <comment id="12731101" author="thetaphi" created="Tue, 14 Jul 2009 21:21:06 +0100">&lt;blockquote&gt;&lt;p&gt;HighlyConcurentForWindows64WithTermDictionaryInRamAndStoredFieldsOnDiskDirectory just for me  &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;By the way, for MMapDirectory the MappedByteBuffer.load() method should be somehow accesible/configureable to create this TermDictionaryInRam part (IndexInput would call load() and tells the OS to swap as much as possible of the mmaped file into RAM). Just an idea.&lt;/p&gt;

&lt;p&gt;The Hyper FileSwitchDirectory was my idea yesterday, too. As Mike said, at least the getDirectory() should be configureable.&lt;/p&gt;

&lt;p&gt;And for some good defaults, a factory could be provided like getUwesBestOfMMapDirectoryFor32BitOSs(File, LockFactory). What I do not like with the current FileSwitchDir is the fact, that you must create instances with the same Dir and LockFactory for each sub-directory (e.g. what happens if you use 2 different LockFactories on the same physical dir inside a FileSwitchDir?). Maybe the FileSwitchDirectory could just get the File and LockFactory once and creates the instances? Many ideas...&lt;/p&gt;</comment>
                    <comment id="12731104" author="eksdev" created="Tue, 14 Jul 2009 21:25:14 +0100">&lt;p&gt;right, it is not everything about reading index, you have to write it as well...&lt;/p&gt;

&lt;p&gt;why not making  it an abstract class with &lt;br/&gt;
abstract Directory getDirectory(String file, int minSize, int maxSize, String &lt;span class="error"&gt;&amp;#91;read/write/append&amp;#93;&lt;/span&gt;, String context);&lt;br/&gt;
String getName(); // for logging&lt;/p&gt;

&lt;p&gt;What do you understand under "context"? Something along the lines /Give me directory for "segment merges", "read only" for search./ &lt;br/&gt;
...Maybe one day we will have possibility not to kill OS cache by merging,&lt;/p&gt;
</comment>
                    <comment id="12731118" author="thetaphi" created="Tue, 14 Jul 2009 21:37:14 +0100">&lt;p&gt;Another idea to think about:&lt;br/&gt;
Maybe we make the base FSDirectory configureable, that you can define rules for the IndexInput or IndexOutput instances used. Why create three directories, when you could only use on that just decides, what is the right IndexInput/IndexOutput? In this case you need no MMapDirectory, no NIOFSDir, no SimpleFSDir. The basic list files, rename and so on are always identical (for FSDirs). The only difference is the IndexInput and IndexOutput.&lt;/p&gt;

&lt;p&gt;In this case you would only have &lt;b&gt;one&lt;/b&gt; LockFactory, which would be good in my opinion (see above).&lt;/p&gt;

&lt;p&gt;So we only provide the Input/Output classes as separate top-level classes: SimpleFSIndexInput, MMapIndexInput (internal two classes for chunking or not), NIOIndexInput and for output at the beginning only the current SimpleFSIndexOutput. You can then configure your FSDirectory to use different impls depending on an method or rule defined like above. For backwards compatibility, the current MMapDirectory and so on are simple subclasses of this universal FSDir with a fixed configuration.&lt;/p&gt;

&lt;p&gt;FileSwitchDirectory should only used, when you really want to separate two directories in complete (stored fields on disk, index on SSD in two physical different dirs).&lt;/p&gt;</comment>
                    <comment id="12731632" author="earwin" created="Wed, 15 Jul 2009 20:07:41 +0100">&lt;p&gt;The initial motive for the issue seems wrong to me.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;"For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods. From the standpoint of performance it is generally only worth mapping relatively large files into memory."&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It is probably right if you're doing a single read through the file. If you're opening/mapping it and do thousands of repeated reads, mmap would be superior, because after initial mapping it's just a memory access VS system call for file.read().&lt;/p&gt;

&lt;p&gt;Add: In case you're not doing repeated reads, and just read these small files once from time to time, you can totally neglect speed difference between mmap and fopen. At least it doesn't warrant increased complexity.&lt;/p&gt;</comment>
                    <comment id="12731638" author="thetaphi" created="Wed, 15 Jul 2009 20:17:25 +0100">&lt;p&gt;A typical example, where MMap would be the wrong thing are e.g. norms. They are read one time and then the file is not accessed anymore. It would only be cool, if MMapDir could directly return the mapped array (but MappedByteBuffer.array() does not work) and use it as norms array. That would be cool.&lt;/p&gt;

&lt;p&gt;My problem was more with all these small files like segments_XXXX and segments.gen or *.del files. They are small and only used one time. Mapping them is just waste of resources and completely useless (even slower that opening them directly). This is why I said, some limit or file extension based mapping would be good.&lt;/p&gt;</comment>
                    <comment id="12731639" author="earwin" created="Wed, 15 Jul 2009 20:26:22 +0100">&lt;blockquote&gt;&lt;p&gt;My problem was more with all these small files like segments_XXXX and segments.gen or *.del files. They are small and only used one time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I can only reiterate my point. These files aren't opened like 10k files per second, so your win is going to be in the order of microseconds per reopen - at the cost of increased complexity.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 14 Jul 2009 19:55:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2902</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25977</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1710] Add byte/short to NumericUtils, NumericField and NumericRangeQuery</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1710</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Although NumericRangeQuery will not profit much from trie-encoding short/byte fields (byte fields with e.g. precisionStep 8 would only create one precision), it may be good to have these two data types available with NumericField to be generally able to store them in prefix-encoded form in index.&lt;/p&gt;

&lt;p&gt;This is important for loading them into FieldCache where they require much less memory.&lt;/p&gt;</description>
                <environment/>
            <key id="12428558">LUCENE-1710</key>
            <summary>Add byte/short to NumericUtils, NumericField and NumericRangeQuery</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Mon, 22 Jun 2009 11:59:55 +0100</created>
                <updated>Fri, 10 May 2013 00:05:35 +0100</updated>
                                    <version>2.9</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/index</component>
                <component>core/search</component>
                        <due/>
                    <votes>1</votes>
                        <watches>1</watches>
                                                            <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2974</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26010</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1698] Change backwards-compatibility policy</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1698</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;These proposed changes might still change slightly:&lt;/p&gt;

&lt;p&gt;I'll call X.Y -&amp;gt; X+1.0 a 'major release', X.Y -&amp;gt; X.Y+1 a&lt;br/&gt;
'minor release' and X.Y.Z -&amp;gt; X.Y.Z+1 a 'bugfix release'. (we can later&lt;br/&gt;
use different names; just for convenience here...)&lt;/p&gt;

&lt;p&gt;1. The file format backwards-compatiblity policy will remain unchanged;&lt;br/&gt;
   i.e. Lucene X.Y supports reading all indexes written with Lucene&lt;br/&gt;
   X-1.Y. That means Lucene 4.0 will not have to be able to read 2.x&lt;br/&gt;
   indexes.&lt;/p&gt;

&lt;p&gt;2. Deprecated public and protected APIs can be removed if they have&lt;br/&gt;
   been released in at least one major or minor release. E.g. an 3.1&lt;br/&gt;
   API can be released as deprecated in 3.2 and removed in 3.3 or 4.0&lt;br/&gt;
   (if 4.0 comes after 3.2).&lt;/p&gt;

&lt;p&gt;3. No public or protected APIs are changed in a bugfix release; except&lt;br/&gt;
   if a severe bug can't be changed otherwise.&lt;/p&gt;

&lt;p&gt;4. Each release will have release notes with a new section&lt;br/&gt;
   "Incompatible changes", which lists, as the names says, all changes that&lt;br/&gt;
   break backwards compatibility. The list should also have information&lt;br/&gt;
   about how to convert to the new API. I think the eclipse releases&lt;br/&gt;
   have such a release notes section. Furthermore, the Deprecation tag &lt;br/&gt;
   comment will state the minimum version when this API is to be removed,  e.g.&lt;br/&gt;
   @deprecated See #fooBar().  Will be removed in 3.3 &lt;br/&gt;
   or&lt;br/&gt;
   @deprecated See #fooBar().  Will be removed in 3.3 or later.&lt;/p&gt;


&lt;p&gt;I'd suggest to treat a runtime change like an API change (unless it's fixing a bug of course),&lt;br/&gt;
i.e. giving a warning, providing a switch, switching the default behavior only after a major &lt;br/&gt;
or minor release was around that had the warning/switch. &lt;/p&gt;</description>
                <environment/>
            <key id="12428017">LUCENE-1698</key>
            <summary>Change backwards-compatibility policy</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Tue, 16 Jun 2009 17:14:58 +0100</created>
                <updated>Fri, 10 May 2013 00:05:35 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12726575" author="hossman" created="Thu, 2 Jul 2009 18:48:57 +0100">



&lt;p&gt;-0&lt;/p&gt;

&lt;p&gt;(Note: this proposal is remeniscent of Grant's suggestion a while back...&lt;br/&gt;
&lt;a href="http://www.nabble.com/Back-Compatibility-to14918202.html" class="external-link"&gt;http://www.nabble.com/Back-Compatibility-to14918202.html&lt;/a&gt; )&lt;/p&gt;

&lt;p&gt;The concern I have with an approach like this (as compared to the current policy) is that it removes information from the version number.  Under the current policy, you can look at any new version Y and be confident about whether it is backwards compatible with some version X you are currently using &amp;#8211; if the version numbers are different enough then you &lt;b&gt;may&lt;/b&gt; need to make changes to use the new version; but if the numbers are similar enough then you &lt;b&gt;definitely&lt;/b&gt; should be able to upgrade without changing code.&lt;/p&gt;

&lt;p&gt;For people who upgrade regularly (or semi-regularly), it gives them a clear indication of how much work should be involved in any given upgrade...&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;4.5.1 -&amp;gt; 4.5.4 ... bug fixes, drop in replacement&lt;/li&gt;
	&lt;li&gt;4.5.1 -&amp;gt; 4.7.0 ... new features/api, should take a look at them, and be on the lookout for new deprecations to save time later.&lt;/li&gt;
	&lt;li&gt;4.5.1 -&amp;gt; 5.0.0 ... major API changes are likely, probably need to modify code, definitely need to modify code if you haven't been keeping up with the deprecation warnings.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Because of point #2 in the new policy being proposed, users of some version Y will have no indication whatsoever of how likely it is that they can upgrade to version Z unless they have been religiously updating to every new minor release and eliminating the use of deprecated methods in their code.&lt;/p&gt;

&lt;p&gt;This could have the adverse effect of discouraging users from upgrading if they haven't been keeping up with the bleeding edge of releases.&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;This loss of information that would come from allowing deprecated methods to be removed in minor releases is one of the reasons that i (half heartedly) suggested moving to a 4 part version numbers as a way to maintain information in the version number about API compatibility without conveying any implications of index format incompatibility or scaring people with "major" version number changes...&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.nabble.com/Re%3A-Back-Compatibility-p15032881.html" class="external-link"&gt;http://www.nabble.com/Re%3A-Back-Compatibility-p15032881.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But I don't think that's really necessary: Ultimately there isn't anything about this proposal that would actually change how easy/hard it is to remove APIs or change functionality between one release and the next &amp;#8211; it just alters what naming convention would be allowed when such changes take place.  &lt;/p&gt;

&lt;p&gt;Currently, users can be (for lack of a better word) "wary" of major version changes (ie: 4.5.1 -&amp;gt; 5.0.0) because they sound like really big releases that might change things in a way that break their apps.  Consequently, developers are wary of having major releases too often and instead we try to have lots of minor releases and leave bad/broken APIs deprecated for a long time and then "batch" up their removal in major releases.  Switching to a policy where we remove deprecated methods (or change runtime behavior) during minor releases is likely just going to make any users who have been wary of major releases in the past wary of minor releases in the future.&lt;/p&gt;

&lt;p&gt;We could accomplish the same underlying goal (increase the flexibility in developing Lucene internals by allowing faster removal of deprecated/broken APIs/functionality that may be in the way) without changing our current policy (and without losing any compatibility info in the version numbers) by getting past whatever hangups we may have about what it means to have a "major" release and start increment the "major" version number whenever necessary to get the job done.  &lt;/p&gt;

&lt;p&gt;If 4.1.0 has run time behavior that we want to change ASAP, then make the next release 5.0.0 &amp;#8211; even if it's only 2 months later.  If 5.0.0 has an API that we realize is completely hamstringing our ability to make some amazing performance improvements to the internals, then make the next release be 6.0.0.&lt;/p&gt;

&lt;p&gt;Rethinking how we use the use the current compatibility policy can accomplish the same goals as changing it without impacting our users in any way beyond how tis proposal would impact them &amp;#8211; the releases would still come out at the same time and have the same changes &amp;#8211; the only difference would be in what we call them, and those names would actually convey useful information.&lt;/p&gt;


</comment>
                    <comment id="12726590" author="markrmiller@gmail.com" created="Thu, 2 Jul 2009 19:01:55 +0100">&lt;p&gt;I agree. I think we have the tools we need, we just need to accelerate our releases if we want to deprecate faster. We could have made 2.4 2.9. We could have released faster with fewer issues.&lt;/p&gt;

&lt;p&gt;I also think that its confusing that you could drop in 4.2 over 4.1, but try 4.4 and your out of luck. Major number bumps and nice and intuitive.&lt;/p&gt;

&lt;p&gt;I was behind a change in back compat (though I had no idea what change - was waiting for the right one &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; ), basically for the case of defaults for new users - out of the box performance/experience. The version stuff Mike came up with appears to fix that rather nicely without tweaking back compat versioning schemes though. We can just jump numbers faster.&lt;/p&gt;</comment>
                    <comment id="12762329" author="michaelbusch" created="Mon, 5 Oct 2009 20:46:01 +0100">&lt;p&gt;Now that 2.9 is out and 3.0 is close, I'd like to get back to this one to get to a conclusion.&lt;/p&gt;

&lt;p&gt;We had several informal +1s on javadev and a -0 here from Hoss and Mark. So no -1 yet.&lt;/p&gt;

&lt;p&gt;I think the final decision should be made with an official vote on java-dev. How does everyone feel about this? Shall we have a vote right away? I think it might be a good idea to get some feedback about this proposal on java-user first? Or we could even wait a month and bring it up in Oakland at the Lucene BOF?&lt;/p&gt;</comment>
                    <comment id="12778262" author="thetaphi" created="Mon, 16 Nov 2009 08:41:29 +0000">&lt;p&gt;This is the last issue blocking 3.0 somehow. Should I remove the fix version, or do we already have some "official" backwards document available somewhere to resolve this?&lt;/p&gt;</comment>
                    <comment id="12778268" author="michaelbusch" created="Mon, 16 Nov 2009 08:57:19 +0000">&lt;p&gt;This doesn't need to block 3.0.&lt;/p&gt;

&lt;p&gt;We discussed this on ApacheCon and I was going to write up a new proposal soon.&lt;/p&gt;</comment>
                    <comment id="12857103" author="michaelbusch" created="Wed, 14 Apr 2010 22:28:24 +0100">&lt;p&gt;&lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12882641" author="shaie" created="Fri, 25 Jun 2010 18:13:25 +0100">&lt;p&gt;Is that issue still relevant? In light of the current policy?&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 Jul 2009 17:48:57 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12060</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26022</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1650] Small fix in CustomScoreQuery JavaDoc</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1650</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I have fixed the javadoc for  "Modified Score" formular in CustomScoreQuery. - Patch attached: customScoreQuery_JavaDoc.patch &lt;/p&gt;

&lt;p&gt;I'm quite curious why the method:&lt;br/&gt;
 public float customScore(int doc, float subQueryScore, float valSrcScores[]) &lt;/p&gt;

&lt;p&gt;calls public float customScore(int doc, float subQueryScore, float valSrcScore])  only in 2 of the 3 cases which makes the choice to override either one of the customScore methods dependent on the number of ValueSourceQuery passed to the constructor. I figure it would be more consistent if it would call the latter in all 3 cases.&lt;/p&gt;

&lt;p&gt;I also attached a patch which proposes a fix for that issue. The patch does also include the JavaDoc issue mentioned above.&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;customScoreQuery_CodeChange+JavaDoc.patch&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12425985">LUCENE-1650</key>
            <summary>Small fix in CustomScoreQuery JavaDoc</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="simonwillnauer">Simon Willnauer</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Thu, 21 May 2009 02:56:33 +0100</created>
                <updated>Mon, 13 May 2013 03:48:14 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/javadocs</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12718286" author="markrmiller@gmail.com" created="Thu, 11 Jun 2009 04:16:09 +0100">&lt;p&gt;This looks like it makes sense to me. Unless anyone objects, I'd like to commit it for 2.9.&lt;/p&gt;</comment>
                    <comment id="12723589" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 15:55:38 +0100">&lt;p&gt;Im going to commit this soon, both changes.&lt;/p&gt;</comment>
                    <comment id="12727612" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 17:30:16 +0100">&lt;p&gt;No I'm not &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Yonik, could you take a peak at this?&lt;/p&gt;

&lt;p&gt;Why the javadoc change? It used to refer to the method parameter - that makes sense to me ? Where is valSrcScore bound?&lt;/p&gt;

&lt;p&gt;As far as the second change, its a logical change, but it is a back compat change. I'm thinking it could be considered a bug fix, but the old weirdness worked if&lt;br/&gt;
you followed the javadoc no?&lt;/p&gt;

&lt;p&gt;I'm pro the change, and it prob doesnt make sense to add a static or property to allow the old behaviour, but another opinion...&lt;/p&gt;</comment>
                    <comment id="12727613" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 17:34:19 +0100">&lt;p&gt;updated to trunk in any case.&lt;/p&gt;</comment>
                    <comment id="12727675" author="yseeley@gmail.com" created="Mon, 6 Jul 2009 19:25:50 +0100">&lt;p&gt;Not sure why you wanted me to take a peek - this isn't part of the original Solr function query stuff, so I won't know any more than anyone else.&lt;br/&gt;
Anyway the current code looks like it's working as designed?  Perhaps it wasn't the best interface, but not worth breaking compatibility over, and not necessary to improve for 2.9 IMO.&lt;/p&gt;</comment>
                    <comment id="12727688" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 19:38:44 +0100">&lt;blockquote&gt;&lt;p&gt;Not sure why you wanted me to take a peek - this isn't part of the original Solr function query stuff, so I won't know any more than anyone else.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because you knew that, and I didnt &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; And because your pretty smart about these things. And your usually happy to ignore when you are not interested, so I figured you always had that out here &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I think there are usually okay odds that you might know more than someone else.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Anyway the current code looks like it's working as designed? Perhaps it wasn't the best interface, but not worth breaking compatibility over, and not necessary to improve for 2.9 IMO.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's  kind of what I started thinking, and I just wanted someone smarter than me to confirm or deny. It also just seemed like it was &lt;b&gt;very&lt;/b&gt; unlikely that it would affect anyone if the change was made, so why not just make it consistent. I could go either way, but technically it is a backcompat break, and it would be easier to just leave it.&lt;/p&gt;

&lt;p&gt;I'll just push it off 2.9 for now.&lt;/p&gt;

&lt;p&gt;I'm just trying to resolve/push 2.9 issues - didnt mean to single you out for any specific work &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12408650" name="customScoreQuery_CodeChange+JavaDoc.patch" size="2101" author="simonwillnauer" created="Thu, 21 May 2009 02:57:21 +0100"/>
                    <attachment id="12408649" name="customScoreQuery_JavaDoc.patch" size="618" author="simonwillnauer" created="Thu, 21 May 2009 02:57:21 +0100"/>
                    <attachment id="12412628" name="LUCENE-1650.patch" size="3664" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 17:34:19 +0100"/>
                    <attachment id="12410828" name="LUCENE-1650.patch" size="3683" author="markrmiller@gmail.com" created="Tue, 16 Jun 2009 17:33:37 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 11 Jun 2009 03:16:09 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12108</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26071</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1574] PooledSegmentReader, pools SegmentReader underlying byte arrays</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1574</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;PooledSegmentReader pools the underlying byte arrays of deleted docs and norms for realtime search.  It is designed for use with IndexReader.clone which can create many copies of byte arrays, which are of the same length for a given segment.  When pooled they can be reused which could save on memory.  &lt;/p&gt;

&lt;p&gt;Do we want to benchmark the memory usage comparison of PooledSegmentReader vs GC?  Many times GC is enough for these smaller objects.&lt;/p&gt;</description>
                <environment/>
            <key id="12419587">LUCENE-1574</key>
            <summary>PooledSegmentReader, pools SegmentReader underlying byte arrays</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="jasonrutherglen">Jason Rutherglen</reporter>
                        <labels>
                    </labels>
                <created>Thu, 26 Mar 2009 23:38:32 +0000</created>
                <updated>Fri, 10 May 2013 00:05:35 +0100</updated>
                                    <version>2.4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/other</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                          <timeoriginalestimate seconds="604800">168h</timeoriginalestimate>
                    <timeestimate seconds="604800">168h</timeestimate>
                                  <comments>
                    <comment id="12695115" author="mikemccand" created="Thu, 2 Apr 2009 21:23:44 +0100">&lt;p&gt;Presumably it wouldn't save on memory (the pool would presumably sometimes be holding onto spares, for future reuse), but could save on time, right?&lt;/p&gt;

&lt;p&gt;Or, maybe instead we could spend our effort making a simple transactional data structure for holding deletes/norms (I think there's already an issue on this &amp;#8211; maybe it's &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1526" title="For near real-time search, use paged copy-on-write BitVector impl"&gt;&lt;del&gt;LUCENE-1526&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="12695130" author="jasonrutherglen" created="Thu, 2 Apr 2009 21:52:00 +0100">&lt;p&gt;True the pool would hold onto spares, but they would expire.&lt;br/&gt;
It's mostly useful for the large on disk segments as those byte&lt;br/&gt;
arrays (for BitVectors) are large, and because there's more docs&lt;br/&gt;
in them would get hit with deletes more often, and so they'd be&lt;br/&gt;
reused fairly often. &lt;/p&gt;

&lt;p&gt;I'm not knowledgeable enough to say whether the transactional&lt;br/&gt;
data structure will be fast enough. We had been using&lt;br/&gt;
&lt;a href="http://fastutil.dsi.unimi.it/docs/it/unimi/dsi/fastutil/ints/IntR" class="external-link"&gt;http://fastutil.dsi.unimi.it/docs/it/unimi/dsi/fastutil/ints/IntR&lt;/a&gt;&lt;br/&gt;
BTreeSet.html in Zoie for deleted docs and it's way slow. Binary&lt;br/&gt;
search of an int array is faster, albeit not fast enough. The&lt;br/&gt;
multi dimensional array thing isn't fast enough (for searching)&lt;br/&gt;
as we implemented this in Bobo. It's implemented in Bobo because&lt;br/&gt;
we have a multi value field cache (which is quite large because&lt;br/&gt;
for each doc we're storing potentially 64 or more values in an&lt;br/&gt;
inplace bitset) and a single massive array kills the GC. In some&lt;br/&gt;
cases this is faster than a single large array because of the&lt;br/&gt;
way Java (or the OS?) transfers memory around through the CPU&lt;br/&gt;
cache. &lt;/p&gt;</comment>
                    <comment id="12718232" author="mikemccand" created="Wed, 10 Jun 2009 23:18:38 +0100">&lt;p&gt;Moving out.&lt;/p&gt;</comment>
                    <comment id="12737950" author="john.wang@gmail.com" created="Sat, 1 Aug 2009 21:37:36 +0100">&lt;p&gt;Re: Zoie and deleted docs:&lt;br/&gt;
That is no longer true, Zoie is using a bloom filter over a intHash set from fastutil for exactly the perf reason Jason pointed.&lt;/p&gt;</comment>
                    <comment id="12776299" author="jasonrutherglen" created="Wed, 11 Nov 2009 06:03:42 +0000">&lt;p&gt;Yonik,&lt;/p&gt;

&lt;p&gt;Do you recommend using the method in SimpleStringInterner for lockless pooling?&lt;/p&gt;</comment>
                    <comment id="12776321" author="jasonrutherglen" created="Wed, 11 Nov 2009 06:50:11 +0000">&lt;p&gt;I suppose as we're on Java 1.5, ConcurrentLinkedQueue can be used.&lt;/p&gt;</comment>
                    <comment id="12776576" author="jasonrutherglen" created="Wed, 11 Nov 2009 19:02:10 +0000">&lt;p&gt;A likely optimization for this patch is we'll only pool if the doc count is above a threshold, 100,000 seems like a good number.  Also pooling will be optional.  &lt;/p&gt;</comment>
                    <comment id="12984996" author="jasonrutherglen" created="Fri, 21 Jan 2011 23:24:09 +0000">&lt;p&gt;I'm going to revive this, and if it works fine for trunk, then we can use the basic system for RT eg, &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2312" title="Search on IndexWriter&amp;#39;s RAM Buffer"&gt;LUCENE-2312&lt;/a&gt;.  I think the only open question is how we'll shrink the pool, most likely there'd be an expiration on the pooled objects.  With RT, the parallel arrays will grow, so the pool will need to be size based, eg, when the arrays are grown, all of the previous arrays may be forcefully evicted, or they may simply expire.&lt;/p&gt;
</comment>
                    <comment id="12985091" author="mikemccand" created="Sat, 22 Jan 2011 11:16:09 +0000">&lt;p&gt;I'm working on an initial patch for this...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think the only open question is how we'll shrink the pool, most likely there'd be an expiration on the pooled objects.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we can simply have a "max pooled free bit vectors"... or we may want to expire by time/staleness as well.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With RT, the parallel arrays will grow, so the pool will need to be size based, eg, when the arrays are grown, all of the previous arrays may be forcefully evicted, or they may simply expire.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True... but, like the other per-doc arrays, the BV can be overallocated (ArrayUtil.oversize) to accommodate further added docs.&lt;/p&gt;</comment>
                    <comment id="12985150" author="jasonrutherglen" created="Sat, 22 Jan 2011 16:18:28 +0000">&lt;p&gt;ThreadPoolExecutor can act as a guide, it's main parameters are corePoolSize, maximumPoolSize, keepAliveTime.  &lt;/p&gt;

&lt;p&gt;In regards to using System.arraycopy for the RT parallel arrays, if they grow to become too large, then SC could become a predominant cost.  However if the default thread states is 8, which'd mean that many DWPTs, the arrays would probably never grow to be too large for their SC to become noticeably expensive, hopefully.&lt;/p&gt;</comment>
                    <comment id="12985408" author="jasonrutherglen" created="Sun, 23 Jan 2011 20:51:58 +0000">&lt;p&gt;We want to record the deletes between getReader calls however there's no way to know ahead of time if a term or query is going to hit many documents or not, meaning we can't always save del docids, because we'd have too many ints in RAM.  I'm curious how we plan on handling this case?&lt;/p&gt;</comment>
                    <comment id="12985877" author="mikemccand" created="Mon, 24 Jan 2011 18:28:39 +0000">&lt;blockquote&gt;&lt;p&gt;I'm curious how we plan on handling this case?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should keep the replay log smallish, or, expire it aggressively with age.  I suspect this opto is only going to be "worth it" for very frequent reopens... but I'm not sure yet.&lt;/p&gt;</comment>
                    <comment id="12985971" author="mikemccand" created="Mon, 24 Jan 2011 20:37:07 +0000">&lt;p&gt;Attached rough patch.&lt;/p&gt;

&lt;p&gt;At least one test fails....&lt;/p&gt;

&lt;p&gt;And, I haven't yet seen that this is in fact worthwhile.  The rough benchmark I have (which hits other issues so the results aren't conclusive yet) doesn't show much difference w/ this patch.  I think this patch may only be worthwhile at insane reopen rates, which I think in practice is rarely a legitimate use case (even though many apps start off thinking it is).&lt;/p&gt;</comment>
                    <comment id="12985975" author="jasonrutherglen" created="Mon, 24 Jan 2011 20:51:06 +0000">&lt;p&gt;What size segments is the benchmark deleting against?  Maybe we're underestimating the speed of arraycopy, eg, it's really a hardware operation that could be optimized?&lt;/p&gt;</comment>
                    <comment id="12986630" author="mikemccand" created="Tue, 25 Jan 2011 19:28:29 +0000">&lt;p&gt;I've been testing on a 25M doc index (all of en Wikipedia, at least as of March 2010).&lt;/p&gt;

&lt;p&gt;Yes, I think likely alloc of big BitVector, System.arraycopy, destroying it, may be a fairly low cost compared to lucene resolving the deleted term, indexing the doc, flushing the tiny segment, etc.&lt;/p&gt;</comment>
                    <comment id="12986637" author="jasonrutherglen" created="Tue, 25 Jan 2011 19:33:54 +0000">&lt;blockquote&gt;&lt;p&gt;I think likely alloc of big BitVector, System.arraycopy, destroying it, may be a fairly low cost compared to lucene resolving the deleted term, indexing the doc, flushing the tiny segment, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, and if we pool the byte[]s we'd take the cost of instantiating and GC'ing out of the picture in the high NRT throughput case.  It's counter intuitive and will require testing.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12469203" name="LUCENE-1574.patch" size="26999" author="mikemccand" created="Mon, 24 Jan 2011 20:37:07 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 Apr 2009 20:23:44 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12181</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26149</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1560] maxDocBytesToAnalyze should be required arg up front</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1560</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We recently changed IndexWriter to require you to specify up-front&lt;br/&gt;
MaxFieldLength, on creation, so that you are aware of this dangerous&lt;br/&gt;
"loses stuff" setting.  Too many developers had fallen into the trap&lt;br/&gt;
of "how come my search can't find this document...".&lt;/p&gt;

&lt;p&gt;I think we should do the same with "maxDocBytesToAnalyze" with&lt;br/&gt;
highlighter?&lt;/p&gt;

&lt;p&gt;Spinoff from this thread:&lt;/p&gt;

&lt;p&gt;    &lt;a href="http://www.nabble.com/Lucene-Highlighting-and-Dynamic-Summaries-p22385887.html" class="external-link"&gt;http://www.nabble.com/Lucene-Highlighting-and-Dynamic-Summaries-p22385887.html&lt;/a&gt;&lt;/p&gt;</description>
                <environment/>
            <key id="12416823">LUCENE-1560</key>
            <summary>maxDocBytesToAnalyze should be required arg up front</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                        <label>dead</label>
                    </labels>
                <created>Fri, 13 Mar 2009 10:14:06 +0000</created>
                <updated>Mon, 13 May 2013 03:50:10 +0100</updated>
                                    <version>2.4.1</version>
                                <fixVersion>4.4</fixVersion>
                                <component>modules/highlighter</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12718234" author="mikemccand" created="Wed, 10 Jun 2009 23:21:42 +0100">&lt;p&gt;Moving out.&lt;/p&gt;</comment>
                    <comment id="13458972" author="steve_rowe" created="Wed, 19 Sep 2012 19:56:29 +0100">&lt;p&gt;I think this issue is still valid?&lt;/p&gt;

&lt;p&gt;The nabble.com email thread link in the description is broken - here's the markmail.org thread: &lt;a href="http://markmail.org/thread/2pcdjsurrrqoxuew" class="external-link"&gt;http://markmail.org/thread/2pcdjsurrrqoxuew&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;maxDocBytesToAnalyze()&lt;/tt&gt; was deprecated as part of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1132" title="Highlighter Documentation updates"&gt;&lt;del&gt;LUCENE-1132&lt;/del&gt;&lt;/a&gt; in favor of &lt;tt&gt;maxDocCharsToAnalyze()&lt;/tt&gt;, and then removed with the 3.0 release deprecation removals as part of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-2022" title="remove contrib deprecations"&gt;&lt;del&gt;LUCENE-2022&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is one commented-out vestige that ought to be renamed or removed (my vote is removal - this was commented out six years ago):&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeHeader panelHeader" style="border-bottom-width: 1px;"&gt;&lt;b&gt;Highlighter.java&lt;/b&gt;&lt;/div&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
265: &lt;span class="code-comment"&gt;//	  &lt;span class="code-keyword"&gt;if&lt;/span&gt;(lastEndOffset&amp;gt;maxDocBytesToAnalyze)
&lt;/span&gt;266: &lt;span class="code-comment"&gt;//   {
&lt;/span&gt;267: &lt;span class="code-comment"&gt;//     &lt;span class="code-keyword"&gt;break&lt;/span&gt;;
&lt;/span&gt;268: &lt;span class="code-comment"&gt;//   }&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 19 Sep 2012 18:56:29 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2890</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26163</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1518] Merge Query and Filter classes</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1518</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This issue presents a patch, that merges Queries and Filters in a way, that the new Filter class extends Query. This would make it possible, to use every filter as a query.&lt;/p&gt;

&lt;p&gt;The new abstract filter class would contain all methods of ConstantScoreQuery, deprecate ConstantScoreQuery. If somebody implements the Filter's getDocIdSet()/bits() methods he has nothing more to do, he could just use the filter as a normal query.&lt;/p&gt;

&lt;p&gt;I do not want to completely convert Filters to ConstantScoreQueries. The idea is to combine Queries and Filters in such a way, that every Filter can automatically be used at all places where a Query can be used (e.g. also alone a search query without any other constraint). For that, the abstract Query methods must be implemented and return a "default" weight for Filters which is the current ConstantScore Logic. If the filter is used as a real filter (where the API wants a Filter), the getDocIdSet part could be directly used, the weight is useless (as it is currently, too). The constant score default implementation is only used when the Filter is used as a Query (e.g. as direct parameter to Searcher.search()). For the special case of BooleanQueries combining Filters and Queries the idea is, to optimize the BooleanQuery logic in such a way, that it detects if a BooleanClause is a Filter (using instanceof) and then directly uses the Filter API and not take the burden of the ConstantScoreQuery (see &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Here some ideas how to implement Searcher.search() with Query and Filter:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;User runs Searcher.search() using a Filter as the only parameter. As every Filter is also a ConstantScoreQuery, the query can be executed and returns score 1.0 for all matching documents.&lt;/li&gt;
	&lt;li&gt;User runs Searcher.search() using a Query as the only parameter: No change, all is the same as before&lt;/li&gt;
	&lt;li&gt;User runs Searcher.search() using a BooleanQuery as parameter: If the BooleanQuery does not contain a Query that is subclass of Filter (the new Filter) everything as usual. If the BooleanQuery only contains exactly one Filter and nothing else the Filter is used as a constant score query. If BooleanQuery contains clauses with Queries and Filters the new algorithm could be used: The queries are executed and the results filtered with the filters.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For the user this has the main advantage: That he can construct his query using a simplified API without thinking about Filters oder Queries, you can just combine clauses together. The scorer/weight logic then identifies the cases to use the filter or the query weight API. Just like the query optimizer of a RDB.&lt;/p&gt;
</description>
                <environment/>
            <key id="12412224">LUCENE-1518</key>
            <summary>Merge Query and Filter classes</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="thetaphi">Uwe Schindler</reporter>
                        <labels>
                    </labels>
                <created>Mon, 12 Jan 2009 19:44:52 +0000</created>
                <updated>Fri, 10 May 2013 00:05:36 +0100</updated>
                                    <version>2.4</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>1</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12663064" author="thetaphi" created="Mon, 12 Jan 2009 19:50:33 +0000">&lt;p&gt;This is the patch.&lt;/p&gt;

&lt;p&gt;Most tests pass. Problems are only in the tests that check the explanations (TestSimpleExplanations) when wrapping the deprecated ConstantScoreQuery. For that the test must be rewritten to directly get the explanation from the Filter class (that is now a query). ConstantScoreQuery is just a no-op that rewrites to the wrapped Filter itsself but returns no weight and no explanation.&lt;/p&gt;

&lt;p&gt;Some small problems are the handling of toString()/toString(fieldname). The abstract Filter class has no toString(fieldname), but Query has one (but abstract). So there must be a default or Field implementations must be extended to provide one (bc break). The patch uses some bad workaround for that, maybe somebody has a better idea how to handle this.&lt;/p&gt;</comment>
                    <comment id="12663069" author="thetaphi" created="Mon, 12 Jan 2009 19:56:56 +0000">&lt;p&gt;Further patches must now remove the deprecated ConstantScoreQuery from the core and contrib classes (RangeQuery gets identical to RangeFilter and so on).&lt;/p&gt;

&lt;p&gt;The rewrite method (inherited from the Filter API) may then be changed in RangeQuery to return a BooleanQuery when useConstantScoreRewrite==false. In this case the RangeFilter gets the same as the Query and when used as Query, can also rewrite to TermQueries. But as I know, this is to be removed in Lucene 3.0. In this case every RangeFilter and the others can simply be constant score or filters.&lt;/p&gt;</comment>
                    <comment id="12663077" author="creamyg" created="Mon, 12 Jan 2009 20:24:14 +0000">&lt;p&gt;&amp;gt; the query can be executed and returns score 1.0 for all matching documents&lt;/p&gt;

&lt;p&gt;Why 1.0 and not 0.0?  I chose to use 0.0 in KS because then a Filter would effectively perform only binary filtering and never affect scores.&lt;/p&gt;

&lt;p&gt;Perhaps your envisioned query optimization algo ensures that the Filter will only serve as a DocIDSetIterator unless it's used as a top-level Query?  Can we agree that a Filter used as a sub-clause in a complex query should not contribute to the aggregate score?&lt;/p&gt;

&lt;p&gt;UPDATE: A closer reading reveals that the original issue text contains the answer to this question (we agree), so the only remaining question is whether a top level query would return hits with scores of 0.0 or 1.0. Which is probably a bikeshed painting issue.&lt;/p&gt;</comment>
                    <comment id="12663090" author="cutting" created="Mon, 12 Jan 2009 20:49:59 +0000">&lt;p&gt;&amp;gt; Why 1.0 and not 0.0?&lt;/p&gt;

&lt;p&gt;0.0 does seem more appropriate, since scores are typically added, not multiplied.  There used to be places that filtered anything with a 0.0 score however.  Are any of those left?&lt;/p&gt;</comment>
                    <comment id="12663091" author="thetaphi" created="Mon, 12 Jan 2009 20:59:38 +0000">&lt;blockquote&gt;&lt;p&gt;Why 1.0 and not 0.0? I chose to use 0.0 in KS because then a Filter would effectively perform only binary filtering and never affect scores.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You are right. I was coming from the ConstantScoreQuery that has a score of 1.0 in the result.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Perhaps your envisioned query optimization algo ensures that the Filter will only serve as a DocIDSetIterator unless it's used as a top-level Query? Can we agree that a Filter used as a sub-clause in a complex query should not contribute to the aggregate score?&lt;/p&gt;

&lt;p&gt;UPDATE: A closer reading reveals that the original issue text contains the answer to this question (we agree), so the only remaining question is whether a top level query would return hits with scores of 0.0 or 1.0. Which is probably a bikeshed painting issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is exactly what I was thinking about. But it is not only a top level query. In the case of a BooleanQuery containing only one clause that is a filter, the constant score implementation must also used (as a Filter alone is useless).&lt;/p&gt;

&lt;p&gt;ConstantScoreQuery returns 1.0 if executed alone or alone in a BooleanQuery.&lt;/p&gt;

&lt;p&gt;For backwards compatibility, I think we should just use the following logic:&lt;br/&gt;
A filter is a query, but can also be used as a query (if alone). The default implementation for this is a constant score query as currently when wrapping a filter with ConstantScoreQuery. In all other cases (combined with other boolean clauses and not alone), the score calculation is removed and you can say 0.0f (if additive).&lt;/p&gt;</comment>
                    <comment id="12663120" author="eksdev" created="Mon, 12 Jan 2009 22:20:13 +0000">&lt;p&gt;nice, &lt;br/&gt;
you did it top down (api), Paul takes it bottom up (speed). &lt;/p&gt;

&lt;p&gt;this makes some really crazy things possible, e.g. implementing normal TermQuery as a "DirectFilter" and when the optimization of the BooleanQuery gets done (no Score calculation, direct usage of DocIdSetIterators) you can speed up some queries containing TermQuery  without really instantiating Filter. Of course only for cases where tf/idf/norm can be ignored. &lt;/p&gt;

&lt;p&gt;Kind of middle-ground between Filter and full ranked TermQuery (better said any BooleanQuery!), Faster than ranked case due to the switched off score calculation and more comfortable than Filter usage, no instantiation of DocIdSet-s... &lt;/p&gt;

&lt;p&gt;very nice indeed, smooth mix between ranked and "pure boolean" model with both benefits.  &lt;/p&gt;</comment>
                    <comment id="12663131" author="paul.elschot@xs4all.nl" created="Mon, 12 Jan 2009 22:36:24 +0000">&lt;blockquote&gt;&lt;p&gt;There used to be places that filtered anything with a 0.0 score however. Are any of those left?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;They should be gone by now, even from the javadocs.&lt;/p&gt;</comment>
                    <comment id="12663133" author="jasonrutherglen" created="Mon, 12 Jan 2009 22:38:03 +0000">&lt;p&gt;Hopefully this will allow caching of individual term queries even if they are a part of a boolean query?&lt;/p&gt;</comment>
                    <comment id="12663134" author="thetaphi" created="Mon, 12 Jan 2009 22:38:45 +0000">&lt;p&gt;In my opinion, both approches could be combined. I do not know how the scoring and the whole BooleanQery works, I did only the merging of the API. If we have consensus, that this may be good, I could remove the rest of deprecated ConstantScoreQuery. The new code then works without any problems and backwards compatible as before, but with no optimization.&lt;/p&gt;

&lt;p&gt;Then Paul could adapt his patch to not create new methods to add Filter clauses to BooleanQueries, but do just some difference in the whole BooleanQuery logic like that:&lt;/p&gt;

&lt;p&gt;if (clause.query instanceof Filter) &lt;/p&gt;
{ do only filter optimization from Paul's patch }
&lt;p&gt; else &lt;/p&gt;
{ do conventional boolean scoring }

&lt;p&gt;If he cannot do the optimization (because the Filter is alone in BooleanQuery) he could just fall back to the standard query logic (that uses implicit the current ConstantScoreQuery algorithm).&lt;/p&gt;

&lt;p&gt;But before start implementing more for removing the deprecated class, I wanted to hear some more ideas, maybe something completely different (like every query can also automatically be a filter): Only one superclass "Query" no filter anymore, every query clause can implement a filter and/or a query with always a Fallback to the other side (if no filter implementation provided, a filter is provided by a algorithm like QueryFilter; if no weight/rewrite, constant score weight is provided).&lt;/p&gt;

&lt;p&gt;Just ideas...&lt;/p&gt;</comment>
                    <comment id="12664432" author="paul.elschot@xs4all.nl" created="Fri, 16 Jan 2009 08:28:27 +0000">&lt;p&gt;Broadly, the patch moves the internal ConstantScorer class from ConstantScoreQuery to Filter.&lt;/p&gt;

&lt;p&gt;That means that to add a Filter to a BooleanQuery as a clause, the user will need an option whether or not to have the Filter take part in the scoring.&lt;br/&gt;
So, with this for BooleanQuery an option of scoring/non scoring in the API for adding clauses will be needed,&lt;br/&gt;
whereas without this the possibility to add a Filter will be needed. I don't care either way.&lt;/p&gt;

&lt;p&gt;Are there other places where a Query is used now in which a Filter could also be useful?&lt;br/&gt;
And are there other use cases for ConstantScoreQuery than on top of a Filter?&lt;/p&gt;</comment>
                    <comment id="12664433" author="thetaphi" created="Fri, 16 Jan 2009 08:33:06 +0000">&lt;blockquote&gt;&lt;p&gt;That means that to add a Filter to a BooleanQuery as a clause, the user will need an option whether or not to have the Filter take part in the scoring.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why is this needed? A constant score query does not effect the scoring at all. Why not simply optimize the scoreing of filters away in BooleanQuery in all cases (if there are other queries available in the BooleanQuery that do scoring)?&lt;/p&gt;</comment>
                    <comment id="12664569" author="paul.elschot@xs4all.nl" created="Fri, 16 Jan 2009 17:18:39 +0000">&lt;p&gt;The option is needed because the coordination factor in the result score depends on the number of matching query clauses.&lt;/p&gt;

&lt;p&gt;Also, without the option, a check will have to be done whether the score is constant and 0. I'm not even sure whether that would always work for subclasses of Filter that change the score to a possibly non constant value.&lt;/p&gt;</comment>
                    <comment id="12665750" author="paul.elschot@xs4all.nl" created="Wed, 21 Jan 2009 08:01:04 +0000">&lt;p&gt;Even now that this has had some time, I still think &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt; should go before this, and implement Filter clauses to BooleanQuery using the current simple Filter in a backward compatible way.&lt;/p&gt;

&lt;p&gt;Then, in case the resulting BooleanQuery really turns out to be too complicated for general use, this issue could be taken up again.&lt;/p&gt;</comment>
                    <comment id="12665796" author="thetaphi" created="Wed, 21 Jan 2009 11:03:40 +0000">&lt;p&gt;I have no problem with this. I was waiting for more comments on this befor starting to factor out ConstantScoreQuery from the rest of the core/contrib lasses. In principle the current patch is backwards compatible, so you could also use it as basis for your work. In the case that my idea is not the way to go, this may be lost time (but: as always queries are rewritten before execution, the unneeded ConstantScoreQuery rewrites to the filter, and your instanceof check in BooleanScorer/-Query should correctly detect all filters).&lt;/p&gt;</comment>
                    <comment id="12665809" author="paul.elschot@xs4all.nl" created="Wed, 21 Jan 2009 12:02:19 +0000">&lt;p&gt;I don't think the time is lost. In case there is a way to really simplify/unify Query and Filter, I want to know, too.&lt;/p&gt;

&lt;p&gt;Btw. there is no instanceof check in the patch at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt;. The reason for that is that adding a Filter as a clause is done by another method than the method to add a Query, so the compiler can deal with the difference.&lt;/p&gt;</comment>
                    <comment id="12671851" author="mikemccand" created="Mon, 9 Feb 2009 13:36:52 +0000">&lt;p&gt;At first blush, this patch seems backwards: shouldn't the base class&lt;br/&gt;
be a Filter (which does "pure matching" with no scoring) and then&lt;br/&gt;
subclass of that (Query) would add in scoring?&lt;/p&gt;

&lt;p&gt;But, then I think I appreciate why we did this: it's because we accept&lt;br/&gt;
Query all over the place now, so it's a far less disruptive change to&lt;br/&gt;
have Filter be the subclass.&lt;/p&gt;

&lt;p&gt;Or... we might want to allow scoring without matching, so maybe they&lt;br/&gt;
should be fully independent?&lt;/p&gt;

&lt;p&gt;EG when I want to dynamically boost relevance scores (eg by recency,&lt;br/&gt;
popularity, personalizaztion, etc.), I would want to make a "pure&lt;br/&gt;
scorer" that has absolutely no matching role, whose score gets "mixed&lt;br/&gt;
in" based on the sub-query's boost.&lt;/p&gt;

&lt;p&gt;ValueSourceQuery is an example: it has a "degenerate" matcher (matches&lt;br/&gt;
all docs) whose only purpose is to produce custom per-doc scoring.&lt;/p&gt;

&lt;p&gt;Shouldn't I be able to add a "purer scorer" (no matching) as a clause&lt;br/&gt;
onto BooleanQuery?&lt;/p&gt;</comment>
                    <comment id="12672199" author="thetaphi" created="Tue, 10 Feb 2009 07:53:55 +0000">&lt;p&gt;This patch does not want to completely merge queries and filters. It only gives the possibility to use a Filter directly as a query. So ValueSourceQuery will stay direct subclass of Query and not of Filter and can be used as scorer only. I think completely merging both is not a good idea.&lt;/p&gt;

&lt;p&gt;But for easy usage, the idea of making filter automatically work as query (using a constant score algorithm) may be good. So queries (e.g. without matching, also conventional queries)  can stay alive, but filters can be used as query (and BooleanQuery could automatically have filters as clauses, and is able to optimize and factor away scoring).&lt;/p&gt;

&lt;p&gt;The user only has an easy understandable API and would not be affected with thinking about "is it better to use a ConstantScoreQuery or should I just filter the results of the other boolean query clauses? What if I only have the filter, do I need a MatchAllDocsQuery and filter it, or better use ConstantScore in this case?".&lt;/p&gt;</comment>
                    <comment id="12700325" author="jasonrutherglen" created="Fri, 17 Apr 2009 22:00:26 +0100">&lt;p&gt;What's the status of this patch? Have we agreed on how the class&lt;br/&gt;
structure is going to look and what the optimizations will look&lt;br/&gt;
like?&lt;/p&gt;</comment>
                    <comment id="12700437" author="thetaphi" created="Sat, 18 Apr 2009 10:15:10 +0100">&lt;p&gt;My opinion is that the attached patch has most backwards-compatibility (with some small toString() issues), but it makes Filter a subclass of query with the default constant score logic. Further work to remove the extra Filter classes for MultiTermQueries (RangeFilter, PrefixFilter,...) and so on can be done later or as part of this patch (did'nt want to start with this before being sure, that the API looks good). &lt;/p&gt;

&lt;p&gt;This will not make anything faster, but make the API for users simplier. No longer differentiate between query and filter.&lt;/p&gt;

&lt;p&gt;The optimization can be part of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt;. In this case, BooleanQuery API does not need to be changed (no extra Filter clauses, because Filters can be added directly as normal Query clauses). The optimizations from 1345 then could directltly test with instanceof, if something is a filter.&lt;/p&gt;

&lt;p&gt;WAS: One problem is Fuzzy Query that can also be used as Filter (subclass of MultiTermQuery), but there scoring is important.&lt;/p&gt;

&lt;p&gt;EDIT:&lt;br/&gt;
This is not a problem: MultiTermQuery will also be a filter and vice versa. It depends on what rewrite does. If ConstantScoreMode is on, it will return itsself (through super.rewrite()), if it rewrites to boolean query, it should do it. In this case the result of rewrite is no longer a filter, and filter optimization do not apply. So important with this new API is, that rewrite is called first (even for filters, but as filters are queries now, this would automatically be done). The optimizations in 1345 then would only get the result of the rewrite (and instanceof checks would be on the rewritten one, which maybe a filter or not).&lt;/p&gt;</comment>
                    <comment id="12700490" author="creamyg" created="Sat, 18 Apr 2009 16:05:07 +0100">&lt;p&gt;&amp;gt; Have we agreed on how the class structure is going to look and what the&lt;br/&gt;
&amp;gt; optimizations will look like?&lt;/p&gt;

&lt;p&gt;I don't think this problem is solved.  &lt;/p&gt;

&lt;p&gt;I thought I had come up with a grand unifying OO design solution with my&lt;br/&gt;
Matcher proposal for Lucy.  Matcher was to be the base class for any matching&lt;br/&gt;
iterator, whether the task was scoring, filtering, or deletions (a specialized&lt;br/&gt;
kind of filter).  A provisional implementation has been completed in KS svn&lt;br/&gt;
trunk.&lt;/p&gt;

&lt;p&gt;Unfortunately, McCandless's benchmarks on iterated vs. random access filters&lt;br/&gt;
have blown a hole in the Matcher approach.  Whether it can be mended, or&lt;br/&gt;
whether starting over from scratch is the best idea, I don't know.  I really&lt;br/&gt;
hate the hasRandomAccess() approach from an OO design standpoint, but I have&lt;br/&gt;
to admit that I don't have anything better.  However, I do think that it's&lt;br/&gt;
essential for any Filter refactoring to answer Mike's challenge in full.&lt;/p&gt;

&lt;p&gt;Right now, we don't have search-time benchmarking capabilities for Lucy/KS,&lt;br/&gt;
and I don't think it's possible for me, at least, to pursue any further&lt;br/&gt;
experimentation until we acquire those capabilities.  For the time being, I've&lt;br/&gt;
turned my attention to other concerns, and I don't expect to push this issue&lt;br/&gt;
forward for a little while.  &lt;/p&gt;</comment>
                    <comment id="12700577" author="mikemccand" created="Sun, 19 Apr 2009 12:14:44 +0100">
&lt;blockquote&gt;&lt;p&gt;I really hate the hasRandomAccess() approach from an OO design standpoint, but I have to admit that I don't have anything better.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we need to approach this as a structural optimization problem;&lt;br/&gt;
I think there should be an "optimize()" step after (or during)&lt;br/&gt;
rewrite().  The optimize phase would structure the matching to run as&lt;br/&gt;
quickly as possible.&lt;/p&gt;

&lt;p&gt;Ie, on detecting somehow that a filter is random access, we should&lt;br/&gt;
multiply it out (down) to each TermScorer.  If deletes are&lt;br/&gt;
pre-multiplied in the filter, we tell each TermScorer not to check&lt;br/&gt;
deletes.&lt;/p&gt;

&lt;p&gt;[We may need a filter manager to go along w/ this, eg that will&lt;br/&gt;
convert a filter to random-access (if it's going to be reused),&lt;br/&gt;
multiply in deletes (and re-do that whenever new reader is opened),&lt;br/&gt;
etc.]&lt;/p&gt;

&lt;p&gt;Likewise, &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1252" title="Avoid using positions when not all required terms are present"&gt;LUCENE-1252&lt;/a&gt; splits matching of queries that consult&lt;br/&gt;
positional information into two steps (roughly "cheap" and&lt;br/&gt;
"expensive") and does all "cheap" tests across each "and" clause and&lt;br/&gt;
"expensive" only when necessary.  So optimize() would return two&lt;br/&gt;
matchers for such queries, and we'd "collate" the cheap matchers&lt;br/&gt;
together first, followed by the expensive one.&lt;/p&gt;

&lt;p&gt;Not requiring an implicit next() after skipTo() ... so optimize would&lt;br/&gt;
decide which matchers should "drive" the iteration, and which others&lt;br/&gt;
should do random-access test.  Some next()'s (eg OR or AND matchers)&lt;br/&gt;
are far more costly than other next()'s (eg, TermScorer).  Some are&lt;br/&gt;
far more restrictive than others, etc.&lt;/p&gt;

&lt;p&gt;Of course, some filters require iterator access, so clearly we must&lt;br/&gt;
accept that.&lt;/p&gt;

&lt;p&gt;At some point, there will be too much splintering of options and&lt;br/&gt;
source code specialization should &lt;span class="error"&gt;&amp;#91;somehow&amp;#93;&lt;/span&gt; take over in enumerating&lt;br/&gt;
all the combinations.  EG the field-sort collectors are already&lt;br/&gt;
getting close to this (record score or not, compute max score or not,&lt;br/&gt;
single field vs multi field, docID required for tie breaking or not,&lt;br/&gt;
etc).&lt;/p&gt;</comment>
                    <comment id="12704499" author="shaie" created="Thu, 30 Apr 2009 07:35:05 +0100">&lt;p&gt;I would like to query why do we need to make Filter and Query of the same type? After all, they both do different things, even though it looks like they are similar. Attempting to do this yields those peculiarities:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;If Filter extends Query, it now has to implement all sorts of methods like weight, toString, rewrite, getTerms and scoresDocInOrder (an addition from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1593" title="Optimizations to TopScoreDocCollector and TopFieldCollector"&gt;&lt;del&gt;LUCENE-1593&lt;/del&gt;&lt;/a&gt;).&lt;/li&gt;
	&lt;li&gt;If Query extends Filter, it has to implement getDocIdSet.&lt;/li&gt;
	&lt;li&gt;Introduce instanceof checks in places just to check if a given Query is actually a Filter or not.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Both (1) and (2) are completely redundant for both Query and Filter, i.e. why should Filter implement toString(term) or scoresDocInOrder when it does score docs? Why should Query implement getDocIdSet when it already implements a weight().scorer() which returns a DocIdSetIterator?&lt;/p&gt;

&lt;p&gt;I read the different posts on this issue and I don't understand why we think that the API is not clear enough today, or is not convenient:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;If I want to just filter the entire index, I have two ways: (1) execute a search with MatchAllDocsQuery and a Filter (2) Wrap a filter with ConstantScoreQuery. I don't see the difference between the two, and I don't think it forces any major/difficult decision on the user.&lt;/li&gt;
	&lt;li&gt;If I want to have a BooleanQuery with several clauses and I want a clause to be a complex one with a Filter, I can wrap the Filter with CSQ.&lt;/li&gt;
	&lt;li&gt;If I want to filter a Query, there is already API today on Searcher which accepts both Query and Filter.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;At least as I understand it, Queries are supposed to score documents, while Filters to just filter. If there is an API which requires Queries only, then I can wrap my Filter with CSQ, but I'd prefer to check if we can change that API first (for example, allowing BooleanClause to accept a Filter, and implement a weight(IndexReader) rather than just getQuery()).&lt;/p&gt;

&lt;p&gt;So if Filters just filter and Queries just score, the API on both is very clear: Filter returns a DISI and Query returns a Scorer (which is also a DISI). I don't see the advantage of having the code unaware to the fact a certain Query is actually a Fitler - I prefer it to be upfront. That way, we can do all sorts of optimizations, like asking the Filter for next() first, if we know it's supposed to filter most of the documents.&lt;/p&gt;

&lt;p&gt;At the end of the day, both Filter and Query iterate on documents. The difference lies in the purpose of iteration. In my code there are several Query implementations today that just filter documents, and I plan to change all of them to implement Filter instead (that was originally the case because Filter had just bits() and now it's more efficient with the iterator() version, at least to me). I want to do this for a couple of reasons, clarity being one of the most important. If Filter just filters, I don't see why it should inherit all the methods from Query (or vice versa BTW), especially when I have this CSQ wrapper.&lt;br/&gt;
To me, as a Lucene user, I make far more complicated decisions every day than deciding whether I want to use a Filter as a Query or not. If I pass it directly to IndexSearcher, I use it as a filter. If I use a different API which accepts just Query, I wrap it with CSQ. As simple as that.&lt;/p&gt;

&lt;p&gt;But that's just my two cents.&lt;/p&gt;</comment>
                    <comment id="12704561" author="eksdev" created="Thu, 30 Apr 2009 11:30:14 +0100">&lt;p&gt;imo, it is really not all that important to make Filter and Query the same (that is just one alternative to achieve goal). &lt;/p&gt;

&lt;p&gt;Basic problem we try  to solve is adding Filter directly to BoolenQuery, and making optimizations after that easier. Wrapping with CSQ is just adding anothe layer between Lucene search machinery and Filter, making these optimizations harder.&lt;/p&gt;

&lt;p&gt;On the other hand, I must accept, conceptually FIter and Query are "the same", supporting together following options:&lt;br/&gt;
1. Pure boolean model: You do not care about scores (today we can do it only wia CSQ, as Filter does not enter BoolenQuery)&lt;br/&gt;
2. Mixed boolean and ranked: you have to define Filter contribution to the documents (CSQ)&lt;br/&gt;
3. Pure ranked: No filters, all gets scored (the same as 2.)&lt;/p&gt;

&lt;p&gt;Ideally, as a user, I define only Query (Filter based or not) and for each clause in my Query define &lt;br/&gt;
Query.setScored(true/false) or useConstantScore(double score); &lt;/p&gt;

&lt;p&gt;also I should be able to say, "Dear Lucene please materialize this "Query_Filter" for me as I would like to have it cached and please store only DocIds (Filter today).  Maybe open possibility to open possibility to cache scores of the documents as well. &lt;/p&gt;

&lt;p&gt;one thing is concept  and another is optimization. From optimization point of view, we have couple of decisions to make:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;DocID Set supports random access, yes or no (my "Materialized Query")&lt;/li&gt;
	&lt;li&gt;Decide if clause should / should not be scored/ or should be constant&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So, for each "Query" we need to decide/support:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;scoring
{yes, no, constant}
&lt;p&gt; and&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;opening option to "materialize Query" (that is how we today create Filters today)&lt;/li&gt;
	&lt;li&gt;these Materialized Queries (aka Filter) should be able to tell us if they support random access, if they cache only doc id's or scores as well&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;nothing usefull in this email, just  thinking aloud, sometimes helps &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;



</comment>
                    <comment id="12704597" author="shaie" created="Thu, 30 Apr 2009 12:52:56 +0100">&lt;blockquote&gt;&lt;p&gt;Wrapping with CSQ is just adding anothe layer between Lucene search machinery and Filter, making these optimizations harder.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right. But making Filter sub-class Query and check in BQ 'if (query instanceof Filter) { Filter f = (Filter) query)' is not going to improve anything. It adds instanceof and casting, and I'd think those are more expensive than wrapping a Filter with CSQ and returning an appropriate Scorer, which will use the Filter in its next() and skipTo() calls.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On the other hand, I must accept, conceptually FIter and Query are "the same", supporting together following options&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that if we allow BooleanClause to implement a Weight(IndexReader) (just like Query) we'll be one more step closer to that goal? BQ uses this method to construct BooleanWeight, only today it calls clause.getQuery().createWeight(). Instead it could do clause.getWeight, and if the BooleanClause holds a Filter it will return a FilterWeight, otherwise delegate that call to the contained Query.&lt;/p&gt;

&lt;p&gt;Regarding pure ranked, CSQ is really what we need, no?&lt;/p&gt;

&lt;p&gt;So how about the following:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Add add(Filter, Occur) to BooleanClause.&lt;/li&gt;
	&lt;li&gt;Add weight(Searcher) to BooleanClause.&lt;/li&gt;
	&lt;li&gt;Create a FilterWeight which wraps a Filter and provide a Scorer implementation with a constant score. (This does not handle the "no scoring" mode, unless "no scoring" can be achieved with score=0.0f, while constant is any other value, defaulting to 1.0f).&lt;/li&gt;
	&lt;li&gt;Add isRandomAccess to Filter.&lt;/li&gt;
	&lt;li&gt;Create a RandomAccessFilter which extends Filter and defines an additional seek(target) method.&lt;/li&gt;
	&lt;li&gt;Add asRandomAccessFilter() to Filter, which will materialize that Filter into memory, or into another RandomAccess data structure (e.g. keeping it on disk but still provide random access to it, even if not very efficient) and return a RandomAccessFilter type, which will implement seek(target) and possibly override next() and skipTo(), but still use whatever other methods this Filter declares.
	&lt;ul&gt;
		&lt;li&gt;I think we should default it to throw UOE providing that we document that isRandomAccess should first be called.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm thinking out loud just like you, so I hope my stuff makes sense &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;/p&gt;</comment>
                    <comment id="12704600" author="mikemccand" created="Thu, 30 Apr 2009 13:01:32 +0100">&lt;p&gt;Let's not forget we also have "provides scores but NOT filtering" type things as well, eg function queries, MatchAllDocsQuery, "I want to boost documents by recency" use case (which sort of a Scorer filter in that it takes another Scorer and modifies its output, per doc), etc.&lt;/p&gt;

&lt;p&gt;It's just that very often the "scoring part" is in fact very much intertwined with the "filtering" part.  EG a TermQuery iterates a SegmentTermDocs, and reads &amp;amp; holds freq/doc in pairs.&lt;/p&gt;</comment>
                    <comment id="12704605" author="paul.elschot@xs4all.nl" created="Thu, 30 Apr 2009 13:13:54 +0100">&lt;blockquote&gt;&lt;p&gt;opening option to "materialize Query" (that is how we today create Filters today)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How about materializing the DocIds &lt;em&gt;and&lt;/em&gt; the score values?&lt;/p&gt;</comment>
                    <comment id="12704609" author="paul.elschot@xs4all.nl" created="Thu, 30 Apr 2009 13:22:48 +0100">&lt;blockquote&gt;&lt;p&gt;Create a FilterWeight which wraps a Filter and provide a Scorer implementation with a constant score. (This does not handle the "no scoring" mode, unless "no scoring" can be achieved with score=0.0f, while constant is any other value, defaulting to 1.0f).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The current patch at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt; does not need such a FilterWeight; the no scoring case is handled by not asking for score values.&lt;br/&gt;
Using score=0.0f for no scoring might not work for BooleanQuery because it also has a coordination factor that depends on the number of matching query clauses. The patch at 1345 does not change that coordination factor for backward compatibility, even though the coordination factor might also depend on the number of a matching filter clauses.&lt;/p&gt;</comment>
                    <comment id="12704613" author="eksdev" created="Thu, 30 Apr 2009 13:32:16 +0100">&lt;p&gt;Shai, &lt;br/&gt;
----Regarding pure ranked, CSQ is really what we need, no? &amp;#8212; &lt;/p&gt;

&lt;p&gt;Yep, it would work for Filters, but why not making it possible to have normal Query "constant score". For these cases,  I am just not sure if this aproach gets max performance (did not look at this code for quite a while).  &lt;/p&gt;

&lt;p&gt;Imagine you have a Query and you are not interested in Scoring at all, this can be acomplished with only DocID iterator arithmetic, ignoring  score() totally.  But that is only an optimization (maybe allready there?)&lt;/p&gt;

&lt;p&gt;Paul, &lt;br/&gt;
---&lt;del&gt;How about materializing the DocIds &lt;em&gt;and&lt;/em&gt; the score values?&lt;/del&gt;---&lt;br/&gt;
exactly,  that would open full caching posibility (original purpose of Filters).  Think Search Results caching ... that is practically another name for search() method. It is easy to create this, but using it again would require some bigger changes &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; &lt;/p&gt;

&lt;p&gt;Filter_on_Steroids materialize(boolean without_score); &lt;/p&gt;
</comment>
                    <comment id="12704618" author="eksdev" created="Thu, 30 Apr 2009 13:40:14 +0100">&lt;p&gt;Paul: ...The current patch at &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt; does not need such a FilterWeight; the no scoring case is handled by not asking for score values...&lt;/p&gt;

&lt;p&gt;Me: ...Imagine you have a Query and you are not interested in Scoring at all, this can be acomplished with only DocID iterator arithmetic, ignoring score() totally. But that is only an optimization (maybe allready there?)...&lt;/p&gt;

&lt;p&gt;I knew Paul will kick in at this place, he sad exactly the same thing I did, but, as oposed to me, he made formulation that executes &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; &lt;br/&gt;
Pfff, I feel bad &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;


</comment>
                    <comment id="12719558" author="markrmiller@gmail.com" created="Mon, 15 Jun 2009 14:50:34 +0100">&lt;p&gt;This issue is marked as part of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1345" title="Allow Filter as clause to BooleanQuery"&gt;&lt;del&gt;LUCENE-1345&lt;/del&gt;&lt;/a&gt;, which has been pushed to 3.1. Also, it has not yet found an assignee. Speak out, or I will push this to 3.1.&lt;/p&gt;</comment>
                    <comment id="12719567" author="thetaphi" created="Mon, 15 Jun 2009 15:07:17 +0100">&lt;p&gt;Push to 3.1! &amp;#8211; Uwe&lt;/p&gt;</comment>
                    <comment id="13495121" author="dsmiley" created="Mon, 12 Nov 2012 06:52:56 +0000">&lt;p&gt;This issue may seem like ancient history but I'm nonetheless looking forward to it being realized.  I think it will really simplify things.  Do we re-target this at 5.0/trunk?&lt;/p&gt;</comment>
                    <comment id="13495138" author="thetaphi" created="Mon, 12 Nov 2012 07:34:27 +0000">&lt;p&gt;Yeah, thanks for touching the issue, so it gets back into my mind.&lt;/p&gt;

&lt;p&gt;Of course the patch non this issue is heavily outdated and the "simple" aproach of making Filter extend ConstantScoreQuery is no longer applicable as implemented there (because Filters can now be applied as random access bits down-low), so making them a query like proposed here would make them stop using RA bits. The better approach would be (as Eks Dev pointed out on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-4548" title="BooleanFilter should optionally pass down further restricted acceptDocs in the MUST case (and acceptDocs in general)"&gt;LUCENE-4548&lt;/a&gt;) to mark the "Filter" as "non-scoring" and then scorers from e.g. BooleanWeight can optimize on that case (and possibly use bits instead of leap-frogging iterators).&lt;/p&gt;

&lt;p&gt;The idea in this patch to make the "Filter" class still available to users (as a convenience way to create a non-scoring query without having to implement a full query with weight and scorers) is what I like from this issue. But a Filter is just a "standard" query and can be used anywhere like e.g. in BooleanQuery. The main work to implement this would be in BooleanQuery to implement the features of FilteredQuery (which would go away together with QueryWrapperFilter and similar classes).&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                                <inwardlinks description="is part of">
                            <issuelink>
            <issuekey id="12400878">LUCENE-1345</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12397710" name="LUCENE-1518.patch" size="9367" author="thetaphi" created="Mon, 12 Jan 2009 19:50:33 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 12 Jan 2009 20:24:14 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2894</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26204</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1486] Wildcards, ORs etc inside Phrase queries</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1486</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;An extension to the default QueryParser that overrides the parsing of PhraseQueries to allow more complex syntax e.g. wildcards in phrase queries.&lt;/p&gt;

&lt;p&gt;The implementation feels a little hacky - this is arguably better handled in QueryParser itself. This works as a proof of concept  for much of the query parser syntax. Examples from the Junit test include:&lt;/p&gt;

&lt;p&gt;		checkMatches("\"j*   smyth~\"", "1,2"); //wildcards and fuzzies are OK in phrases&lt;br/&gt;
		checkMatches("\"(jo* -john)  smith\"", "2"); // boolean logic works&lt;br/&gt;
		checkMatches("\"jo*  smith\"~2", "1,2,3"); // position logic works.&lt;/p&gt;

&lt;p&gt;		checkBadQuery("\"jo*  id:1 smith\""); //mixing fields in a phrase is bad&lt;br/&gt;
		checkBadQuery("\"jo* \"smith\" \""); //phrases inside phrases is bad&lt;br/&gt;
		checkBadQuery("\"jo* &lt;span class="error"&gt;&amp;#91;sma TO smZ&amp;#93;&lt;/span&gt;\" \""); //range queries inside phrases not supported&lt;/p&gt;

&lt;p&gt;Code plus Junit test to follow...&lt;/p&gt;
</description>
                <environment/>
            <key id="12410389">LUCENE-1486</key>
            <summary>Wildcards, ORs etc inside Phrase queries</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="4" iconUrl="https://issues.apache.org/jira/images/icons/statuses/reopened.png">Reopened</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="markh">Mark Harwood</reporter>
                        <labels>
                    </labels>
                <created>Wed, 10 Dec 2008 18:18:22 +0000</created>
                <updated>Fri, 10 May 2013 00:05:36 +0100</updated>
                                    <version>2.4</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>12</votes>
                        <watches>20</watches>
                                                    <comments>
                    <comment id="12655300" author="markh" created="Wed, 10 Dec 2008 18:19:51 +0000">&lt;p&gt;Junit test&lt;/p&gt;</comment>
                    <comment id="12655301" author="markh" created="Wed, 10 Dec 2008 18:20:23 +0000">&lt;p&gt;QueryParser extension&lt;/p&gt;</comment>
                    <comment id="12655664" author="markh" created="Thu, 11 Dec 2008 14:34:37 +0000">&lt;p&gt;Added tests for range queries and plain PhraseQueries&lt;/p&gt;</comment>
                    <comment id="12655665" author="markh" created="Thu, 11 Dec 2008 14:35:20 +0000">&lt;p&gt;Fixed bug with plain phrase query, added support for range queries&lt;/p&gt;</comment>
                    <comment id="12655966" author="mikemccand" created="Fri, 12 Dec 2008 10:45:09 +0000">&lt;p&gt;(Added 2.9 fix version in addition to 2.4.1).&lt;/p&gt;</comment>
                    <comment id="12656034" author="markh" created="Fri, 12 Dec 2008 15:23:04 +0000">&lt;p&gt;Added support for "Nots" in phrase queries e.g. "-not interested"&lt;/p&gt;</comment>
                    <comment id="12656035" author="markh" created="Fri, 12 Dec 2008 15:23:22 +0000">&lt;p&gt;More tests for Nots&lt;/p&gt;</comment>
                    <comment id="12678015" author="markh" created="Mon, 2 Mar 2009 15:17:06 +0000">&lt;p&gt;Updated to cater for phrase clauses that produce no matches&lt;/p&gt;</comment>
                    <comment id="12678017" author="markh" created="Mon, 2 Mar 2009 15:18:09 +0000">&lt;p&gt;Updated Junit test to test for phrases with clauses that produce no matches&lt;/p&gt;</comment>
                    <comment id="12718281" author="markrmiller@gmail.com" created="Thu, 11 Jun 2009 03:25:35 +0100">&lt;p&gt;What do you think about this for 2.9 Mark H?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The implementation feels a little hacky - this is arguably better handled in QueryParser itself. This works as a proof of concept for much of the query parser syntax.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That leads me to think we might want to push to 3.0? Or have you moved beyond that with all of these updates?&lt;/p&gt;</comment>
                    <comment id="12718573" author="markh" created="Thu, 11 Jun 2009 19:11:32 +0100">&lt;p&gt;Perhaps "hacky" was too strong a word. I think it's a reasonable approach to handling the complexity involved in this logic. &lt;/p&gt;

&lt;p&gt;A colleague of mine has this running in production on a big installation with lots of users&lt;/p&gt;</comment>
                    <comment id="12719105" author="mikemccand" created="Sat, 13 Jun 2009 10:48:53 +0100">&lt;p&gt;Is there some reason not to include this in QueryParser instead?  Ie, it accepts a superset of QueryParser's current syntax?&lt;/p&gt;</comment>
                    <comment id="12719115" author="markh" created="Sat, 13 Jun 2009 12:58:23 +0100">&lt;p&gt;The primary reason (and perhaps not a particularly good one) was I didn't want to wade around in the Javacc syntax of the .jj file that generates the QueryParser and the required extensions could be made in a subclass.&lt;/p&gt;

&lt;p&gt;Also there is invariably a performance hit for supporting things like wildcards in phrase queries so rather than adding another "off by default" flag in the main parser  and conditional logic to test if "wildcards etc in phrases" are allowed, the subclass could be seen as a specialised extension that is to be used by those that understand the trade-offs between functionality and performance.  &lt;/p&gt;

&lt;p&gt;I can sympathise with the purist approach of having all parser syntax defined in Javacc though.&lt;/p&gt;</comment>
                    <comment id="12719639" author="markrmiller@gmail.com" created="Mon, 15 Jun 2009 17:52:47 +0100">&lt;p&gt;Should this go in contrib rather than core? That seems to have been the approach so far, any reason to vary it up here?&lt;/p&gt;

&lt;p&gt;Well, actually, looks like I see the multi field parser in core. Makes sense to put subclasses there I guess.&lt;/p&gt;

&lt;p&gt;You think this is ready to commit Mark? If so, I should be able to review it (unless you want to commit it yourself).&lt;/p&gt;</comment>
                    <comment id="12719644" author="markrmiller@gmail.com" created="Mon, 15 Jun 2009 18:00:37 +0100">&lt;p&gt;Reformatted to lucene formatting, removed author tag, removed a couple unused fields, changed to patch format&lt;/p&gt;

&lt;p&gt;Tests don't pass because it doesnt work quite correctly with the new constantscore multi term queries yet.&lt;/p&gt;</comment>
                    <comment id="12723699" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 20:30:33 +0100">&lt;p&gt;Hey Mark, this doesn't work correctly with the new constant score mode. I'm hesitant to put something in core that only works with boolean expansion.&lt;/p&gt;

&lt;p&gt;I'm not sure what needs to be done (I started and realized my interest wasn't high enough). Could you update this? Otherwise I'm tempted to push off to 3.0...&lt;/p&gt;

&lt;p&gt;Unless another brave soul steps of course. Or I may jump back in - my brain is fickle.&lt;/p&gt;</comment>
                    <comment id="12723737" author="markh" created="Wed, 24 Jun 2009 22:05:57 +0100">&lt;p&gt;Added fix for ConstantScoreQuery changes&lt;/p&gt;</comment>
                    <comment id="12723742" author="markh" created="Wed, 24 Jun 2009 22:12:38 +0100">&lt;p&gt;The fix was relatively straight-forward from what I could see. Just temporarily unset the QueryParser's ConstantScoreRewrite mode when performing the pass that is just evaluating query elements inside phrase queries. These clauses need to resolve to traditional BooleanQuery-full-of-termQueries in order that they can be inspected and rewritten as Span equivalents for complex phrases.&lt;/p&gt;

&lt;p&gt;Should do the job.&lt;/p&gt;

&lt;p&gt;Cheers&lt;br/&gt;
Mark&lt;br/&gt;
(Been far too busy with other things and missing getting my hands dirty here with Lucene!)&lt;/p&gt;</comment>
                    <comment id="12723747" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 22:25:08 +0100">&lt;p&gt;Figured thats all it would take. I just was feeling a bit too lazy to try and understand the whole class after I put it up in front of me for a few seconds &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Figured I'd try and pawn off a piece. I made some adjustments to the patch last time, but they were basically cosmetic.&lt;/p&gt;

&lt;p&gt;Looks like I didnt escape much work this time though - I'll review and commit shortly.&lt;/p&gt;

&lt;p&gt;Thanks a lot.&lt;/p&gt;</comment>
                    <comment id="12727622" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 18:03:06 +0100">&lt;p&gt;Whoops - almost let some 1.5 slip by:  throw new IllegalArgumentException(pe.getMessage(), pe) is not in 1.4.&lt;/p&gt;

&lt;p&gt;Last patch. I'll commit later today.&lt;/p&gt;</comment>
                    <comment id="12727685" author="markh" created="Mon, 6 Jul 2009 19:37:06 +0100">&lt;p&gt;Hi Mark,&lt;br/&gt;
Mind if I try committing this patch?&lt;br/&gt;
I've just switched from PC to Mac and my dev environment is all changed (Subclipse vs TortoiseSvn etc) and I wouldn't mind checking my config and commit rights still work in this new environment.&lt;br/&gt;
If anyone has any  mac/subclipse-related "gotchas" I should be aware of, do let me know. &lt;/p&gt;

&lt;p&gt;Cheers&lt;br/&gt;
Mark&lt;/p&gt;</comment>
                    <comment id="12727692" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 19:47:18 +0100">&lt;p&gt;Please, by all means ! &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12727726" author="markh" created="Mon, 6 Jul 2009 20:41:11 +0100">&lt;p&gt;Committed in 791579 -  &lt;a href="http://svn.apache.org/viewvc?rev=791579&amp;amp;view=rev" class="external-link"&gt;http://svn.apache.org/viewvc?rev=791579&amp;amp;view=rev&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12733889" author="adriano_crestani" created="Wed, 22 Jul 2009 00:18:08 +0100">&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I'm trying to understand what kind of syntax this query parser supports. I read the code and it does not say much. Is there any documentation (wiki, javadoc, etc) that specifies the syntax? Because it's not clear for me.&lt;/p&gt;

&lt;p&gt;Thanks in advance,&lt;br/&gt;
Adriano Crestani Campos&lt;/p&gt;</comment>
                    <comment id="12733893" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 00:21:33 +0100">&lt;p&gt;You might check the test class - it has a few basic examples. Its not much different than whats posted in the summary:&lt;/p&gt;

&lt;p&gt;Just experiment.&lt;/p&gt;

&lt;p&gt;+    checkMatches("\"john smith\"", "1"); // Simple multi-term still works&lt;br/&gt;
+    checkMatches("\"j*   smyth~\"", "1,2"); // wildcards and fuzzies are OK in&lt;br/&gt;
+    // phrases&lt;br/&gt;
+    checkMatches("\"(jo* -john)  smith\"", "2"); // boolean logic works&lt;br/&gt;
+    checkMatches("\"jo*  smith\"~2", "1,2,3"); // position logic works.&lt;br/&gt;
+    checkMatches("\"jo* &lt;span class="error"&gt;&amp;#91;sma TO smZ&amp;#93;&lt;/span&gt;\" ", "1,2"); // range queries supported&lt;br/&gt;
+    checkMatches("\"john\"", "1,3"); // Simple single-term still works&lt;br/&gt;
+    checkMatches("\"(john OR johathon)  smith\"", "1,2"); // boolean logic with&lt;br/&gt;
+    // brackets works.&lt;br/&gt;
+    checkMatches("\"(jo* -john) smyth~\"", "2"); // boolean logic with&lt;br/&gt;
+    // brackets works.&lt;br/&gt;
+&lt;br/&gt;
+    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work&lt;br/&gt;
+    // currently &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;.&lt;br/&gt;
+&lt;br/&gt;
+    checkMatches("\"john  nosuchword*\"", ""); // phrases with clauses producing&lt;br/&gt;
+    // empty sets&lt;br/&gt;
+&lt;br/&gt;
+    checkBadQuery("\"jo*  id:1 smith\""); // mixing fields in a phrase is bad&lt;br/&gt;
+    checkBadQuery("\"jo* \"smith\" \""); // phrases inside phrases is bad&lt;/p&gt;</comment>
                    <comment id="12733917" author="adriano_crestani" created="Wed, 22 Jul 2009 01:25:25 +0100">&lt;p&gt;Thanks for the quick response Mark!&lt;/p&gt;

&lt;p&gt;OK, I'm trying now to figure out what is supported reading the junits only, and I ran into some issues:&lt;/p&gt;

&lt;p&gt;What do you mean on the last check by phrase inside phrase, I don't see any phrase inside a phrase (I'm not sure either what it would be, because there is no open and close phrase delimiter), all I see is a phrase &amp;lt;"jo*"&amp;gt;, followed by a term &amp;lt;smith&amp;gt; and an empty phrase &amp;lt;" "&amp;gt;. And the check passes because the query parser throws an exception complaning about the empty phrase, it seems to not be supported. I just changed the empty phrase to a valid phrase and the query works (failing the test case). But as I said, I'm not sure what you were exactly trying to do there, could you give me more explation about that?&lt;/p&gt;

&lt;p&gt;I'm also getting a java.util.ConcurrentModificationException when I type an escaped double quotes inside phrases. So, I suppose it's not supported, but shouldn't it throw a better exception?&lt;/p&gt;

&lt;p&gt;I also have an issue with the parse exceptions, if it comes from inside a phrase, it does not tell the correct position in the query string. I think it considers the beginning of the phrase as the beginning of the query and it only prints the phrase that contains the problem.&lt;/p&gt;

&lt;p&gt;I'm attaching some changes I did in the TestComplexPhraseQuery junit that shows these problems I'm getting, I think it's easier to understand if you read and run it.&lt;/p&gt;

&lt;p&gt;Sorry for so many questions, but I'm just trying to understand what exactly this query parser supports or not.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Adriano Crestani Campos&lt;/p&gt;</comment>
                    <comment id="12733933" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 02:11:34 +0100">&lt;p&gt;You may have to wait for the author, Mark Harwood to respond. I just reviewed the issue. A couple points though:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What do you mean on the last check by phrase inside phrase, I don't see any phrase inside a phrase (I'm not sure either what it would be, because there is no open and close phrase delimiter), all I see is a phrase &amp;lt;"jo*"&amp;gt;, followed by a term &amp;lt;smith&amp;gt; and an empty phrase &amp;lt;" "&amp;gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its kind of a phrase within a phrase (though the "smith" phrase could be turned into a term query) - unescaped: "jo* "smith"" - the full thing is phrase one, and smith is the inner phrase (though yes, only a term in the phrase).&lt;/p&gt;

&lt;p&gt;If Mark Harwood doesn't have time to answer soon, I'll dig in more and respond to your other questions/comments.&lt;/p&gt;</comment>
                    <comment id="12733940" author="michaelbusch" created="Wed, 22 Jul 2009 02:25:20 +0100">&lt;p&gt;Looking at the problems Adriano is seeing it almost seems like this was a bit prematurely committed? It seems like a lot of queries you could enter here are not really supported and might throw strange exceptions.&lt;/p&gt;

&lt;p&gt;Maybe it should live in contrib for now (with experimental warnings)?&lt;/p&gt;</comment>
                    <comment id="12733946" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 02:42:36 +0100">&lt;p&gt;I originally thought it might live in contrib as well (see above), but I'm personally fine with it being in core.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It seems like a lot of queries you could enter here are not really supported and might throw strange exceptions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A lot of queries? I think Adriano is just having trouble with phrases inside phrases, which is unsupported. Other things that are not supported might throw exceptions too, but I think thats to be expected? I see what Adriano was talking about now - technically the first 2 quotes would match, and then the second two - I think Mark H was just demonstrating that you shouldn't try that query though - a user might think they are quoting smith, but for the example, it doesn't matter. I think he just trying to show that you shouldn't try and "nest" phrases - even though they wouldn't be interpreted that way anyway.&lt;/p&gt;

&lt;p&gt;It only supports a limited subset of the Lucene query language - perhaps we could improve the exceptions being thrown, but the exceptions the queryparser throws often leave just as much to be desired. I don't think its experimental because of that.&lt;/p&gt;

&lt;p&gt;Personally, I think the class does what it intends - allows a limited subset of the Lucene query language in phrases. Though of course it could be improved.&lt;/p&gt;

&lt;p&gt;I'll let Mark H respond though. I also don't mind seeing it moved to contrib, but I'm not sure anything glaring points to it being moved at the moment. It lives up to its limited contract I think.&lt;/p&gt;</comment>
                    <comment id="12733956" author="adriano_crestani" created="Wed, 22 Jul 2009 03:37:01 +0100">&lt;blockquote&gt;
&lt;p&gt;I see what Adriano was talking about now - technically the first 2 quotes would match, and then the second two - I think Mark H was just demonstrating that you shouldn't try that query though - a user might think they are quoting smith, but for the example, it doesn't matter. I think he just trying to show that you shouldn't try and "nest" phrases - even though they wouldn't be interpreted that way anyway.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, if you guessed his intention correctly, the comment is misleading: "phrases inside phrases is bad". But lets wait for his response.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Other things that are not supported might throw exceptions too&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think a user would expect a ParseException. Probably, every query parser user catches ParserException and show a nice message to its final user. Now, if the query parser starts throwing random exception to say the syntax is invalid, every software that uses Lucene query parser is gonna start crashing. For me it's like if a compiler started throwing segmentation fault every time you forget a } in the code.&lt;/p&gt;</comment>
                    <comment id="12733957" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 03:45:06 +0100">&lt;blockquote&gt;
&lt;p&gt;I think a user would expect a ParseException. Probably, every query parser user catches ParserException and show a nice message to its final user. Now, if the query parser starts throwing random exception to say the syntax is invalid, every software that uses Lucene query parser is gonna start crashing. For me it's like if a compiler started throwing segmentation fault every time you forget a } in the code.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That's a fair point - addressable though - we can likely catch and rethrow in the worst case.&lt;/p&gt;

&lt;p&gt;I'll admit, the ... non exactness ... of this parser troubled me at first - one of the reasons I liked contrib as a landing spot early on. I took it for what it is in the end I suppose. I think the shortfalls brought up so far can be addressed to a large degree though.&lt;/p&gt;</comment>
                    <comment id="12733958" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 03:50:22 +0100">&lt;blockquote&gt;&lt;p&gt;Well, if you guessed his intention correctly, the comment is misleading: "phrases inside phrases is bad". But lets wait for his response.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think thats a bit of judgement call. We know that the way the query is parsed, you cannot really ever do "phrases inside phrases". However, a user of this parser might think, that like the other syntax, perhaps you can use "phrases inside phrases" - and if you thought that, the example given is likely how you'd imagine it to work. The outside phrase, and then the inside phrase. I certainly agree some comments would clear it up, but I think its a useful example.&lt;/p&gt;</comment>
                    <comment id="12733961" author="adriano_crestani" created="Wed, 22 Jul 2009 04:02:41 +0100">&lt;blockquote&gt;
&lt;p&gt;I'll admit, the ... non exactness ... of this parser troubled me at first - one of the reasons I liked contrib as a landing spot early on. I took it for what it is in the end I suppose. I think the shortfalls brought up so far can be addressed to a large degree though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think contrib would be a good place for now, until it gets more stable and better documented.&lt;/p&gt;</comment>
                    <comment id="12733966" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 04:12:12 +0100">&lt;blockquote&gt;&lt;p&gt;I think contrib would be a good place for now, until it gets more stable and better documented.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If Mark H thinks it should be moved, I won't disagree. But I still don't see a convincing reason. It could use some more documentation, but so could quite a few other classes in core. Its something of a subjective call,  and  more importantly, it can be addressed now.&lt;/p&gt;

&lt;p&gt;I'm not yet convinced its unstable - the only major issue I see so far is the exception issue - but that wouldn't seem to prompt a move to contrib, but an update to address the concern. Moving to contrib is always an option, but I don't think its the default move based on whats been brought up. The standard move would be to address whatever issues are brought up ... so far I am just seeing the exception issue as a large one, and I think that is fairly easily addressable.&lt;/p&gt;</comment>
                    <comment id="12733982" author="michaelbusch" created="Wed, 22 Jul 2009 05:43:23 +0100">&lt;blockquote&gt;
&lt;p&gt;It only supports a limited subset of the Lucene query language - perhaps we could improve the exceptions being thrown, but the exceptions the queryparser throws often leave just as much to be desired. I don't think its experimental because of that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because it only supports a limited subset of the language, I feel like we could have taken a different approach here? Why not add the features that are supported and make sense to the main query parser? &lt;/p&gt;

&lt;p&gt;The documentation does not tell me what is supported and what is not currently. And looking through the code some methods now throw RuntimeExceptions, because the overridden methods themselves don't throw anything. These things feel a bit unfinished. &lt;/p&gt;

&lt;p&gt;I'm not saying these issues are not fixable. But maybe we should rethink the design. My biggest concern is that this new parser doesn't seem to have a well-defined syntax. So since it doesn't check if a query is actually valid or not, it might be hard to maintain. E.g. if you add new language features to the main QP, it's currently not defined what will happen if you use them with this one.&lt;/p&gt;

&lt;p&gt;That's why I'm proposing to move it to contrib and mark it as experimental. Then we have more time to decide if the approach of adding the new features to the main QP makes more sense. &lt;/p&gt;</comment>
                    <comment id="12734015" author="lafa" created="Wed, 22 Jul 2009 07:43:00 +0100">&lt;p&gt;I share same opinion as Michael,&lt;br/&gt;
the implementation has a lot of undefined/undocumented behaviors,&lt;br/&gt;
simple because it reuses the queryparser to parse the text inside a phrase. &lt;br/&gt;
All the lucene syntax needs to be accounted on this design, but it does not seem to be the case.&lt;/p&gt;

&lt;p&gt;Problems like Adriano described, phrase inside a phrase, position reporting for errors.&lt;/p&gt;

&lt;p&gt;I also have a lot of concerns about having the full lucene syntax inside phrases &lt;br/&gt;
and trying to restrict this by throwing exceptions for particular cases does not seem the best design.&lt;/p&gt;

&lt;p&gt;Here is a example of with OR, AND, PARENTESIS with a proximity search&lt;br/&gt;
"(( jakarta OR green) AND (blue AND orange)  AND black~0.5) apache"~10&lt;/p&gt;

&lt;p&gt;What should a user expect from this query, without looking at the code. I'm not sure.&lt;br/&gt;
Does it even make sense to support this complex syntax? In my opinion. no&lt;/p&gt;

&lt;p&gt;I think we should define what is the subset of the language we want to support inside the phrases with a well defined behavior.&lt;br/&gt;
If Mark describes all the syntax he wants to support inside phrases, I actually don't mind to implement a new parser.for this.&lt;/p&gt;

&lt;p&gt;My view is, contrib is probably a better place to have this code, until we figure out a implementation that does not impose as many restrictions on changes to the original queryparser and describes a well defined syntax to be applied inside phrases.&lt;/p&gt;
</comment>
                    <comment id="12734141" author="lafa" created="Wed, 22 Jul 2009 15:53:43 +0100">&lt;p&gt;I added 2 testcases that return doc 3.&lt;br/&gt;
These queries do not make much sense,&lt;br/&gt;
I added it just to prove the point that we need more information&lt;br/&gt;
describing the use case for complex phrase qp.&lt;br/&gt;
We also should define a subset of the supported syntax we want to support inside phrases, &lt;br/&gt;
with well defined behaviors.&lt;/p&gt;

&lt;p&gt;	checkMatches("\"(goos~0.5 AND (mike OR smith) AND NOT ( percival AND john) ) vacation\"~3","3"); // proximity with fuzzy, OR, AND, NOT&lt;br/&gt;
	checkMatches("\"(goos~0.5 AND (mike OR smith) AND ( percival AND john) ) vacation\"~3","3"); // proximity with fuzzy, OR, AND&lt;/p&gt;</comment>
                    <comment id="12734148" author="markh" created="Wed, 22 Jul 2009 16:16:30 +0100">&lt;p&gt;I'll try and catch up with some of the issues raised here:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What do you mean on the last check by phrase inside phrase, I don't see any phrase inside a phrase&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Correct, the "inner phrase" example was a term not a phrase. This is perhaps a better example:&lt;/p&gt;

&lt;p&gt;		checkBadQuery("\"jo* \"percival smith\" \""); //phrases inside phrases is bad&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I'm trying now to figure out what is supported &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The Junit is currently the main form of documentation - unlike the XMLQueryParser (which has a DTD) there is no syntax to formally capture the logic. &lt;br/&gt;
Here is a basic summary of the syntax supported and how it differs from normal non-phrase use of the same operators:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Wildcard/fuzzy/range clauses can be used to define a phrase element (as opposed to simply single terms)&lt;/li&gt;
	&lt;li&gt;Brackets are used to group/define the acceptable variations for a given phrase element  e.g. "(john OR jonathon) smith"&lt;/li&gt;
	&lt;li&gt;"AND" is irrelevant - there is effectively an implied "AND_NEXT_TO" binding all phrase elements&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To move this forward I would suggest we consider following one of these options:&lt;/p&gt;

&lt;p&gt;1) Keep in core and improve error reporting and documentation&lt;br/&gt;
2) Move into "contrib" as experimental &lt;br/&gt;
3) Retain in core but simplify it to support only the simplest syntax (as in my Britney~ example)&lt;br/&gt;
4) Re-engineer the QueryParser.jj to support a formally defined syntax for acceptable "within phrase" operators e.g. *, ~, ( ) &lt;/p&gt;

&lt;p&gt;I think 1) is achievable if we carefully define where the existing parser breaks (e.g. ANDs and nested brackets)&lt;br/&gt;
2) is unnecessary if we can achieve 1).&lt;br/&gt;
3) would be a shame if we lost useful features for some very convoluted edge cases&lt;br/&gt;
4) is beyond my JavaCC skills.&lt;/p&gt;
















</comment>
                    <comment id="12734150" author="markrmiller@gmail.com" created="Wed, 22 Jul 2009 16:24:57 +0100">&lt;p&gt;My first thought is, if we can address some of the issues brought up, there is no reason to keep this out of core IMHO.&lt;/p&gt;

&lt;p&gt;My second thought is, I have a feeling a lot of this concern stems from the fact that these guys (or one of them) has to duplicate this thing with the QueryParser code in contrib. That could be reason enough to move it to contrib. But it doesn't solve the issue longer term when the old QueryParser is removed. It would need to be replaced then, or dropped from contrib.&lt;/p&gt;

&lt;p&gt;With the new info from Mark H, how hard would it be to create a new imp for the new parser that did a lot of this, in a more defined way? It seems you basically just want to be able to use multiterm queries and group/or things, right? We could even relax a little if we have to. This hasn't been released, so there is still a lot of wiggle room I think. But there does have to be a resolution with this and the new parser at some point either way.&lt;/p&gt;</comment>
                    <comment id="12734241" author="adriano_crestani" created="Wed, 22 Jul 2009 19:37:38 +0100">&lt;p&gt;Hi Mark H.,&lt;/p&gt;

&lt;p&gt;Thanks for the response, some comments inline:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Correct, the "inner phrase" example was a term not a phrase. This is perhaps a better example:&lt;/p&gt;

&lt;p&gt;checkBadQuery("\"jo* \"percival smith\" \""); //phrases inside phrases is bad&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think you did not get what I meant, even with your new example, there is no inner phrase, it is: a phrase &amp;lt;"jo* "&amp;gt;, followed by a term &amp;lt;percival&amp;gt;, followed by another term &amp;lt;smith&amp;gt;, and an empty phrase &amp;lt;" "&amp;gt;. So, with your change, the junit passes, but for the wrong reason. It gets an exception complaining about the empty phrase and not because there is an inner phrase (I still don't see how you can type an inner phrase with the current syntax). I think it's not a big deal, but I'm just trying to understand and raise a probable wrong test. I expect you understood what I mean, let me know if I did not make it clear.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Junit is currently the main form of documentation&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But not the ideal, because the source code (junit code) is not released in the binary release. So, the ideal place should be in the javadocs.&lt;/p&gt;

&lt;blockquote&gt;

&lt;ul&gt;
	&lt;li&gt;Wildcard/fuzzy/range clauses can be used to define a phrase element (as opposed to simply single terms)&lt;/li&gt;
	&lt;li&gt;Brackets are used to group/define the acceptable variations for a given phrase element e.g. "(john OR jonathon) smith"&lt;/li&gt;
	&lt;li&gt;"AND" is irrelevant - there is effectively an implied "AND_NEXT_TO" binding all phrase elements&lt;/li&gt;
&lt;/ul&gt;

&lt;/blockquote&gt;

&lt;p&gt;Thanks, now it's clearer for me what is supported or not. I have some questions:&lt;/p&gt;

&lt;p&gt;I understand this AND_NEXT_TO implicit operator between the queries inside the phrase. However, what happens if the user do not type any explicit boolean operator between two terms inside parentheses: "(query parser) lucene". Is the operator between 'query' and 'parser' the implicit AND_NEXT_TO or the default boolean operator (usually OR)?&lt;/p&gt;

&lt;p&gt;What happens if I type "(query AND parser) lucene". In my point of view it is: "(query AND parser) AND_NEXT_TO lucene". Which means for me: find any document that contains the term 'query' and the term 'parser' in the position x, and the term 'lucene' in the position x+1. Is this the expected behaviour?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1) Keep in core and improve error reporting and documentation&lt;br/&gt;
2) Move into "contrib" as experimental&lt;br/&gt;
3) Retain in core but simplify it to support only the simplest syntax (as in my Britney~ example)&lt;br/&gt;
4) Re-engineer the QueryParser.jj to support a formally defined syntax for acceptable "within phrase" operators e.g. *, ~, ( )&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;1 is good, but I would prefer 4 too. Documentation and throw the right exception are necessary. I just don't feel confortable on the complex phrase query parser relying on the main query parser syntax, any change on the main one could easialy brake the complex phrase QP. Anyway, 4 may be done in future &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;Mark M.:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;With the new info from Mark H, how hard would it be to create a new imp for the new parser that did a lot of this, in a more defined way? It seems you basically just want to be able to use multiterm queries and group/or things, right? We could even relax a little if we have to. This hasn't been released, so there is still a lot of wiggle room I think. But there does have to be a resolution with this and the new parser at some point either way.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I am working on the new query parser code. I started recently to read and understand how the ComplexPhraseQP works, so I could reproduce the behaviour using the new QP framework. I first tried to look at this QP as a user and could not figure out what exactly I can or not do with it. I think now we are hitting a big problem, which is related to documentation. That is why I started raising these question, because others could also have the same issues in future.&lt;/p&gt;

&lt;p&gt;So, yes, I can start coding some equivalent QP using the new QP framework, I'm just questioning and trying to understand everything before I start any coding. I don't wanna code anything that wil throw ConcurrentModificationExceptions, that's why I'm raising these issues now, before I start moving it to the new QP.&lt;/p&gt;

&lt;p&gt;Best Regards,&lt;br/&gt;
Adriano Crestani Campos&lt;/p&gt;</comment>
                    <comment id="12734296" author="michaelbusch" created="Wed, 22 Jul 2009 21:24:38 +0100">&lt;p&gt;I think the best thing to do here is do exactly define what syntax is supposed to be supported (which Mark H. did in his latest comment), and then implement the new syntax with the new queryparser. It will enforce correct syntax and give meaningful exceptions if a query was entered that is not supported.&lt;/p&gt;

&lt;p&gt;I think we can still reuse big portions of Mark's patch: we should be able to write a new QueryBuilder that produces the new ComplexPhraseQuery.&lt;/p&gt;

&lt;p&gt;Adriano/Luis: how long would it take to implement? Can we contain it for 2.9?&lt;/p&gt;

&lt;p&gt;This would mean that these new features would go into contrib in 2.9 as part of the new query parser framework, and then be moved to core in 3.0. Also from 3.0 these new features would then be part of Lucene's main query syntax. Would this makes sense?&lt;/p&gt;</comment>
                    <comment id="12734298" author="michaelbusch" created="Wed, 22 Jul 2009 21:26:25 +0100">&lt;p&gt;Reopening this issues; we haven't made a final decision on how we want to go forward yet, but in any case there's remaining work here.&lt;/p&gt;</comment>
                    <comment id="12734300" author="lafa" created="Wed, 22 Jul 2009 21:27:46 +0100">&lt;p&gt;Hi Mark H&lt;/p&gt;

&lt;p&gt;I would like to propose 5,&lt;br/&gt;
5) Re-engineer the QueryParser.jj to support a formally defined syntax for acceptable "within phrase" operators e.g. *, ~, ( ) &lt;br/&gt;
    I propose doing this using using the new QP implementation. (I can write the new javacc QP for this)&lt;br/&gt;
    (this implies that the code will be in contrib in 2.9 and be part of core on 3.0)&lt;/p&gt;

&lt;p&gt;I also want to propose to change the complexphrase to use single quotes,&lt;br/&gt;
this way we can have both implementation for phrases.&lt;/p&gt;

&lt;p&gt;Here is a summary:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;the complexqueryparser would support all Lucene syntax even for phrases&lt;/li&gt;
	&lt;li&gt;and we could add singlequoted text to identify complexphrases&lt;br/&gt;
    1) Wildcard/fuzzy/range clauses can be used to define a phrase element (as opposed to simply single terms)&lt;br/&gt;
    2) Brackets are used to group/define the acceptable variations for a given phrase element e.g. "(john OR jonathon) smith"&lt;br/&gt;
    3) supported operators: OR, *, ~, ( ), ?&lt;br/&gt;
    4) disallow fields, proximity, boosting and operators on single quoted phrases (I'm making an assumption here, Mark H please comment)&lt;br/&gt;
    5) singlequotes need to be escaped, double quotes will be treated as regular punctuation characters inside single quoted strings&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Mark H, can you please elaborate more on the these other operators "+" "-" "^" "AND" "&amp;amp;&amp;amp;" "||" "NOT" "!" ":" "&lt;span class="error"&gt;&amp;#91;&amp;quot; &amp;quot;&amp;#93;&lt;/span&gt;" "&lt;/p&gt;
{" "}
&lt;p&gt;".&lt;/p&gt;

&lt;p&gt;Example:&lt;br/&gt;
A query with single quoted (complexphrase) followed by a term and a normal phrase:&lt;/p&gt;

&lt;p&gt;query: '(john OR jonathon) smith~0.3 order*' order:sell  "stock market"  &lt;/p&gt;
</comment>
                    <comment id="12734323" author="lafa" created="Wed, 22 Jul 2009 22:10:32 +0100">&lt;p&gt;Mark H - &lt;/p&gt;

&lt;p&gt;Question 1)&lt;/p&gt;

&lt;p&gt;I added a doc 5 and 6&lt;/p&gt;
&lt;div class="code panel" style="border-style: solid;border-width: 1px;"&gt;&lt;div class="codeHeader panelHeader" style="border-bottom-width: 1px;border-bottom-style: solid;"&gt;&lt;b&gt;TestComplexPhraseQuery.java&lt;/b&gt;&lt;/div&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
...
  DocData docsContent[] = { &lt;span class="code-keyword"&gt;new&lt;/span&gt; DocData(&lt;span class="code-quote"&gt;"john smith"&lt;/span&gt;, &lt;span class="code-quote"&gt;"1"&lt;/span&gt;),
      &lt;span class="code-keyword"&gt;new&lt;/span&gt; DocData(&lt;span class="code-quote"&gt;"johathon smith"&lt;/span&gt;, &lt;span class="code-quote"&gt;"2"&lt;/span&gt;),      
      &lt;span class="code-keyword"&gt;new&lt;/span&gt; DocData(&lt;span class="code-quote"&gt;"john percival smith goes on  a b c vacation"&lt;/span&gt;, &lt;span class="code-quote"&gt;"3"&lt;/span&gt;),
      &lt;span class="code-keyword"&gt;new&lt;/span&gt; DocData(&lt;span class="code-quote"&gt;"jackson waits tom"&lt;/span&gt;, &lt;span class="code-quote"&gt;"4"&lt;/span&gt;),
      &lt;span class="code-keyword"&gt;new&lt;/span&gt; DocData(&lt;span class="code-quote"&gt;"johathon smith john"&lt;/span&gt;, &lt;span class="code-quote"&gt;"5"&lt;/span&gt;),
      &lt;span class="code-keyword"&gt;new&lt;/span&gt; DocData(&lt;span class="code-quote"&gt;"johathon mary gomes smith"&lt;/span&gt;, &lt;span class="code-quote"&gt;"6"&lt;/span&gt;),
      };
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;for test &lt;br/&gt;
    checkMatches("\"(jo* -john) smyth\"", "2"); // boolean logic with&lt;/p&gt;

&lt;p&gt;would document 5 be returned or just doc 2 should be returned,&lt;br/&gt;
I'm assuming position is always important and doc 5 is supposed to be returned.&lt;br/&gt;
Is this the correct behavior?&lt;/p&gt;

&lt;p&gt;Question 2)&lt;br/&gt;
Should these 2 queries behave the same when we fix the problem&lt;br/&gt;
    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work&lt;br/&gt;
    // checkMatches("\"john (-percival)\"", "1"); // not logic doesn't work&lt;/p&gt;

&lt;p&gt;Question 3)&lt;br/&gt;
for query:&lt;br/&gt;
checkMatches("\"jo*  smith\"~2", "1,2,3,5"); // position logic works.&lt;br/&gt;
doc 6 is also returned, so this feature does not seem to be working.&lt;/p&gt;

&lt;p&gt;Question 4)&lt;br/&gt;
The usage of AND and AND_NEXT_TO is confusing to me&lt;br/&gt;
the query &lt;br/&gt;
checkMatches("\"(jo* AND mary)  smith\"", "1,2,5"); // boolean logic with&lt;/p&gt;

&lt;p&gt;returns 1,2,5 and not 6, but I was only expecting 6 to be returned,&lt;br/&gt;
seems that like the AND is converted into a OR.&lt;br/&gt;
What is the behavior you want to implement?&lt;/p&gt;

</comment>
                    <comment id="12734333" author="lafa" created="Wed, 22 Jul 2009 22:30:32 +0100">&lt;p&gt;Sorry for all the emails, &lt;br/&gt;
I'm still new to JIRA and only now I realized that for every edit I do,a email is sent.&lt;/p&gt;

&lt;p&gt;But now that I found the preview button, it won't happen again. &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12734337" author="markh" created="Wed, 22 Jul 2009 22:32:21 +0100">&lt;blockquote&gt;&lt;p&gt;I think it's not a big deal, but I'm just trying to understand and raise a probable wrong test.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Granted, the test fails for a reason other than the one for which I wanted it to fail. &lt;br/&gt;
We can probably strike the test and leave a note saying phrase-within-a-phrase just does not make sense and is not supported.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is the operator between 'query' and 'parser' the implicit AND_NEXT_TO or the default boolean operator (usually OR)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In brackets it's an OR - the brackets are used to suggest that the current phrase element at position X is composed of some choices that are evaluated as a subclause in the same way that in normal query logic sub-clauses are defined in brackets e.g. +a +(b OR c). There seems to be a reasonable logic to this.&lt;/p&gt;

&lt;p&gt;Ideally the ComplexPhraseQueryParser should explicitly turn this setting on while evaluating the bracketed innards of phrases just in case the base class has AND as the default.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Mark H, can you please elaborate more on the these other operators "+" "-" "^" "AND" "&amp;amp;&amp;amp;" "||" "NOT" "!" ":" "&lt;span class="error"&gt;&amp;#91;&amp;quot; &amp;quot;&amp;#93;&lt;/span&gt;" "{" "}".&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I'll try and deal with them one by one but these are not necessarily definitive answers or guarantees of correctly implemented support&lt;/p&gt;

&lt;p&gt;OR,||,+, AND, &amp;amp;&amp;amp; ..... ignored. The implicit operator is AND_NEXT_TO apart from in bracketed sections where all elements at this level are ORed&lt;br/&gt;
^ .....boosts are carried through from TermQuerys to SpanTermQuerys&lt;br/&gt;
NOT, ! ....Creates SpanNotQueries &lt;br/&gt;
[]{} ....range queries are supported as are wildcards *, fuzzies  ~, ?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;query: '(john OR jonathon) smith~0.3 order*' order:sell "stock market"&lt;/p&gt;&lt;/blockquote&gt;


&lt;p&gt;I'll post the XML query syntax equivalent of what should be parsed here shortly (just seen your next comment come in) &lt;/p&gt;


</comment>
                    <comment id="12734349" author="markh" created="Wed, 22 Jul 2009 22:59:28 +0100">&lt;blockquote&gt;&lt;p&gt;for test checkMatches("\"(jo* -john) smyth\"", "2"); &lt;br/&gt;
would document 5 be returned or just doc 2 should be returned,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I presume you mean smith not smyth here otherwise nothing would match? If so, doc 5 should match and position is relevant (subject to slop factors).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Question 2)&lt;br/&gt;
Should these 2 queries behave the same when we fix the problem&lt;br/&gt;
// checkMatches("\"john -percival\"", "1"); // not logic doesn't work&lt;br/&gt;
// checkMatches("\"john (-percival)\"", "1"); // not logic doesn't work&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suppose there's an open question as to if the second example is legal (the brackets are unnecessary)&lt;/p&gt;



&lt;blockquote&gt;
&lt;p&gt;Question 3)&lt;br/&gt;
checkMatches("\"jo* smith\"~2", "1,2,3,5"); // position logic works.&lt;br/&gt;
doc 6 is also returned, so this feature does not seem to be working.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That looks like a bug related to slop factor?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Question 4)&lt;br/&gt;
The usage of AND and AND_NEXT_TO is confusing to me&lt;br/&gt;
the query &lt;br/&gt;
checkMatches("\"(jo* AND mary) smith\"", "1,2,5"); // boolean logic with&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ANDs are ignored and turned into ORs (see earlier comments) but maybe a query parse error should be thrown to emphasise this.&lt;/p&gt;


</comment>
                    <comment id="12734355" author="markh" created="Wed, 22 Jul 2009 23:14:20 +0100">&lt;blockquote&gt;
&lt;p&gt;query: '(john OR jonathon) smith~0.3 order*' order:sell "stock market"&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Would be parsed as follows (shown as equivalent XMLQueryParser syntax)&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-xml"&gt; 
&lt;span class="code-tag"&gt;&amp;lt;BooleanQuery&amp;gt;&lt;/span&gt;
  &lt;span class="code-tag"&gt;&amp;lt;Clause occurs=&lt;span class="code-quote"&gt;"should"&lt;/span&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="code-tag"&gt;&amp;lt;SpanNear &amp;gt;&lt;/span&gt;		
			&lt;span class="code-tag"&gt;&amp;lt;SpanOr&amp;gt;&lt;/span&gt;
				&lt;span class="code-tag"&gt;&amp;lt;SpanOrTerms&amp;gt;&lt;/span&gt;john jonathon &lt;span class="code-tag"&gt;&amp;lt;/SpanOrTerms&amp;gt;&lt;/span&gt;
			&lt;span class="code-tag"&gt;&amp;lt;/SpanOr&amp;gt;&lt;/span&gt;
			&lt;span class="code-tag"&gt;&amp;lt;SpanOr&amp;gt;&lt;/span&gt;
				&lt;span class="code-tag"&gt;&amp;lt;SpanOrTerms&amp;gt;&lt;/span&gt; smith smyth&lt;span class="code-tag"&gt;&amp;lt;/SpanOrTerms&amp;gt;&lt;/span&gt;
			&lt;span class="code-tag"&gt;&amp;lt;/SpanOr&amp;gt;&lt;/span&gt;
			&lt;span class="code-tag"&gt;&amp;lt;SpanOr&amp;gt;&lt;/span&gt;
				&lt;span class="code-tag"&gt;&amp;lt;SpanOrTerms&amp;gt;&lt;/span&gt; order orders&lt;span class="code-tag"&gt;&amp;lt;/SpanOrTerms&amp;gt;&lt;/span&gt;
			&lt;span class="code-tag"&gt;&amp;lt;/SpanOr&amp;gt;&lt;/span&gt;
   &lt;span class="code-tag"&gt;&amp;lt;/SpanNear&amp;gt;&lt;/span&gt;
 &lt;span class="code-tag"&gt;&amp;lt;/Clause&amp;gt;&lt;/span&gt;
&lt;span class="code-tag"&gt;&amp;lt;Clause occurs=&lt;span class="code-quote"&gt;"should"&lt;/span&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="code-tag"&gt;&amp;lt;TermQuery fieldName=&lt;span class="code-quote"&gt;"order"&lt;/span&gt; &amp;gt;&lt;/span&gt;sell&lt;span class="code-tag"&gt;&amp;lt;/TermQuery&amp;gt;&lt;/span&gt;		
 &lt;span class="code-tag"&gt;&amp;lt;/Clause&amp;gt;&lt;/span&gt;
&lt;span class="code-tag"&gt;&amp;lt;Clause occurs=&lt;span class="code-quote"&gt;"should"&lt;/span&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="code-tag"&gt;&amp;lt;UserQuery&amp;gt;&lt;/span&gt;&lt;span class="code-quote"&gt;"stock market"&lt;/span&gt;&lt;span class="code-tag"&gt;&amp;lt;/UserQuery &amp;gt;&lt;/span&gt;		
 &lt;span class="code-tag"&gt;&amp;lt;/Clause&amp;gt;&lt;/span&gt;
&lt;span class="code-tag"&gt;&amp;lt;/BooleanQuery&amp;gt;&lt;/span&gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12734398" author="adriano_crestani" created="Thu, 23 Jul 2009 01:09:21 +0100">&lt;blockquote&gt;
&lt;p&gt;I propose doing this using using the new QP implementation. (I can write the new javacc QP for this)&lt;br/&gt;
(this implies that the code will be in contrib in 2.9 and be part of core on 3.0)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That would be good!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Granted, the test fails for a reason other than the one for which I wanted it to fail.&lt;br/&gt;
We can probably strike the test and leave a note saying phrase-within-a-phrase just does not make sense and is not supported.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Cool, I agree to remove it. But I still don't see how an user can type a phrase inside a phrase with the current syntax definition, can you give me an example?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In brackets it's an OR - the brackets are used to suggest that the current phrase element at position X is composed of some choices that are evaluated as a subclause in the same way that in normal query logic sub-clauses are defined in brackets e.g. +a +(b OR c). There seems to be a reasonable logic to this.&lt;/p&gt;

&lt;p&gt;Ideally the ComplexPhraseQueryParser should explicitly turn this setting on while evaluating the bracketed innards of phrases just in case the base class has AND as the default.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we use the implemented java cc code Luis suggested, we would have already a query parser that throws ParseExceptions whenever the user types an AND inside a phrase.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;OR,||,+, AND, &amp;amp;&amp;amp; ..... ignored&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So we should throw an excpetion if any of these is found inside a phrase. It could confuse the user if we just ignore it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    Question 2)&lt;br/&gt;
    Should these 2 queries behave the same when we fix the problem&lt;br/&gt;
    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work&lt;br/&gt;
    // checkMatches("\"john (-percival)\"", "1"); // not logic doesn't work&lt;/p&gt;

&lt;p&gt;I suppose there's an open question as to if the second example is legal (the brackets are unnecessary)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, the second is unnecessary, but I don't think it's illegal. The user could type &amp;lt;(smith)&amp;gt; outside the phrase, it makes sense to support it inside also.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    Question 3)&lt;br/&gt;
    checkMatches("\"jo* smith\"~2", "1,2,3,5"); // position logic works.&lt;br/&gt;
    doc 6 is also returned, so this feature does not seem to be working.&lt;/p&gt;

&lt;p&gt;That looks like a bug related to slop factor?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have not checked yet, but I think it's working fine. The slop means how many switches between the terms inside the phrase is allowed to match the query. It matches doc 6, because the term &amp;lt;smith&amp;gt; switches twice to the right and matched "johathon mary gomes smith". Twice = slop 2 &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ANDs are ignored and turned into ORs (see earlier comments) but maybe a query parse error should be thrown to emphasise this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we could support AND also. I agree there are few cases where the user would use that. It would work as I explained before:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What happens if I type "(query AND parser) lucene". In my point of view it is: "(query AND parser) AND_NEXT_TO lucene". Which means for me: find any document that contains the term 'query' and the term 'parser' in the position x, and the term 'lucene' in the position x+1. Is this the expected behaviour?&lt;/p&gt;&lt;/blockquote&gt;</comment>
                    <comment id="12735041" author="iorixxx" created="Fri, 24 Jul 2009 13:19:09 +0100">&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I am using your ComplexPhraseQueryParser. I integrated it into Solr. &lt;br/&gt;
I am interested in it mainly because it supports OR operator and wildcards inside proximity search.&lt;/p&gt;

&lt;p&gt;Specifically : "(john johathon) smith"~10   and   "j* smith"&lt;br/&gt;
They both work perfectly, thank you for your work.&lt;/p&gt;

&lt;p&gt;I downloaded source code of it from &lt;a href="http://svn.apache.org/viewvc?view=rev&amp;amp;revision=791579" class="external-link"&gt;http://svn.apache.org/viewvc?view=rev&amp;amp;revision=791579&lt;/a&gt;&lt;br/&gt;
And then edited the code a little bit since I am using lucene 2.4.1:&lt;/p&gt;

&lt;p&gt;I replaced those:&lt;br/&gt;
1-) TermRangeQuery to RangeQuery.&lt;br/&gt;
2-) getConstantScoreRewrite() to getUseOldRangeQuery();&lt;br/&gt;
3-) setConstantScoreRewrite(false); to setUseOldRangeQuery(true);&lt;br/&gt;
4-) On line 168 of ComplexPhraseQueryParser.java there are two semicolons ( ; ; )&lt;/p&gt;

&lt;p&gt;I am not sure what I did is the way to start using this query parser with latest versions of lucene/solr.&lt;br/&gt;
If it is not can you suggest me better ways or where to get/download latest source code of query parser.&lt;/p&gt;

&lt;p&gt;I am having problems with multi-field searches.&lt;/p&gt;

&lt;p&gt;Query  "(john johathon) smith"~10 works on default field, e.g. text.&lt;/p&gt;

&lt;p&gt;But when I want to run the same query on another field (other than default field)&lt;br/&gt;
title:"(john johathon) smith"~10 &lt;br/&gt;
it gives exception below:&lt;br/&gt;
Cannot have clause for field "text" nested in phrase for field "title"&lt;/p&gt;

&lt;p&gt;When I ran the query distibuting field name to all terms it works:&lt;br/&gt;
title:"(title:john title:johathon) title:smith"~10&lt;/p&gt;

&lt;p&gt;Is there an easy way to set field of all terms (without specifying)?&lt;/p&gt;

&lt;p&gt;And about boosts of multi-field queries, is this query legal? (default operator = OR, default field = text)&lt;/p&gt;

&lt;p&gt;title:"(title:john title:johathon) title:smith"~10^1.5 OR "(john johathon) smith"~10^3.0&lt;/p&gt;

&lt;p&gt;Shortly I want to use this queryparser to query on multi-fields with different boosts.&lt;/p&gt;

&lt;p&gt;I am not sure if I am allowed to ask such question in here, if not please accept my apologies.&lt;/p&gt;

&lt;p&gt;Thank you for your consideration.&lt;/p&gt;

&lt;p&gt;Ahmet Arslan&lt;/p&gt;</comment>
                    <comment id="12735058" author="markh" created="Fri, 24 Jul 2009 14:57:09 +0100">&lt;p&gt;Fix for phrases using QueryParser's non-default field e.g. &lt;br/&gt;
     author:"j* smith"&lt;/p&gt;</comment>
                    <comment id="12736851" author="markrmiller@gmail.com" created="Wed, 29 Jul 2009 22:30:45 +0100">&lt;p&gt;If we don't have a clear path for this very soon I think we should pull it from this release.&lt;/p&gt;</comment>
                    <comment id="12736965" author="lafa" created="Thu, 30 Jul 2009 03:37:20 +0100">&lt;p&gt;My understanding is that with "New flexible query parser" (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1567" title="New flexible query parser"&gt;&lt;del&gt;LUCENE-1567&lt;/del&gt;&lt;/a&gt;),&lt;br/&gt;
the old QueryParser classes will be deprecated in 2.9&lt;br/&gt;
and removed in 3.0 (or moved to contrib in 3.0).&lt;/p&gt;

&lt;p&gt;This change will also make ComplexPhraseQueryParser deprecated&lt;br/&gt;
because it currently extends the old queryparser.&lt;/p&gt;

&lt;p&gt;ComplexPhraseQueryParser was not part of any lucene release&lt;br/&gt;
and was only checked in 2 months ago in trunk.&lt;/p&gt;

&lt;p&gt;For the reasons above I think we should re-implement this functionality&lt;br/&gt;
using the new flexible query parser.&lt;/p&gt;

&lt;p&gt;3.0 and 2.9 releases will be very similar &lt;br/&gt;
but 3.0 will have all deprecated APIs removed (at least this is my understanding).&lt;/p&gt;

&lt;p&gt;In my view the path should be:&lt;/p&gt;
&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Wait for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1567" title="New flexible query parser"&gt;&lt;del&gt;LUCENE-1567&lt;/del&gt;&lt;/a&gt; to be in trunk&lt;/li&gt;
	&lt;li&gt;re-implement this feature using the "New flexible query parser"&lt;/li&gt;
	&lt;li&gt;and probably do it using a super set of the current syntax with a new TextParser.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I'm not sure if I'll have the time to implement a compatible implementation of&lt;br/&gt;
ComplexPhraseQueryParser before 2.9 release &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/sad.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I'm currently working on 1567 to finalize the patch,&lt;br/&gt;
cleaning up javadocs and some small clean up to the APIs.&lt;/p&gt;

&lt;p&gt;I'll try to work on ComplexPhraseQueryParser,&lt;br/&gt;
once lucene-1567 is in the trunk.&lt;/p&gt;

&lt;p&gt;So in my view, ComplexPhraseQueryParser depends on 1567, &lt;br/&gt;
and will require some extra work after 1567 is in the trunk.&lt;/p&gt;

&lt;p&gt;I think we have the following, options:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;We could wait until 1567 is in trunk and wait for a compatible implementation of ComplexPhraseQueryParser using 1567,&lt;br/&gt;
  before we release 2.9. (this would still remove the current ComplexPhraseQueryParser class, and provide this features with LuceneQueryParserHelper class, or with a new TextParser name complexphrase)&lt;/li&gt;
	&lt;li&gt;We can release 2.9 with only 1567, but that will require ComplexPhraseQueryParser to be removed from trunk or at least deprecated in 2.9, and in 3.X re-implement it using the "New flexible query parser" APIs&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I hope this helps &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;
</comment>
                    <comment id="12736966" author="markrmiller@gmail.com" created="Thu, 30 Jul 2009 03:39:44 +0100">&lt;p&gt;Okay thanks. I think we should pull it for 2.9.&lt;/p&gt;</comment>
                    <comment id="12737212" author="markrmiller@gmail.com" created="Thu, 30 Jul 2009 19:17:07 +0100">&lt;p&gt;Okay, so I guess the question is - who objects to pulling this from 2.9? I don't think we should release a class that extends a deprecated class and I don't think we want to hold up 2.9 waiting for an adequate non deprecated replacement.&lt;/p&gt;</comment>
                    <comment id="12737270" author="markh" created="Thu, 30 Jul 2009 21:41:50 +0100">&lt;p&gt;No objections to pulling from core given the impending deprecation of the QueryParser base class.&lt;/p&gt;

&lt;p&gt;I know of at least 2 folks using it so moving it to contrib would help provide somewhere to maintain fixes while we wait for the new QueryParser to incorporate the complex phrase features.&lt;/p&gt;</comment>
                    <comment id="12737311" author="michaelbusch" created="Thu, 30 Jul 2009 22:47:24 +0100">&lt;p&gt;+1 for moving it to conrib. Then the users Mark H. mentioned can consume it from a contrib jar until these features are in the new QP.&lt;/p&gt;</comment>
                    <comment id="12737314" author="markrmiller@gmail.com" created="Thu, 30 Jul 2009 22:53:06 +0100">&lt;p&gt;Alright, then - do you have time to handle that soon Mark H? If not I can probably make some time for it.&lt;/p&gt;</comment>
                    <comment id="12737913" author="markrmiller@gmail.com" created="Sat, 1 Aug 2009 17:42:57 +0100">&lt;p&gt;patch that moves to contrib&lt;/p&gt;</comment>
                    <comment id="12739824" author="mikemccand" created="Thu, 6 Aug 2009 00:33:22 +0100">&lt;p&gt;Reopening so we don't forget to do this one...&lt;/p&gt;

&lt;p&gt;Come 3.0, how will this work, even in contrib?  (Because the plan is to replace the old queryParser with the new one for 3.0).&lt;/p&gt;</comment>
                    <comment id="12739826" author="markrmiller@gmail.com" created="Thu, 6 Aug 2009 00:39:47 +0100">&lt;p&gt;The plan is to remove it and add a replacement built on the new QueryParser. The replacement may not be exactly the same, but it should be very similar.&lt;/p&gt;

&lt;p&gt;My inclination was to leave it out of this release - its a single class and so easy to manage and plug it in separately if you want. I don't know that we should release a class that may or may not get a replacement (promises, promises &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; ) and extends a deprecated class. Contrib was once called sandbox though and consensus appeared to be to put it in contrib - so I went with that and added a warning that the class might change soon.&lt;/p&gt;</comment>
                    <comment id="12747982" author="gsingers" created="Wed, 26 Aug 2009 16:27:35 +0100">&lt;p&gt;I'm not sure why the ComplexPhraseQuery itself is buried in the Parser.  Can't the query stand on it's own?  Seems like it could be a useful class outside of the specific content of a QueryParser, no?&lt;/p&gt;</comment>
                    <comment id="12748046" author="markh" created="Wed, 26 Aug 2009 18:42:01 +0100">&lt;p&gt;It does not stand on it's own as it is merely a temporary object used as a peculiarity in the way the parsing works. The SpanQuery family would be the legitimate standalone equivalents of this class.&lt;/p&gt;

&lt;p&gt;ComplexPhraseQuery objects are constructed during the the first pass of parsing to capture everything between quotes as an opaque string.&lt;br/&gt;
The ComplexPhraseQueryParser then calls "parsePhraseElements(...)" on these objects to complete the process of parsing in a second pass where in this context any brackets etc take on a different meaning&lt;br/&gt;
There is no merit in making this externally visible.&lt;/p&gt;


</comment>
                    <comment id="12749557" author="lafa" created="Mon, 31 Aug 2009 19:04:16 +0100">&lt;p&gt;We hope to implement this on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1823" title="QueryParser with new features for Lucene 3"&gt;LUCENE-1823&lt;/a&gt;, along with other features.&lt;/p&gt;</comment>
                    <comment id="12776113" author="thetaphi" created="Tue, 10 Nov 2009 22:19:41 +0000">&lt;p&gt;Move to 3.1 as this is a new feature.&lt;/p&gt;</comment>
                    <comment id="12782254" author="iorixxx" created="Wed, 25 Nov 2009 00:17:50 +0000">&lt;p&gt;Hi Mark,&lt;/p&gt;

&lt;p&gt;Up to now, I was consuming ComplexPhraseQueryParser.java by means of copy paste into my source code, so I didn't notice.&lt;br/&gt;
Today I find out that ComplexPhraseQuery.java in Lucene 2.9.1 Misc has missed the non default field.patch.&lt;br/&gt;
It gives exception with author:"fred* smith" style queries.&lt;br/&gt;
I am writing a solr plugin to contribute for this query parser and Solr 1.4.0 directly depends on lucene-misc-2.9.1.jar.&lt;br/&gt;
Should I edit and include source code of  ComplexPhraseQueryParser.java in my patch to solve this problem?&lt;br/&gt;
Or is there a more convenient way to do it?&lt;/p&gt;

&lt;p&gt;Thank you for your consideration.&lt;/p&gt;</comment>
                    <comment id="12782521" author="markh" created="Wed, 25 Nov 2009 17:31:35 +0000">&lt;p&gt;Ugh. There's probably two separate actions required here then:&lt;br/&gt;
1) a bug needs raising on Lucene.&lt;br/&gt;
2) guidance needed from the Solr team about preferred course of action&lt;/p&gt;</comment>
                    <comment id="12787053" author="dkaelbling@blackducksoftware.com" created="Mon, 7 Dec 2009 19:12:57 +0000">&lt;p&gt;Could someone link the new Lucene bug mentioned above to this issue?  I couldn't find it.&lt;/p&gt;</comment>
                    <comment id="12835961" author="markh" created="Fri, 19 Feb 2010 21:29:18 +0000">&lt;p&gt;Double Ugh. Applying the patch for the "non-default field" bug doesn't work any more because the latest ComplexPhraseQueryParser source sitting in contrib now has a different package to the QueryParser base class . This means that this subclass doesn't have the required write access to the package-protected "field" variable. This is needed to temporarily set the context of the parser when processing the inner contents of the phrase.&lt;/p&gt;

&lt;p&gt;Fixing this would require changing the package name of ComplexPhraseQueryParser or changing the visibility of "field" in the QueryParser base class to "protected".&lt;br/&gt;
Anyone have any strong feelings about which of these is the most acceptable?&lt;/p&gt;</comment>
                    <comment id="12900278" author="terje_eggestad" created="Thu, 19 Aug 2010 14:21:04 +0100">&lt;p&gt;Hi &lt;/p&gt;

&lt;p&gt;I'm about begin using the ComplexPhraseQueryParser with 3.0.2 as we need wildcard with phrases and proximity &lt;/p&gt;

&lt;p&gt;Our customers have a habit of including '-' in phrases which seem to trigger a bug :&lt;/p&gt;

&lt;p&gt;If you add the following tests to the TestComplexPhraseQueryParser class:&lt;/p&gt;

&lt;p&gt;		checkMatches("\"joe john nosuchword\"", "");  &lt;br/&gt;
		checkMatches("\"joe-john-nosuchword\"", "");  &lt;br/&gt;
		checkMatches("\"john-nosuchword smith\"", "");  &lt;/p&gt;

&lt;p&gt;AND add a rewrite() in checkMatches() just after parse :&lt;br/&gt;
 			Query q = qp.parse(qString);&lt;br/&gt;
 			IndexReader reader = searcher.getIndexReader();  // need for rewrite&lt;br/&gt;
  			q = q.rewrite(reader); &lt;/p&gt;


&lt;p&gt;The first two is OK, and is rewritten to:&lt;/p&gt;

&lt;p&gt;spanNear(&lt;span class="error"&gt;&amp;#91;name:joe, name:john, name:nosuchword&amp;#93;&lt;/span&gt;, 0, true)&lt;br/&gt;
name:"joe john nosuchword"&lt;/p&gt;


&lt;p&gt;The third bomb out on &lt;/p&gt;

&lt;p&gt;java.lang.IllegalArgumentException: Unknown query type "org.apache.lucene.search.PhraseQuery" found in phrase query string "john-nosuchword smith"&lt;br/&gt;
	at org.apache.lucene.queryParser.ComplexPhraseQueryParser$ComplexPhraseQuery.rewrite(ComplexPhraseQueryParser.java:281)&lt;br/&gt;
	at org.apache.lucene.queryParser.TestComplexPhraseQuery.checkMatches(TestComplexPhraseQuery.java:120)&lt;br/&gt;
.&lt;br/&gt;
.&lt;br/&gt;
.&lt;/p&gt;


&lt;p&gt;I made a fix that &lt;b&gt;seem&lt;/b&gt; to fixit, but I feel on very shaky ground here.&lt;br/&gt;
I've made so many debugging hack around that I can't make a propper patch, but I added this fix to ComplexPhraseQueryParser::rewrite()&lt;br/&gt;
just before the place the exception is thrown:&lt;/p&gt;

&lt;p&gt;       } else {&lt;br/&gt;
        	if (qc instanceof TermQuery) &lt;/p&gt;
{
        		TermQuery tq = (TermQuery) qc;
        		allSpanClauses[i] = new SpanTermQuery(tq.getTerm());

// START  FIX "A-B C" phrases
        	}
&lt;p&gt; else if (qc instanceof PhraseQuery) {&lt;br/&gt;
        		PhraseQuery pq = (PhraseQuery) qc;&lt;br/&gt;
        		Term[] subterms = pq.getTerms();&lt;/p&gt;

&lt;p&gt;        		SpanQuery[] clauses = new SpanQuery&lt;span class="error"&gt;&amp;#91;subterms.length&amp;#93;&lt;/span&gt;;&lt;br/&gt;
        		for (int j = 0; j &amp;lt; subterms.length; j++) &lt;/p&gt;
{
        			clauses[j] = new SpanTermQuery(subterms[j]);
        		}
&lt;p&gt;        		allSpanClauses&lt;span class="error"&gt;&amp;#91;i&amp;#93;&lt;/span&gt; = new SpanNearQuery(clauses, 0, true);&lt;br/&gt;
// END FIX&lt;br/&gt;
        	}	else &lt;/p&gt;
{

        		throw new IllegalArgumentException("Unknown query type \""
        				+ qc.getClass().getName()
        				+ "\" found in phrase query string \""
        				+ phrasedQueryStringContents + "\"");
        	}



</comment>
                    <comment id="13200258" author="markrmiller@gmail.com" created="Sat, 4 Feb 2012 01:39:25 +0000">&lt;blockquote&gt;&lt;p&gt;changing the visibility of "field" in the QueryParser base class to "protected".&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This seems reasonable?&lt;/p&gt;</comment>
                    <comment id="13202409" author="tomasflobbe" created="Tue, 7 Feb 2012 14:10:16 +0000">&lt;p&gt;Hi I'm working in this change to allow field queries. I noted that queries like:&lt;br/&gt;
name:"de*" &lt;br/&gt;
name:de* &lt;br/&gt;
fail due to the exception thrown in the "rewrite" method: &lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
    &lt;span class="code-keyword"&gt;public&lt;/span&gt; Query rewrite(IndexReader reader) &lt;span class="code-keyword"&gt;throws&lt;/span&gt; IOException {
      &lt;span class="code-comment"&gt;// ArrayList spanClauses = &lt;span class="code-keyword"&gt;new&lt;/span&gt; ArrayList();
&lt;/span&gt;      &lt;span class="code-keyword"&gt;if&lt;/span&gt; (contents &lt;span class="code-keyword"&gt;instanceof&lt;/span&gt; TermQuery) {
        &lt;span class="code-keyword"&gt;return&lt;/span&gt; contents;
      }
      
      &lt;span class="code-comment"&gt;// Build a sequence of Span clauses arranged in a SpanNear - child
&lt;/span&gt;      &lt;span class="code-comment"&gt;// clauses can be complex
&lt;/span&gt;      &lt;span class="code-comment"&gt;// Booleans e.g. nots and ors etc
&lt;/span&gt;      &lt;span class="code-object"&gt;int&lt;/span&gt; numNegatives = 0;
      &lt;span class="code-keyword"&gt;if&lt;/span&gt; (!(contents &lt;span class="code-keyword"&gt;instanceof&lt;/span&gt; BooleanQuery)) {
        &lt;span class="code-keyword"&gt;throw&lt;/span&gt; &lt;span class="code-keyword"&gt;new&lt;/span&gt; IllegalArgumentException(&lt;span class="code-quote"&gt;"Unknown query type \"&lt;/span&gt;"
            + contents.getClass().getName()
            + &lt;span class="code-quote"&gt;"\"&lt;/span&gt; found in phrase query string \"" + phrasedQueryStringContents
            + &lt;span class="code-quote"&gt;"\"&lt;/span&gt;");
      }
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;By changing it to something like:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;if&lt;/span&gt; (!(contents &lt;span class="code-keyword"&gt;instanceof&lt;/span&gt; BooleanQuery)) {
        &lt;span class="code-keyword"&gt;return&lt;/span&gt; contents;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;queries like the one above work, together with all the other queries available in the unit test. Is there something I'm missing with the previous change? I know the ComplexPhraseQueryParser is not intended to be used for queries like the ones I'm proposing, but why does it needs to fail in those cases?&lt;/p&gt;</comment>
                    <comment id="13202523" author="tomasflobbe" created="Tue, 7 Feb 2012 16:49:45 +0000">&lt;p&gt;I attached a patch with the change of my previous comment plus the change that allows fielded queries.&lt;/p&gt;</comment>
                    <comment id="13203507" author="iorixxx" created="Wed, 8 Feb 2012 12:16:32 +0000">&lt;p&gt;Mark's and Tomas' non default field patches are combined.&lt;/p&gt;</comment>
                    <comment id="13203514" author="iorixxx" created="Wed, 8 Feb 2012 12:33:01 +0000">&lt;p&gt;Thanks for looking into this, Mark and Tomas. Do you think this issue is the right place to introduce boolean inOrder parameter? Currently always inOrder=true is passed to SpanNearQuery's ctor.&lt;/p&gt;</comment>
                    <comment id="13203652" author="tomasflobbe" created="Wed, 8 Feb 2012 15:01:42 +0000">&lt;p&gt;Ahmet, I created a Jira for the "inOrder" in the ComplexPhraseQueryParser. See &lt;a href="https://issues.apache.org/jira/browse/LUCENE-3758" title="Allow the ComplexPhraseQueryParser to search order or un-order proximity queries."&gt;LUCENE-3758&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="13501679" author="otis" created="Wed, 21 Nov 2012 03:11:39 +0000">&lt;p&gt;The JIRA cleaning man is here.  I thought this was committed long ago, but I just noticed it's open and set for 4.1. Huh?&lt;br/&gt;
Last activity on this pretty popular issue was from &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markh" class="user-hover" rel="markh"&gt;Mark Harwood&lt;/a&gt; back in 2008!&lt;/p&gt;</comment>
                    <comment id="13580828" author="dmitry_key" created="Mon, 18 Feb 2013 20:54:44 +0000">&lt;p&gt;Can someone give me a hand on this parser (despite the jira is so old)?&lt;/p&gt;

&lt;p&gt;We need to have the NOT logic work properly in the boolean sense, that is the following should work correctly:&lt;/p&gt;

&lt;p&gt;a AND NOT b&lt;br/&gt;
a AND NOT (b OR c)&lt;br/&gt;
a AND NOT ((b OR c) AND (d OR e))&lt;/p&gt;

&lt;p&gt;Can anybody guide me here? Is it at all possible to accomplish this with this original CPQP implementation? I would not be afraid of changing QueryParser.jj lexical specification, if the task requires it.&lt;/p&gt;</comment>
                    <comment id="13582207" author="dmitry_key" created="Wed, 20 Feb 2013 14:30:43 +0000">&lt;p&gt;OK, after some study, here is what we did:&lt;/p&gt;

&lt;p&gt;we treat the AND clauses as spanNearQuery objects. So, the&lt;/p&gt;

&lt;p&gt;a AND b&lt;/p&gt;

&lt;p&gt;becomes %a b%&lt;sub&gt;slop, where %%&lt;/sub&gt; operator is an unordered SpanNear query (change to QueryParser.jj was required for this).&lt;/p&gt;

&lt;p&gt;When there is a case of NOT clause with nested clauses:&lt;/p&gt;

&lt;p&gt;NOT( (a AND b) OR (c AND d) ) = NOT ( %a b%~slop OR %c d%~slop ) ,&lt;/p&gt;

&lt;p&gt;we need to handle SpanNearQueries in the addComplexPhraseClause method. In order to handle this, we just added to the if statement:&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;code&amp;#93;&lt;/span&gt;&lt;br/&gt;
    if (qc instanceof BooleanQuery) {&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;/code&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;the following else if statement:&lt;/p&gt;

&lt;p&gt;&lt;span class="error"&gt;&amp;#91;code&amp;#93;&lt;/span&gt;&lt;br/&gt;
else if (childQuery instanceof SpanNearQuery) {&lt;br/&gt;
    ors.add((SpanQuery)childQuery);&lt;br/&gt;
}&lt;br/&gt;
&lt;span class="error"&gt;&amp;#91;/code&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12417241">LUCENE-1567</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12433389">LUCENE-1823</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12441718">SOLR-1604</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="12310040">
                <name>Required</name>
                                                <inwardlinks description="is required by">
                            <issuelink>
            <issuekey id="12441718">SOLR-1604</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12401245" name="ComplexPhraseQueryParser.java" size="12324" author="markh" created="Mon, 2 Mar 2009 15:17:06 +0000"/>
                    <attachment id="12414166" name="junit_complex_phrase_qp_07_21_2009.patch" size="7747" author="adriano_crestani" created="Wed, 22 Jul 2009 01:25:25 +0100"/>
                    <attachment id="12414217" name="junit_complex_phrase_qp_07_22_2009.patch" size="1505" author="lafa" created="Wed, 22 Jul 2009 15:53:43 +0100"/>
                    <attachment id="12414443" name="Lucene-1486 non default field.patch" size="1532" author="markh" created="Fri, 24 Jul 2009 14:57:09 +0100"/>
                    <attachment id="12513804" name="LUCENE-1486.patch" size="5976" author="iorixxx" created="Wed, 8 Feb 2012 12:16:32 +0000"/>
                    <attachment id="12513628" name="LUCENE-1486.patch" size="5660" author="tomasflobbe" created="Tue, 7 Feb 2012 16:49:45 +0000"/>
                    <attachment id="12415242" name="LUCENE-1486.patch" size="44172" author="markrmiller@gmail.com" created="Sat, 1 Aug 2009 17:42:57 +0100"/>
                    <attachment id="12412631" name="LUCENE-1486.patch" size="21140" author="markrmiller@gmail.com" created="Mon, 6 Jul 2009 18:03:06 +0100"/>
                    <attachment id="12412195" name="LUCENE-1486.patch" size="21124" author="markrmiller@gmail.com" created="Tue, 30 Jun 2009 20:29:43 +0100"/>
                    <attachment id="12411703" name="LUCENE-1486.patch" size="19071" author="markh" created="Wed, 24 Jun 2009 22:05:57 +0100"/>
                    <attachment id="12410682" name="LUCENE-1486.patch" size="18831" author="markrmiller@gmail.com" created="Mon, 15 Jun 2009 18:00:37 +0100"/>
                    <attachment id="12401246" name="TestComplexPhraseQuery.java" size="5234" author="markh" created="Mon, 2 Mar 2009 15:18:09 +0000"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 12 Dec 2008 10:45:09 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12265</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26235</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1361] QueryParser should have a setDateFormat(DateFormat) method</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1361</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently the only way to change the date format used by QueryParser.java is to override the getRangeQuery method. This seems a bit excessive to me. Since QueryParser isn't threadsafe (like DateFormat) I would suggest that a DateFormat field be introduced (protected DateFormat dateFormat) and a setter be introduced (public void setDateFormat(DateFormat format)) so that it's easier to customize the date format in queries. If there are good reasons against this (can't imagine, but who knows) why not introduce a protected 'DateFormat:createDateFormat())' method so that, again, it's easier for clients to override this logic.&lt;/p&gt;</description>
                <environment/>
            <key id="12402826">LUCENE-1361</key>
            <summary>QueryParser should have a setDateFormat(DateFormat) method</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="ocean">ocean</reporter>
                        <labels>
                    </labels>
                <created>Fri, 22 Aug 2008 00:08:36 +0100</created>
                <updated>Fri, 10 May 2013 00:05:36 +0100</updated>
                                    <version>2.3.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12625471" author="hossman" created="Mon, 25 Aug 2008 20:19:10 +0100">&lt;p&gt;to clarify: the DateFormat used by QueryParser when dealing with RangeQueries is currently controlled by calling setLocale ... it's not actually necessary to override the getRangeQuery method.&lt;/p&gt;

&lt;p&gt;but that doesn't mean allowing an explicit DateFormat to be set is a bad idea.&lt;/p&gt;</comment>
                    <comment id="12718287" author="markrmiller@gmail.com" created="Thu, 11 Jun 2009 04:18:14 +0100">&lt;p&gt;If a patch is supplied, we could consider for 2.9, but otherwise I am moving out for now&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 25 Aug 2008 19:19:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12387</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26361</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-1000] queryparsersyntax.html escaping section needs beefed up</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1000</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;the query syntax documentation is currently lacking several key pieces of info:&lt;br/&gt;
 1) that unicode style escapes are valid&lt;br/&gt;
 2) that any character can be escaped with a backslash, not just special chars.&lt;/p&gt;

&lt;p&gt;..we should probably beef up the "Escaping Special Characters" section&lt;/p&gt;</description>
                <environment/>
            <key id="12378333">LUCENE-1000</key>
            <summary>queryparsersyntax.html escaping section needs beefed up</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="hossman">Hoss Man</reporter>
                        <labels>
                        <label>newdev</label>
                    </labels>
                <created>Sat, 15 Sep 2007 03:43:16 +0100</created>
                <updated>Fri, 10 May 2013 00:05:36 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>general/website</component>
                        <due/>
                    <votes>1</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12744900" author="michaelbusch" created="Wed, 19 Aug 2009 06:36:20 +0100">&lt;p&gt;Luis &amp;amp; Adriano: as the queryparser "experts" maybe you could create a patch for this issue and for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1122" title="queryparser whitespace escaping and documentation?"&gt;&lt;del&gt;LUCENE-1122&lt;/del&gt;&lt;/a&gt;? &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12385757">LUCENE-1122</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>0.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 19 Aug 2009 05:36:20 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2973</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26723</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-989] Statistics from ValueSourceQuery </title>
                <link>https://issues.apache.org/jira/browse/LUCENE-989</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Patch forthcoming that adds a DocValuesStats object that is optionally computer for a ValueSourceQuery.  This ~replaces the getMin/Max/Avg from the DocValues which were previously unaccessible without reasonably heavy subclassing.  in addition it add a few more stats and provides a single object to encapsulate all statistics going forward.  The stats object is tied to the ValueSourceQuery so that the values can be cached without having to maintain the full set of DocValues.  Test and javadocs included.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;will&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12376936">LUCENE-989</key>
            <summary>Statistics from ValueSourceQuery </summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="willjohnson3">Will Johnson</reporter>
                        <labels>
                    </labels>
                <created>Mon, 27 Aug 2007 16:35:28 +0100</created>
                <updated>Fri, 10 May 2013 00:05:37 +0100</updated>
                                    <version>2.2</version>
                                <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12523030" author="willjohnson3" created="Mon, 27 Aug 2007 16:36:52 +0100">&lt;p&gt;patch as described above.&lt;/p&gt;</comment>
                    <comment id="12628197" author="doronc" created="Thu, 4 Sep 2008 00:32:06 +0100">&lt;p&gt;This should be look at with &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1085" title="search.function should support all capabilities of Solr&amp;#39;s search.function"&gt;&lt;del&gt;LUCENE-1085&lt;/del&gt;&lt;/a&gt; - removing myself to not block others who can do it sooner.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12384266">LUCENE-1085</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12364614" name="functionStats.patch" size="9544" author="willjohnson3" created="Mon, 27 Aug 2007 16:36:52 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 3 Sep 2008 23:32:06 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12754</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26734</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>

<item>
            <title>[LUCENE-949] AnalyzingQueryParser can't work with leading wildcards.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-949</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards:&lt;/p&gt;

&lt;p&gt;	protected Query getWildcardQuery(String field, String termStr) throws ParseException&lt;br/&gt;
	{&lt;br/&gt;
		String useTermStr = termStr;&lt;br/&gt;
		String leadingWildcard = null;&lt;br/&gt;
		if ("*".equals(field))&lt;/p&gt;
		{
			if ("*".equals(useTermStr))
				return new MatchAllDocsQuery();
		}
&lt;p&gt;		boolean hasLeadingWildcard = (useTermStr.startsWith("*") || useTermStr.startsWith("?")) ? true : false;&lt;/p&gt;

&lt;p&gt;		if (!getAllowLeadingWildcard() &amp;amp;&amp;amp; hasLeadingWildcard)&lt;br/&gt;
			throw new ParseException("'*' or '?' not allowed as first character in WildcardQuery");&lt;/p&gt;

&lt;p&gt;		if (getLowercaseExpandedTerms())&lt;/p&gt;
		{
			useTermStr = useTermStr.toLowerCase();
		}

&lt;p&gt;		if (hasLeadingWildcard)&lt;/p&gt;
		{
			leadingWildcard = useTermStr.substring(0, 1);
			useTermStr = useTermStr.substring(1);
		}

&lt;p&gt;		List tlist = new ArrayList();&lt;br/&gt;
		List wlist = new ArrayList();&lt;br/&gt;
		/*&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;somewhat a hack: find/store wildcard chars in order to put them back&lt;/li&gt;
	&lt;li&gt;after analyzing&lt;br/&gt;
		 */&lt;br/&gt;
		boolean isWithinToken = (!useTermStr.startsWith("?") &amp;amp;&amp;amp; !useTermStr.startsWith("*"));&lt;br/&gt;
		isWithinToken = true;&lt;br/&gt;
		StringBuffer tmpBuffer = new StringBuffer();&lt;br/&gt;
		char[] chars = useTermStr.toCharArray();&lt;br/&gt;
		for (int i = 0; i &amp;lt; useTermStr.length(); i++)&lt;br/&gt;
		{&lt;br/&gt;
			if (chars&lt;span class="error"&gt;&amp;#91;i&amp;#93;&lt;/span&gt; == '?' || chars&lt;span class="error"&gt;&amp;#91;i&amp;#93;&lt;/span&gt; == '*')
			&lt;div class="error"&gt;&lt;span class="error"&gt;Unknown macro: {				if (isWithinToken)				{
					tlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}				isWithinToken = false;			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;			else&lt;/p&gt;
			&lt;div class="error"&gt;&lt;span class="error"&gt;Unknown macro: {				if (!isWithinToken)				{
					wlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}				isWithinToken = true;			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;			tmpBuffer.append(chars&lt;span class="error"&gt;&amp;#91;i&amp;#93;&lt;/span&gt;);&lt;br/&gt;
		}&lt;br/&gt;
		if (isWithinToken)&lt;/p&gt;
		{
			tlist.add(tmpBuffer.toString());
		}
&lt;p&gt;		else&lt;/p&gt;
		{
			wlist.add(tmpBuffer.toString());
		}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;		// get Analyzer from superclass and tokenize the term&lt;br/&gt;
		TokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr));&lt;br/&gt;
		org.apache.lucene.analysis.Token t;&lt;/p&gt;

&lt;p&gt;		int countTokens = 0;&lt;br/&gt;
		while (true)&lt;br/&gt;
		{&lt;br/&gt;
			try&lt;/p&gt;
			{
				t = source.next();
			}
&lt;p&gt;			catch (IOException e)&lt;/p&gt;
			{
				t = null;
			}
&lt;p&gt;			if (t == null)&lt;/p&gt;
			{
				break;
			}
&lt;p&gt;			if (!"".equals(t.termText()))&lt;br/&gt;
			{&lt;br/&gt;
				try&lt;/p&gt;
				{
					tlist.set(countTokens++, t.termText());
				}
&lt;p&gt;				catch (IndexOutOfBoundsException ioobe)&lt;/p&gt;
				{
					countTokens = -1;
				}
&lt;p&gt;			}&lt;br/&gt;
		}&lt;br/&gt;
		try&lt;/p&gt;
		{
			source.close();
		}
&lt;p&gt;		catch (IOException e)&lt;/p&gt;
		{
			// ignore
		}

&lt;p&gt;		if (countTokens != tlist.size())&lt;/p&gt;
		{
			/*
			 * this means that the analyzer used either added or consumed
			 * (common for a stemmer) tokens, and we can't build a WildcardQuery
			 */
			throw new ParseException("Cannot build WildcardQuery with analyzer " + getAnalyzer().getClass()
					+ " - tokens added or lost");
		}

&lt;p&gt;		if (tlist.size() == 0)&lt;/p&gt;
		{
			return null;
		}
&lt;p&gt;		else if (tlist.size() == 1)&lt;br/&gt;
		{&lt;br/&gt;
			if (wlist.size() == 1)&lt;br/&gt;
			{&lt;br/&gt;
				/*&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;if wlist contains one wildcard, it must be at the end,&lt;/li&gt;
	&lt;li&gt;because: 1) wildcards at 1st position of a term by&lt;/li&gt;
	&lt;li&gt;QueryParser where truncated 2) if wildcard was &lt;b&gt;not&lt;/b&gt; in end,&lt;/li&gt;
	&lt;li&gt;there would be &lt;b&gt;two&lt;/b&gt; or more tokens&lt;br/&gt;
				 */&lt;br/&gt;
				StringBuffer sb = new StringBuffer();&lt;br/&gt;
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}&lt;br/&gt;
				sb.append((String) tlist.get(0));&lt;br/&gt;
				sb.append(wlist.get(0).toString());&lt;br/&gt;
				return super.getWildcardQuery(field, sb.toString());&lt;br/&gt;
			}&lt;br/&gt;
			else if (wlist.size() == 0 &amp;amp;&amp;amp; hasLeadingWildcard)&lt;br/&gt;
			{&lt;br/&gt;
				/*&lt;br/&gt;
				 * if wlist contains no wildcard, it must be at 1st position&lt;br/&gt;
				 */&lt;br/&gt;
				StringBuffer sb = new StringBuffer();&lt;br/&gt;
				if (hasLeadingWildcard)&lt;br/&gt;
				{					// adding leadingWildcard					sb.append(leadingWildcard);				}
&lt;p&gt;				sb.append((String) tlist.get(0));&lt;br/&gt;
				sb.append(wlist.get(0).toString());&lt;br/&gt;
				return super.getWildcardQuery(field, sb.toString());&lt;br/&gt;
			}&lt;br/&gt;
			else&lt;/p&gt;
			{
				/*
				 * we should never get here! if so, this method was called with
				 * a termStr containing no wildcard ...
				 */
				throw new IllegalArgumentException("getWildcardQuery called without wildcard");
			}
&lt;p&gt;		}&lt;br/&gt;
		else&lt;br/&gt;
		{&lt;br/&gt;
			/*&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;the term was tokenized, let's rebuild to one token with wildcards&lt;/li&gt;
	&lt;li&gt;put back in postion&lt;br/&gt;
			 */&lt;br/&gt;
			StringBuffer sb = new StringBuffer();&lt;br/&gt;
			if (hasLeadingWildcard)
			{
				// adding leadingWildcard
				sb.append(leadingWildcard);
			}
&lt;p&gt;			for (int i = 0; i &amp;lt; tlist.size(); i++)&lt;/p&gt;
			&lt;div class="error"&gt;&lt;span class="error"&gt;Unknown macro: {				sb.append((String) tlist.get(i));				if (wlist != null &amp;amp;&amp;amp; wlist.size() &amp;gt; i)				{
					sb.append((String) wlist.get(i));
				}			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;			return super.getWildcardQuery(field, sb.toString());&lt;br/&gt;
		}&lt;br/&gt;
	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment/>
            <key id="12372906">LUCENE-949</key>
            <summary>AnalyzingQueryParser can't work with leading wildcards.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="steve_rowe">Steve Rowe</assignee>
                                <reporter username="stefan.klein">Stefan Klein</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 Jul 2007 13:41:35 +0100</created>
                <updated>Fri, 10 May 2013 23:57:23 +0100</updated>
                    <resolved>Tue, 7 May 2013 00:03:19 +0100</resolved>
                            <version>2.2</version>
                                <fixVersion>5.0</fixVersion>
                <fixVersion>4.4</fixVersion>
                                <component>core/queryparser</component>
                        <due/>
                    <votes>0</votes>
                        <watches>6</watches>
                                                    <comments>
                    <comment id="12986910" author="shaie" created="Wed, 26 Jan 2011 09:13:41 +0000">&lt;p&gt;Way outdated code and it's not really clear what needs to be done. If this is still a problem, I suggest reopening and post a proper patch.&lt;/p&gt;</comment>
                    <comment id="13108413" author="vidda" created="Tue, 20 Sep 2011 08:18:04 +0100">&lt;p&gt;Hi.&lt;/p&gt;

&lt;p&gt;Is there some way to re-open and fix this behavior/bug in AnalyzingQueryParser?&lt;br/&gt;
I have discover this opened (and closed 4 years later) bug. We are working with Lucene 3.2 and we use AnalyzingQueryParser because we need to parse with analyzer every query, even wildcard queries. &lt;/p&gt;

&lt;p&gt;This works great with most queries, and with the ones that don't work (for example in cases analyzer add/remove words and query have wildcards) we use QueryParser although it doesn't analyze wildcard queries.&lt;/p&gt;

&lt;p&gt;In our application there are some cases when we need to allow leading wildcard queries, and AnalyzingQueryParser fails although I set to true 'AllowLeadingWildcard' flag. Strings like '&lt;b&gt;ucene' is converted into WildcardQuery like this 'ucene&lt;/b&gt;'. This is another strange behavior, the ending wildcard.&lt;/p&gt;

&lt;p&gt;I know QueryParser doesn't have this leading wildcard bug, but I need to parse query (I am Spanish and we have special characters (ñ, ü, vocals with accent on them) and we parse indexed data, and to search we need to parse query too.&lt;/p&gt;



&lt;p&gt;Thanks in advance. Regards!&lt;/p&gt;
</comment>
                    <comment id="13108415" author="vidda" created="Tue, 20 Sep 2011 08:22:04 +0100">&lt;p&gt;Sorry, is my first post here and I didn't know special characters like &lt;/p&gt;
&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;*&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; is bold. For that mistake, in my previous comment there one bold line that I didn't want to be bold. This is what I want to put:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;'*ucene' is converted into WildcardQuery like this 'ucene*'&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13620032" author="dsmiley" created="Tue, 2 Apr 2013 18:36:16 +0100">&lt;p&gt;Reopening at the request of &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tallison%40mitre.org" class="user-hover" rel="tallison@mitre.org"&gt;Tim Allison&lt;/a&gt; who intends to attach a patch.  Regular users can't re-open JIRA issues, apparently.&lt;/p&gt;</comment>
                    <comment id="13620036" author="tallison@mitre.org" created="Tue, 2 Apr 2013 18:40:15 +0100">&lt;p&gt;This allows for leading wildcards in the AnalyzingQueryParser if setAllowLeadingWildcard is set to true.  &lt;/p&gt;</comment>
                    <comment id="13626606" author="rcmuir" created="Tue, 9 Apr 2013 14:33:00 +0100">&lt;p&gt;Hello Timothy, can you turn these changes into a patch?&lt;/p&gt;

&lt;p&gt;See &lt;a href="http://wiki.apache.org/lucene-java/HowToContribute#Creating_a_patch" class="external-link"&gt;http://wiki.apache.org/lucene-java/HowToContribute#Creating_a_patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                    <comment id="13629337" author="tallison@mitre.org" created="Thu, 11 Apr 2013 21:42:43 +0100">&lt;p&gt;First patch.  Let me know if this actually works.&lt;/p&gt;</comment>
                    <comment id="13630226" author="rcmuir" created="Fri, 12 Apr 2013 17:03:17 +0100">&lt;p&gt;Thank you Timothy. the patch looks very good to me, thanks also for adding tests!&lt;/p&gt;

&lt;p&gt;A few questions:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The regex simplification looks good to me, but I'm not a regex expert. Maybe someone that is better with regex like &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=steve_rowe" class="user-hover" rel="steve_rowe"&gt;Steve Rowe&lt;/a&gt; can have a look. If nobody objects after a few days I'm inclined to move forward though.&lt;/li&gt;
	&lt;li&gt;What about the case where someone has escaped wildcards? I'm not sure whats even happening today in this case... perhaps it already has surprising behavior and should really be a separate bug, or maybe its working and I just dont see it. I doubt its tested though... but it seems the regex would need to accomodate that?&lt;/li&gt;
	&lt;li&gt;The String.format() invocations should probably pass getLocale() from the superclass as the first argument, rather than depending on the default locale. Since they are locale sensitive I think its best to use the one that someone configured on the queryparser (e.g. via setLocale)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13630354" author="steve_rowe" created="Fri, 12 Apr 2013 18:15:52 +0100">&lt;p&gt;Hi Timothy, I agree with Robert, the patch reduces line count while improving clarity and adding functionality - sweet!&lt;/p&gt;

&lt;p&gt;A few nitpicks (+1 otherwise):&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;You don't handle escaped metachars ? and * in the query, though the original doesn't either, so this shouldn't stop the patch from being committed.&lt;/li&gt;
	&lt;li&gt;The &lt;tt&gt;(?s)&lt;/tt&gt; has no effect in &lt;tt&gt;"(?s)(&lt;span class="error"&gt;&amp;#91;^\\?\\*&amp;#93;&lt;/span&gt;+)"&lt;/tt&gt;, since it only affects the expansion of the dot metachar, which isn't present (see &lt;a href="http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html#DOTALL" class="external-link"&gt;http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html#DOTALL&lt;/a&gt;): an inverted charset will always include newlines unless the charset to be inverted explicitly includes them.  Of course, this is all moot, since the query parser will already have split on whitespace before sending &lt;tt&gt;termStr&lt;/tt&gt; to &lt;tt&gt;getWildcardQuery()&lt;/tt&gt; &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/li&gt;
	&lt;li&gt;You can drop the following line in &lt;tt&gt;normalizeMustBeSingleTerm()&lt;/tt&gt;: &lt;tt&gt;nonWildcardMatcher.reset(termStr);&lt;/tt&gt;, since nonWildcardMatcher is never reused.&lt;/li&gt;
	&lt;li&gt;The presence of term breaking chars is not the only condition under which analysis of a single term can produce multiple terms (e.g. synonyms, multiple readings), so the following exception message should be generalized: "There is a term breaking character between %s and %s".&lt;/li&gt;
	&lt;li&gt;In your new test, the following assertion message seems to have too many words? "Testing wildcard with wildcard with initial wildcard not allowed"&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13630370" author="steve_rowe" created="Fri, 12 Apr 2013 18:25:42 +0100">&lt;p&gt;One more minor thing: the javadoc on &lt;tt&gt;getWildcardQuery()&lt;/tt&gt; needs fixing - probably previously should have been "&lt;tt&gt;but not for &amp;lt;code&amp;gt;user*&amp;lt;/code&amp;gt;&lt;/tt&gt;" to illustrate not being called for prefix queries, but with your patch it's just plain wrong:&lt;/p&gt;

&lt;div class="preformatted panel" style="border-width: 1px;"&gt;&lt;div class="preformattedContent panelContent"&gt;
&lt;pre&gt;* Example: will be called for &amp;lt;code&amp;gt;H?user&amp;lt;/code&amp;gt; or for &amp;lt;code&amp;gt;H*user&amp;lt;/code&amp;gt; 
* but not for &amp;lt;code&amp;gt;*user&amp;lt;/code&amp;gt;.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="13635415" author="tallison@mitre.org" created="Thu, 18 Apr 2013 18:49:40 +0100">&lt;p&gt;Thank you very much for the feedback. I refactored a bit and added escaped wildcard handling.  Let me know how this looks.&lt;/p&gt;</comment>
                    <comment id="13635425" author="steve_rowe" created="Thu, 18 Apr 2013 18:54:29 +0100">&lt;p&gt;Tim, thanks, I'll take a look at your new patch later today.&lt;/p&gt;

&lt;p&gt;FYI, you shouldn't remove old patches - when you upload a file with the same name, the older versions still appear, but their names appear in grey, and you can see the date/time each was uploaded.  See e.g. &lt;a href="https://issues.apache.org/jira/browse/SOLR-3251" title="dynamically add fields to schema"&gt;&lt;del&gt;SOLR-3251&lt;/del&gt;&lt;/a&gt;, where I've uploaded the same-named patch multiple times.&lt;/p&gt;</comment>
                    <comment id="13642457" author="tallison@mitre.org" created="Fri, 26 Apr 2013 01:59:37 +0100">&lt;p&gt;Refactored a bit and added a few more tests.&lt;/p&gt;</comment>
                    <comment id="13649555" author="steve_rowe" created="Mon, 6 May 2013 08:11:39 +0100">&lt;p&gt;Hi &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tallison%40mitre.org" class="user-hover" rel="tallison@mitre.org"&gt;Tim Allison&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Sorry it took so long, I've attached a patch based on your patch with some fixes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Removed tabs.&lt;/li&gt;
	&lt;li&gt;Restored license header and class javadoc to &lt;tt&gt;AnalyzingQueryParser.java&lt;/tt&gt; (your patch removed them for some reason?).&lt;/li&gt;
	&lt;li&gt;Converted all code indentation to 2 spaces per level (you had a lot of 3 space per level indentation).&lt;/li&gt;
	&lt;li&gt;Converted the &lt;tt&gt;wildcardPattern&lt;/tt&gt; to allow anything to be escaped, not just backslashes and wildcard chars '?' and '*'.  Also removed the optional backslashes from group 2 (the actual wildcards) - when iterating over wildcardPattern matches, your patch would throw away any number of real wildcards following an escaped wildcard.  I added a test for this.&lt;/li&gt;
	&lt;li&gt;When multiple output tokens are produced (and there should only be one), now reporting all of them in the exception message instead of just the first two.&lt;/li&gt;
	&lt;li&gt;Removed all references to "chunklet" in favor of "output token" - this non-standard terminology made the code harder to read.&lt;/li&gt;
	&lt;li&gt;Changed descriptions of multiple output tokens to not necessarily be as the result of splitting (e.g. synonyms).&lt;/li&gt;
	&lt;li&gt;In &lt;tt&gt;analyzeSingleChunk()&lt;/tt&gt;, moved exception throwing to the source of problems.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I also added a &lt;tt&gt;CHANGES.txt&lt;/tt&gt; entry.  &lt;/p&gt;

&lt;p&gt;Tim, let me know if you think my changes are okay - if so, I think it's ready to commit.&lt;/p&gt;</comment>
                    <comment id="13649560" author="steve_rowe" created="Mon, 6 May 2013 08:20:08 +0100">&lt;p&gt;One other change I forgot to mention, Tim: I substituted MockAnalyzer where you used StandardAnalyzer in the test code - this allowed me to remove the analyzers-common dependency you introduced (and also the memory dependency, which didn't seem to be used for anything in your patch).&lt;/p&gt;</comment>
                    <comment id="13649670" author="tallison@mitre.org" created="Mon, 6 May 2013 12:40:14 +0100">&lt;p&gt;Steve, no problem on the delay.  Thank you for your help!  Changes sound great.  Thank you.&lt;/p&gt;
</comment>
                    <comment id="13650208" author="steve_rowe" created="Tue, 7 May 2013 00:03:19 +0100">&lt;p&gt;Committed to trunk and branch_4x.&lt;/p&gt;

&lt;p&gt;Thanks Tim!&lt;/p&gt;</comment>
                    <comment id="13654988" author="steve_rowe" created="Fri, 10 May 2013 23:57:23 +0100">&lt;p&gt;Although this is labelled as a Major Bug, it feels more like a new feature to me, so I'm not motivated to backport this to 4.3.1.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12581853" name="LUCENE-949.patch" size="24298" author="steve_rowe" created="Mon, 6 May 2013 08:11:39 +0100"/>
                    <attachment id="12580635" name="LUCENE-949.patch" size="33999" author="tallison@mitre.org" created="Fri, 26 Apr 2013 01:59:37 +0100"/>
                    <attachment id="12579378" name="LUCENE-949.patch" size="10109" author="tallison@mitre.org" created="Thu, 18 Apr 2013 18:48:50 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 26 Jan 2011 09:13:41 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12793</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26774</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>

<item>
            <title>[LUCENE-831] Complete overhaul of FieldCache API/Implementation</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-831</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Motivation:&lt;br/&gt;
1) Complete overhaul the API/implementation of "FieldCache" type things...&lt;br/&gt;
    a) eliminate global static map keyed on IndexReader (thus&lt;br/&gt;
        eliminating synch block between completley independent IndexReaders)&lt;br/&gt;
    b) allow more customization of cache management (ie: use &lt;br/&gt;
        expiration/replacement strategies, disk backed caches, etc)&lt;br/&gt;
    c) allow people to define custom cache data logic (ie: custom&lt;br/&gt;
        parsers, complex datatypes, etc... anything tied to a reader)&lt;br/&gt;
    d) allow people to inspect what's in a cache (list of CacheKeys) for&lt;br/&gt;
        an IndexReader so a new IndexReader can be likewise warmed. &lt;br/&gt;
    e) Lend support for smarter cache management if/when&lt;br/&gt;
        IndexReader.reopen is added (merging of cached data from subReaders).&lt;br/&gt;
2) Provide backwards compatibility to support existing FieldCache API with&lt;br/&gt;
    the new implementation, so there is no redundent caching as client code&lt;br/&gt;
    migrades to new API.&lt;/p&gt;
</description>
                <environment/>
            <key id="12364883">LUCENE-831</key>
            <summary>Complete overhaul of FieldCache API/Implementation</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png">Open</status>
                    <resolution id="-1">Unresolved</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="hossman">Hoss Man</reporter>
                        <labels>
                    </labels>
                <created>Wed, 14 Mar 2007 07:25:23 +0000</created>
                <updated>Fri, 10 May 2013 00:05:37 +0100</updated>
                                                    <fixVersion>4.4</fixVersion>
                                <component>core/search</component>
                        <due/>
                    <votes>6</votes>
                        <watches>17</watches>
                                                    <comments>
                    <comment id="12480676" author="hossman" created="Wed, 14 Mar 2007 07:27:38 +0000">
&lt;p&gt;This is based on an idea i had a few months ago and was recently reminded of because of several mail threads about FieldCache .. so i started fleshing it out on the plane last week.&lt;/p&gt;

&lt;p&gt;I'm not entirely happy with it in it's current state, but I wanted to post it and see what people think of the overall approach.&lt;/p&gt;

&lt;p&gt;if people like the direction this is going, I would definitely appreciate some help with API critique and good unit tests (so far i've been relying solely on the existing Unit Tests to validate that i'm not breaking anything &amp;#8211; but that doesn't really prove that the new APIs work the way they are intended to)&lt;/p&gt;


&lt;p&gt;TODO List&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;lots of little :TODO:s in code, mainly about javadocs&lt;/li&gt;
	&lt;li&gt;add merge methods to StringIndexCacheKey&lt;br/&gt;
   (a bit complicated, but should be possible/efficient)&lt;/li&gt;
	&lt;li&gt;figure out if there is any better way of dealing with&lt;br/&gt;
   SortComparatorCacheKey and the visibility issues of &lt;br/&gt;
   SortComparator.getComparable &lt;/li&gt;
	&lt;li&gt;change norm caching to use new caches (if not the same&lt;br/&gt;
   main cache, then at least the same classes in a private cache)&lt;/li&gt;
	&lt;li&gt;write an ass load more tests&lt;/li&gt;
	&lt;li&gt;is there a better way to deal with merging then to pass starts[] ?&lt;br/&gt;
   (pass a new datastructure encapsulating starts/subReaders?)&lt;/li&gt;
	&lt;li&gt;CacheFactory seemed like a good idea initially so that MultiReader&lt;br/&gt;
   on a multi-segment index could cascade down, but what if people &lt;br/&gt;
   only want caching at the outermost level (regardless of wether &lt;br/&gt;
   the key is mergable) ... factory can't assuming anything if reader &lt;br/&gt;
   is not an instance of MultiReader&lt;/li&gt;
	&lt;li&gt;audit/change all core code using FieldCache to use new API&lt;/li&gt;
	&lt;li&gt;performance test that this doesn't hurt things in some way.&lt;/li&gt;
&lt;/ul&gt;

</comment>
                    <comment id="12484518" author="otis" created="Tue, 27 Mar 2007 18:52:10 +0100">&lt;p&gt;I haven't looked at the patch yet.  However, I do know that a colleague of mine is about to start porting some FieldCache-based Filter stuff to Lucene's trunk.  Because that work may conflict with Hoss' changes here, we should see if this get applied first.&lt;/p&gt;</comment>
                    <comment id="12501086" author="hossman" created="Mon, 4 Jun 2007 04:39:31 +0100">&lt;p&gt;i haven't had any time to do further work on this issue ... partly because i haven't had a lot of time, but mainly because i'm hoping to get some feedback on the overall approach before any more serious effort investment.&lt;/p&gt;

&lt;p&gt;updated patch to work against the trunk (r544035)&lt;/p&gt;</comment>
                    <comment id="12509958" author="markrmiller@gmail.com" created="Tue, 3 Jul 2007 18:55:01 +0100">&lt;p&gt;I think this patch is great.  Not only does it make all of the sort caching logic much easier to decipher, being able to throw in a sophisticated cache manager like ehcache (which can let caches overflow to disk) could end up being pretty powerful. I am also very interested in the possibility of pre-warming a Reader.&lt;/p&gt;

&lt;p&gt;I will spend some time playing with what is here so far.&lt;/p&gt;</comment>
                    <comment id="12513777" author="hossman" created="Thu, 19 Jul 2007 01:56:30 +0100">&lt;p&gt;thanks for the feedback mark ... i honestly haven't looked at this patch since the last time i updated the issue ... (i'm not sure if i've even thought about it once since then).  it's the kind of things that seemed really cool important at the time, but then ... you know, other things come up.&lt;/p&gt;

&lt;p&gt;by all means, feel free to update it.&lt;/p&gt;

&lt;p&gt;as i recall, the biggest thing about this patch that was really just pie in the sky and may not make any sense is the whole concept of merging and letting subreaders of MultiReader do their own caching which could then percolate up.  I did it on the assumption that it would come in handy when reopening an IndexReader that contains several segments &amp;#8211; many of which may not have changed since the last time you opened the index.  but i really didn't have any idea how the whole reopening things would work.  i see now there is some reopen code in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-743" title="IndexReader.reopen()"&gt;&lt;del&gt;LUCENE-743&lt;/del&gt;&lt;/a&gt;, but frankly i'm still not sure wether the API makes sense, or is total overkill.&lt;/p&gt;

&lt;p&gt;it might be better to gut the merging logic from the patch and add it later if/when there becomes a more real use case for it (the existing mergeData and isMergable methods could always be re-added to the abstract base classes if it turns out they do make sense)&lt;/p&gt;</comment>
                    <comment id="12559008" author="thetaphi" created="Tue, 15 Jan 2008 10:14:38 +0000">&lt;p&gt;I did some extensive tests with Lucene 2.3 today and was wondering, that IndexReader.reopen() in combination with FieldCache/Sorting does not bring a performance increase.&lt;/p&gt;

&lt;p&gt;Until now, even if you reopen an Index using IndexReader.reopen(), you will get a new IndexReader instance which is not available in the default FieldCache. Because of that, all values from the cached fields must be reloaded. As reopen() only opens new/changed segments, a FieldCache implementation directly embedded into IndexReader as propsed by this issue would help here. Each segment would have its own FieldCache and reopening would be quite fast.&lt;/p&gt;

&lt;p&gt;As with Lucene 2.3 the reopen is possible, how about this issue? Would this be possible for 2.4?&lt;/p&gt;</comment>
                    <comment id="12559341" author="michaelbusch" created="Wed, 16 Jan 2008 03:15:38 +0000">&lt;blockquote&gt;
&lt;p&gt;As with Lucene 2.3 the reopen is possible, how about this issue? Would this be possible for 2.4?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, this is a limitation currently of reopen(). I'm planning to work on it after the 2.3 release is&lt;br/&gt;
out and &lt;a href="https://issues.apache.org/jira/browse/LUCENE-584" title="Decouple Filter from BitSet"&gt;&lt;del&gt;LUCENE-584&lt;/del&gt;&lt;/a&gt; is committed!&lt;/p&gt;</comment>
                    <comment id="12581189" author="markrmiller@gmail.com" created="Fri, 21 Mar 2008 21:08:51 +0000">&lt;p&gt;I spent a little time getting this patch somewhat updated to the trunk and running various benchmarks with reopen. As expected, the sequence of searching a large index with a sort, adding a few docs and then reopening a Reader to perform a sorted search, can be 10 of times faster.&lt;/p&gt;

&lt;p&gt;I think the new API is great too - I really like being able to experiment with other caching strategies. I find the code easier to follow as well.&lt;/p&gt;

&lt;p&gt;Did you have any ideas for merging a String index? That is something that still needs to be done...&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Mark&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12581291" author="markrmiller@gmail.com" created="Sat, 22 Mar 2008 21:13:53 +0000">&lt;p&gt;Patch roughly moves things forward.&lt;/p&gt;

&lt;p&gt;I've added stuff for Long, Short, and Byte parsing, changed getAuto to a static cachkey method, switched core Lucene from using the FieldCache API to the new API, added some javadoc, and roughly have things going with the reopen method.&lt;/p&gt;

&lt;p&gt;In short, still a lot to do, but this advances things enough so that you can apply it to trunk and check out the goodness that this can bring to sorting and reopen.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Mark&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12581307" author="hossman" created="Sat, 22 Mar 2008 23:35:59 +0000">&lt;p&gt;Mark:&lt;/p&gt;

&lt;p&gt;I haven't looked at this issue or any of the code in my patch since my last updated, nor have i had a chance to look at your updated patch, but a few comments...&lt;/p&gt;

&lt;p&gt;1) you rock.&lt;/p&gt;

&lt;p&gt;2) I have no idea what i had in mind for dealing with StringIndex when i said "a bit complicated, but should be possible/efficient".  I do distinctly remember thinking that we should add support for &lt;b&gt;just&lt;/b&gt; getting the string indexes (ie: the current int&lt;span class="error"&gt;&amp;#91;numDocs&amp;#93;&lt;/span&gt;) for when you don't care what the strings are, just the sort order or &lt;b&gt;just&lt;/b&gt; getting a String&lt;span class="error"&gt;&amp;#91;numDocs&amp;#93;&lt;/span&gt; when you aren't doing sorting and just want an "inverted inverted index" on the field ... but obviously we still need to support the curent StringIndex (it's used in MultiSearcher right?)&lt;/p&gt;</comment>
                    <comment id="12581375" author="markrmiller@gmail.com" created="Sun, 23 Mar 2008 12:47:24 +0000">&lt;p&gt;Right, I think its used in MultiSearcher and Parallel-MultSearcher and I believe its because you cannot compare simple ord arrays across Searchers and so you need the original sort term?&lt;/p&gt;

&lt;p&gt;I havn't been able to come up with anything that would be very efficient for merging a StringIndex, but I have not thought to much about it yet. Anyone out there have any ideas? Fastest way to merge two String[], each with an int[] indexing into the String[]? Is there a &lt;b&gt;really&lt;/b&gt; fast way?&lt;/p&gt;

&lt;p&gt;I agree that it would be nice to skip the String[] if a MultiSearcher was not being used.&lt;/p&gt;

&lt;p&gt;I'll keep playing with it&lt;/p&gt;</comment>
                    <comment id="12581758" author="yseeley@gmail.com" created="Tue, 25 Mar 2008 01:19:42 +0000">&lt;p&gt;&amp;gt; I agree that it would be nice to skip the String[] if a MultiSearcher was not being used.&lt;/p&gt;

&lt;p&gt;If you're going to incrementally update a FieldCache of a MultiReader, it's the same issue... can't merge the ordinals without the original (String) values.&lt;/p&gt;</comment>
                    <comment id="12582422" author="mikemccand" created="Wed, 26 Mar 2008 20:43:00 +0000">
&lt;p&gt;One question here: should we switch to a method call, instead of a&lt;br/&gt;
straight array, to retrieve a cached value for a doc?&lt;/p&gt;

&lt;p&gt;If we did that, then MultiSearchers would forward the request to the&lt;br/&gt;
right IndexReader.&lt;/p&gt;

&lt;p&gt;The benefit then is that reopen() of a reader would not have to&lt;br/&gt;
allocate &amp;amp; bulk copy massive arrays when updating the caches.  It&lt;br/&gt;
would keep the cost of reopen closer to the size of the new segments.&lt;br/&gt;
And this way the old reader &amp;amp; the new one would not double-allocate&lt;br/&gt;
the RAM required to hold the common parts of the cache.&lt;/p&gt;

&lt;p&gt;We could always still provide a "give me the full array" fallback if&lt;br/&gt;
people really wanted that (and were willing to accept the cost).&lt;/p&gt;
</comment>
                    <comment id="12582443" author="michaelbusch" created="Wed, 26 Mar 2008 21:55:45 +0000">&lt;blockquote&gt;
&lt;p&gt;The benefit then is that reopen() of a reader would not have to&lt;br/&gt;
allocate &amp;amp; bulk copy massive arrays when updating the caches. It&lt;br/&gt;
would keep the cost of reopen closer to the size of the new segments.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, Mike. Currently during reopen() the MultiSegmentReader &lt;br/&gt;
allocates a new norms array with size maxDoc(), which is, as you said,&lt;br/&gt;
inefficient if only some (maybe even small) segments changed.&lt;/p&gt;

&lt;p&gt;The method call might be a little slower than the array lookup, but&lt;br/&gt;
I doubt that this would be very significant. We can make this change for&lt;br/&gt;
the norms and run performance tests to measure the slowdown.&lt;/p&gt;</comment>
                    <comment id="12582471" author="markrmiller@gmail.com" created="Wed, 26 Mar 2008 23:03:25 +0000">&lt;p&gt;&amp;gt;If you're going to incrementally update a FieldCache of a MultiReader, it's the same issue... can't merge the ordinals without the original (String) &amp;gt;values.&lt;/p&gt;

&lt;p&gt;That is a great point.&lt;/p&gt;

&lt;p&gt;&amp;gt;should we switch to a method call, instead of a straight array, to retrieve a cached value for a doc?&lt;/p&gt;

&lt;p&gt;Sounds like a great idea to me. Solves the StringIndex merge and eliminates all merge costs at the price of a method call per access.&lt;/p&gt;</comment>
                    <comment id="12582480" author="markrmiller@gmail.com" created="Wed, 26 Mar 2008 23:44:57 +0000">&lt;p&gt;Hmm...how do we avoid having to pull the cached field values through a sync on every call? The field data has to be cached...and the method to return the single cached field value has to be multi-threaded...&lt;/p&gt;</comment>
                    <comment id="12582576" author="mikemccand" created="Thu, 27 Mar 2008 09:41:51 +0000">&lt;p&gt;I think if we can finally move to having read-only IndexReaders then they would not sync on this method?&lt;/p&gt;

&lt;p&gt;Also, we should still provide the "give me the full array as of right now" fallback which in a read/write usage would allow you to spend lots of RAM in order to not synchronize.  Of course you'd also have to update your array (or, periodically ask for a new one) if you are altering fields.&lt;/p&gt;</comment>
                    <comment id="12583230" author="markrmiller@gmail.com" created="Fri, 28 Mar 2008 23:13:19 +0000">&lt;p&gt;Here is a quick proof-of-concept type patch for using a method call rather than arrays. Speed pertaining to reopen.&lt;/p&gt;

&lt;p&gt;In my quick test of 'open 500000 tiny docs index, repeat(3): add couple docs/sort search' the total time taken was:&lt;/p&gt;

&lt;p&gt;Orig FieldCache impl: 27 seconds&lt;br/&gt;
New impl with arrays: 12 seconds&lt;br/&gt;
New impl with method call: 3 seconds&lt;/p&gt;

&lt;p&gt;Its kind of a worse case scenerio, but much faster is much faster&amp;lt;g&amp;gt; The bench does not push through the point where method 3 would have to reload all of the segments, so that would affect it some...but method one is reloading all of the segments every single time...&lt;/p&gt;

&lt;p&gt;This approach keeps the original approach for those that want to use the arrays. In that case everything still merges except for the StringIndex, so String sorting is slow. Lucene core is rigged to use the new method call though, so String sort is as sped up as the other field types when not using the arrays.&lt;/p&gt;

&lt;p&gt;Not sure everything is completely on the level yet, but all core tests pass (core sort tests can miss a lot).&lt;/p&gt;

&lt;p&gt;I lied about changing all core to use the new api...I havn't changed the function package yet.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Mark&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12583443" author="markrmiller@gmail.com" created="Sun, 30 Mar 2008 13:35:32 +0100">&lt;p&gt;Another push forward:&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Comparators are cached again (left it out before to think about).&lt;/li&gt;
	&lt;li&gt;Lots of new JavaDoc&lt;/li&gt;
	&lt;li&gt;Naming, usage refinements&lt;/li&gt;
	&lt;li&gt;Still doesn't touch the norms.&lt;/li&gt;
	&lt;li&gt;Cleanup, fixup, finishup, type stuff.&lt;/li&gt;
	&lt;li&gt;Deprecated some &lt;b&gt;function&lt;/b&gt; package methods that used FieldCache, but still need alternatives.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12583650" author="markrmiller@gmail.com" created="Mon, 31 Mar 2008 12:57:07 +0100">&lt;p&gt;-Further refinements and code changes.&lt;br/&gt;
-Fixed ord comparisons across IndexReaders - as yonik pointed out. Standard sort tests didn't catch and I missed even with the reminder.&lt;br/&gt;
-Added a bunch of CacheKey unit tests.&lt;/p&gt;</comment>
                    <comment id="12622052" author="michaelbusch" created="Wed, 13 Aug 2008 01:49:24 +0100">&lt;p&gt;I'm not sure how soon I'll have time to work on this; I don't want to block progress here, so I'm unassigning it for now in case someone else has time.&lt;/p&gt;</comment>
                    <comment id="12624217" author="markrmiller@gmail.com" created="Thu, 21 Aug 2008 02:22:42 +0100">&lt;p&gt;Brings this patch back up to trunk level.&lt;/p&gt;</comment>
                    <comment id="12624529" author="markrmiller@gmail.com" created="Fri, 22 Aug 2008 01:59:10 +0100">&lt;p&gt;Deprecating the function package is problematic. Doing it by method leaves the classes open to issues maintaining two possible states that could be mixed - the FieldCache and parsers are used as method params - you can't really replace the old internal representation with the new one. You really have to support the old method and new method and tell people not to use both or something. Doing it by class means duplicating half a dozen classes with new names. Neither is very satisfying. This stuff is all marked experimental and subject to change though - could it just be done clean sweep style? A little abrupt but...experimental is experimental &amp;lt;g&amp;gt;&lt;/p&gt;</comment>
                    <comment id="12624835" author="funtick" created="Fri, 22 Aug 2008 15:07:15 +0100">&lt;p&gt;Would be nice to have TermVectorCache (if term vectors are stored in the index)&lt;/p&gt;</comment>
                    <comment id="12624844" author="markrmiller@gmail.com" created="Fri, 22 Aug 2008 15:26:14 +0100">&lt;p&gt;That patch may have a goof , I'll peel off another soon. Unfortunately, it looks like this is slower than the orig implementation. I have to run more benchmarks, but it might even be in the 10-20% mark. My guess is that its the method calls - you may gain much faster reopen, but you appear to lose quite a bit on standard sort speed...&lt;/p&gt;

&lt;p&gt;Could give the choice of going with the arrays, if they prove as fast as orig, but then back to needing an efficient StringIndex merge...&lt;/p&gt;</comment>
                    <comment id="12625084" author="markrmiller@gmail.com" created="Sat, 23 Aug 2008 18:54:35 +0100">&lt;p&gt;I've got the function package happily deprecated with replacement methods.&lt;/p&gt;

&lt;p&gt;I am going to ballpark the sort speed with method calls to be about 10% slower, but there are a lot of variables and I am still benchmarking.&lt;/p&gt;

&lt;p&gt;I've added a check for a system property so that by default, sorting will still use primitive arrays, but if you want to pay the small sort cost you can turn on the method calls and get faster reopen without cache merging.&lt;/p&gt;

&lt;p&gt;So that wraps up everything I plan to do unless comments, criticisms bring up anything else.&lt;/p&gt;

&lt;p&gt;The one piece that is missing and should be addressed is an efficient merge for the StringIndex.&lt;/p&gt;

&lt;p&gt;Patch to follow.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;mark&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12625089" author="markrmiller@gmail.com" created="Sat, 23 Aug 2008 20:15:48 +0100">&lt;p&gt;Here is the patch - plenty to look over.&lt;/p&gt;

&lt;p&gt;The method access might not be as much slower as I thought - might be closer to a couple to 5% than the 10% I first guessed. &lt;/p&gt;</comment>
                    <comment id="12625098" author="markrmiller@gmail.com" created="Sat, 23 Aug 2008 20:30:44 +0100">&lt;blockquote&gt;&lt;p&gt;change norm caching to use new caches (if not the same&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;main cache, then at least the same classes in a private cache)&lt;/p&gt;


&lt;p&gt;What benefit do you see to this? Does it offer anything over the simple map used now?&lt;/p&gt;</comment>
                    <comment id="12625470" author="hossman" created="Mon, 25 Aug 2008 20:14:49 +0100">&lt;blockquote&gt;&lt;p&gt;What benefit do you see to this? Does it offer anything over the simple map used now?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I &lt;b&gt;think&lt;/b&gt; what i ment there was that if the norm caching used the same functionality then it could simplify the code, and norms would be optimized in the reopen case as well ... plus custom Cache Impls could report stats on norm usage (just like anything else) so people could  see when norms were getting used for fields they didn't expect them to be.&lt;/p&gt;

&lt;p&gt;But as i've said before ... most of my early comments were pie in the sky theoretical brainstorming comments &amp;#8211; don't read too much into them if they don't really make sense.&lt;/p&gt;</comment>
                    <comment id="12642459" author="markrmiller@gmail.com" created="Fri, 24 Oct 2008 14:24:55 +0100">&lt;p&gt;Updated to trunk.&lt;/p&gt;

&lt;p&gt;Took out an optimization last patch - was using ordinals rather than strings to sort when Reader was not Multi - didn't like the isMulti on IndexReader though, so this patch and the last don't have it.&lt;/p&gt;</comment>
                    <comment id="12644786" author="markrmiller@gmail.com" created="Mon, 3 Nov 2008 19:12:58 +0000">&lt;p&gt;This is missing a good way to manage the caching of the field values per cachekey (allowing for a more fine grained  weak or disk strategy for example). That type of stuff would have to be built into the cachkey currently - so to add a new strategy, youd have to implement a lot of new cachekeys and multiply your options to manage...for every int/long/byte/etc array and object array you would have to implement a new cachkey for a weakref solution. Is there an API that can overcome this?&lt;/p&gt;

&lt;p&gt;The current cache that you can easily override, cachkey to data, is pretty course...I am not sure how many helpful things you can do easily.&lt;/p&gt;</comment>
                    <comment id="12649180" author="alexvigdor" created="Wed, 19 Nov 2008 22:10:05 +0000">&lt;p&gt;Another useful feature that seems like it would be pretty easy to implement on top of this patch is cache warming; that is, the ability to reopen an IndexReader and repopulate its caches before making it "live".  The main thing missing is a Cache.getKeys() method which could be used to discover what caches are already in use, in order to repopulate them after reopening the IndexReader.  This cache warming could be performed externally to the IndexReader (by calling getCacheData for each of the keys after reopening), or perhaps the reopen method could be overloaded with a boolean "reloadCaches" to perform this in the same method call.&lt;/p&gt;

&lt;p&gt;The rationale for this I hope is clear; my application, like many I'm sure, keeps a single IndexReader for handling searches, and in a separate thread from search request handling commits writes and reopens the IndexReader before replacing the IndexReader reference for new searches (reference counting is then used to eventually close the old reader instances).  It would be ideal for that separate thread to also bear the expense of cache warming; even with this patch against our 4 GB indices, along with -Duse.object.array.sort=true,  a search request coming immediately after reopening the index will pause 20-40 seconds while the caches refill.  Preferably that could be done in the background and request handling threads would never be slowed down.&lt;/p&gt;
</comment>
                    <comment id="12649188" author="markrmiller@gmail.com" created="Wed, 19 Nov 2008 22:35:30 +0000">&lt;p&gt;Also - your reopen time will vary greatly depending on how many segments need to be reopened and what size they are...so I think a lower merge factor would help, while hurting overall search speed of course.&lt;/p&gt;</comment>
                    <comment id="12649191" author="markrmiller@gmail.com" created="Wed, 19 Nov 2008 22:36:45 +0000">&lt;p&gt;You've tried the patch? Awesome!&lt;/p&gt;

&lt;p&gt;How long did it take to fill the cache before the patch?&lt;/p&gt;

&lt;p&gt;I agree that we should be as friendly to warming as we can be. Keep in &lt;br/&gt;
mind that you can warm the reader yourself by issuing a sorted search &lt;br/&gt;
before putting the reader into duty - of course you don't get to warm &lt;br/&gt;
from RAM like with what you suggest.&lt;/p&gt;

&lt;p&gt;Keep that feedback coming. I've been building momentum on coming back to &lt;br/&gt;
this issue, unless a commiter beats me to it.&lt;/p&gt;

</comment>
                    <comment id="12649385" author="alexvigdor" created="Thu, 20 Nov 2008 14:56:03 +0000">&lt;p&gt;To be honest, the cache never successfully refilled before the patch - or at least I gave up after waiting 10 minutes.  I was about to give up on sorting.  It could have to do with the fact that we're running with a relatively modest amount of RAM (768M) given our index size. But with the patch at least sorting is a realistic option!&lt;/p&gt;

&lt;p&gt;I will look at adding the warming to my own code as you suggest; it is another peculiarity of this project that I can't know in the code what fields will be used for sorting, but I'll just track the searches coming through and aggregate any sorts they perform into a warming query.&lt;/p&gt;</comment>
                    <comment id="12649391" author="markrmiller@gmail.com" created="Thu, 20 Nov 2008 15:24:12 +0000">&lt;blockquote&gt;&lt;p&gt;i haven't had any time to do further work on this issue ... partly because i haven't had a lot of time, but mainly because i'm hoping to get some feedback on the overall approach before any more serious effort investment. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wheres that investment Hoss? You've orphaned your baby. There is a fairly decent amount of feedback here.&lt;/p&gt;</comment>
                    <comment id="12649393" author="markrmiller@gmail.com" created="Thu, 20 Nov 2008 15:32:31 +0000">&lt;p&gt;I think this would actually be better if all cachekey types had to implement both ObjectArray access as well as primitive Array access. Makes the code cleaner and cuts down on the cachekey explosion. Should have done it this way to start, but couldnt see the forest through the trees back then i suppose.&lt;/p&gt;</comment>
                    <comment id="12653546" author="markrmiller@gmail.com" created="Thu, 4 Dec 2008 23:38:23 +0000">&lt;p&gt;Updated to trunk.&lt;/p&gt;

&lt;p&gt;I've combined all of the dual (primitive array/ObjectArray) CachKeys into one. Each cache key can support both modes or throw UnsupportedException or something.&lt;/p&gt;

&lt;p&gt;I've also tried something a bit experimental to allow users to eventually use custom or alternate cachekeys (payload or sparse arrays or something) that work with internal sorting. A cache implementation can now supply a ComparatorFactory (name will prob be tweaked) that handles creating comparators. You can subclass ComparatorFactory and add new or override current supported CacheKeys.&lt;/p&gt;

&lt;p&gt;CustomComparators still needs to be twiddled with some.&lt;/p&gt;

&lt;p&gt;I've converted some of the sort tests to run with both primitive and object arrays as well.&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Mark&lt;br/&gt;
I&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12653560" author="markrmiller@gmail.com" created="Fri, 5 Dec 2008 00:12:27 +0000">&lt;p&gt;Couple of needed tweaks and a test for a custom ComparatorFactory.&lt;/p&gt;</comment>
                    <comment id="12653757" author="mikemccand" created="Fri, 5 Dec 2008 11:35:13 +0000">&lt;blockquote&gt;&lt;p&gt;change norm caching to use new caches (if not the same&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we could go even further, and &lt;span class="error"&gt;&amp;#91;eventually&amp;#93;&lt;/span&gt; change norms to use an iterator API, which'd also have the same benefit of not requiring costly materialization of a full byte[] array for every doc in the index (ie, reopen() cost would be in proportion to changed segments not total index size).&lt;/p&gt;

&lt;p&gt;Likewise field cache / stored fields / column stride fields could eventually open up an iterator API as well.  This API would be useful if eg in a custom HitCollector you wanted to look at a field's value in order to do custom filtering/scoring.&lt;/p&gt;</comment>
                    <comment id="12654055" author="mikemccand" created="Sat, 6 Dec 2008 11:51:40 +0000">
&lt;p&gt;[Note: my understanding of this area in general, and this patch in&lt;br/&gt;
particular, is still rather spotty... so please correct my&lt;br/&gt;
misconceptions in what follows...]&lt;/p&gt;

&lt;p&gt;This change is a great improvement, since the cache management would&lt;br/&gt;
be per-IndexReader, and more public so that you could see what's&lt;br/&gt;
cached, access the cache via the reader, swap in your own cache&lt;br/&gt;
management, etc.&lt;/p&gt;

&lt;p&gt;But I'm concerned, because this change continues the "materialize&lt;br/&gt;
massive array for entire index" approach, which is the major remaining&lt;br/&gt;
cost when (re)opening readers.  EG, isMergable()/mergeData() methods&lt;br/&gt;
build up the whole array from sub readers.&lt;/p&gt;

&lt;p&gt;What would it take to never require materializing the full array for&lt;br/&gt;
the index, for Lucene's internal purposes (external users may continue&lt;br/&gt;
to do so if they want)?  Ie, leave the array bound to the "leaf"&lt;br/&gt;
IndexReader (ie, SegmentReader).  It was briefly touched on here:&lt;/p&gt;

&lt;p&gt;  &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1458?focusedCommentId=12650964#action_12650964" class="external-link"&gt;https://issues.apache.org/jira/browse/LUCENE-1458?focusedCommentId=12650964#action_12650964&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I realize this is a big change, but I think we need to get there&lt;br/&gt;
eventually.&lt;/p&gt;

&lt;p&gt;EG I can see in this patch that MultiReader &amp;amp; MultiSegmentReader do&lt;br/&gt;
expose a CacheData that has get and get2 (why do we have get2?) that&lt;br/&gt;
delegate to child readers, which is good, but it's not good that they&lt;br/&gt;
return Object (requires casting for every lookup).  We don't have&lt;br/&gt;
per-atomic-type variants?  Couldn't we expose eg an IntData class (and&lt;br/&gt;
all other types) that has int get(docID) abstract method, that&lt;br/&gt;
delegate to child readers?  (I'm also generally confused by why we&lt;br/&gt;
have the per-atomic-type switching happening in CacheKey subclasses&lt;br/&gt;
and not CacheData.)&lt;/p&gt;

&lt;p&gt;Then... and probably the hardest thing to fix here: for all the&lt;br/&gt;
comparators we now materialize the full array.  I realize we use the&lt;br/&gt;
full array when sorting during a search of an&lt;br/&gt;
IndexSearcher(MultiReader(...)), because FieldSortedHitQueue is called&lt;br/&gt;
for every doc visited and must be able to quickly make its comparison.&lt;/p&gt;

&lt;p&gt;However, stepping back, this is poor approach.  We should instead be&lt;br/&gt;
doing what MultiSearcher does, which is gather top results&lt;br/&gt;
per-sub-reader, and then merge-sort the results.  At that point, to do&lt;br/&gt;
the merge, we only need actual field values for those docs in the top&lt;br/&gt;
N.&lt;/p&gt;

&lt;p&gt;If we could fix field-sorting like that (and I'm hazy on exactly how&lt;br/&gt;
to do so), I think Lucene internally would then never need the full&lt;br/&gt;
array?&lt;/p&gt;

&lt;p&gt;This change also adds USE_OA_SORT, which is scary to me because Object&lt;br/&gt;
overhead per doc can be exceptionally costly.  Why do we need to even&lt;br/&gt;
offer that?&lt;/p&gt;</comment>
                    <comment id="12654057" author="mikemccand" created="Sat, 6 Dec 2008 12:10:11 +0000">
&lt;p&gt;One more thing here... while random-access lookup of a field's value&lt;br/&gt;
via MultiReader dispatch requires the binary search to find the right&lt;br/&gt;
sub-reader, I think in most internal uses, the access could be&lt;br/&gt;
switched to an iterator instead, in which case the lookup should be&lt;br/&gt;
far faster.&lt;/p&gt;

&lt;p&gt;EG when sorting by field, we could pull say an IntData iterator from&lt;br/&gt;
the reader, and then access the int values in docID order as we visit&lt;br/&gt;
the docs.&lt;/p&gt;

&lt;p&gt;For norms, which we should eventually switch to FieldCache +&lt;br/&gt;
column-stride fields, it would be the same story.&lt;/p&gt;

&lt;p&gt;Accessing via iterator should go a long ways to reducing the overhead&lt;br/&gt;
of "using a method" instead of accessing the full array directly.&lt;/p&gt;</comment>
                    <comment id="12654064" author="markrmiller@gmail.com" created="Sat, 6 Dec 2008 13:13:51 +0000">&lt;p&gt;Ah, the dirty secret of 831 - there is plenty more to do &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I've been pushing it down the path, but I've expected radical changes to be needed before it goes in.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But I'm concerned, because this change continues the "materialize massive array for entire index" approach, which is the major remaining cost when (re)opening readers. EG, isMergable()/mergeData() methods build up the whole array from sub readers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Originally, 3.0 wasn't so close, so there was more concern with back compatibility than there might be now. I think the method call will be a slight slowdown no matter what as well...even with an iterator approach. Perhaps other "wins" will make up for it though. Its certainly cleaner to support 'one' mode.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What would it take to never require materializing the full array for the index, for Lucene's internal purposes (external users may continue to do so if they want)? Ie, leave the array bound to the "leaf" IndexReader (ie, SegmentReader). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm not sure I fully understand yet. If you use the ObjectArray mode, this is what happens right? Each sub array is bound to the IndexReader and MultiReader will distribute the requests to the right subreader. Only if you use the primitive arrays and merging do you get the full arrays (when not using USE_OA_SORT).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I realize this is a big change, but I think we need to get there eventually.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds good to me.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(why do we have get2?) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because a StringIndex needs to access both the array of Strings and a second array indexing into that. None of the other types need to access two arrays.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Couldn't we expose eg an IntData class (and all other types) that has int get(docID) abstract method, that delegate to child readers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I think this would be possible. If casting does indeed cost so much, this may bring things closer to the primitive array speed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I'm also generally confused by why we have the per-atomic-type switching happening in CacheKey subclasses and not CacheData.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;From Hoss' original design. What are your concerns here? The right key gets you the right data &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I've actually mulled this over some, buts its too early in the morning to remember I suppose. I'll look at it some more.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we could fix field-sorting like that (and I'm hazy on exactly how to do so), I think Lucene internally would then never need the full array?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That would be cool. Again, I'll try to explore in this direction. It doesn't need the full array when using the ObjectArray stuff now though (well, it kind of does, just split up over the readers).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This change also adds USE_OA_SORT, which is scary to me because Object overhead per doc can be exceptionally costly. Why do we need to even offer that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;All this does at the moment (and I hate system properties, but for the moment, thats whats working) is switch between using the primitive arrays and merging or using the distributed ObjectArray for internal sorting. It defaults to using the primitive arrays and merging because its 5-10% faster than using the ObjectArrays. The ObjectArray approach is just an ObjectArray backed by an array for each Reader - a MultiReader distributes a requests for a doc field to the right Readers ObjectArray.&lt;/p&gt;

&lt;p&gt;To your second comment...I'm gong to have to spend some more time &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;No worries though, this is very much a work in progress. I'd love to have it in by 3.0 though. Glad to see someone else taking more of an interest - very hard for me to find the time to dig into it all that often. I'll work with the code some as I can, thinking more about your comments, and perhaps I can come up with some better responses/ideas. &lt;/p&gt;
</comment>
                    <comment id="12654069" author="rnewson" created="Sat, 6 Dec 2008 13:50:09 +0000">&lt;p&gt;This enhancement is particularly interesting to me (and the application my team is building). I'm not sure how much time I can donate but since I'd likely have to enhance this area of Lucene for our app anyway and it would be better to have it in core, I'd like to help out where I can.&lt;/p&gt;

&lt;p&gt;The patch applies more or less cleanly against 2.4.0, but not trunk, btw. Is it possible to get the patch committed to a feature branch off of trunk perhaps?&lt;/p&gt;

&lt;p&gt;Finally, I'm most interested in the ability to make disk-backed caches. A very quick attempt to put the cache into JDBM failed as the CacheKey classes are not Comparable, which seems necessary for most kinds of disk lookup structures. SimpleMapCache uses a HashMap, which just needs equals/hashcode methods.&lt;/p&gt;

&lt;p&gt;The other benefit to this approach is that it allows the data structures needed for sorting to be reused for range filtering. My application needs both, though on numeric field types (dates predominantly).&lt;/p&gt;

&lt;p&gt;Finally, this might also be a good time to add first class support for non-String field types. The formatting that NumberTools supplies is incompatible with SortField (the former outputs in base 36, the latter parses with parseLong), so there's clearly been several approaches to the general problem. In my case, I wrote a new Document class with addLong(name, value), etc.&lt;/p&gt;</comment>
                    <comment id="12654101" author="markrmiller@gmail.com" created="Sat, 6 Dec 2008 14:10:53 +0000">&lt;p&gt;Hmmm - not sure what is up. There is already one small conflict for me (trunk is a rapidly changing target &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; ), but its a pretty simple conflict.&lt;/p&gt;

&lt;p&gt;There are revision number issues (I was connected to an older revision apparently). If thats the problem, try this patch (which also resolves the new simple conflict).&lt;/p&gt;</comment>
                    <comment id="12654105" author="thetaphi" created="Sat, 6 Dec 2008 14:16:50 +0000">&lt;p&gt;Maybe every asignee should tag his issues that are related to sorting to be related (or similar) to this one. I am thinking about the latest developments in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1478" title="Missing possibility to supply custom FieldParser when sorting search results"&gt;&lt;del&gt;LUCENE-1478&lt;/del&gt;&lt;/a&gt; and &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1481" title="Sort and SortField does not have equals() and hashCode()"&gt;&lt;del&gt;LUCENE-1481&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12654106" author="thetaphi" created="Sat, 6 Dec 2008 14:24:42 +0000">&lt;p&gt;Maybe we need two trunks or branches or whatever &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;. One for 2.9 and one for 3.0. This is a typical example for that in my opinion.&lt;/p&gt;</comment>
                    <comment id="12654109" author="rnewson" created="Sat, 6 Dec 2008 14:26:50 +0000">
&lt;p&gt;The conflict was easy to resolve, it was just an FYI, I appreciate trunk changes rapidly. I was just wondering if a feature branch would make synchronization easier.&lt;/p&gt;

&lt;p&gt;Some meta-task to combine or track these things would be great. I hit the problem that &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1478" title="Missing possibility to supply custom FieldParser when sorting search results"&gt;&lt;del&gt;LUCENE-1478&lt;/del&gt;&lt;/a&gt; describes. I'd previously indexed numeric values with NumberTools, which gives String-based sorting, the most memory-intensive one.&lt;/p&gt;

&lt;p&gt;It seems with this field cache approach and the recent FieldCacheRangeFilter on trunk, that Lucene has a robust and coherent answer to performing efficient sorting and range filtering for float, double, short, int and long values, perhaps it's time to enhance Document. That might cut down the size of the API, which in turn makes it easy to test and tune. Document could preclude tokenization for such fields, I suspect I'm not the only one to build a type-safe replacement to Document.&lt;/p&gt;

&lt;p&gt;For what it's worth, I'm currently indexed longs using String.format("%019d") and treating dates as longs (getTime()) coupled with a long[] version of FieldCacheRangeFilter. It achieves a similar goal to this task, the long[] used for sorting is the same as for range filtering. &lt;/p&gt;</comment>
                    <comment id="12654413" author="mikemccand" created="Mon, 8 Dec 2008 13:44:49 +0000">
&lt;blockquote&gt;&lt;p&gt;It seems with this field cache approach and the recent FieldCacheRangeFilter on trunk, that Lucene has a robust and coherent answer to performing efficient sorting and range filtering for float, double, short, int and long values, perhaps it's time to enhance Document. That might cut down the size of the API, which in turn makes it easy to test and tune. Document could preclude tokenization for such fields, I suspect I'm not the only one to build a type-safe replacement to Document.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is an interesting idea.  Say we create IntField, a subclass of&lt;br/&gt;
Field.  It could directly accept a single int value and not accept&lt;br/&gt;
tokenization options.  It could assert "not null", if the field wanted&lt;br/&gt;
that.  FieldInfo could store that it's an int and expose more stronly&lt;br/&gt;
typed APIs from IndexReader.document as well.  If in the future we&lt;br/&gt;
enable Term to be things-other-than-String, we could do the right&lt;br/&gt;
thing with typed fields.  Etc....&lt;/p&gt;</comment>
                    <comment id="12654417" author="thetaphi" created="Mon, 8 Dec 2008 14:03:11 +0000">&lt;blockquote&gt;&lt;p&gt;This is an interesting idea. Say we create IntField, a subclass of&lt;br/&gt;
Field. It could directly accept a single int value and not accept&lt;br/&gt;
tokenization options. It could assert "not null", if the field wanted&lt;br/&gt;
that. FieldInfo could store that it's an int and expose more stronly&lt;br/&gt;
typed APIs from IndexReader.document as well. If in the future we&lt;br/&gt;
enable Term to be things-other-than-String, we could do the right&lt;br/&gt;
thing with typed fields. Etc....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe this new Document class could also manage the encoding of these fields to the index format. With that it would be possible to extend Document, to automatically use my trie-based encoding for storing the raw term values. On the otrher hand RangeQuery would be aware of the field encoding (from field metadata) and can switch dynamically to the correct search/sort algorithm. Great!&lt;/p&gt;</comment>
                    <comment id="12654418" author="rnewson" created="Mon, 8 Dec 2008 14:03:14 +0000">
&lt;p&gt;Yes, something like that. I made a Document class with an add method for each primitive type which allowed only the sensible choices for Store and Index. Field subclasses would achieve the same thing. A subclass per primitive type might be excessive, they'd be 99% identical to each other. A NumericField that could hold a single short, int, long, float, double or Date might be enough (new NumericField(name, 99.99F, true), the final boolean toggling YES/NO for Store, since Index is always UNANALYZED_NO_NORMS).&lt;/p&gt;

&lt;p&gt;Adding this to FieldInfo would change the on-disk format such that it remembers that a particular field is of a special type?  That way all the places that Lucene currently has a multiplicity of classes or constants (SortField.INT, etc) could be eliminated, replaced by first class support in Document/Field.&lt;/p&gt;

&lt;p&gt;A remaining question would be whether field name is sufficient for uniqueness, I suggest it becomes fieldname+type. This also implies changes to the Query and Filter hierarchy. &lt;/p&gt;

&lt;p&gt;If it helps, I can post my Document class, which had helper methods for RangeFilter and TermQuery's for each type. It's not a complicated class, you can probably already picture it.&lt;/p&gt;</comment>
                    <comment id="12654421" author="rnewson" created="Mon, 8 Dec 2008 14:11:33 +0000">
&lt;p&gt;Type-safe Document-style object. Doesn't extend Document as it is final.&lt;/p&gt;
</comment>
                    <comment id="12654488" author="jasonrutherglen" created="Mon, 8 Dec 2008 18:07:43 +0000">&lt;p&gt;M. McCandless:&lt;/p&gt;

&lt;p&gt;"This is an interesting idea. Say we create IntField, a subclass of&lt;br/&gt;
Field. It could directly accept a single int value and not accept&lt;br/&gt;
tokenization options. It could assert "not null", if the field wanted&lt;br/&gt;
that. FieldInfo could store that it's an int and expose more stronly&lt;br/&gt;
typed APIs from IndexReader.document as well. If in the future we&lt;br/&gt;
enable Term to be things-other-than-String, we could do the right&lt;br/&gt;
thing with typed fields. Etc...."&lt;/p&gt;

&lt;p&gt;+1 For 3.0 this will be of great benefit in the effort to remove the excessive string creation&lt;br/&gt;
that happens right now with Lucene.  Term should also be more generic such that it&lt;br/&gt;
can also accept primitive or user defined types (and index format encodings).  &lt;/p&gt;</comment>
                    <comment id="12654820" author="mikemccand" created="Tue, 9 Dec 2008 15:15:51 +0000">&lt;p&gt;Marvin, does KS/Lucy have something like FieldCache?  If so, what API do you use?  Is it iterator-only?&lt;/p&gt;</comment>
                    <comment id="12655081" author="creamyg" created="Wed, 10 Dec 2008 03:10:23 +0000">&lt;p&gt;&amp;gt; Marvin, does KS/Lucy have something like FieldCache? If so, what API do you&lt;br/&gt;
&amp;gt; use? Is it iterator-only? &lt;/p&gt;

&lt;p&gt;At present, KS only caches the docID -&amp;gt; ord map as an array.  It builds that&lt;br/&gt;
array by iterating over the terms in the sort field's Lexicon and mapping the&lt;br/&gt;
docIDs from each term's posting list.&lt;/p&gt;

&lt;p&gt;Building the docID -&amp;gt; ord array is straightforward for a single-segment&lt;br/&gt;
SegLexicon.  The multi-segment case requires that several SegLexicons be&lt;br/&gt;
collated using a priority queue.  In KS, there's a MultiLexicon class which&lt;br/&gt;
handles this; I don't believe that Lucene has an analogous class.&lt;/p&gt;

&lt;p&gt;Relying on the docID -&amp;gt; ord array alone works quite well until you get to the&lt;br/&gt;
MultiSearcher case.  As you know, at that point you need to be able to&lt;br/&gt;
retrieve the actual field values from the ordinal numbers, so that you can&lt;br/&gt;
compare across multiple searchers (since the ordinal values are meaningless).&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;Lex_Seek_By_Num(lexicon, term_num);
field_val = Lex_Get_Term(lexicon);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is that seeking by ordinal value on a MultiLexicon iterator&lt;br/&gt;
requires a gnarly implementation and is very expensive.  I got it working, but&lt;br/&gt;
I consider it a dead-end design and a failed experiment.&lt;/p&gt;

&lt;p&gt;The planned replacement for these iterator-based quasi-FieldCaches involves&lt;br/&gt;
several topics of recent discussion:&lt;/p&gt;

&lt;p&gt;  1) A "keyword" field type, implemented using a format similar to what Nate &lt;br/&gt;
     and I came up with for the lexicon index.&lt;br/&gt;
  2) Write per-segment docID -&amp;gt; ord maps at index time for sort fields.&lt;br/&gt;
  3) Memory mapping.&lt;br/&gt;
  4) Segment-centric searching.&lt;/p&gt;

&lt;p&gt;We'd mmap the pre-composed docID -&amp;gt; ord map and use it for intra-segment&lt;br/&gt;
sorting.  The keyword field type would be implemented in such a way that we'd&lt;br/&gt;
be able to mmap a few files and get a per-segment field cache, which we'd then&lt;br/&gt;
use to sort hits from multiple segments.&lt;/p&gt;</comment>
                    <comment id="12655988" author="mikemccand" created="Fri, 12 Dec 2008 12:12:09 +0000">
&lt;blockquote&gt;
&lt;p&gt;&amp;gt; At present, KS only caches the docID -&amp;gt; ord map as an array. It builds that&lt;br/&gt;
&amp;gt; array by iterating over the terms in the sort field's Lexicon and mapping the&lt;br/&gt;
&amp;gt; docIDs from each term's posting list.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, that corresponds to the "order" array in Lucene's&lt;br/&gt;
FieldCache.StringIndex class.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;gt; Building the docID -&amp;gt; ord array is straightforward for a single-segment&lt;br/&gt;
&amp;gt; SegLexicon. The multi-segment case requires that several SegLexicons be&lt;br/&gt;
&amp;gt; collated using a priority queue. In KS, there's a MultiLexicon class which&lt;br/&gt;
&amp;gt; handles this; I don't believe that Lucene has an analogous class.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lucene achieves the same functionality by using a MultiReader to read&lt;br/&gt;
the terms in order (which uses MultiSegmentReader.MultiTermEnum, which&lt;br/&gt;
uses a pqueue under the hood) and building up StringIndex from that.&lt;br/&gt;
It's very costly.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;gt; Relying on the docID -&amp;gt; ord array alone works quite well until you get to the&lt;br/&gt;
&amp;gt; MultiSearcher case. As you know, at that point you need to be able to&lt;br/&gt;
&amp;gt; retrieve the actual field values from the ordinal numbers, so that you can&lt;br/&gt;
&amp;gt; compare across multiple searchers (since the ordinal values are meaningless).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, and we are trying to move towards pushing searcher down to the&lt;br/&gt;
segment.  Then we can use the per-segment ords for within-segment&lt;br/&gt;
collection, and then the real values for merging the separate pqueues&lt;br/&gt;
at the end (but, initial results from &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt; show that collecting&lt;br/&gt;
N queues then merging in the end adds ~20% slowdown for N = 100&lt;br/&gt;
segments).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;gt; Lex_Seek_By_Num(lexicon, term_num);&lt;br/&gt;
&amp;gt; field_val = Lex_Get_Term(lexicon);&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; The problem is that seeking by ordinal value on a MultiLexicon iterator&lt;br/&gt;
&amp;gt; requires a gnarly implementation and is very expensive. I got it working, but&lt;br/&gt;
&amp;gt; I consider it a dead-end design and a failed experiment.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;gt; The planned replacement for these iterator-based quasi-FieldCaches involves&lt;br/&gt;
&amp;gt; several topics of recent discussion:&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; 1) A "keyword" field type, implemented using a format similar to what Nate&lt;br/&gt;
&amp;gt; and I came up with for the lexicon index.&lt;br/&gt;
&amp;gt; 2) Write per-segment docID -&amp;gt; ord maps at index time for sort fields.&lt;br/&gt;
&amp;gt; 3) Memory mapping.&lt;br/&gt;
&amp;gt; 4) Segment-centric searching.&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; We'd mmap the pre-composed docID -&amp;gt; ord map and use it for intra-segment&lt;br/&gt;
&amp;gt; sorting. The keyword field type would be implemented in such a way that we'd&lt;br/&gt;
&amp;gt; be able to mmap a few files and get a per-segment field cache, which we'd then&lt;br/&gt;
&amp;gt; use to sort hits from multiple segments.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK so your "keyword" field type would expose random-access to field&lt;br/&gt;
values by docID, to be used to merge the N segments' pqueues into a&lt;br/&gt;
single final pqueue?&lt;/p&gt;

&lt;p&gt;The alternative is to use iterator but pull the values into your&lt;br/&gt;
pqueues when they are inserted.  The benefit is iterator-only&lt;br/&gt;
exposure, but the downside is likely higher net cost of insertion.&lt;br/&gt;
And if the "assumption" is these fields can generally be ram resident&lt;br/&gt;
(explicitly or via mmap), then the net benefit of iterator-only API is&lt;br/&gt;
not high.&lt;/p&gt;</comment>
                    <comment id="12656150" author="creamyg" created="Fri, 12 Dec 2008 20:13:14 +0000">&lt;p&gt;&amp;gt;&amp;gt; Building the docID -&amp;gt; ord array is straightforward for a single-segment&lt;br/&gt;
&amp;gt;&amp;gt; SegLexicon. The multi-segment case requires that several SegLexicons be&lt;br/&gt;
&amp;gt;&amp;gt; collated using a priority queue. In KS, there's a MultiLexicon class which&lt;br/&gt;
&amp;gt;&amp;gt; handles this; I don't believe that Lucene has an analogous class.&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Lucene achieves the same functionality by using a MultiReader to read&lt;br/&gt;
&amp;gt; the terms in order (which uses MultiSegmentReader.MultiTermEnum, which&lt;br/&gt;
&amp;gt; uses a pqueue under the hood) and building up StringIndex from that.&lt;br/&gt;
&amp;gt; It's very costly.&lt;/p&gt;

&lt;p&gt;Ah, you're right, that class is analogous.  The difference is that&lt;br/&gt;
MultiTermEnum doesn't implement seek(), let alone seekByNum().  I was pretty&lt;br/&gt;
sure you wouldn't have bothered, since by loading the actual term values into&lt;br/&gt;
an array you eliminate the need for seeking the iterator.&lt;/p&gt;

&lt;p&gt;&amp;gt; OK so your "keyword" field type would expose random-access to field&lt;br/&gt;
&amp;gt; values by docID, &lt;/p&gt;

&lt;p&gt;Yes.  There would be three files for each keyword field in a segment.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;docID -&amp;gt; ord map.  A stack of i32_t, one per doc.&lt;/li&gt;
	&lt;li&gt;Character data.  Each unique field value would be stored as uncompressed&lt;br/&gt;
    UTF-8, sorted lexically (by default).&lt;/li&gt;
	&lt;li&gt;Term offsets.  A stack of i64_t, one per term plus one, demarcating the&lt;br/&gt;
    term text boundaries in the character data file.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Assuming that we've mmap'd those files &amp;#8211; or slurped them &amp;#8211; here's the&lt;br/&gt;
function to find the keyword value associated with a doc num:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;void
KWField_Look_Up(KeyWordField *self, i32_t doc_num, ViewCharBuf *target)
{
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; (doc_num &amp;gt; self-&amp;gt;max_doc) {
        CONFESS(&lt;span class="code-quote"&gt;"Doc num out of range: %u32 %u32"&lt;/span&gt;, 
    }
    &lt;span class="code-keyword"&gt;else&lt;/span&gt; {
        i64_t offset      = self-&amp;gt;offsets[doc_num];
        i64_t next_offset = self-&amp;gt;offsets[doc_num + 1];
        i64_t len         = next_offset - offset;
        ViewCB_Assign_Str(target, self-&amp;gt;chardata + offset, len);
    }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I'm not sure whether IndexReader.fetchDoc() should retrieve the values for&lt;br/&gt;
keyword fields by default, but I lean towards yes.  The locality isn't ideal,&lt;br/&gt;
but I don't think it'll be bad enough to contemplate storing keyword values&lt;br/&gt;
redundantly alongside the other stored field values.&lt;/p&gt;

&lt;p&gt;&amp;gt; to be used to merge the N segments' pqueues into a&lt;br/&gt;
&amp;gt; single final pqueue?&lt;/p&gt;

&lt;p&gt;Yes, although I think you only need one two priority queues total: one&lt;br/&gt;
dedicated to iterating intra-segment, which gets emptied out after each&lt;br/&gt;
seg into the other, final queue.&lt;/p&gt;

&lt;p&gt;&amp;gt; The alternative is to use iterator but pull the values into your&lt;br/&gt;
&amp;gt; pqueues when they are inserted. The benefit is iterator-only&lt;br/&gt;
&amp;gt; exposure, but the downside is likely higher net cost of insertion.&lt;br/&gt;
&amp;gt; And if the "assumption" is these fields can generally be ram resident&lt;br/&gt;
&amp;gt; (explicitly or via mmap), then the net benefit of iterator-only API is&lt;br/&gt;
&amp;gt; not high.&lt;/p&gt;

&lt;p&gt;If I understand where you're going, you'd like to apply the design of the&lt;br/&gt;
deletions iterator to this problem?&lt;/p&gt;

&lt;p&gt;For that to work, we'd need to store values for each document, rather than&lt;br/&gt;
only unique values... right?  And they couldn't be stored in sorted order,&lt;br/&gt;
because we aren't pre-sorting the docs in the segment according to the value&lt;br/&gt;
of a keyword field &amp;#8211; which means string diffs don't help.  You'd have a&lt;br/&gt;
single file, with each doc's values encoded as a vbyte byte-count followed by&lt;br/&gt;
UTF-8 character data.&lt;/p&gt;</comment>
                    <comment id="12657151" author="rnewson" created="Tue, 16 Dec 2008 20:27:21 +0000">&lt;p&gt;I was wondering if the next version of the patch could include a sample disk-based cache? It seems that CacheKey classes are fine for an in-memory HashMap (since SimpleMapCache works just fine) but I wonder if equals/hashCode is sufficient when the data is on disk?&lt;/p&gt;</comment>
                    <comment id="12657401" author="jvolkman" created="Wed, 17 Dec 2008 14:28:55 +0000">&lt;p&gt;A couple things:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Looking at the getCachedData method for MultiReader and MultiSegmentReader, it doesn't appear that the CacheData objects from merge operations are cached.  Is there any reason for this?&lt;/li&gt;
	&lt;li&gt;I've written a merge method for StringIndexCacheKey. The process isn't all that complicated (apart from all of the off-by-ones), but it's expensive.&lt;/li&gt;
&lt;/ol&gt;


&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-object"&gt;boolean&lt;/span&gt; isMergable() {
    &lt;span class="code-keyword"&gt;return&lt;/span&gt; &lt;span class="code-keyword"&gt;true&lt;/span&gt;;
  }

  &lt;span class="code-keyword"&gt;private&lt;/span&gt; &lt;span class="code-keyword"&gt;static&lt;/span&gt; class OrderNode {
      &lt;span class="code-object"&gt;int&lt;/span&gt; index;
      OrderNode next;
  }
  
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; CacheData mergeData(&lt;span class="code-object"&gt;int&lt;/span&gt;[] starts, CacheData[] data) 
  &lt;span class="code-keyword"&gt;throws&lt;/span&gt; UnsupportedOperationException {
    &lt;span class="code-object"&gt;int&lt;/span&gt;[] mergedOrder = &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt;[starts[starts.length - 1]];
    &lt;span class="code-comment"&gt;// Lookup map is 1-based
&lt;/span&gt;    &lt;span class="code-object"&gt;String&lt;/span&gt;[] mergedLookup = &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;String&lt;/span&gt;[starts[starts.length - 1] + 1];
    
    &lt;span class="code-comment"&gt;// Unwrap cache payloads and flip order arrays
&lt;/span&gt;    StringIndex[] unwrapped = &lt;span class="code-keyword"&gt;new&lt;/span&gt; StringIndex[data.length];

    /* Flip the order arrays (reverse indices and values)
     * Since the ord map has a many-to-one relationship with the lookup table,
     * the flipped structure must be one-to-many which results in an array of
     * linked lists.
     */
    OrderNode[][] flippedOrders = &lt;span class="code-keyword"&gt;new&lt;/span&gt; OrderNode[data.length][];
    &lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; i = 0; i &amp;lt; data.length; i++) {
        StringIndex si = (StringIndex) data[i].getCachePayload();
        unwrapped[i] = si;
        flippedOrders[i] = &lt;span class="code-keyword"&gt;new&lt;/span&gt; OrderNode[si.lookup.length];
        &lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; j = 0; j &amp;lt; si.order.length; j++) {
            OrderNode a = &lt;span class="code-keyword"&gt;new&lt;/span&gt; OrderNode();
            a.index = j;
            a.next = flippedOrders[i][si.order[j]];
            flippedOrders[i][si.order[j]] = a;
        }
    }

    &lt;span class="code-comment"&gt;// Lookup map is 1-based
&lt;/span&gt;    &lt;span class="code-object"&gt;int&lt;/span&gt;[] lookupIndices = &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt;[unwrapped.length];
    Arrays.fill(lookupIndices, 1);

    &lt;span class="code-object"&gt;int&lt;/span&gt; lookupIndex = 0;
    &lt;span class="code-object"&gt;String&lt;/span&gt; currentVal;
    &lt;span class="code-object"&gt;int&lt;/span&gt; currentSeg;
    &lt;span class="code-keyword"&gt;while&lt;/span&gt; (&lt;span class="code-keyword"&gt;true&lt;/span&gt;) {
        currentVal = &lt;span class="code-keyword"&gt;null&lt;/span&gt;;
        currentSeg = -1;
        &lt;span class="code-object"&gt;int&lt;/span&gt; remaining = 0;
        &lt;span class="code-comment"&gt;// Find the next ordered value from all the segments
&lt;/span&gt;        &lt;span class="code-keyword"&gt;for&lt;/span&gt; (&lt;span class="code-object"&gt;int&lt;/span&gt; i = 0; i &amp;lt; unwrapped.length; i++) {
            &lt;span class="code-keyword"&gt;if&lt;/span&gt; (lookupIndices[i] &amp;lt; unwrapped[i].lookup.length) {
                remaining++;
                &lt;span class="code-object"&gt;String&lt;/span&gt; that = unwrapped[i].lookup[lookupIndices[i]];
                &lt;span class="code-keyword"&gt;if&lt;/span&gt; (currentVal == &lt;span class="code-keyword"&gt;null&lt;/span&gt; || currentVal.compareTo(that) &amp;gt; 0) {
                    currentVal = that;
                    currentSeg = i;
                }
            }
        }
        &lt;span class="code-keyword"&gt;if&lt;/span&gt; (remaining == 1) {
            &lt;span class="code-keyword"&gt;break&lt;/span&gt;;
        } &lt;span class="code-keyword"&gt;else&lt;/span&gt; &lt;span class="code-keyword"&gt;if&lt;/span&gt; (remaining == 0) {
            /* The only way &lt;span class="code-keyword"&gt;this&lt;/span&gt; could happen is &lt;span class="code-keyword"&gt;if&lt;/span&gt; there are 0 segments or &lt;span class="code-keyword"&gt;if&lt;/span&gt;
             * all segments have 0 terms. In either &lt;span class="code-keyword"&gt;case&lt;/span&gt;, we can &lt;span class="code-keyword"&gt;return&lt;/span&gt;
             * early.
             */
            &lt;span class="code-keyword"&gt;return&lt;/span&gt; &lt;span class="code-keyword"&gt;new&lt;/span&gt; CacheData(&lt;span class="code-keyword"&gt;new&lt;/span&gt; StringIndex(
                    &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt;[starts[starts.length - 1]], &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;String&lt;/span&gt;[1]));
        }
        &lt;span class="code-keyword"&gt;if&lt;/span&gt; (!currentVal.equals(mergedLookup[lookupIndex])) {
            lookupIndex++;
            mergedLookup[lookupIndex] = currentVal;
        }
        OrderNode a = flippedOrders[currentSeg][lookupIndices[currentSeg]];
        &lt;span class="code-keyword"&gt;while&lt;/span&gt; (a != &lt;span class="code-keyword"&gt;null&lt;/span&gt;) {
            mergedOrder[a.index + starts[currentSeg]] = lookupIndex;
            a = a.next;
        }
        lookupIndices[currentSeg]++;
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                    <comment id="12657404" author="markrmiller@gmail.com" created="Wed, 17 Dec 2008 14:40:41 +0000">&lt;p&gt;Thanks Jeremey! Thats great news. &lt;/p&gt;

&lt;p&gt;I think the issues is going to heavily affected by &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt;, so once thats done, I hope I'll be able to come right back to this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Looking at the getCachedData method for MultiReader and MultiSegmentReader, it doesn't appear that the CacheData objects from merge operations are cached. Is there any reason for this?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think its just not done yet - I havn't looked at the merging since I started playing with the ObjectArrays - I think most of this stuff becomes moot with 1483 though - this will turn more into an API overhaul than an IndexReader reopen time saver.&lt;/p&gt;</comment>
                    <comment id="12657409" author="mikemccand" created="Wed, 17 Dec 2008 14:47:11 +0000">&lt;blockquote&gt;
&lt;p&gt;&amp;gt;  this will turn more into an API overhaul than an IndexReader reopen time saver.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;...and given the progress on &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt; (copying values into the sort queues), I think this new FieldCache API should probably be primarily an iteration API.&lt;/p&gt;</comment>
                    <comment id="12674023" author="jvolkman" created="Mon, 16 Feb 2009 18:03:44 +0000">&lt;p&gt;Are there still things planned for this issue now that &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt; has been committed? &lt;/p&gt;</comment>
                    <comment id="12674255" author="mikemccand" created="Tue, 17 Feb 2009 16:09:37 +0000">&lt;blockquote&gt;&lt;p&gt;Are there still things planned for this issue now that &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt; has been committed? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good question... I think it'd still be nice to 1) have the IndexReader&lt;br/&gt;
expose the API for accessing the FieldCache values, 2) allow for&lt;br/&gt;
customization of the caching policy.&lt;/p&gt;

&lt;p&gt;Though maybe we should hold off on those changes until we do&lt;br/&gt;
&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1231" title="Column-stride fields (aka per-document Payloads)"&gt;&lt;del&gt;LUCENE-1231&lt;/del&gt;&lt;/a&gt; (column stride fields), which I think would use exactly&lt;br/&gt;
the same API with the only difference being whether under-the-hood&lt;br/&gt;
there was a more efficient (column-stride storage) representation for&lt;br/&gt;
the field values vs the slower uninvert &amp;amp; resort (for StringIndex)&lt;br/&gt;
approach that FieldCache does today.&lt;/p&gt;

&lt;p&gt;Also, in the new API I'd like to make it not-so-easy to materialize&lt;br/&gt;
the full array.  I think it's OK to ask for the full array of a&lt;br/&gt;
sub-reader, but if you want to access @ the MultiReader level, we&lt;br/&gt;
should encourage either random access getX(int docID), iteration or&lt;br/&gt;
get-sub-arrays and append yourself.&lt;/p&gt;</comment>
                    <comment id="12689620" author="michaelbusch" created="Thu, 26 Mar 2009 19:47:01 +0000">&lt;p&gt;Shall we try to gather all requirements we have for this feature here?&lt;br/&gt;
I think recently new requirements were mentioned and they are now scattered accross different issues and email threads.&lt;/p&gt;</comment>
                    <comment id="12689624" author="thetaphi" created="Thu, 26 Mar 2009 19:56:25 +0000">&lt;p&gt;I will attach my comments regarding the problem with the TrieRangeFilter and sorting (stop collecting terms into cache when lower precisions begin or only collect terms using a specific range (like a range filter). So you could fill a FieldCache and specify a starting term and ending term, all terms inbetween could be put into the cache, others outside left out. In this way, it would be possible to just use TrieUtils.prefixCodeLong() to specify the upper and lower integer bound encoded in the highest precision.&lt;/p&gt;</comment>
                    <comment id="12689629" author="tsmith" created="Thu, 26 Mar 2009 20:09:09 +0000">&lt;p&gt;One requirement i would like to request is the ability to attach an arbitrary object to each Segment.&lt;br/&gt;
This will allow people using lucene to store any arbitrary per segment caches and statistics that their application requires (fully free form)&lt;/p&gt;

&lt;p&gt;Would like to see the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;add SegmentReader.setCustomCacheManager(CacheManager m) // mabye add a string for a CacheManager id (to allow registration of multiple cache managers)&lt;/li&gt;
	&lt;li&gt;add SegmentReader.getCustomCacheManager() // to allow accessing the manager&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;CacheManager should be a very light interface (just a close() method that is called when the SegmentReader is closed)&lt;/p&gt;
</comment>
                    <comment id="12689863" author="mikemccand" created="Fri, 27 Mar 2009 09:45:54 +0000">&lt;p&gt;I'd like to see the new FieldCache API de-emphasize "get me a single array holding all values for all docs in the index" for a MultiReader. That invocation is exceptionally costly in the context of reopened readers, and providing the illusion that one can simply get this array is dangerous. It's a "leaky API", like how virtual memory API pretends you can use more memory than is physically available. &lt;/p&gt;

&lt;p&gt;I think it's OK to return an array-of-arrays (ie, one contiguous array per underlying segment); if the app really wants to make a massive array &amp;amp; concatenate it, they can do so outside of the FieldCache API. &lt;/p&gt;

&lt;p&gt;Or an method you call to get a given doc's value (like DocValues in o.a.l.search.function). Or an iterator API to step through all the values. &lt;/p&gt;

&lt;p&gt;We should also set this API up as much as possible for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1231" title="Column-stride fields (aka per-document Payloads)"&gt;&lt;del&gt;LUCENE-1231&lt;/del&gt;&lt;/a&gt;. Ie, the current "un-invert the field" approach that FieldCache takes is merely one source of values per doc. Column stride fields in the future will be a different (faster) source of values, that should be able to "just plug in" under the hood somehow to this same exposure API. &lt;/p&gt;

&lt;p&gt;On Uwe's suggestion for some flexibility on how the un-inversion takes place, I think allowing differing degrees of extension makes sense. EG we already allow you to provide a custom parser. We need to allow control on whether a given value replaces the already-seen value (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1372" title="Proposal: introduce more sensible sorting when a doc has multiple values for a term"&gt;&lt;del&gt;LUCENE-1372&lt;/del&gt;&lt;/a&gt;), or whether to stop the looping early (Uwe's needs for improving Trie). We should also allow outright entire custom class that creates the value array.&lt;/p&gt;</comment>
                    <comment id="12689867" author="earwin" created="Fri, 27 Mar 2009 10:25:27 +0000">&lt;p&gt;Adding to Tim, I'd like to see the ability not only to be notified of SegmentReader destruction, but of SegmentReader creation (within reopen) too. And new FieldCache logic should be built on these notifications.&lt;br/&gt;
Then it's possible to extend/replace Lucene's native FieldCache, then it's possible to create a cache specialized for trie-fields.&lt;/p&gt;

&lt;p&gt;Linking objects to each other with WeakHashmaps is insanely evil, especially in the case when object creation/destruction is clearly visible.&lt;/p&gt;</comment>
                    <comment id="12693464" author="mikemccand" created="Sat, 28 Mar 2009 16:05:42 +0000">&lt;p&gt;Let's make sure the new API fixes &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1579" title="Cloned SegmentReaders fail to share FieldCache entries"&gt;&lt;del&gt;LUCENE-1579&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12697485" author="markrmiller@gmail.com" created="Thu, 9 Apr 2009 13:02:07 +0100">&lt;blockquote&gt;
&lt;p&gt;I'd like to see the new FieldCache API de-emphasize "get me a single array holding all values for all docs in the index" for a MultiReader. That invocation is exceptionally costly in the context of reopened readers, and providing the illusion that one can simply get this array is dangerous. It's a "leaky API", like how virtual memory API pretends you can use more memory than is physically available.&lt;/p&gt;

&lt;p&gt;I think it's OK to return an array-of-arrays (ie, one contiguous array per underlying segment); if the app really wants to make a massive array &amp;amp; concatenate it, they can do so outside of the FieldCache API. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is there much difference in one massive array or an array of arrays? Its just as much space and just as dangerous, right? Some apps will need random access to the field cache for any given document right? Don't we always have to support that in some way, and won't it always be severely limited by RAM (until IO is as fast)?&lt;/p&gt;

&lt;p&gt;I like the idea of an iterator API, but it seems we will still have to provide random access with all its problems, right?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We should also set this API up as much as possible for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1231" title="Column-stride fields (aka per-document Payloads)"&gt;&lt;del&gt;LUCENE-1231&lt;/del&gt;&lt;/a&gt;. Ie, the current "un-invert the field" approach that FieldCache takes is merely one source of values per doc. Column stride fields in the future will be a different (faster) source of values, that should be able to "just plug in" under the hood somehow to this same exposure API.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Definitely.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On Uwe's suggestion for some flexibility on how the un-inversion takes place, I think allowing differing degrees of extension makes sense. EG we already allow you to provide a custom parser. We need to allow control on whether a given value replaces the already-seen value (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-1372" title="Proposal: introduce more sensible sorting when a doc has multiple values for a term"&gt;&lt;del&gt;LUCENE-1372&lt;/del&gt;&lt;/a&gt;), or whether to stop the looping early (Uwe's needs for improving Trie). We should also allow outright entire custom class that creates the value array.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Allow a custom field cache loader for each type?&lt;/p&gt;</comment>
                    <comment id="12697492" author="mikemccand" created="Thu, 9 Apr 2009 13:18:56 +0100">&lt;blockquote&gt;&lt;p&gt;Is there much difference in one massive array or an array of arrays? Its just as much space and just as dangerous, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;One massive array is far more dangerous during reopen() (ie that's why we did &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt;) since it's non-incremental.&lt;/p&gt;

&lt;p&gt;Array-per-segment I think is OK.&lt;/p&gt;

&lt;p&gt;But yes both of them consume RAM, but I don't consider that "dangerous".&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I like the idea of an iterator API, but it seems we will still have to provide random access with all its problems, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Some apps will need random access to the field cache for any given document right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes but I think such apps should move to the per-segment model (eg a Filter's getDocIdSet is called per segment reader).&lt;/p&gt;

&lt;p&gt;If an app really wants to make a single massive array, they can certainly do so, outside of Lucene.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Allow a custom field cache loader for each type?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, possibly w/ different degrees of extension (much like Collector).  EG maybe you just want to override how you parse an int, or maybe you want to take control over the entire uninversion.&lt;/p&gt;</comment>
                    <comment id="12697495" author="markrmiller@gmail.com" created="Thu, 9 Apr 2009 13:38:41 +0100">&lt;blockquote&gt;
&lt;p&gt;One massive array is far more dangerous during reopen() (ie that's why we did &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt;) since it's non-incremental.&lt;/p&gt;

&lt;p&gt;Array-per-segment I think is OK.&lt;/p&gt;

&lt;p&gt;But yes both of them consume RAM, but I don't consider that "dangerous".&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Okay, I got you now. We will force everyone to migrate to use fieldcache at the segment level rather than MR (or create their own array from the subarrays).&lt;/p&gt;</comment>
                    <comment id="12697519" author="markrmiller@gmail.com" created="Thu, 9 Apr 2009 15:21:47 +0100">&lt;blockquote&gt;&lt;p&gt;But yes both of them consume RAM, but I don't consider that "dangerous".&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess you meant dangerous as in dangerous to reopen then? I actually thought you meant as in dangerous because it could require too may resources. Dangerous is a tough to pin down word &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;So what are the advantages of the iterator API again then? It not likely you are going to stream the values, and random access will likely still have a use as mentioned.&lt;/p&gt;

&lt;p&gt;Just trying to get a clearer picture in my head - I doubt I'll have time, but I'd love to put a little sweat into this issue.&lt;/p&gt;

&lt;p&gt;It probably makes sense to start from one of Hoss's original patches or even from scratch.&lt;/p&gt;</comment>
                    <comment id="12697540" author="mikemccand" created="Thu, 9 Apr 2009 16:42:14 +0100">&lt;blockquote&gt;
&lt;p&gt;I guess you meant dangerous as in dangerous to reopen then? I actually thought you meant as in dangerous because it could require too may resources. Dangerous is a tough to pin down word &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Dangerous is a dangerous word &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I meant: I don't like exposing non-performant APIs; they are sneaky traps.  (EG TermEnum.skipTo is another such API).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So what are the advantages of the iterator API again then?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The big advantage is the possibility of backing it with eg an IndexInput, so that the values need to all be in RAM for one segment.  Though, as Lucy is doing, we could leave things on disk and still have random access via mmap, which perhaps should be an option for Lucene as well.  However, iterator only messes up out-of-order scoring (BooleanScorer for OR queries), so I'm tentatively leaning against iterator only at this point.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but I'd love to put a little sweat into this issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That would be AWESOME (if you can somehow make time)!&lt;/p&gt;

&lt;p&gt;We should hash out the design a bit before figuring out how/where to start.&lt;/p&gt;</comment>
                    <comment id="12697812" author="markrmiller@gmail.com" created="Fri, 10 Apr 2009 15:09:12 +0100">&lt;p&gt;Some random thoughts:&lt;/p&gt;

&lt;p&gt;If we are going to allow random access, I like the idea of sticking with the arrays. They are faster than hiding behind a method, and it allows easier movement from the old API. It would be nice if we can still deprecate all of that by backing it with the new impl (as done with the old patch).&lt;/p&gt;

&lt;p&gt;The current API (from this patch) still looks fairly good to me - a given cachekey gets your data, and knows how to construct it. You get data something like: return (byte[]) reader.getCachedData(new ByteCacheKey(field, parser)). It could be improved, but it seems a good start to me.&lt;/p&gt;

&lt;p&gt;The immediate problem I see is how to handle multireader vs reader. Not being able to treat them the same is a real pain. In the segment case, you just want an array back, in the multi-segment perhaps an array of arrays? Or unsupported? I havn't thought of anything nice.&lt;/p&gt;

&lt;p&gt;We have always been able to customize a lot of behavior with our custom sort types - I guess the real issue is making the built in sort types customizable. So I guess we need someway to say, use this "cachekey" for this built in type?&lt;/p&gt;

&lt;p&gt;When we load the new caches in FieldComparator, can we count on those being segmentreaders? We can Lucene wise, but not API wise right? Does that matter? I suppose its really tied in with the multireader vs reader API.&lt;/p&gt;</comment>
                    <comment id="12697820" author="mikemccand" created="Fri, 10 Apr 2009 15:40:50 +0100">&lt;blockquote&gt;
&lt;p&gt;If we are going to allow random access, I like the idea of sticking&lt;br/&gt;
with the arrays. They are faster than hiding behind a method, and it&lt;br/&gt;
allows easier movement from the old API.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It would be nice if we can&lt;br/&gt;
still deprecate all of that by backing it with the new impl (as done&lt;br/&gt;
with the old patch).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That seems fine?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The current API (from this patch) still looks fairly good to me - a given cachekey gets your data, and knows how to construct it. You get data something like: return (byte[]) reader.getCachedData(new ByteCacheKey(field, parser)). It could be improved, but it seems a good start to me.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The immediate problem I see is how to handle multireader vs reader. Not being able to treat them the same is a real pain. In the segment case, you just want an array back, in the multi-segment perhaps an array of arrays? Or unsupported? I havn't thought of anything nice.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would lean towards throwing UOE, and suggesting that you call&lt;br/&gt;
getSequentialReaders instead.&lt;/p&gt;

&lt;p&gt;Eg with the new getUniqueTermCount() we do that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We have always been able to customize a lot of behavior with our custom sort types - I guess the real issue is making the built in sort types customizable. So I guess we need someway to say, use this "cachekey" for this built in type?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't quite follow that last sentence.&lt;/p&gt;

&lt;p&gt;We'll have alot of customizability here, ie, if you want to change how&lt;br/&gt;
String is parsed to int, if you want to fully override how uninversion&lt;br/&gt;
works, etc.  At first the core will only support uninversion as a&lt;br/&gt;
source of values, but once CSF is online that should be an alternate&lt;br/&gt;
pluggable source, presumably plugging in the same way that&lt;br/&gt;
customization would allow you to override uninversion.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When we load the new caches in FieldComparator, can we count on those being segmentreaders? We can Lucene wise, but not API wise right? Does that matter? I suppose its really tied in with the multireader vs reader API.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Once getSequentialSubReaders() is called (and, recursively if needed),&lt;br/&gt;
then those "atomic" readers should be able to provide values.  I guess&lt;br/&gt;
that's the contract we require of a given IndexReader impl?&lt;/p&gt;</comment>
                    <comment id="12697822" author="markrmiller@gmail.com" created="Fri, 10 Apr 2009 15:49:55 +0100">&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We have always been able to customize a lot of behavior with our custom sort types - I guess the real issue is making the built in sort types customizable. So I guess we need someway to say, use this "cachekey" for this built in type?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't quite follow that last sentence.&lt;/p&gt;

&lt;p&gt;We'll have alot of customizability here, ie, if you want to change how&lt;br/&gt;
String is parsed to int, if you want to fully override how uninversion&lt;br/&gt;
works, etc.  At first the core will only support uninversion as a&lt;br/&gt;
source of values, but once CSF is online that should be an alternate&lt;br/&gt;
pluggable source, presumably plugging in the same way that&lt;br/&gt;
customization would allow you to override uninversion.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right - since a custom cachekey builds the array from a reader, you can pretty much do anything. What I meant was that you could do anything before with a custom sort type as well - the problem was that you could not say use this custom sort type when sorting on a built in type (eg INT, BYTE, STRING). So thats all we need, right? A way to say, use this builder (cachekey) for LONG, use this one for INT, etc. When we get CSF, you would set it to use cachekeys that built arrays from that data.&lt;/p&gt;</comment>
                    <comment id="12697830" author="mikemccand" created="Fri, 10 Apr 2009 16:01:31 +0100">&lt;blockquote&gt;&lt;p&gt;A way to say, use this builder (cachekey) for LONG, use this one for INT, etc. When we get CSF, you would set it to use cachekeys that built arrays from that data.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That sounds right, though it'd presumably be field dependent rather than relying on only the native type?  Ie I may have 3 fields that should load long[]'s, but each has its own custom decoding to be done.&lt;/p&gt;</comment>
                    <comment id="12697832" author="markrmiller@gmail.com" created="Fri, 10 Apr 2009 16:06:19 +0100">&lt;p&gt;Yes, good point. Okay, I think I have a much clearer picture of what needs to be done - this may be less work than I thought - a lot of what has been done is probably still helpful.&lt;/p&gt;</comment>
                    <comment id="12698048" author="markrmiller@gmail.com" created="Sat, 11 Apr 2009 03:19:31 +0100">&lt;p&gt;Here is fairly decent base to start from.&lt;/p&gt;

&lt;p&gt;Still needs a lot, but a surprising amount of tests pass. Essentially stripped out what isnt needed due to 1483, updated to trunk (new 1483 API), and added a bit of the new stuff we need.&lt;/p&gt;

&lt;p&gt;Some of the work to do:&lt;/p&gt;

&lt;p&gt;I think cache needs a clone for indexreader cloning.&lt;/p&gt;

&lt;p&gt;Need to come up with a good way to specify type to cachekey configuration - I threw some paint on the wall. Once we go filed/type - cachekey, almost seems we are getting into schema territory...&lt;/p&gt;

&lt;p&gt;Have to deal with new Parser interface in FieldCache (back compat)&lt;/p&gt;

&lt;p&gt;We may need the multireader to do the full array for back compat.&lt;/p&gt;

&lt;p&gt;I dont think any of the custom type stuff is setup to work yet.&lt;/p&gt;

&lt;p&gt;Iterate iterate iterate I suppose.&lt;/p&gt;</comment>
                    <comment id="12698089" author="mikemccand" created="Sat, 11 Apr 2009 13:57:40 +0100">&lt;blockquote&gt;&lt;p&gt;Iterate iterate iterate I suppose.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Here here!  Ready, set, GO!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We may need the multireader to do the full array for back compat.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can't we just create the "make massive array &amp;amp; copy sub arrays in"&lt;br/&gt;
inside the old FieldCache?  (And deprecate the old FieldCache&lt;br/&gt;
entirely).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I dont think any of the custom type stuff is setup to work yet.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How about we create a ValueSource abstract base class, that defines&lt;br/&gt;
abstract byte[] getBytes(IndexReader r, String field),&lt;br/&gt;
int[] getInts(IndexReader r, String field), etc.  (Just like&lt;br/&gt;
ExtendedFieldCache).&lt;/p&gt;

&lt;p&gt;This is subclassed to things like UninversionValueSource (what&lt;br/&gt;
FieldCache does today), CSFValueSource (in the future) both of which&lt;br/&gt;
take an IndexReader when created.&lt;/p&gt;

&lt;p&gt;UninversionValueSource should provide basic ways to customize the&lt;br/&gt;
uninversion.  Hopefully, we can share mode code than the current&lt;br/&gt;
FieldCacheImpl does (eg, a single "enum terms &amp;amp; terms docs" loop that&lt;br/&gt;
switches out to a "handler" to deal with each term &amp;amp; doc, w/&lt;br/&gt;
subclasses that handle to byte, int, etc.).&lt;/p&gt;

&lt;p&gt;And then I can also make MyFunkyValueSource (for extensibility) that&lt;br/&gt;
does whatever to produce the values.&lt;/p&gt;

&lt;p&gt;Then we make CachingValueSource, that wraps any other ValueSource.&lt;/p&gt;

&lt;p&gt;And finally expose a way in IndexReader to set its ValueSource when&lt;br/&gt;
you open it?  It would default to&lt;br/&gt;
CachedValueSource(UninversionValueSource()).  I think we should&lt;br/&gt;
require that you set this on opening the reader, and you can't later&lt;br/&gt;
change it.&lt;/p&gt;

&lt;p&gt;This would mean a single CachingValueSource can be used for more than&lt;br/&gt;
one reader, which is good because IndexReader.open would send it down&lt;br/&gt;
to all SegmentReaders it opens.&lt;/p&gt;

&lt;p&gt;This would then replace *CacheKey.&lt;/p&gt;

&lt;p&gt;This approach is not that different from what we have today, but I&lt;br/&gt;
think there are important differences:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Decouple value generation (ValueSource) from caching&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Tell IndexReader what its ValueSource is, so eg when you do&lt;br/&gt;
    sorting the sort pulls from your ValueSource and not a global&lt;br/&gt;
    default one.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hopefully don't duplicate so much code (eg uninversion)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Other thoughts:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Presumably, at this point, the arrays returned by field cache&lt;br/&gt;
    should be considered readonly by the app, right?  So cloning&lt;br/&gt;
    a reader should simply make a shallow clone of the cache.  (Longer&lt;br/&gt;
    term, with CSF as the source, we think updating fields should be&lt;br/&gt;
    possible, so we'd need a copy-on-write solution, like we now do w/&lt;br/&gt;
    deleted docs).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Looks like some accidental regressions snuck in, eg in&lt;br/&gt;
    DirIndexReader:
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
-    &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;String&lt;/span&gt;[] files = dir.listAll();
+    &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;String&lt;/span&gt;[] files = dir.list();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;    and in IndexReader:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;protected&lt;/span&gt; IndexReader(Directory directory) {
     &lt;span class="code-keyword"&gt;this&lt;/span&gt;();
-    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.directory = directory;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Do we even need ComparatorFactory*?  Seems like this patch&lt;br/&gt;
    shouldn't be be in the business of creating comparators.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;You should hit UOE if you try to getXXX() on a MultiReader&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Shouldn't FieldCache be deprecated entirely?  I would think, going&lt;br/&gt;
    forward, I interact only w/ the IndexReader's default ValueSource?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12698096" author="markrmiller@gmail.com" created="Sat, 11 Apr 2009 15:01:02 +0100">&lt;p&gt;Thanks Mike! Everything makes sense on first read, so I'll work in that direction.&lt;/p&gt;

&lt;p&gt;In regards to FieldCache - yes it def will be deprecated (pretty rough patch to start so I might have missed some of that - I know plenty is undone).&lt;/p&gt;

&lt;p&gt;As far as back compat with it, I was trying to make it so that if you happened to have code that still used it, and you used the new cache, you woudn't double your mem needs (the original point of backing FieldCache with the new API). Thats pretty restrictive though - indeed, things look much nicer if we don't attempt that (and in the case of a MultiReader cache array, we wouldn't be able to avoid it anyway, so I guess it does make sense we don't worry about it so much).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Do we even need ComparatorFactory*?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Probably not then - I didn't really touch any of the custom type stuff yet. I mainly just got all of the sort tests except remote and custom to pass (though tests elsewhere are still failing).&lt;/p&gt;

&lt;p&gt;I'll make another push with your suggestions.&lt;/p&gt;</comment>
                    <comment id="12698101" author="thetaphi" created="Sat, 11 Apr 2009 16:31:25 +0100">&lt;blockquote&gt;
&lt;p&gt;How about we create a ValueSource abstract base class, that defines&lt;br/&gt;
abstract byte[] getBytes(IndexReader r, String field),&lt;br/&gt;
int[] getInts(IndexReader r, String field), etc. (Just like&lt;br/&gt;
ExtendedFieldCache).&lt;/p&gt;

&lt;p&gt;This is subclassed to things like UninversionValueSource (what&lt;br/&gt;
FieldCache does today), CSFValueSource (in the future) both of which&lt;br/&gt;
take an IndexReader when created.&lt;/p&gt;

&lt;p&gt;UninversionValueSource should provide basic ways to customize the&lt;br/&gt;
uninversion. Hopefully, we can share mode code than the current&lt;br/&gt;
FieldCacheImpl does (eg, a single "enum terms &amp;amp; terms docs" loop that&lt;br/&gt;
switches out to a "handler" to deal with each term &amp;amp; doc, w/&lt;br/&gt;
subclasses that handle to byte, int, etc.).&lt;/p&gt;

&lt;p&gt;And then I can also make MyFunkyValueSource (for extensibility) that&lt;br/&gt;
does whatever to produce the values.&lt;/p&gt;

&lt;p&gt;Then we make CachingValueSource, that wraps any other ValueSource.&lt;/p&gt;

&lt;p&gt;And finally expose a way in IndexReader to set its ValueSource when&lt;br/&gt;
you open it? It would default to&lt;br/&gt;
CachedValueSource(UninversionValueSource()). I think we should&lt;br/&gt;
require that you set this on opening the reader, and you can't later&lt;br/&gt;
change it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like this idea, but i am a little bit concerned about only one ValueSource for the Reader. This makes plugging in different sources for different field types hard.&lt;/p&gt;

&lt;p&gt;E.g.: One have a CSF and a TrieField and several normal int/float fields. For each of these fields he needs another ValueSource. The CSF field can be loaded from Payloads, the TrieField by decoding the prefix encoded values and the others like it is now.&lt;/p&gt;

&lt;p&gt;So the IndexReaders ValueSource should be a Map of FieldValueSources, so the user could register FieldValueSources for different field types.&lt;/p&gt;

&lt;p&gt;The idea of setting the ValueSource for the IndexReader is nice, we then could simply remove the extra SortField constructors, I added in &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1478" title="Missing possibility to supply custom FieldParser when sorting search results"&gt;&lt;del&gt;LUCENE-1478&lt;/del&gt;&lt;/a&gt;, as it would be possible to specify the type for Sorting when creating the IndexReader. Search code then would simply say, sort by fields a, b, c without knowing what type of field it is. The sort code would get the type and the arrays from the underlying IndexReaders.&lt;/p&gt;

&lt;p&gt;The same with the current function query value sources (just a question: are the functions query value sources then obsolete and can be merged with the "new" ValueSource)?&lt;/p&gt;</comment>
                    <comment id="12698104" author="mikemccand" created="Sat, 11 Apr 2009 17:15:04 +0100">&lt;blockquote&gt;&lt;p&gt;E.g.: One have a CSF and a TrieField and several normal int/float fields. For each of these fields he needs another ValueSource.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Couldn't we make a "PerFieldValueSource" impl to handle this?  (And leave the switching logic out of IndexReader).&lt;/p&gt;

&lt;p&gt;I think another useful ValueSource would be one that first consults CSF and uses that, if present, else falls back to the uninversion source.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The sort code would get the type and the arrays from the underlying IndexReaders.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm not sure this'll work &amp;#8211; IndexReader still won't know what type to ask for, for a given field?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;are the functions query value sources then obsolete and can be merged with the "new" ValueSource&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, the API is a little different (this API returns int[], but the function query's ValueSource has int intVal(int docID)), so I think we'd wrap the new API to match function query's  (ie, cut over function query's use of FieldCache to this new FieldCache).&lt;/p&gt;</comment>
                    <comment id="12698135" author="markrmiller@gmail.com" created="Sat, 11 Apr 2009 22:11:51 +0100">&lt;p&gt;Any ideas on where parser fits in with valuesource? Its easy enough to kind of keep it how it is, but then what if CSF can be stored as a byte rep of an int or something? parse(String) won't make any sense. If we move Parser up to an Impl of valuesource, we have to special case things -&lt;/p&gt;

&lt;p&gt;Any thoughts? Just stick with allowing the passing of a 'String to type' Parser and worry about possible byte handling later? A different parser object of some kind?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;I guess its not so bad if parser is still first class and something that read bytes would just ignore the given parser? The parsing would just be built into something specialized like that, and it would be free to ignore a given String parser?&lt;/p&gt;</comment>
                    <comment id="12698151" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 02:59:22 +0100">&lt;p&gt;or parsing is just done by the FieldValue implementation, with overrides or something? To change parsers you override UnivertedValuedSource returning your parsers in the callbacks (or something similiar?)&lt;/p&gt;</comment>
                    <comment id="12698195" author="mikemccand" created="Sun, 12 Apr 2009 10:11:53 +0100">&lt;blockquote&gt;&lt;p&gt;Any ideas on where parser fits in with valuesource?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the UninversionValueSource would accept a custom parser (String -&amp;gt; native type), like what's done today.&lt;/p&gt;

&lt;p&gt;Maybe it should also allow stopping the loop early (which Trie* uses), or perhaps outright overriding of the inversion loop itself (which if we make that class subclass-able should be simple).&lt;/p&gt;</comment>
                    <comment id="12698196" author="mikemccand" created="Sun, 12 Apr 2009 10:21:49 +0100">&lt;p&gt;&amp;gt; I like this idea, but i am a little bit concerned about only one ValueSource for the Reader. &lt;/p&gt;

&lt;p&gt;Thinking more about this...&lt;/p&gt;

&lt;p&gt;Over in KS/Lucy, the approach Marvin is taking is something called a&lt;br/&gt;
FieldSpec, to define the "extended type" for a field.  The idea is to&lt;br/&gt;
strongly decouple a field's type from its value, allowing that type to&lt;br/&gt;
be shared across different fields &amp;amp; instances of the same field.&lt;/p&gt;

&lt;p&gt;So in KS/Lucy, presumably IndexReader would simply consult the&lt;br/&gt;
FieldSpec for a given field, to determine which ValueSource impl is&lt;br/&gt;
responsible for producing values for this field.&lt;/p&gt;

&lt;p&gt;Right now details for a field are scattered about (PerFieldValueSource&lt;br/&gt;
and PerFieldAnalyzerWrapper and Field.Index/Store/TermVector.*,&lt;br/&gt;
FieldInfo, etc.). This then requires alot of app-level code to&lt;br/&gt;
properly use Trie* fields &amp;#8211; you have to use Trie* to analyze the&lt;br/&gt;
field, use Trie* to construct the query, use PerFieldValueSource to&lt;br/&gt;
populate the FieldCache, etc.&lt;/p&gt;

&lt;p&gt;Maybe, as part of the cleanup of our three *Field classes, and index&lt;br/&gt;
vs search time documents, we should make steps towards having a&lt;br/&gt;
consolidated class that represents the "extended type" of a field.&lt;br/&gt;
Then in theory one could make a Field, attach a NumericFieldType() to&lt;br/&gt;
it (after renaming Trie* -&amp;gt; Numeric*), and then everything would&lt;br/&gt;
default properly.&lt;/p&gt;</comment>
                    <comment id="12698214" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 12:41:27 +0100">&lt;blockquote&gt;
&lt;p&gt;I think the UninversionValueSource would accept a custom parser (String -&amp;gt; native type), like what's done today.&lt;/p&gt;

&lt;p&gt;Maybe it should also allow stopping the loop early (which Trie* uses), or perhaps outright overriding of the inversion loop itself (which if we make that class subclass-able should be simple).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its the accepting that seems tricky though. If the getInts() calls take the parser, you have to use instanceof code to work with ValueSource.  Thats why I was thinking maybe callbacks - if a new type is added you just add a new one returning a default parser. Then you can just extend and replace the parsers you want to. I wasn't a big fan of that idea, but I am not sure of a nice, clean, extensible way to specify a bunch of parsers to UninversionValueSource that allows the API to cleanly be used from ValueSource. It already kind of seemed annoying that you would have to set a new ValueSource on the reader just to specify different parsers. I guess at least that has to be accepted though.&lt;/p&gt;</comment>
                    <comment id="12698215" author="earwin" created="Sun, 12 Apr 2009 12:47:10 +0100">&lt;p&gt;I'm using a similar approach.&lt;/p&gt;

&lt;p&gt;There's a FieldType, that governs conversions from Java type into Lucene strings and declares 'abilities' of that type. Like - conversion is order-preserving (all numerics + some others), converted values can be meaningfully prefix-searched (like TreeId, that is essentially an int[], used to represent things like nested category trees). Some types can also declare themselves as derivatives of others, like DateType being derived from LongType.&lt;/p&gt;

&lt;p&gt;Then there's a FieldInfo, that defines field name, FieldType used for it, and actions we're going to take on the field. E.g. if we want to sort on it, build clusters with certain characteristics, load values for this field for each found document, use fast rangefilters, store/filter on field being null/notnull, apply transforms on the field before storing/searching, copy value of the field to another field (with probable transformation) when indexing, etc. From FieldType and desired actions, FieldInfo is able to deduce tokenize/index/store/cache behaviour, and can say that additional lucene fields are required (e.g. for handling null/notnull searches, or trie ranges, or a special sort-form).&lt;/p&gt;

&lt;p&gt;Then there's an interface that contains FieldInfo constants and a special constant FieldEnum FIELDS = fieldsOf(ResumeFields.class); that is essentially a navigable list of all FieldInfos defined in this interface and interfaces it extends (allows me to have CommonFields + ResumeFields extends CommonFields, VacancyFields extends CommonFields).&lt;/p&gt;

&lt;p&gt;FieldType, and consequently FieldInfo is type-parameterized with the java type associated with the field, so you get the benefit of type-safety when storing/loading/searching the field. All Filters/Queries/Sorters/Loaders/Document accept FieldInfo instead of String for field name, so for example Filters.Range(field, fromValue, fromInclusive, toValue, toInclusive) knows whether to use a simple range filter or a trie one, ensures from/toValues are of a proper type and converts them properly. Filters.IsSet(field) can consult an additional field created during indexation, or access a FieldCache. DocLoader will either get a value for the field from index or from the cache. etc, etc, etc.&lt;/p&gt;

&lt;p&gt;While I like resulting schema-style very much, I don't want to see the likes of it within Lucene core. Better to have some contrib/extension/whatever that builds on core-defined primitives. That way if one needs to build his own somewhat divergent schema, they can easily do it, instead of trying to fit theirs over Lucene's. For the very same reason I'd like to see fieldcaches moved away from the core, and depending on the same in-core IndexReader segment creation/deletion/whatever hooks that users will use to build their extensions. &lt;/p&gt;</comment>
                    <comment id="12698219" author="creamyg" created="Sun, 12 Apr 2009 14:18:18 +0100">&lt;p&gt;"FieldType" is probably a better name than "FieldSpec", as it implies&lt;br/&gt;
subclasses with "Type" as a suffix: FullTextType, StringType, BlobType,&lt;br/&gt;
Float32Type, etc.&lt;/p&gt;</comment>
                    <comment id="12698224" author="mikemccand" created="Sun, 12 Apr 2009 15:46:38 +0100">&lt;blockquote&gt;&lt;p&gt;"FieldType" is probably a better name than "FieldSpec"&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12698225" author="mikemccand" created="Sun, 12 Apr 2009 15:59:14 +0100">
&lt;p&gt;How about something like this (NOTE: not compiled/tested):&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
&lt;span class="code-keyword"&gt;abstract&lt;/span&gt; class Uninverter {
  &lt;span class="code-keyword"&gt;abstract&lt;/span&gt; void newTerm(&lt;span class="code-object"&gt;String&lt;/span&gt; text);
  &lt;span class="code-keyword"&gt;abstract&lt;/span&gt; void handleDoc(&lt;span class="code-object"&gt;int&lt;/span&gt; docID);
  void go(IndexReader r) {
    ... TermEnum/TermDocs uninvert code...
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then:&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
class IntUninverter &lt;span class="code-keyword"&gt;extends&lt;/span&gt; Uninverter {
  &lt;span class="code-keyword"&gt;final&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt;[] values;
  IntUninverter(IndexReader r) {
    values = &lt;span class="code-keyword"&gt;new&lt;/span&gt; &lt;span class="code-object"&gt;int&lt;/span&gt;[r.maxDoc()];
  }

  &lt;span class="code-object"&gt;int&lt;/span&gt; currentVal;
  void newTerm(&lt;span class="code-object"&gt;String&lt;/span&gt; text) {
    currentVal = Intger.parseInt(text);
  }

  void handleDoc(&lt;span class="code-object"&gt;int&lt;/span&gt; docID) {
    values[docID] = currentVal;
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12698226" author="thetaphi" created="Sun, 12 Apr 2009 16:04:50 +0100">&lt;p&gt;Looks good, one addition: newTerm() could return false to stop iterating (for trie).&lt;/p&gt;

&lt;p&gt;But I do not know how performat this is with the class-global variable currentVal...&lt;/p&gt;</comment>
                    <comment id="12698229" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 16:39:15 +0100">&lt;p&gt;This should give us some more concrete to start from. Still early and rough (I started to put nocommits, but too early to worry much yet). I have pushed things in general to the proposed API, but its still fairly rough. Core sort tests pass (no custom comparator, remote), but custom parsers are currently ignored. A bunch of the unfun back compat / little stuff is undone.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How about something like this (NOTE: not compiled/tested):&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats kind of what I did (though not named as nicely, I'll update), but does that solve easily telling the IndexReader (valuesource) what parsers to use by a user? I suppose what your proposing is no custom parser? Instead a custom inverter - but still the problem of a user simply specifying inverters per field when initing the indexreader? Of course you can do a whole new UninversionVS imp, but it just seemed a little heavy handed compared to giving a parser the SortField... I'll update the code a bit and think some...&lt;/p&gt;</comment>
                    <comment id="12698230" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 17:03:31 +0100">&lt;blockquote&gt;&lt;p&gt;I like this idea, but i am a little bit concerned about only one ValueSource for the Reader. This makes plugging in different sources for different field types hard.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess you would just have to set a TrieEnabledValueSource when creating your IndexReaders? I suppose it could extend UninversionValueSource and for given fields do Trie unencoding, uninverting, and all other fields, yeld to the super impl?&lt;/p&gt;</comment>
                    <comment id="12698232" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 17:12:18 +0100">&lt;p&gt;Note: I think we should add the option to sort nulls first or last with this.&lt;/p&gt;</comment>
                    <comment id="12698233" author="earwin" created="Sun, 12 Apr 2009 17:14:17 +0100">&lt;blockquote&gt;&lt;p&gt;I guess you would just have to set a TrieEnabledValueSource when creating your IndexReaders? I suppose it could extend UninversionValueSource and for given fields do Trie unencoding, uninverting, and all other fields, yeld to the super impl?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;And then if you get some other XXX encoding, you'll end up with XXXVS extends UVS, TrieVS extends UVS, XXXAndTrieVS extends XXXVS or TrieVS + duplicate code from the other one. Ugly.&lt;/p&gt;</comment>
                    <comment id="12698234" author="mikemccand" created="Sun, 12 Apr 2009 17:26:46 +0100">&lt;p&gt;That is why I'd love to somehow move to a FieldType class that holds such per-field details.&lt;/p&gt;

&lt;p&gt;As things stand now (or, soon) you have to do many separate things to use TrieXXX:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Create the right tokenStream for it, and stick that into your Field&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Make the right query type at search time&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Make the right sort-field parser&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Make the right ValueSource&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It's crazy.  I should be able to make a TrieFieldType (SingleNumberFieldType, or something, after the rename), make that the type of my field when I add it to my document, and then have all these places that do range search, sorting, value retrieval consult the FieldInfo and see that this is a trie field, and act accordingly.&lt;/p&gt;</comment>
                    <comment id="12698235" author="mikemccand" created="Sun, 12 Apr 2009 17:29:08 +0100">&lt;blockquote&gt;&lt;p&gt;Note: I think we should add the option to sort nulls first or last with this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mean for getStringIndex()?  I agree!&lt;/p&gt;

&lt;p&gt;Actually, it'd be nice to disallow nulls entirely, somehow, since this forces us to sprinkle null checks all over the place in StringOrdVarlComparator.  Maybe we would allow you to pass in a "null equivalent", eg you could use "", "UNDEFINED", whatever, as long as it's a valid string.&lt;/p&gt;</comment>
                    <comment id="12698238" author="mikemccand" created="Sun, 12 Apr 2009 17:41:31 +0100">&lt;p&gt;Another thing that'd be great to fix about FieldCache is its&lt;br/&gt;
intermittent checking of the "some docs had more than one token in the&lt;br/&gt;
field" error.  The current check only catches it in limited cases,&lt;br/&gt;
which is deadly because you can test like crazy and think you're OK&lt;br/&gt;
only in production months later to index slightly different content&lt;br/&gt;
and hit the exception. RuntimeException&lt;/p&gt;

&lt;p&gt;But I can't think of a cheap way to do it reliably.&lt;/p&gt;

&lt;p&gt;At least, we should upgrade the exception from RuntimeException to a&lt;br/&gt;
checked exception.  Or, we could turn the check off entirely (which I&lt;br/&gt;
think is better than intermittently catching it).&lt;/p&gt;

&lt;p&gt;We should also somehow allow turning off the check on a case by case&lt;br/&gt;
basis, since there are really times when it's OK.  (Though maybe you&lt;br/&gt;
just make your own ValueSource, or maybe subclass Uninverter,&lt;br/&gt;
or... something?).&lt;/p&gt;</comment>
                    <comment id="12698239" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 17:42:12 +0100">&lt;blockquote&gt;&lt;p&gt;Ugly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well no worries yet &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Still in early design mode, so if it can be made better, I'm sure it will. Of course I'd love to get to 'everything works right for the right field automagically' as well - not sure that will fit into the scope of this issue though (though nothing saying this issue can't be further delayed). We will do the best we can regardless though.&lt;/p&gt;

&lt;p&gt;I'm kind of worried that any change is going to hurt Apps like Solr - if you end up using the new built in API, but also have code that must stick for a while with the old API (for multireader fieldcache or something), you'll likely increase RAM usage more than before - how much of a concern that ends up being, I'm not sure. I suppose eventually it has to become up to the upgrader to consider and deal with it if we want to move to segment level caching.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Little behind on that worry I guess - we already pumped that problem out the door with 1483. Half in the clouds over here.&lt;/p&gt;</comment>
                    <comment id="12698241" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 17:58:52 +0100">&lt;blockquote&gt;&lt;p&gt;Or, we could turn the check off entirely (which I think is better than intermittently catching it).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 - agreed that the check is nasty - better to never trip it if your only going to trip it depending...&lt;/p&gt;</comment>
                    <comment id="12698242" author="creamyg" created="Sun, 12 Apr 2009 18:00:33 +0100">&lt;p&gt;&amp;gt; Another thing that'd be great to fix about FieldCache is its&lt;br/&gt;
&amp;gt; intermittent checking of the "some docs had more than one token in the&lt;br/&gt;
&amp;gt; field" error.&lt;/p&gt;

&lt;p&gt;Add a FieldType that only allows one value per document. At index-time, &lt;br/&gt;
verify when the doc is added that indeed, only one value was supplied.&lt;/p&gt;

&lt;p&gt;In Lucy, I expect StringType to fill this role. FullTextType is for multi-token&lt;br/&gt;
fields.&lt;/p&gt;

&lt;p&gt;Optionally, add a "NOT NULL" check to verify that each doc supplies a&lt;br/&gt;
value, or allow the FieldType object to specify a default value that should&lt;br/&gt;
be inserted.&lt;/p&gt;</comment>
                    <comment id="12698243" author="earwin" created="Sun, 12 Apr 2009 18:09:30 +0100">&lt;blockquote&gt;&lt;p&gt;At least, we should upgrade the exception from RuntimeException to a checked exception.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Exceptions are for expected conditions that can be adequately handled by the caller. RuntimeExceptions are for possible, but unexpected conditions that can theoretically be handled by the caller, but most of the times caller will terminate anyway.&lt;br/&gt;
Having several values in place where only one should be at all times is obviously an unexpected indexer's fault. So by using checked exception here you'll only provoke some ugly rethrowing/wrapping code, or propagation of said checked exception up the method hierarchy, without gaining any benefit at all.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Or, we could turn the check off entirely.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes!&lt;/p&gt;</comment>
                    <comment id="12698247" author="mikemccand" created="Sun, 12 Apr 2009 18:51:20 +0100">&lt;blockquote&gt;
&lt;p&gt;&amp;gt; Or, we could turn the check off entirely (which I think is better than intermittently catching it).&lt;/p&gt;

&lt;p&gt;+1 - agreed that the check is nasty - better to never trip it if your only going to trip it depending...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OK &amp;#8211; let's turn this check off entirely.&lt;/p&gt;

&lt;p&gt;I like Marvin's approach but we don't quite have a FieldType just yet...&lt;/p&gt;</comment>
                    <comment id="12698248" author="earwin" created="Sun, 12 Apr 2009 18:52:50 +0100">&lt;blockquote&gt;&lt;p&gt;I'm kind of worried that any change is going to hurt Apps like Solr - if you end up using the new built in API, but also have code that must stick for a while with the old API (for multireader fieldcache or something), you'll likely increase RAM usage more than before - how much of a concern that ends up being, I'm not sure.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;My personal stance is that until you have one perfectly thought out API, nothing should restrain you from changing it. It's better to feel pain once or twice, when you adapt to API changes, then to feel it constantly, each time you're using that half-assed thing you're keeping around for back-compat. Look at google-collections. They did some really breaking changes since they released, but most of them eased my life after I made my project compile and run with the new version of their library.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In Lucy, I expect StringType to fill this role. FullTextType is for multi-token fields.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In our case multiplicity is defined on FieldInfo level. Because we can have one Int field that holds some value, and another Int field that holds several ids.&lt;br/&gt;
Same goes for the String field - you might want tags on a document that are represented as untokenized strings, but each document can have 0..n of them.&lt;/p&gt;</comment>
                    <comment id="12698249" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 18:54:40 +0100">&lt;p&gt;Rolling forward with Uninverter ...&lt;/p&gt;</comment>
                    <comment id="12698254" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 19:58:20 +0100">&lt;p&gt;I began to make the switch of allowing an Uninverter to be attached to a SortField (like Parser) with the idea of doing backcompat by wrapping the Parser with an Uninverter. I caught myself though, because the reason I wasn't doing that before is that Uninverter may not make sense for a given ValueSource. By forcing you to set the Uninverters per type on the UninversionValueSource instead (current posted patch), this issue is kind of avoided - the possible problem is that it seems somewhat less dynamic in that you set it once on the ValueSource and leave it instead of being able to pass any impl any time on a SortField. Perhaps not so bad. But then how do I set the Uninverter for back compat with an old Parser? It doesn't seem wise to allow arbitrary Uninverter updates to the UninversionValueSource does it? Thread safety issues, and ... but then how to handle back compat with the parser? ... &lt;/p&gt;</comment>
                    <comment id="12698255" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 20:17:50 +0100">&lt;p&gt;I guess we do need to have Uninverters settable per run somehow ... Then if a Parser comes in on SortField, we can downcast to UninversionValueSource and pass the Uninverter wrapping the Parser. I don't think we should pass the Uninverter on SortField though, going forward. Uninverter may not apply to ValueSource, and internally, things will work with ValueSource.&lt;/p&gt;

&lt;p&gt;So a user cannot pass an Uninverter for internal sorting per sort, it has to init the UninversionValueSource with the right Uninverters, but for backcompat, FieldComparator would be able to pass an Uninverter that takes precedence?&lt;/p&gt;

&lt;p&gt;That sounds somewhat alright to me, I'll roll that way a bit for now.&lt;/p&gt;</comment>
                    <comment id="12698257" author="mikemccand" created="Sun, 12 Apr 2009 20:22:15 +0100">&lt;p&gt;How about SortField taking ValueSource?&lt;/p&gt;</comment>
                    <comment id="12698258" author="mikemccand" created="Sun, 12 Apr 2009 20:25:09 +0100">&lt;p&gt;This issue is fast moving!  Here're some thoughts on the last patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Need to pass in ValueSource impl to IndexReader.open, defaulting&lt;br/&gt;
    to Cached(Uninverted)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Maybe ValueSource should provide default "throws UOE" exceptions&lt;br/&gt;
    methods (instead of abstract) since it's rather cumbersome for a&lt;br/&gt;
    subclass that only intends to provide eg getInts().&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Should CachingValueSource really include IndexReader in its key?&lt;br/&gt;
    I think it shouldn't?  The cache is already "scoped" to a reader&lt;br/&gt;
    because each SegmentReader will have its own instance.  Also, one&lt;br/&gt;
    might cache ValueSources that don't "have" a reader (eg pull from&lt;br/&gt;
    a DB or custom file format or something).  Cloning an SR for now&lt;br/&gt;
    should just copy over the same cache.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we should back deprecated FieldCache with ValueSource for&lt;br/&gt;
    the reader; one simple way to not ignore the Parser passed in is&lt;br/&gt;
    to anonymously subclass Uninverter and invoke the passed in&lt;br/&gt;
    ByteParser from its "newTerm" method?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why do we need to define StringInverter, IntInverter, etc in&lt;br/&gt;
    UninversionValueSource?  Couldn't someone instead subclass&lt;br/&gt;
    UninversionValueSource, and override whichever getXXX's they want&lt;br/&gt;
    using their own anonymous uninverter subclass?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Should we deprecate function.FieldCacheSource, and make a new&lt;br/&gt;
    function.ReaderValueSource (ie pulls its values from the&lt;br/&gt;
    IndexReader's ValueSource)?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12698264" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 20:46:46 +0100">&lt;blockquote&gt;&lt;p&gt;How about SortField taking ValueSource? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right - theres the right thought I think. I'll play with that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Need to pass in ValueSource impl to IndexReader.open, defaulting to Cached(Uninverted)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes - only havn't because that code is kind of unfirm - it already seems like it will prob be moved out to SortField &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; So I was just short-cut initing it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Should CachingValueSource really include IndexReader in its key?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Probably not then &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; I'll be going over that tighter - I was in speed mode and havn't considered the CachingValueSource much yet - I kind of just banged out a quick impl (gotto love eclipses generate hashcode/equals), put it in and tested it.  To handle all of this 'do it for Long, repeat for Int, repeat for Byte, etc' I go somewhat into robot mode. Also, this early, I know you'll have me changing directions fast enough that I'm still in pretty rough hammer out mode.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why do we need to define StringInverter, IntInverter, etc in UninversionValueSource? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes true. I was using anon classes in the prev patch, but upon switch to Uninverter, I just did mostly what came to mind quickest looking at your example code and what I had.&lt;br/&gt;
Indeed, overriding getXXX is simple and effective. As I think about it - now I  am thinking I did it for the getArray call to return the right type (easy with anon class, but custom passed Uninverter ?). Could return object and cast though ...&lt;/p&gt;

&lt;p&gt;Do we need Uninverter if overriding getXXX is easy enough and we pass the ValueSource on SortField?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Should we deprecate function.FieldCacheSource,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah for sure. I'll take a closer look at the function package soon. Thus far, I've just got it compiling and the main sort tests passing. As soon as I feel the API is firming (which, sweet, it already is a bit I think), I'll start polishing and filling in the missing pieces, more thorough nocommits, comments.&lt;/p&gt;</comment>
                    <comment id="12698272" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 21:43:04 +0100">&lt;blockquote&gt;&lt;p&gt;Should CachingValueSource really include IndexReader in its key?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I see now - I wasnt always holding the ValueSource in the Reader - I was using getXXX(reader, field). Now I am back to that - and since the CachingValueSource holds the map, I am back to needing that. Being able to change the ValueSource on the fly now with SortField has implications for CachingValueSource. There has to be some kind of default ValueSource, which would likely do caching. Once you started using ValueSources from SortField, they won't share the cache. Get enough of them going and the RAM reqs jump.&lt;/p&gt;</comment>
                    <comment id="12698286" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 22:45:36 +0100">&lt;p&gt;I really like CachingValueSource, but somehow the cache has to move to the segmentreader or something... (now that there is not a segmentreader -&amp;gt; valuesource instance mapping)&lt;/p&gt;</comment>
                    <comment id="12698295" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 00:16:46 +0100">&lt;p&gt;Moves ValueSource to SortField, giving us a caching problem. I've got a DEFAULT_VALUESOURCE in IndexReader that needs movement to some refinement.&lt;/p&gt;

&lt;p&gt;No custom field comparator support yet, but parser back compat test passes.&lt;/p&gt;

&lt;p&gt;I really am liking how its feeling now - have to think about the caching though.&lt;/p&gt;</comment>
                    <comment id="12698297" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 01:43:16 +0100">&lt;p&gt;Might as well throw another one up with custom comparator test passing. Still a bunch to do beyond tests, but I think any left that are failing will be easily fixed with a resolution to how we are going to cache.&lt;/p&gt;

&lt;p&gt;Then javadoc, consider all the back compat issues a bit more, tweak API, consider what new tests make sense ...&lt;/p&gt;</comment>
                    <comment id="12698298" author="thetaphi" created="Mon, 13 Apr 2009 01:51:49 +0100">&lt;p&gt;I did some tests with the new API and TrieRange. Without patch it did not work because the missing StopFillCacheException (but this was intended, as it was only a hack).&lt;/p&gt;

&lt;p&gt;I created a TrieValueSource, that is similar like the UninversionValueSource, but iterates only over shift=0 prefix encoded values.&lt;/p&gt;

&lt;p&gt;The original patch only missed the following ctor of SortField:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
  &lt;span class="code-keyword"&gt;public&lt;/span&gt; SortField (&lt;span class="code-object"&gt;String&lt;/span&gt; field, &lt;span class="code-object"&gt;int&lt;/span&gt; type, ValueSource valueSource, &lt;span class="code-object"&gt;boolean&lt;/span&gt; reverse) {
    initFieldType(field, type);
    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.reverse = reverse;
    &lt;span class="code-keyword"&gt;this&lt;/span&gt;.valueSource = valueSource;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After adding this, it worked (this ctor creates a "normal" sortfield but without custom comparator, but with a specific value-source). This ctor would replace the deprecated parser ctor.&lt;/p&gt;</comment>
                    <comment id="12698300" author="thetaphi" created="Mon, 13 Apr 2009 01:58:55 +0100">&lt;p&gt;By the way: If this new API goes into 2.9, the SortField ctors with parser and the whole parser support in SortField/Search-code can be removed, if Parser itsself is deprecated (as support for &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1478" title="Missing possibility to supply custom FieldParser when sorting search results"&gt;&lt;del&gt;LUCENE-1478&lt;/del&gt;&lt;/a&gt; was not released until now). New code could always use ValueSource.&lt;/p&gt;</comment>
                    <comment id="12698309" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 03:29:15 +0100">&lt;p&gt;I guess the caching is not the problem I thought. It's got to be per  &lt;br/&gt;
value source anyway. I guess I just have to stick the default  &lt;br/&gt;
valuesource in a better place. Seems cachingvaluesource is still legit.&lt;/p&gt;

&lt;p&gt;Still, I introduced reader back into the caching , and you wanted to  &lt;br/&gt;
avoid that..&lt;/p&gt;

&lt;ul class="alternate" type="square"&gt;
	&lt;li&gt;Mark&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;a href="http://www.lucidimagination.com" class="external-link"&gt;http://www.lucidimagination.com&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On Apr 12, 2009, at 8:45 PM, "Mark Miller (JIRA)" &amp;lt;jira@apache.org&amp;gt;  &lt;/p&gt;
</comment>
                    <comment id="12698394" author="mikemccand" created="Mon, 13 Apr 2009 15:10:36 +0100">&lt;blockquote&gt;&lt;p&gt;Also, this early, I know you'll have me changing directions fast enough that I'm still in pretty rough hammer out mode.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I hear you &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; Things're fast moving... I'm glad you're good with&lt;br/&gt;
The Eclipse.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Do we need Uninverter if overriding getXXX is easy enough and we pass the ValueSource on SortField?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good question... though it is nice/important to not have to implement&lt;br/&gt;
your own TermEnum/TermDocs iteration logic.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Being able to change the ValueSource on the fly now with SortField has implications for CachingValueSource.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think an app would need to pay attention here, ie, if caching is&lt;br/&gt;
needed the app should always pass the same CachingValueSource to all&lt;br/&gt;
sort fields.  It's up to the app to scope their ValueSources&lt;br/&gt;
correctly.  You're right that if you have a bug and don't always use a&lt;br/&gt;
consistent CachingValueSource, you can use too much RAM since a given&lt;br/&gt;
field's int[] can be double cached; but I think that's a bug in the&lt;br/&gt;
app?  It's analogous to using 2 IndexReaders instead of sharing?&lt;/p&gt;

&lt;p&gt;Though... I'm not sure.  Something's not yet right about our approach&lt;br/&gt;
but I can't think of how to fix it... will mull it over.&lt;/p&gt;

&lt;p&gt;I wonder if we can somehow fit norms handling into this?  Norms ought&lt;br/&gt;
to be some sort of XXX.getBytes() somewhere, but I'm not sure how&lt;br/&gt;
yet.  It's tricky because norms accept changes and thus must implement&lt;br/&gt;
copy-on-write.  So maybe we levae them out for now...&lt;/p&gt;</comment>
                    <comment id="12698397" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 15:18:08 +0100">&lt;p&gt;I'd like to get the cache back into the segment readers somehow ... or somehow get away from the large indexreader cache map.&lt;/p&gt;

&lt;p&gt;IndexReader plugin could come in useful there &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;p&gt;I've got the tests passing in general, but still have to figure out better/right caching (the way I allow you to pass in an Uninverter skips caching completely! plus the above), need to deal with ValueSource as a field on SortField - require it being serializable...ugg.&lt;/p&gt;

&lt;p&gt;I'll be away from this for while now prob though, so plenty of time to mull it over.&lt;/p&gt;</comment>
                    <comment id="12698409" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 16:14:41 +0100">&lt;p&gt;Might as well post as far as I got in case anyone else ends up wanting to take a few swings.&lt;/p&gt;

&lt;p&gt;I think all tests pass except for one remote in testsort (probably related to caching?), and clone checks in an IndexReader test that try and ensure the fieldcache contents are the same instance (they are not because of the caching - havnt gotten that far).&lt;/p&gt;

&lt;p&gt;So the major issue remaining I think (other than natural cleanup and back test thinking) is getting the caching right. There will probably be other changes too, but I think getting the caching right might drive them.&lt;/p&gt;</comment>
                    <comment id="12698418" author="thetaphi" created="Mon, 13 Apr 2009 16:53:48 +0100">&lt;blockquote&gt;&lt;p&gt;I've got the tests passing in general, but still have to figure out better/right caching (the way I allow you to pass in an Uninverter skips caching completely! plus the above), need to deal with ValueSource as a field on SortField - require it being serializable...ugg.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mh, FieldCache.Parser is also not serializable and the others like the Comparators are not serializable, too. Why not simply pass the ValueSource to SortField like the Parser or Locale? It worked until now without serialization and so I think we should remove serialization from SortField. The factory is silly. If we have a factory there, we should also have a factory for parsers...&lt;/p&gt;

&lt;p&gt;By the way, can you add the ctor mentioned above to your patch, I need it to sucessfully test the new TrieValueSource I wrote yesterday (see patch). This is a good test case for the extensibility of your new API.&lt;br/&gt;
By the way: For TrieRange, I made the ValueSource a static variable. In general the ValueSources should be singletons (maybe this is why you created the factory).&lt;/p&gt;</comment>
                    <comment id="12698419" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 17:00:40 +0100">&lt;blockquote&gt;&lt;p&gt;It worked until now without serialization and so I think we should remove serialization from SortField. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don't think we can right now because of remote searchable - I think?. I agree the factories are silly, but now I know why they exist! It had eluded me before.&lt;/p&gt;

&lt;p&gt;I can roll your patch in Uwe - sorry I missed it with that last one - I had meant to, but it slipped my mind.&lt;/p&gt;

&lt;p&gt;I've been waiting to pin down how ValueSource handles its cache (either lightning will strike my mind, or more likely, Mike will tell me what to do) - but the main reason for the factory at the moment is to get the remote tests to pass - since SortField is serializable, it allows us to pass the ValueSource without it being seriazable.&lt;/p&gt;</comment>
                    <comment id="12698421" author="thetaphi" created="Mon, 13 Apr 2009 17:08:29 +0100">&lt;p&gt;I am just wondering why the parsers and locales work, which all are not serializable. But they are NULL per default. So in principle, if I do a remote search with a custom parser or comparator, it would fail, too.&lt;/p&gt;</comment>
                    <comment id="12698423" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 17:13:41 +0100">&lt;p&gt;Okay, good point. I've got to take a closer look at what we are required to support for remote searching.&lt;/p&gt;

&lt;p&gt;I think your right though, we probably won't end up needing the factory. For right now, the way I set the DEFAULT, we need it to get remote tests to pass. But that is all very loose right now - when the caching mechanism is fixed up, I think the whole DEFAULT_VALUE_SOURCE will be cleaned right up - especially having to set it there on the SortField.&lt;/p&gt;

&lt;p&gt;I think we have to support remote for CustomFieldComparator, but I guess wouldnt have to for ValueSource.&lt;/p&gt;</comment>
                    <comment id="12699016" author="markrmiller@gmail.com" created="Wed, 15 Apr 2009 02:26:35 +0100">&lt;p&gt;I was going to throw in that constructor Uwe, and I got caught up doing a few things. I'm not so sure we can stick with attaching the ValueSource on the SortField. In the end, we'd really like the cache to be held per segment in the reader, rather than a monster reader cache. So I have moved the standard ValueSource back to the reader. I've got a separate massive reader cache (as in, everything goes in one cache keyed by reader) for handling the backcompat issues with custom parsers in FieldCache (it would be awesome to get this out before having to deprecate the SortField parser stuff).&lt;/p&gt;

&lt;p&gt;This doesnt really jive well with passing in ValueSources on the fly with SortField. Which sucks, because that was nice.&lt;/p&gt;

&lt;p&gt;What do you think about having to pull off Trie from the ValueSource set on your reader at reader init? I'm not thinking its super pretty - I guess you take which fields to override at init, and then do trie stuff for certain fields, but pass through to the default ValueSource impl for other fields?&lt;/p&gt;

&lt;p&gt;I hope that can be improved, but i don't see how at the moment...&lt;/p&gt;</comment>
                    <comment id="12699028" author="markrmiller@gmail.com" created="Wed, 15 Apr 2009 03:19:15 +0100">&lt;p&gt;Or maybe we can just keep all 3 options? Backcompat parser stuff goes through a reader keyed cache, built in stuff goes through a segment level ValueSource, and custom stuff using SortField can do whatever to override the ValueSource - they would cache if they wanted, etc.&lt;/p&gt;

&lt;p&gt;So you could either override the default ValueSource, and provide your own override on the fly.&lt;/p&gt;

&lt;p&gt;I guess that does seem to work out anyway ... ?&lt;/p&gt;</comment>
                    <comment id="12699166" author="markrmiller@gmail.com" created="Wed, 15 Apr 2009 13:14:58 +0100">&lt;p&gt;Thinking a bit on this this morning:&lt;/p&gt;

&lt;p&gt;I think that will work out right. We would have 3 different attacks at ValueSource.&lt;/p&gt;

&lt;p&gt;1. A ValueSource per segment reader that handles all default ValueSource needs - you get it with IndexReader.getValueSource. Its an UninversionValueSource wrapped by a CachingValueSource by default.&lt;/p&gt;

&lt;p&gt;2. A singleton back compat value source that is wrapped by CacheByReaderValueSource. It has extra methods that takes Uninverters, allowing custom Uninverters and caching by Uninverter.&lt;/p&gt;

&lt;p&gt;3. You can override the ValueSource used for Sorting by attaching it to the SortField. Likely, you would wrap in a CacheByReaderValueSource and have your own singleton.&lt;/p&gt;

&lt;p&gt;I think that gives us back compat and the best of both worlds?&lt;/p&gt;</comment>
                    <comment id="12699428" author="markrmiller@gmail.com" created="Wed, 15 Apr 2009 23:46:29 +0100">&lt;p&gt;I've added your SortField constructor Uwe.&lt;/p&gt;

&lt;p&gt;I've also switched things to what I proposed.&lt;/p&gt;

&lt;p&gt;You can access a ValueSource from SegmentReaders with getValueSource. You get a USO Exception on non SegmentReaders with the hint to use getSequentialSubReaders.&lt;br/&gt;
The built in FieldCache usage uses this new API instead, allowing caching per segment without global caching keyed by IndexReader. Its hardwired at the moment, but you would be able to set it on IndexReader open.&lt;/p&gt;

&lt;p&gt;There is a back compat CacheByReaderUninverterValueSource that caches in a WeakHashMap by IndexReader and Uninverter (Uninverter is actually keyed by the Parser it wraps). This&lt;br/&gt;
works as a Singleton. Deprecated code, like the FieldCache uses this. You wouldn't want to straddle both API's, because for identical types / uninversion you would duplicate data (eg if you asked both APIs for the same data type with&lt;br/&gt;
the same parser/uninverter and field, you would get twice the data in RAM). This method still works with MultiReaders, and so nicely solves back compat with regards to only allowing access to ValueSource from the SegmentReader.&lt;/p&gt;

&lt;p&gt;You can also set a ValueSource on a SortField. This would override the ValueSource used at sort time to one provided. There is a convenience class called CacheByReaderValueSource that caches&lt;br/&gt;
by Reader, field, type, key.&lt;/p&gt;

&lt;p&gt;I think that solves all three needs (and my caching concerns) in a pretty nice way. Patch is not done, but all tests now pass except for TrieRange (have not run back compat tests yet due to Trie failure - havnt looked into yet either).&lt;/p&gt;</comment>
                    <comment id="12699433" author="markrmiller@gmail.com" created="Wed, 15 Apr 2009 23:59:35 +0100">&lt;p&gt;Bah, just wrote a bunch and hit cancel.&lt;/p&gt;

&lt;p&gt;Attacking this from the old incarnation of the patch has me trying to hard to back FieldCache with the new API. Looking from closer eyes now, I don't see that being necessary. We just want the SegmentReader level ValueSource and the option for SortField override. FieldCache can use its current implementation. The motivation to back it is to minimize the RAM reqs of straddling the two API's. If we want the new API to work at the SegmentReader level though, we can never really achieve that. Might as well not half *ss it. I'll return the FieldCache to as is at some point and just deprecate.&lt;/p&gt;</comment>
                    <comment id="12699435" author="thetaphi" created="Thu, 16 Apr 2009 00:00:38 +0100">&lt;blockquote&gt;&lt;p&gt;Patch is not done, but all tests now pass except for TrieRange (have not run back compat tests yet due to Trie failure - havnt looked into yet either).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The TrieRange tests do not pass, because the FieldCache.StopFillCacheException is not handled in the uninverter code (part of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1582" title="Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded values"&gt;&lt;del&gt;LUCENE-1582&lt;/del&gt;&lt;/a&gt;, FieldCache.StopFillCacheException). When Trie gets switched to an own ValueSource, StopFillCacheException can be removed (you can do this in this patch, it was just a temporary hack).&lt;/p&gt;</comment>
                    <comment id="12699439" author="thetaphi" created="Thu, 16 Apr 2009 00:11:57 +0100">&lt;p&gt;I do not get the patch applied to trunk (merging works) but it gots lots of compile failures (because of the changes of &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1575" title="Refactoring Lucene collectors (HitCollector and extensions)"&gt;&lt;del&gt;LUCENE-1575&lt;/del&gt;&lt;/a&gt;). Mostly because the comparators got new ctors and so on.&lt;/p&gt;</comment>
                    <comment id="12699440" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 00:14:21 +0100">&lt;p&gt;Thanks Uwe. I had it in my mind to update to trunk about an hour ago and...&lt;/p&gt;

&lt;p&gt;I'll repost in a moment.&lt;/p&gt;</comment>
                    <comment id="12699448" author="thetaphi" created="Thu, 16 Apr 2009 00:31:45 +0100">&lt;p&gt;By the way: In the patch, the ValueSource set in SortField seems to be never used when building the comparators. If it is used, when applying the patch attached by me some days ago (&lt;a href="https://issues.apache.org/jira/browse/LUCENE-831" title="Complete overhaul of FieldCache API/Implementation"&gt;LUCENE-831&lt;/a&gt;-trieimpl.patch), the trie tests should also work.&lt;/p&gt;</comment>
                    <comment id="12699456" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 01:22:08 +0100">&lt;p&gt;Okay, all fixed up and updated to trunk. All tests pass. Still some rough edges for sure. Still probably going to revert FieldCache to its former self (though internally, the API won't be used).&lt;/p&gt;

&lt;p&gt;Back compat test run hasnt finished yet.&lt;/p&gt;</comment>
                    <comment id="12699545" author="thetaphi" created="Thu, 16 Apr 2009 07:23:13 +0100">&lt;p&gt;Hi, looks good:&lt;/p&gt;

&lt;p&gt;I am only not sure, what would be the right caching ValueSource. If you use a caching value source externally from IndexReader, what should I use? The original trie patch used the CachingValueSource (as when the patch was done, there only existed CacingValueSource):&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;
+  &lt;span class="code-keyword"&gt;public&lt;/span&gt; &lt;span class="code-keyword"&gt;static&lt;/span&gt; &lt;span class="code-keyword"&gt;final&lt;/span&gt; ValueSource TRIE_VALUE_SOURCE = &lt;span class="code-keyword"&gt;new&lt;/span&gt; CachingValueSource(&lt;span class="code-keyword"&gt;new&lt;/span&gt; TrieValueSource());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But correct would be CacheByReaderValueSource as a per-JVM singleton? For the tests is its not a problem, because there is only one index with one segment. If I use CachingValueSurce as a singleton, it would cache all values from all index readers mixed together?&lt;/p&gt;
</comment>
                    <comment id="12699644" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 12:31:32 +0100">&lt;p&gt;Right, you really want to use CacheByReaderValueSource. Better would probably be to get that cache on the segment reader as well. But I think that would mean bringing back some sort of general cache to IndexReader. You would have to be able to attach arbitrary ValueSources to the reader. We will see what ends up materializing. I am agonizingly slow at understanding anything, but quick to move anyway &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12699649" author="thetaphi" created="Thu, 16 Apr 2009 12:41:46 +0100">&lt;p&gt;This was the idea behin the "FieldType": You register at the top-level IndexReader/MultiReader/whatever the parsers/valuesources (e.g. in a map coded by field), all subreaders would also get this map (passed through) and if one asks for cache values for a specific field, he would get the correctly decoded fields (from CSF, Univerter, TrieUniverter, Stored Fields &lt;span class="error"&gt;&amp;#91;not really, but would be possible&amp;#93;&lt;/span&gt;). This was the original approach of this issue: attach caching to the single index/segmentreaders (with possibility to "register" valuesources for specific fields).&lt;br/&gt;
In this case the SortField ctors taking ValueSource or Parser can be cancelled (and we can do this for 2.9, as the Parser ctor of SortField was not yet released!).&lt;/p&gt;</comment>
                    <comment id="12699663" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 13:02:18 +0100">
&lt;p&gt;Thats somewhat possible now (with the exception that you can't yet set the value source for the segment reader yet - it would likely become an argument to the static open methods): ValueSource gets a field as an argument, so it is also easy enough to set a ValueSource that does trie encoding for arbitrary fields on the SegmentReader, eg FieldTypeValueSource could take arguments to configure it per field and then you set it on the IndexReader when you open it. Thats all still in the patch - its just a bit more of a pain than being able to set it at any time on the SortField as an override.&lt;/p&gt;

&lt;p&gt;I guess I almost see things going just to the segment reader valuesource option though - once FieldCache goes back to standard, it might make sense to drop the SortField valuesource support too, and just do the segment ValueSource. Being able to init the SegmentReader with a ValueSource really allows for anything needed - I just wasn't sure if it was too much of a pain in comparison to also having a dynamic SortField override.&lt;/p&gt;</comment>
                    <comment id="12699678" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 13:42:13 +0100">&lt;p&gt;So I'm flopping around on this, but I guess my latest take is that:&lt;/p&gt;

&lt;p&gt;I want to drop the SortField ValueSource override option. Everything would need to be handled by overriding the segment reader ValueSource.&lt;/p&gt;

&lt;p&gt;Drop the current back compat code for FieldCache - its mostly unnecessary I think. Instead, perhaps go back to orig FieldCache impl, except if the Reader is a segment reader, use the new ValueSource API ? Grrr - except if someone has mucked with the ValueSource or used a custom FieldCache Parser, it won't match correctly...thats it - you just can't straddle the two APIs. So I'll revert FieldCache to its former self and just deprecate.&lt;/p&gt;</comment>
                    <comment id="12699880" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 22:34:43 +0100">&lt;p&gt;Okay, now that I half way understand this issue, I think I have to go back to the basic motivations. The original big win was taken away by 1483, so lets see if we really need a new API for the wins we have left.&lt;/p&gt;

&lt;h3&gt;&lt;a name="AdvantageofnewAPI%28kindofasitisinthepatch%29"&gt;&lt;/a&gt;Advantage of new API (kind of as it is in the patch)&lt;/h3&gt;
&lt;p&gt;FieldCache is interface and it would be nice to move to abstract class, ExtendedFieldCache is ugly&lt;br/&gt;
Avoid global sync by IndexReader to access cache&lt;br/&gt;
its easier/cleaner to block caching by multireaders (though I am almost thinking I would prefer warnings/advice about performance and encouragement to move to per segment)&lt;br/&gt;
It becomes easier to share a ValueSource instance across readers.&lt;/p&gt;

&lt;h3&gt;&lt;a name="DisadvantagesofnewAPI"&gt;&lt;/a&gt;Disadvantages of new API&lt;/h3&gt;
&lt;p&gt;If we want only SegmentReaders to have a ValueSource, you can't efficiently back the old API with the new, causing RAM reqs jumps if you straddle the two APIs and ask for the same array data from each.&lt;/p&gt;

&lt;p&gt;Its probably a higher barrier to a custom Parser to implement and init a Reader with a ValueSource (presumably that works per field) than to simply pass the Parser on a SortField. However, Parser stops making sense if we end up being able to back ValueSource with column stride fields. We could allow ValueSource to be passed on the SortField (the current incarnation of this patch), but then you have to go back to a global cache by reader the ValueSources passed that way (you would also still have the per segment reader, settable ValueSource).&lt;/p&gt;

&lt;h3&gt;&lt;a name="AdvantagesofstayingwitholdAPI"&gt;&lt;/a&gt;Advantages of staying with old API&lt;/h3&gt;
&lt;p&gt;Avoid forcing large migration for users, with possible RAM req penalties if they don't switch from deprecated code (we are doing something similar with 1483 even without deprecated code though - if you were using an external multireader FieldCache that matched a sort FieldCache key, youd double your RAM reqs).&lt;/p&gt;

&lt;h3&gt;&lt;a name="Thoughts"&gt;&lt;/a&gt;Thoughts&lt;/h3&gt;
&lt;p&gt;If we stayed with the old API, we could still allow a custom FieldCache to be supplied. We could still back FieldCacheImpl with Uninverter to reduce code. We could still have CachingFieldCache. Though CachingValueSource is much better &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; FieldCache implies caching, and so the name would be confusing. We could also avoid CachingFieldCache though, as just making a pluggable FieldCache would allow alternate caching implementations (with a bit more effort).&lt;/p&gt;

&lt;p&gt;We could deprecate the Parser methods and force supplying a new FieldCache impl for custom uninversion to get to an API suitable to be backed by CSF.&lt;/p&gt;

&lt;p&gt;Or:&lt;/p&gt;

&lt;p&gt;We could also move to ValueSource, but allow a ValueSource on multi-readers. That would probably make straddling the API's much more possible (and efficient) in the default case. We could advise that its best to work per segment, but leave the option to the user.&lt;/p&gt;

&lt;h3&gt;&lt;a name="Conclusion"&gt;&lt;/a&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I am not sure. I thought I was convinced we might as well not even move from FieldCache at all, but now that I've written a bit out, I'm thinking it would be worth going to ValueSource. I'm just not positive on what we should support. SortField ValueSource override keyed by reader? ValueSources on MultiReaders?&lt;/p&gt;</comment>
                    <comment id="12699893" author="thetaphi" created="Thu, 16 Apr 2009 22:53:24 +0100">&lt;p&gt;We have the problem with the ValueSource-override not only with SortField. Also Functions Queries need the additional ValueSource-override and other places too. So a central place to register a "ValueSource per field" for a IndexReader (MultiReader,... passing down to segments) would really be nice.&lt;/p&gt;

&lt;p&gt;For the caching problem: Possibly the ValueSource given to SortField etc. behaves like the current parser. The cache in IndexReader should also be keyed by the ValueSource. So the SortField/FunctionQuery ValueSource override is passed down to IndexReader's cache. If the IndexReader has an entry in its cache for same (field, ValueSource, ...) key, it could use the arrays from there, if not fill cache with an array from the overridden ValueSource. I would really make the ValueSource per-field.&lt;/p&gt;

&lt;p&gt;Univerter inner class should be made public and the Univerter should accept a starting term to iterate (overwrite "") and the newTerm() method should be able to return false to stop iterating (see my ValueSource example for trie). With that one could easily create a subclass of univerter with a own parser logic (like trie).&lt;/p&gt;</comment>
                    <comment id="12699993" author="markrmiller@gmail.com" created="Fri, 17 Apr 2009 04:44:14 +0100">&lt;p&gt;I think we don't want to expose Uninverter though? The API should be neutral enough to naturally support loading from CSF, in which case Uninverter doesnt make sense...so we were going to go with having to override the value source to handle uninverter type stuff.&lt;/p&gt;</comment>
                    <comment id="12700154" author="thetaphi" created="Fri, 17 Apr 2009 13:36:57 +0100">&lt;p&gt;I think, it would be a good idea to have the Univerter static class not package private, but "protected" (if possible) or "public". Then it would be simple, to subclass the UniverterValueSource (e.g. for the TriePackage) and then just use own univerter subclasses to fill the cache. The idea is to make this more simple.&lt;br/&gt;
As you may have seen in my TrieValueSource I had to redeclare the Univerter.&lt;/p&gt;</comment>
                    <comment id="12700175" author="markrmiller@gmail.com" created="Fri, 17 Apr 2009 14:36:02 +0100">&lt;p&gt;Yes, I agree - will do.&lt;/p&gt;</comment>
                    <comment id="12700177" author="mikemccand" created="Fri, 17 Apr 2009 14:43:46 +0100">&lt;p&gt;I've been struggling with the "right" way forward here... despite&lt;br/&gt;
following all comments and aggressive ongoing mulling, I still don't&lt;br/&gt;
have much clarity.&lt;/p&gt;

&lt;p&gt;It feels like one of those features that just hasn't quite "clicked"&lt;br/&gt;
yet (to me at least).  In fact, the more I try to think about it, the&lt;br/&gt;
less clarity I get!&lt;/p&gt;

&lt;p&gt;I think there're some cncrete reasons to create a new API (some&lt;br/&gt;
overlap w/ Mark's list above):&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Make caching "external"/public so you can control when things are&lt;br/&gt;
    evicted&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Cleaner API &amp;#8211; it's just awkward that you now must call a separate&lt;br/&gt;
    place (ExtendedFieldCache.EXT_DEFAULT) to getInts.  FieldCache &amp;amp;&lt;br/&gt;
    ExtendedFieldCache are awkward, and they are interfaces.  It makes&lt;br/&gt;
    more sense to ask the reader directly for ints (or a future&lt;br/&gt;
    component of the reader).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Better extensibility on uninversion (either via "you make your own&lt;br/&gt;
    ValueSource entirely", or "you can subclass Uninverted and tweak&lt;br/&gt;
    it").  Trie needs this (though, we have a viable approach in field&lt;br/&gt;
    cache).  Fields with more than one value want custom control to&lt;br/&gt;
    pick one.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Making it not-so-easy to get all field values at the reader level&lt;br/&gt;
    (don't set dangerous API traps)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Honestly these reasons are not net/net compelling enough to warrant a&lt;br/&gt;
whole new API?  They are fairly minor.  And I agree: &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt; has&lt;br/&gt;
already achieved the biggest step forward here.&lt;/p&gt;

&lt;p&gt;Furthermore, there are other innovations happening that may affect how&lt;br/&gt;
we do this. EG &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1597" title="New Document and Field API"&gt;&lt;del&gt;LUCENE-1597&lt;/del&gt;&lt;/a&gt; introduces type information for fields (at&lt;br/&gt;
least at indexing time), and Earwin is working on "componentizing"&lt;br/&gt;
SegmentReader.  Normally I don't like letting "big distant future&lt;br/&gt;
feature X" prevent progess on "today's feature Y", but since we lack&lt;br/&gt;
clarity on Y...&lt;/p&gt;

&lt;p&gt;I can imagine a future when the FieldType would be the central place&lt;br/&gt;
that records all details for a field:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The analyzer to use (so we don't need PerFieldAnalyzerWrapper)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The ValueSource&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;It's "native" type (now "switched" in many places, like&lt;br/&gt;
    FieldComparator, SortField, FieldCache, etc.)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;All the index-time configuration&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And then instead of having ValueSource dispatch per field, we'd simply&lt;br/&gt;
ask the FieldType what it's source is.&lt;/p&gt;

&lt;p&gt;Finally, there are a number of future improvements we should take into&lt;br/&gt;
account.  We wouldn't try to accomplish these right now, but we ought&lt;br/&gt;
to think about them (eg, not preclude them) in whatever approach we&lt;br/&gt;
settle on:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We need source pluggability for when CSF arrives (but, admittedly,&lt;br/&gt;
    we could wait until CSF actually does arrive)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Allowing values to change, just like we can call&lt;br/&gt;
    IndexReader.setNorm/deleteDoc to change norms/deletes. We'd need a&lt;br/&gt;
    copy-on-write approach, like norms &amp;amp; deleted docs.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;How would norms be folded into this?  Ideally, each field could&lt;br/&gt;
    choose to pull its norms from any source.  Document level norms&lt;br/&gt;
    was discussed somewhere, and should easily "fit" as another norms&lt;br/&gt;
    source.  We'd need to relax how per-doc-field boosting is computed&lt;br/&gt;
    at runtime to pull from such "arbitrary" sources.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Deleted docs could also be represented as a ValueSource?  Just one&lt;br/&gt;
    bit per doc.  This way one could swap in whatever source for&lt;br/&gt;
    "deleted docs" one wanted.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Allowing for docs that have more than one value.  (We'd also need&lt;br/&gt;
    to extend sorting to be able to compare multiple vlaues).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;An mmap implementation (like Lucy/KS) &amp;#8211; should feel just like CSF&lt;br/&gt;
    or uninversion (ie, "just another impl").&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Impls of getStrings and getStringIndex that are based on offsets&lt;br/&gt;
    into char[] (not actual individual String object).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Good impls for the enum case (all strings could be considered&lt;br/&gt;
    enums), eg if there are only 100 unique strings in that field, you&lt;br/&gt;
    only need 7 bits per ord derefing into the char[] values.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Possible future when Lucene computes sort cache (for text fields)&lt;br/&gt;
    and stores in the index&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Allowing field sort to use an entirely external source of values&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There's alot to think about &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;</comment>
                    <comment id="12700391" author="markrmiller@gmail.com" created="Sat, 18 Apr 2009 01:24:35 +0100">&lt;p&gt;I've got a bit of the same feeling. My list was more or less cherry picked from all of the above comments, and my initial feeling was their was not enough motivation as well. But the more I thought about it, the more kind of ugly field cache is. And we would want to lose exposing Parser so that CFS can be a seamless backing. That makes FieldCache even uglier for a while. Clickless thus far here too, but I think we have a good base to work with still.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Honestly these reasons are not net/net compelling enough to warrant a&lt;br/&gt;
whole new API? They are fairly minor. And I agree: &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1483" title="Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector"&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt; has&lt;br/&gt;
already achieved the biggest step forward here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not only that, but almost all of those reasons can be handled by allowing a custom FieldCache to be used, rather than just hard coding to the default singleton.&lt;/p&gt;

&lt;p&gt;A couple responses:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We need source pluggability for when CSF arrives (but, admittedly,&lt;br/&gt;
we could wait until CSF actually does arrive)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We have it? Just pass the CSFValueSource at IndexReader creation?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Allowing values to change, just like we can call&lt;br/&gt;
IndexReader.setNorm/deleteDoc to change norms/deletes. We'd need a&lt;br/&gt;
copy-on-write approach, like norms &amp;amp; deleted docs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good point. We need a way to update, that can throw USO Exception?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How would norms be folded into this? Ideally, each field could&lt;br/&gt;
choose to pull its norms from any source. Document level norms&lt;br/&gt;
was discussed somewhere, and should easily "fit" as another norms&lt;br/&gt;
source. We'd need to relax how per-doc-field boosting is computed&lt;br/&gt;
at runtime to pull from such "arbitrary" sources.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good point again. Getting norms under this API will add a bit more meat to this issue.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Deleted docs could also be represented as a ValueSource? Just one&lt;br/&gt;
bit per doc. This way one could swap in whatever source for&lt;br/&gt;
"deleted docs" one wanted.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You've got me here at the moment. I don't know the delete code very well, but I will in time &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/smile.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;      Allowing for docs that have more than one value. (We'd also need&lt;br/&gt;
      to extend sorting to be able to compare multiple values).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is an interesting one, because I wonder if we can do it and stick with arrays? A multi dimensional array seems a bit much...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;An mmap implementation (like Lucy/KS) - should feel just like CSF&lt;br/&gt;
or uninversion (ie, "just another impl").&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is already fairly independent I think...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Good impls for the enum case (all strings could be considered&lt;br/&gt;
enums), eg if there are only 100 unique strings in that field, you&lt;br/&gt;
only need 7 bits per ord derefing into the char[] values.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1. Yes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Possible future when Lucene computes sort cache (for text fields)&lt;br/&gt;
and stores in the index&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I'm not familiar with that idea, so not sure what affect this has...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Allowing field sort to use an entirely external source of values&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think both options allow that now - if you pass the ValueSource from the reader, it can get its values from everywhere. If you override the reader valuesource with the sortfield valuesource, it too can load from anywhere. I am just not sure both options are really needed. I am kind of liking Uwe's idea of assigning ValueSources per field, though that could probably get messy. Perhaps a default, and then per field overrides? &lt;/p&gt;</comment>
                    <comment id="12700430" author="earwin" created="Sat, 18 Apr 2009 08:53:46 +0100">&lt;blockquote&gt;
&lt;p&gt;Allowing values to change, just like we can call&lt;br/&gt;
IndexReader.setNorm/deleteDoc to change norms/deletes. We'd need a&lt;br/&gt;
copy-on-write approach, like norms &amp;amp; deleted docs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On the other hand, maybe, we shouldn't?&lt;br/&gt;
Deleted docs should definetly be mutable, but that's it.&lt;br/&gt;
Anybody is updating norms on a regular basis on a serious project? But still everyone pays for the feature with running ugly synchronization code for norms. Let's dump it!&lt;br/&gt;
As for mutable fields, okay, users of Sphinx have them. They use them mostly.. hehehe.. for implementing deletions that Sphinx lacks. I bet there could exist some other usecases, but they can be handled with a custom ValueSource without the need to bring it into API everyone must implement.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Deleted docs could also be represented as a ValueSource? Just one&lt;br/&gt;
bit per doc. This way one could swap in whatever source for&lt;br/&gt;
"deleted docs" one wanted.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That's why I think this is a misfeature. Deleted docs have different meaning from field values. They can be updated, and they should be checked against uberfast.&lt;br/&gt;
Swapping in another impl is cool, while forcing everyone and his dog under the same usage API is not so cool.&lt;/p&gt;</comment>
                    <comment id="12700479" author="markrmiller@gmail.com" created="Sat, 18 Apr 2009 14:13:52 +0100">&lt;blockquote&gt;&lt;p&gt;Deleted docs could also be represented as a ValueSource? Just one&lt;br/&gt;
bit per doc. This way one could swap in whatever source for&lt;br/&gt;
"deleted docs" one wanted.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Some of your comments seem to indicate you think we will need to end up with an object rather than raw arrays?&lt;/p&gt;</comment>
                    <comment id="12700523" author="mikemccand" created="Sat, 18 Apr 2009 19:40:12 +0100">&lt;blockquote&gt;&lt;p&gt;Some of your comments seem to indicate you think we will need to end up with an object rather than raw arrays?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, really I threw out all these future items to stir up the pot and&lt;br/&gt;
see if some clarity comes out of it &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; This is what I try to do&lt;br/&gt;
whenever I'm stuck on how to design something... some sort of defense&lt;br/&gt;
mechanism.&lt;/p&gt;

&lt;p&gt;That said, what requires object instead of array?  EG for binary&lt;br/&gt;
fields (deleted docs) we'd have eg "BitVector getBits(...)".&lt;/p&gt;

&lt;p&gt;For multi-valued fields, I'm not sure what's best.  I think Yonik did&lt;br/&gt;
something neat with Solr for holding multi-valued fields but I can't&lt;br/&gt;
find it now.  But, with ValueSource, we have the freedom to use arrays&lt;br/&gt;
for simple cases and something else for interesting ones?  It's not&lt;br/&gt;
either/or?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And we would want to lose exposing Parser so that CFS can be a seamless backing. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I see the CFS/CSF confusion has already struck!&lt;/p&gt;

&lt;p&gt;But yes cleaner API would be a nice step forward...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We have it? Just pass the CSFValueSource at IndexReader creation?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes I think we have this one.&lt;/p&gt;

&lt;p&gt;Though... I feel like ValueSource should represent a single field's&lt;br/&gt;
values, and something else (FieldType?) returns the ValueSource for&lt;br/&gt;
that field.  Ie, I think we are overloading ValueSource now?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Good point. We need a way to update, that can throw USO Exception?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe... or we can defer for future.  We don't need full answers nor&lt;br/&gt;
impls for all of these now...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;gt; Possible future when Lucene computes sort cache (for text fields)&lt;br/&gt;
&amp;gt; and stores in the index&lt;/p&gt;

&lt;p&gt;I'm not familiar with that idea, so not sure what affect this has...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sort cache is just getStringIndex()... all other types just use the&lt;br/&gt;
values directly (no need for separate ords).  If it's costly to&lt;br/&gt;
compute per-reopen we may want to store it in the index.  But&lt;br/&gt;
honestly, since we load the full thing into RAM, I wonder how&lt;br/&gt;
different the time'd really be loading it vs recomputing it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Good point again. Getting norms under this API will add a bit more meat to this issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I'm not sure whether norms/deleted docs "fit"; certainly we'd&lt;br/&gt;
need updatability first.  It's just that, from a distance, they are&lt;br/&gt;
clearly a "value per doc" for every doc in the index.  If we had norms&lt;br/&gt;
&amp;amp; deletions under this API then suddenly, &lt;span class="error"&gt;&amp;#91;almost&amp;#93;&lt;/span&gt; for free, we'd get&lt;br/&gt;
pluggability of deleted docs &amp;amp; norms.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am kind of liking Uwe's idea of assigning ValueSources per field, though that could probably get messy. Perhaps a default, and then per field overrides?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I'm also more liking "per field" to be somehow handled.  Whether&lt;br/&gt;
IndexReader exposes that vs a FieldType (that also holds other&lt;br/&gt;
per-field stuff), I'm not sure.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Anybody is updating norms on a regular basis on a serious project?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is a good question &amp;#8211; I'd love to know too.&lt;/p&gt;

&lt;p&gt;But I think updating CSFs would be compelling; having to reindex the&lt;br/&gt;
entire doc because only 1 or 2 metadata fields had changed is a common&lt;br/&gt;
annoyance.  Of course we'd have to figure out (or rule out) updating&lt;br/&gt;
the postings for such changes...&lt;/p&gt;</comment>
                    <comment id="12700529" author="markrmiller@gmail.com" created="Sat, 18 Apr 2009 20:38:56 +0100">&lt;blockquote&gt;&lt;p&gt;But, with ValueSource, we have the freedom to use arrays&lt;br/&gt;
for simple cases and something else for interesting ones? It's not&lt;br/&gt;
either/or?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point. I was also thinking that some of these issues could force back up to multi-reader support though. But I guess that is not such a worry now that we search per segment is it. A lot of that could probably be deprecated (though I really don't know how easily - I hope to spend a lot more time getting more familiar with IndexReader code).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;span class="error"&gt;&amp;#91;almost&amp;#93;&lt;/span&gt; for free, we'd get&lt;br/&gt;
pluggability of deleted docs &amp;amp; norms.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I like that idea as well. Plugability is nice.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I'm also more liking "per field" to be somehow handled. Whether&lt;br/&gt;
IndexReader exposes that vs a FieldType (that also holds other&lt;br/&gt;
per-field stuff), I'm not sure.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I want field handling to become easier in Lucene, but I hope we don't lose any of our super on the fly settings. +1 on making field handing easier, but I am much more weary of a fixed schema type thing.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But I think updating CSFs would be compelling; having to reindex the&lt;br/&gt;
entire doc because only 1 or 2 metadata fields had changed is a common&lt;br/&gt;
annoyance. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am very interested in having updatable CSF's (much too easy to mistype that). There are many cool things to use it for, especially in combination with near realtime search (tagging variations).&lt;/p&gt;</comment>
                    <comment id="12700571" author="mikemccand" created="Sun, 19 Apr 2009 10:55:30 +0100">&lt;blockquote&gt;&lt;p&gt;I was also thinking that some of these issues could force back up to multi-reader support though. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hopefully not...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I want field handling to become easier in Lucene, but I hope we don't lose any of our super on the fly settings. +1 on making field handing easier, but I am much more weary of a fixed schema type thing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think "consolidating per-field details" (FieldType) is well decoupled from "forcing every occurrence of a field to be the same" (fixed schema).  We can (and I think should) do FieldType without forcing a fixed schema.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am very interested in having updatable CSF's (much too easy to mistype that). There are many cool things to use it for, especially in combination with near realtime search (tagging variations).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For tags we'd presumably want multi-valued fields handled in ValueSource, plus updatability, plus NRT.&lt;/p&gt;

&lt;p&gt;Updatability is tricky... ValueSource would maybe need a "startChanges()" API, which would copy the array (copy-on-write) if it's not already private.  The problem is... direct array access precludes more efficient data structures that amortize the copy-on-write cost (eg by block), which we are wanting to eventually get to for deleted docs &amp;amp; norms (it's likely a large cost in NRT reader turnaround, though I hven't yet measured just how costly).&lt;/p&gt;</comment>
                    <comment id="12700583" author="markrmiller@gmail.com" created="Sun, 19 Apr 2009 13:30:52 +0100">&lt;blockquote&gt;&lt;p&gt;&amp;gt;&amp;gt;I was also thinking that some of these issues could force back up to multi-reader support though. &lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;&amp;gt;Hopefully not...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I don't know enough yet to know for sure. My thought was things like norms and deletes that are available from multireader now will have to either still be, or straddle multi/segment for a while. I guess that doesnt become much of an issue if we go with the same method of just don't load from both single and multi or you will double your reqs? It just gets ugly trying to prevent multireader use with valuesource, but then have to support it due to all the back compat reqs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We can (and I think should) do FieldType without forcing a fixed schema.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fair enough, fair enough. I wasn't really taking this completely from this discussion, but from a variety of ideas about fields that have been spilling out on the list. Of course we can still get a lot better (easier) without hitting fixed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For tags we'd presumably want multi-valued fields handled in ValueSource, plus updatability, plus NRT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well I'm glad its a small order. Yonik did do some multi value faceting work that I never really looked at. I'll go dig it up.&lt;/p&gt;

&lt;p&gt;It may just be best if this sits for a while and we see what happens with a couple other issues floating around it. I said I had sweat to pump into this, not intelligence &lt;img class="emoticon" src="https://issues.apache.org/jira/images/icons/emoticons/wink.gif" height="20" width="20" align="absmiddle" alt="" border="0"/&gt; If we hit all this stuff (and yes, your not saying we need to or should, but) this ends up touching most things in IndexReader, than possibly writing and merging and what not in IndexWriter (pluggable norms etc still need to be written, merged, loaded, etc), and ... &lt;/p&gt;
</comment>
                    <comment id="12700585" author="thetaphi" created="Sun, 19 Apr 2009 13:45:20 +0100">&lt;p&gt;I am still thinking about the difference between function query's ValueSource and the new ValueSource and I would really like to combine both.&lt;br/&gt;
I know for sorting, the array approach is faster, but maybe the new ValueSource could provide both ways to access. In the array approach, one would only get arrays for single segments, but the method-like access could still map the document ids to the correct segment, to have a uniform access even to multi readers.&lt;br/&gt;
So, maybe there is a possibility to merge both approaches and only provide one ValueSource supplying both access strategies.&lt;/p&gt;</comment>
                    <comment id="12700586" author="markrmiller@gmail.com" created="Sun, 19 Apr 2009 14:07:18 +0100">&lt;blockquote&gt;&lt;p&gt;but maybe the new ValueSource could provide both ways to access&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, this goes with with what Mike pointed out above - we can return arrays, objects, or anything and your grandmother. My main worry with that idea is the ValueSource API - it could have 10's of accessors, but only 1 or 2 are generally implemented and you have to know the right one to call - it could work of course, but on first thought, its fairly ugly. You could make a fair point that we are already a ways down that path with the design we already have I guess though.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, maybe there is a possibility to merge both approaches and only provide one ValueSource supplying both access strategies. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its a good point. Something makes me think we will still be a bit hindered by back compat with deletes, norms though.&lt;/p&gt;</comment>
                    <comment id="12701751" author="jasonrutherglen" created="Thu, 23 Apr 2009 01:13:52 +0100">&lt;p&gt;I'm trying to figure out how to integrate Bobo faceting field&lt;br/&gt;
caches with this patch, I applied the patch, browsed the&lt;br/&gt;
ValueSource API and yeah, it's not what I expected. "we can&lt;br/&gt;
return arrays, objects, or anything and your grandmother" not&lt;br/&gt;
Grandma! But yeah we need to somehow support probably plain Java&lt;br/&gt;
objects rather than every primitive derivative? &lt;/p&gt;

&lt;p&gt;(In reference to Mark's post 2nd to last post) Bobo efficiently&lt;br/&gt;
nicely calculates facets for multiple values per doc which is&lt;br/&gt;
the same thing as "multi value faceting"? &lt;/p&gt;

&lt;p&gt;&amp;gt; by back compat with deletes, norms though.&lt;/p&gt;

&lt;p&gt;Are norms and deletes implemented? These would just be byte&lt;br/&gt;
arrays in the current approach? If not how would they be&lt;br/&gt;
represented? It seems like for deleted docs we'd want the&lt;br/&gt;
BitVector returned from a ValueSource.get type of method?&lt;/p&gt;

&lt;p&gt;M.M.: "Updatability is tricky... ValueSource would maybe need a&lt;br/&gt;
"startChanges()" API, which would copy the array (copy-on-write)&lt;br/&gt;
if it's not already private"&lt;/p&gt;

&lt;p&gt;Hmm... Does this mean we'd replace the current IndexReader&lt;br/&gt;
method of performing updates on norms and deletes with this more&lt;br/&gt;
generic update mechanism?&lt;/p&gt;

&lt;p&gt;It would be cool to get CSF going?&lt;/p&gt;</comment>
                    <comment id="12702526" author="mikemccand" created="Fri, 24 Apr 2009 21:19:29 +0100">&lt;blockquote&gt;
&lt;p&gt;Grandma! But yeah we need to somehow support probably plain Java&lt;br/&gt;
objects rather than every primitive derivative?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mean big arrays (one per doc) of plain-java-objects?  Is Bobo doing that today?  Or do you mean a single Java obect that, internally, deals with lookup by docID?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(In reference to Mark's post 2nd to last post) Bobo efficiently&lt;br/&gt;
nicely calculates facets for multiple values per doc which is&lt;br/&gt;
the same thing as "multi value faceting"?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Neat.  How do you compactly represent (in RAM) multiple values per doc?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Are norms and deletes implemented? These would just be byte&lt;br/&gt;
arrays in the current approach? If not how would they be&lt;br/&gt;
represented? It seems like for deleted docs we'd want the&lt;br/&gt;
BitVector returned from a ValueSource.get type of method?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The current patch doesn't do this &amp;#8211; but we should think about how this change could absorb norms/deleted docs, in the future.  We would add a "bit" variant of getXXX (eg that returns BitVector, BitSet, something).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hmm... Does this mean we'd replace the current IndexReader&lt;br/&gt;
method of performing updates on norms and deletes with this more&lt;br/&gt;
generic update mechanism?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Probably we'd still leave the "sugar" APIs in place, but under the hood their impls would be switched to this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It would be cool to get CSF going?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Most definitely!!&lt;/p&gt;</comment>
                    <comment id="12708307" author="markrmiller@gmail.com" created="Tue, 12 May 2009 03:56:32 +0100">&lt;p&gt;I won't likely be getting to this anytime soon if someone else wants to work on it. I'll get back at it at some point if not though.&lt;/p&gt;

&lt;p&gt;I believe the latest patch is a nice base to work from.&lt;/p&gt;

&lt;p&gt;I'm still not clear to me if its best to start merging using the ValueSource somehow, or do something where the ValueSource has a merge implementation (allowing for a more efficient private merge). It seems the merge code for fields, norms, dels, is fairly specialized now, but could become a bit more generic. Then perhaps you could add any old ValueSource (other than norms, fields, dels)  and easily hook into the merge process. Maybe even in RAM merges of RAM based ValueSources - FieldCache etc. Of course, I guess you could also still do things specialized as now, and just provide access to the files through a ValueSource. That really crimps the pluggability though.&lt;/p&gt;

&lt;p&gt;The next step (in terms of the current patch) seems to be to start working ValueSource into norms, dels, possibly stored fields. Eventually they should become pluggable, but I'm not sure how best to plug them in. I was thinking you could set a default ValueSource by field for the FieldCache using the Reader open method with a new param. Perhaps it should take a ValueSourceFactory that can provide a variety of ValueSources based on field, norms, dels, stored fields, with variations for read-only? The proposed componentization of IndexReader could be another approach if it materializes, or worked into this issue.&lt;/p&gt;

&lt;p&gt;I don't think I'll understand whats needed for updatability until I'm in deeper. It almost seems like something like setInt(int doc, int n), setByte(int doc, byte b) on the ValueSource might work. They could possibly throw Unsupported. I know there are a lot of little difficulties involved in all of this though, so I'm not very sure of anything at the moment. The backing impl would be free to update in RAM (say synced dels), or do a copy on write, etc. I guess all methods would throw Unsupported by default, but if you override a getXXX you would have the option of  overriding a setXXX. &lt;/p&gt;

&lt;p&gt;ValueSources also need the ability to be sharable across IndexReaders with the ability to do copy on write if they are shared and updatable.&lt;/p&gt;
</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12391410">LUCENE-1231</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12474983">LUCENE-2665</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12403462">LUCENE-1372</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12358344">LUCENE-743</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12519669">LUCENE-3390</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12442706">LUCENE-2133</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12421860">LUCENE-1582</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12395557" name="ExtendedDocument.java" size="15999" author="rnewson" created="Mon, 8 Dec 2008 14:11:33 +0000"/>
                    <attachment id="12378441" name="fieldcache-overhaul.032208.diff" size="148373" author="markrmiller@gmail.com" created="Sat, 22 Mar 2008 21:13:53 +0000"/>
                    <attachment id="12358796" name="fieldcache-overhaul.diff" size="49802" author="hossman" created="Mon, 4 Jun 2007 04:39:31 +0100"/>
                    <attachment id="12353253" name="fieldcache-overhaul.diff" size="49931" author="hossman" created="Wed, 14 Mar 2007 07:27:38 +0000"/>
                    <attachment id="12378827" name="LUCENE-831.03.28.2008.diff" size="206521" author="markrmiller@gmail.com" created="Fri, 28 Mar 2008 23:13:19 +0000"/>
                    <attachment id="12378895" name="LUCENE-831.03.30.2008.diff" size="221633" author="markrmiller@gmail.com" created="Sun, 30 Mar 2008 13:35:32 +0100"/>
                    <attachment id="12378953" name="LUCENE-831.03.31.2008.diff" size="250668" author="markrmiller@gmail.com" created="Mon, 31 Mar 2008 12:57:06 +0100"/>
                    <attachment id="12405596" name="LUCENE-831.patch" size="124863" author="markrmiller@gmail.com" created="Thu, 16 Apr 2009 01:22:08 +0100"/>
                    <attachment id="12405589" name="LUCENE-831.patch" size="124879" author="markrmiller@gmail.com" created="Wed, 15 Apr 2009 23:46:29 +0100"/>
                    <attachment id="12405325" name="LUCENE-831.patch" size="110360" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 16:14:41 +0100"/>
                    <attachment id="12405291" name="LUCENE-831.patch" size="107080" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 01:43:16 +0100"/>
                    <attachment id="12405290" name="LUCENE-831.patch" size="104540" author="markrmiller@gmail.com" created="Mon, 13 Apr 2009 00:16:46 +0100"/>
                    <attachment id="12405278" name="LUCENE-831.patch" size="87235" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 18:54:40 +0100"/>
                    <attachment id="12405274" name="LUCENE-831.patch" size="90442" author="markrmiller@gmail.com" created="Sun, 12 Apr 2009 16:39:15 +0100"/>
                    <attachment id="12405217" name="LUCENE-831.patch" size="145792" author="markrmiller@gmail.com" created="Sat, 11 Apr 2009 03:19:31 +0100"/>
                    <attachment id="12395488" name="LUCENE-831.patch" size="187536" author="markrmiller@gmail.com" created="Sat, 6 Dec 2008 14:10:53 +0000"/>
                    <attachment id="12395351" name="LUCENE-831.patch" size="187158" author="markrmiller@gmail.com" created="Fri, 5 Dec 2008 00:12:27 +0000"/>
                    <attachment id="12395349" name="LUCENE-831.patch" size="183441" author="markrmiller@gmail.com" created="Thu, 4 Dec 2008 23:38:23 +0000"/>
                    <attachment id="12392789" name="LUCENE-831.patch" size="177782" author="markrmiller@gmail.com" created="Fri, 24 Oct 2008 14:24:55 +0100"/>
                    <attachment id="12388801" name="LUCENE-831.patch" size="177396" author="markrmiller@gmail.com" created="Sat, 23 Aug 2008 20:15:48 +0100"/>
                    <attachment id="12388649" name="LUCENE-831.patch" size="160258" author="markrmiller@gmail.com" created="Thu, 21 Aug 2008 02:22:42 +0100"/>
                    <attachment id="12405292" name="LUCENE-831-trieimpl.patch" size="8133" author="thetaphi" created="Mon, 13 Apr 2009 01:51:49 +0100"/>
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>22.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 27 Mar 2007 17:52:10 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2971</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26893</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                        </customfields>
    </item>
</channel>
</rss>